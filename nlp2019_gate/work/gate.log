model-.h5
e2e-stack model-
gpu: 0
6 3 2
vocab size:  29396
vocab size:  29396
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse
gpu: 0
6 3 2
vocab size:  29396
vocab size:  29396
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse
gpu: 0
6 3 2
vocab size:  29396
vocab size:  29396
model-e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse
gpu: 0
6 3 2
vocab size:  29396
model-e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse
gpu: 0
6 3 2
vocab size:  29396
vocab size:  29396
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2016_preFalse
gpu: 0
6 3 2
vocab size:  29396
vocab size:  29396
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2016_preFalse
gpu: 0
6 3 2
vocab size:  29396
vocab size:  29396
model-e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2016_preFalse
gpu: 0
6 3 2
vocab size:  29396
vocab size:  29396
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse
gpu: 0
6 3 2
vocab size:  29396
vocab size:  29396
vocab size:  29396
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
vocab size:  29396
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(3645.5591) lr: 0.0001 time: 1383.62
pred_count_train 41644

Test...

ga 	p: 75.05 	r: 70.43 	f1: 72.67 	 4344 	 5788 	 6168
wo 	p: 91.38 	r: 82.32 	f1: 86.61 	 2789 	 3052 	 3388
ni 	p: 81.28 	r: 76.74 	f1: 78.95 	 1168 	 1437 	 1522
dev_num_of_high:  0
best_thres [0.47, 0.37, 0.14]
f 0.7774291734956684
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 1 	 [0.47, 0.37, 0.14] 	 lr: 0.0001 	 f: 77.74291734956684
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 2 [0m
Train...
loss: tensor(3653.6636) lr: 0.0001 time: 1421.21
pred_count_train 41644

Test...

ga 	p: 73.47 	r: 71.38 	f1: 72.41 	 4403 	 5993 	 6168
wo 	p: 90.71 	r: 82.7 	f1: 86.52 	 2802 	 3089 	 3388
ni 	p: 82.96 	r: 75.16 	f1: 78.87 	 1144 	 1379 	 1522
dev_num_of_high:  0
best_thres [0.39, 0.37, 0.22]
f 0.7752449045916711
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 1 	 [0.39, 0.37, 0.22] 	 lr: 0.0001 	 f: 77.52449045916711
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 2 [0m
Train...
loss: tensor(2232.3115) lr: 0.0001 time: 1389.41
pred_count_train 41644

Test...

ga 	p: 79.09 	r: 75.02 	f1: 77.0 	 4627 	 5850 	 6168
wo 	p: 93.46 	r: 83.94 	f1: 88.45 	 2844 	 3043 	 3388
ni 	p: 83.45 	r: 81.14 	f1: 82.28 	 1235 	 1480 	 1522
dev_num_of_high:  0
best_thres [0.34, 0.35, 0.2]
f 0.8117104097711063
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 2 	 [0.34, 0.35, 0.2] 	 lr: 0.0001 	 f: 81.17104097711064
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 3 [0m
Train...
loss: tensor(2238.5032) lr: 0.0001 time: 1389.35
pred_count_train 41644

Test...

ga 	p: 80.92 	r: 73.43 	f1: 76.99 	 4529 	 5597 	 6168
wo 	p: 93.33 	r: 84.27 	f1: 88.57 	 2855 	 3059 	 3388
ni 	p: 82.63 	r: 80.95 	f1: 81.78 	 1232 	 1491 	 1522
dev_num_of_high:  0
best_thres [0.39, 0.34, 0.21]
f 0.8118727915194346
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 2 	 [0.39, 0.34, 0.21] 	 lr: 0.0001 	 f: 81.18727915194346
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 3 [0m
Train...
loss: tensor(1850.7885) lr: 0.0001 time: 1401.45
pred_count_train 41644

Test...

ga 	p: 82.98 	r: 72.97 	f1: 77.66 	 4501 	 5424 	 6168
wo 	p: 94.35 	r: 84.36 	f1: 89.08 	 2858 	 3029 	 3388
ni 	p: 85.24 	r: 80.09 	f1: 82.59 	 1219 	 1430 	 1522
dev_num_of_high:  0
best_thres [0.51, 0.48, 0.15]
f 0.8184724011259006
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 3 	 [0.51, 0.48, 0.15] 	 lr: 0.0001 	 f: 81.84724011259006
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 4 [0m
Train...
loss: tensor(1853.0269) lr: 0.0001 time: 1483.83
pred_count_train 41644

Test...

ga 	p: 80.46 	r: 74.09 	f1: 77.14 	 4570 	 5680 	 6168
wo 	p: 91.94 	r: 86.19 	f1: 88.97 	 2920 	 3176 	 3388
ni 	p: 84.76 	r: 80.42 	f1: 82.54 	 1224 	 1444 	 1522
dev_num_of_high:  0
best_thres [0.47, 0.33, 0.13]
f 0.8152306109084104
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 3 	 [0.47, 0.33, 0.13] 	 lr: 0.0001 	 f: 81.52306109084104
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 4 [0m
Train...
loss: tensor(1560.9156) lr: 0.0001 time: 1712.99
pred_count_train 41644

Test...
loss: tensor(1561.7477) lr: 0.0001 time: 1518.67
pred_count_train 41644

Test...

ga 	p: 80.6 	r: 74.09 	f1: 77.21 	 4570 	 5670 	 6168
wo 	p: 93.06 	r: 85.89 	f1: 89.33 	 2910 	 3127 	 3388
ni 	p: 86.49 	r: 80.75 	f1: 83.52 	 1229 	 1421 	 1522
dev_num_of_high:  0
best_thres [0.26, 0.53, 0.24]
f 0.8179000751314801
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 3 	 [0.51, 0.48, 0.15] 	 lr: 0.0001 	 f: 81.84724011259006
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 5 [0m
Train...

ga 	p: 79.72 	r: 74.38 	f1: 76.96 	 4588 	 5755 	 6168
wo 	p: 90.95 	r: 86.87 	f1: 88.86 	 2943 	 3236 	 3388
ni 	p: 87.09 	r: 80.22 	f1: 83.52 	 1221 	 1402 	 1522
dev_num_of_high:  0
best_thres [0.24, 0.33, 0.22]
f 0.8152391597969354
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 4 	 [0.24, 0.33, 0.22] 	 lr: 0.0001 	 f: 81.52391597969354
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 5 [0m
Train...
loss: tensor(1280.8274) lr: 0.0001 time: 1428.12
pred_count_train 41644

Test...
loss: tensor(1284.0964) lr: 0.0001 time: 1429.26
pred_count_train 41644

Test...

ga 	p: 79.57 	r: 77.56 	f1: 78.56 	 4784 	 6012 	 6168
wo 	p: 92.83 	r: 86.36 	f1: 89.48 	 2926 	 3152 	 3388
ni 	p: 82.16 	r: 79.57 	f1: 80.84 	 1211 	 1474 	 1522
dev_num_of_high:  0
best_thres [0.3, 0.47, 0.14]
f 0.8216061889850801
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 5 	 [0.3, 0.47, 0.14] 	 lr: 0.0001 	 f: 82.160618898508
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 6 [0m
Train...
vocab size:  29396
[34me2e-stack_ve256_vu256_depth3_adam_lr0.0001_du0.1_dh0.0_True_size5_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(689.1609) lr: 0.0001 time: 122.45
pred_count_train 3278

Test...
> [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(46)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     45 [0;31m[0;34m[0m[0m
[0m[0;32m---> 46 [0;31m        [0;32mfor[0m [0mscore[0m [0;32min[0m [0mscores[0m[0;34m:[0m [0;31m# iteration æ¯Žã®è§£æž[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     47 [0;31m            [0;32mfor[0m [0mi[0m [0;32min[0m [0mrange[0m[0;34m([0m[0myss[0m[0;34m.[0m[0msize[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> tensor([[-3.8076e+00, -5.3289e+00, -5.7302e+00, -3.0765e-02],
        [-9.4559e+00, -1.0076e+01, -1.0267e+01, -1.5497e-04],
        [-6.9102e+00, -8.2743e+00, -8.0443e+00, -1.5745e-03],
        [-7.1650e+00, -8.3468e+00, -7.5456e+00, -1.5402e-03],
        [-7.6142e-01, -2.9715e+00, -2.6595e+00, -8.8726e-01],
        [-2.9889e+00, -5.4640e+00, -5.9041e+00, -5.9014e-02],
        [-7.6138e+00, -9.5345e+00, -9.3670e+00, -6.5184e-04],
        [-6.2585e+00, -8.3335e+00, -7.3110e+00, -2.8267e-03],
        [-2.5493e+00, -5.0142e+00, -4.6193e+00, -9.9419e-02],
        [-7.4028e+00, -9.6880e+00, -9.3266e+00, -7.6103e-04],
        [-5.7722e+00, -7.7436e+00, -6.9220e+00, -4.5424e-03],
        [-4.2431e+00, -5.6115e+00, -5.2083e+00, -2.3770e-02],
        [-8.0123e+00, -8.7334e+00, -7.4647e+00, -1.0662e-03],
        [-2.9252e+00, -3.5137e+00, -3.0551e+00, -1.3991e-01],
        [-5.5433e+00, -6.3869e+00, -6.5046e+00, -7.1187e-03],
        [-7.5965e+00, -8.8868e+00, -8.4208e+00, -8.6117e-04],
        [-4.6167e+00, -5.0483e+00, -5.6109e+00, -2.0165e-02],
        [-9.1892e+00, -1.0027e+01, -9.5883e+00, -2.1505e-04],
        [-4.3855e+00, -5.8887e+00, -5.3075e+00, -2.0388e-02],
        [-5.1050e+00, -6.4905e+00, -6.5190e+00, -9.1004e-03],
        [-6.7443e+00, -7.5424e+00, -7.6037e+00, -2.2087e-03],
        [-4.0963e+00, -4.1371e+00, -5.3998e+00, -3.7827e-02],
        [-7.3600e+00, -7.1736e+00, -8.6137e+00, -1.5855e-03],
        [-7.3965e+00, -7.5098e+00, -7.8029e+00, -1.5707e-03],
        [-7.3947e+00, -7.9729e+00, -7.2032e+00, -1.7047e-03],
        [-8.1052e-01, -1.9199e+00, -2.0043e+00, -1.2946e+00],
        [-5.7512e+00, -7.4520e+00, -7.1679e+00, -4.5404e-03],
        [-5.8450e+00, -7.6808e+00, -7.0736e+00, -4.2119e-03],
        [-4.9418e+00, -5.3388e+00, -5.0575e+00, -1.8475e-02],
        [-2.6290e+00, -2.6638e+00, -3.8418e+00, -1.7828e-01],
        [-8.6356e+00, -7.5271e+00, -7.7766e+00, -1.1358e-03],
        [-3.8055e+00, -2.8193e+00, -3.4201e+00, -1.2172e-01],
        [-8.5284e+00, -8.3790e+00, -8.1109e+00, -7.2765e-04],
        [-3.6123e+00, -3.5216e+00, -3.9065e+00, -7.9752e-02],
        [-9.4151e+00, -9.0472e+00, -9.2293e+00, -2.9755e-04],
        [-7.8083e+00, -7.6438e+00, -7.5628e+00, -1.4057e-03],
        [-5.1968e+00, -5.9446e+00, -5.2314e+00, -1.3592e-02],
        [-8.2342e-01, -1.7089e+00, -2.0219e+00, -1.3959e+00],
        [-6.1895e+00, -7.3115e+00, -7.2786e+00, -3.4146e-03],
        [-5.1629e+00, -5.8882e+00, -5.5464e+00, -1.2476e-02],
        [-3.9153e+00, -5.0850e+00, -5.2555e+00, -3.1843e-02],
        [-8.6644e+00, -8.3559e+00, -8.3977e+00, -6.3324e-04]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(47)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     46 [0;31m        [0;32mfor[0m [0mscore[0m [0;32min[0m [0mscores[0m[0;34m:[0m [0;31m# iteration æ¯Žã®è§£æž[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 47 [0;31m            [0;32mfor[0m [0mi[0m [0;32min[0m [0mrange[0m[0;34m([0m[0myss[0m[0;34m.[0m[0msize[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     48 [0;31m                [0mys[0m [0;34m=[0m [0myss[0m[0;34m[[0m[0mi[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(48)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     47 [0;31m            [0;32mfor[0m [0mi[0m [0;32min[0m [0mrange[0m[0;34m([0m[0myss[0m[0;34m.[0m[0msize[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 48 [0;31m                [0mys[0m [0;34m=[0m [0myss[0m[0;34m[[0m[0mi[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     49 [0;31m                [0mpredicted[0m [0;34m=[0m [0mtorch[0m[0;34m.[0m[0mt[0m[0;34m([0m[0mscore[0m[0;34m[[0m[0mi[0m[0;34m][0m[0;34m.[0m[0mcpu[0m[0;34m([0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(49)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     48 [0;31m                [0mys[0m [0;34m=[0m [0myss[0m[0;34m[[0m[0mi[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 49 [0;31m                [0mpredicted[0m [0;34m=[0m [0mtorch[0m[0;34m.[0m[0mt[0m[0;34m([0m[0mscore[0m[0;34m[[0m[0mi[0m[0;34m][0m[0;34m.[0m[0mcpu[0m[0;34m([0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     50 [0;31m                [0;31m# print(torch.pow(torch.zeros(predicted.size()) + math.e, predicted.data))[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> tensor([3, 3, 3, 3, 2, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(52)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     51 [0;31m[0;34m[0m[0m
[0m[0;32m---> 52 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     53 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> tensor([-3.8076, -5.3290, -5.7302, -0.0308], grad_fn=<TBackward>)
ipdb> tensor([3, 3, 3, 3, 2, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])
ipdb> tensor([-3.8076, -5.3290, -5.7302, -0.0308], grad_fn=<TBackward>)
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(53)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     52 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 53 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     54 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mthres_lists[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> 0
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(54)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     53 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 54 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mthres_lists[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     55 [0;31m            [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> tensor(-3.8076, grad_fn=<SelectBackward>)
ipdb> 0
ipdb> tensor(-3.8076, grad_fn=<SelectBackward>)
ipdb> {0.1: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}
ipdb> 
ga 	p: 82.15 	r: 74.68 	f1: 78.23 	 4606 	 5607 	 6168
wo 	p: 91.68 	r: 86.81 	f1: 89.18 	 2941 	 3208 	 3388
ni 	p: 83.82 	r: 76.94 	f1: 80.23 	 1171 	 1397 	 1522
dev_num_of_high:  0
best_thres [0.39, 0.34, 0.17]
f 0.8189760450915924
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 5 	 [0.39, 0.34, 0.17] 	 lr: 0.0001 	 f: 81.89760450915924
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 6 [0m
Train...
[0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7]
ipdb> tensor([3, 3, 3, 3, 2, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(52)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     51 [0;31m[0;34m[0m[0m
[0m[0;32m---> 52 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     53 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(53)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     52 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 53 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     54 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mthres_lists[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(54)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     53 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 54 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mthres_lists[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     55 [0;31m            [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(52)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     51 [0;31m[0;34m[0m[0m
[0m[0;32m---> 52 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     53 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(53)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     52 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 53 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     54 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mthres_lists[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(54)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     53 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 54 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mthres_lists[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     55 [0;31m            [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(52)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     51 [0;31m[0;34m[0m[0m
[0m[0;32m---> 52 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     53 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(47)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     46 [0;31m        [0;32mfor[0m [0mscore[0m [0;32min[0m [0mscores[0m[0;34m:[0m [0;31m# iteration æ¯Žã®è§£æž[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 47 [0;31m            [0;32mfor[0m [0mi[0m [0;32min[0m [0mrange[0m[0;34m([0m[0myss[0m[0;34m.[0m[0msize[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     48 [0;31m                [0mys[0m [0;34m=[0m [0myss[0m[0;34m[[0m[0mi[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(48)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     47 [0;31m            [0;32mfor[0m [0mi[0m [0;32min[0m [0mrange[0m[0;34m([0m[0myss[0m[0;34m.[0m[0msize[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 48 [0;31m                [0mys[0m [0;34m=[0m [0myss[0m[0;34m[[0m[0mi[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     49 [0;31m                [0mpredicted[0m [0;34m=[0m [0mtorch[0m[0;34m.[0m[0mt[0m[0;34m([0m[0mscore[0m[0;34m[[0m[0mi[0m[0;34m][0m[0;34m.[0m[0mcpu[0m[0;34m([0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(49)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     48 [0;31m                [0mys[0m [0;34m=[0m [0myss[0m[0;34m[[0m[0mi[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 49 [0;31m                [0mpredicted[0m [0;34m=[0m [0mtorch[0m[0;34m.[0m[0mt[0m[0;34m([0m[0mscore[0m[0;34m[[0m[0mi[0m[0;34m][0m[0;34m.[0m[0mcpu[0m[0;34m([0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     50 [0;31m                [0;31m# print(torch.pow(torch.zeros(predicted.size()) + math.e, predicted.data))[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(52)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     51 [0;31m[0;34m[0m[0m
[0m[0;32m---> 52 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     53 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(53)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     52 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 53 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     54 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mthres_lists[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(54)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     53 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 54 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mthres_lists[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     55 [0;31m            [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> <module 'torch.nn' from '/home/miyawaki_shumpei/.pyenv/versions/miniconda3-4.0.5/envs/taiwa_Project_2019/lib/python3.6/site-packages/torch/nn/__init__.py'>
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(52)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     51 [0;31m[0;34m[0m[0m
[0m[0;32m---> 52 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     53 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> Exiting Debugger.
loss: tensor(1017.8372) lr: 0.0001 time: 1459.42
pred_count_train 41644

Test...
loss: tensor(1022.3210) lr: 0.0001 time: 1416.84
pred_count_train 41644

Test...
vocab size:  29396
[34me2e-stack_ve256_vu256_depth3_adam_lr0.0001_du0.1_dh0.0_True_size5_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...

ga 	p: 79.64 	r: 76.22 	f1: 77.89 	 4701 	 5903 	 6168
wo 	p: 92.02 	r: 86.78 	f1: 89.32 	 2940 	 3195 	 3388
ni 	p: 84.77 	r: 76.41 	f1: 80.37 	 1163 	 1372 	 1522
dev_num_of_high:  0
best_thres [0.43, 0.32, 0.08]
f 0.8171524039354
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 5 	 [0.3, 0.47, 0.14] 	 lr: 0.0001 	 f: 82.160618898508
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 7 [0m
Train...
loss: tensor(689.1609) lr: 0.0001 time: 109.18
pred_count_train 3278

Test...

ga 	p: 7.95 	r: 1.0 	f1: 1.78 	 24 	 302 	 2389
wo 	p: 0.0 	r: 0.0 	f1: 0.0 	 0 	 26 	 1352
ni 	p: 8.33 	r: 0.33 	f1: 0.64 	 2 	 24 	 601
> [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(58)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     57 [0;31m    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 58 [0;31m    [0mprint[0m[0;34m([0m[0;34m'dev_num_of_high: '[0m[0;34m,[0m [0mdev_num_of_high[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     59 [0;31m    [0mprint[0m[0;34m([0m[0;34m'best_thres'[0m[0;34m,[0m [0mbest_thres[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> *** SyntaxError: invalid syntax
ipdb> dev_num_of_high:  0
> [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(59)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     58 [0;31m    [0mprint[0m[0;34m([0m[0;34m'dev_num_of_high: '[0m[0;34m,[0m [0mdev_num_of_high[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 59 [0;31m    [0mprint[0m[0;34m([0m[0;34m'best_thres'[0m[0;34m,[0m [0mbest_thres[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     60 [0;31m    [0mprint[0m[0;34m([0m[0;34m'f'[0m[0;34m,[0m [0mf[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> best_thres [0.1, 0.2, 0.07]
> [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(60)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     59 [0;31m    [0mprint[0m[0;34m([0m[0;34m'best_thres'[0m[0;34m,[0m [0mbest_thres[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 60 [0;31m    [0mprint[0m[0;34m([0m[0;34m'f'[0m[0;34m,[0m [0mf[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     61 [0;31m[0;34m[0m[0m
[0m
ipdb> f 0.011077971878994461
> [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(62)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     61 [0;31m[0;34m[0m[0m
[0m[0;32m---> 62 [0;31m    [0;32mreturn[0m [0mbest_thres[0m[0;34m,[0m [0mf[0m[0;34m,[0m [0mnum_test_instance[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     63 [0;31m[0;34m[0m[0m
[0m
ipdb> [0.1, 0.2, 0.07]
ipdb> 235
ipdb> --Return--
([0.1, 0.2, 0.07], 0.011077971878994461, 235)
> [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(62)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     61 [0;31m[0;34m[0m[0m
[0m[0;32m---> 62 [0;31m    [0;32mreturn[0m [0mbest_thres[0m[0;34m,[0m [0mf[0m[0;34m,[0m [0mnum_test_instance[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     63 [0;31m[0;34m[0m[0m
[0m
ipdb> 
ga 	p: 82.25 	r: 73.25 	f1: 77.49 	 4518 	 5493 	 6168
wo 	p: 93.33 	r: 85.45 	f1: 89.21 	 2895 	 3102 	 3388
ni 	p: 86.05 	r: 74.18 	f1: 79.68 	 1129 	 1312 	 1522
dev_num_of_high:  0
best_thres [0.56, 0.68, 0.09]
f 0.8141053133190375
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 5 	 [0.39, 0.34, 0.17] 	 lr: 0.0001 	 f: 81.89760450915924
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 7 [0m
Train...
> [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/train_edit.py[0m(149)[0;36mtrain[0;34m()[0m
[0;32m    148 [0;31m        [0mthres[0m[0;34m,[0m [0mobj_score[0m[0;34m,[0m [0mnum_test_batch_instance[0m [0;34m=[0m [0mevaluate_multiclass_without_none[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata_dev[0m[0;34m,[0m [0mlen_dev[0m[0;34m,[0m [0mlabels[0m[0;34m,[0m[0mthres_lists[0m[0;34m,[0m [0mmodel_id[0m[0;34m,[0m [0mthreshold[0m[0;34m,[0m [0miterate_num[0m[0;34m,[0m [0mep[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 149 [0;31m        [0mf[0m [0;34m=[0m [0mobj_score[0m [0;34m*[0m [0;36m100[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    150 [0;31m        [0;32mif[0m [0mf[0m [0;34m>[0m [0mbest_performance[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> Exiting Debugger.
vocab size:  29396
[34me2e-stack_ve256_vu256_depth2_adam_lr0.0001_du0.1_dh0.0_True_size1_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(199.4973) lr: 0.0001 time: 27.26
pred_count_train 737

Test...
> [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(25)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     24 [0;31m    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 25 [0;31m    [0mdev_num_of_high[0m [0;34m=[0m [0;36m0[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     26 [0;31m    [0mdic_file[0m [0;34m=[0m [0;34m{[0m[0;34m}[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> defaultdict(<class 'dict'>, {0: {0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 1: {0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.71: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.72: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.73: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.74: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.75: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.76: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.77: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.78: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.79: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.8: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.81: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.82: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.83: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.84: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.85: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 2: {0.0: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.01: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.02: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.03: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.04: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.05: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.06: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.07: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.08: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.09: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}})
ipdb> {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}
ipdb> Exiting Debugger.
vocab size:  29396
[34me2e-stack_ve256_vu256_depth2_adam_lr0.0001_du0.1_dh0.0_True_size1_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(199.4973) lr: 0.0001 time: 27.51
pred_count_train 737

Test...
model-e2e-stack_ve256_vu256_depth2_adam_lr0.0001_du0.1_dh0.0_True_size1_sub1030_th0.2_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth2_adam_lr0.0001_du0.1_dh0.0_True_size1_sub1030_th0.2_it3_rs2016_preFalse
gpu: 0
2 3 2
vocab size:  29396
vocab size:  29396
[34me2e-stack_ve256_vu256_depth2_adam_lr0.0001_du0.1_dh0.0_True_size1_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(199.4973) lr: 0.0001 time: 27.22
pred_count_train 737

Test...
> [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(53)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     52 [0;31m[0;34m[0m[0m
[0m[0;32m---> 53 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(54)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     53 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     55 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0miter_result[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(55)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 55 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0miter_result[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     56 [0;31m                    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(56)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     55 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0miter_result[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 56 [0;31m                    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     57 [0;31m[0;34m[0m[0m
[0m
ipdb> [defaultdict(<class 'dict'>, {0: {0.1: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 1: {0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.71: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.72: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.73: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.74: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.75: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.76: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.77: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.78: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.79: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.8: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.81: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.82: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.83: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.84: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.85: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 2: {0.0: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.01: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.02: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.03: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.04: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.05: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.06: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.07: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.08: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.09: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}}), defaultdict(<class 'dict'>, {0: {0.1: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 1: {0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.71: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.72: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.73: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.74: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.75: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.76: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.77: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.78: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.79: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.8: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.81: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.82: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.83: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.84: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.85: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 2: {0.0: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.01: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.02: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.03: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.04: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.05: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.06: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.07: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.08: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.09: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}}), defaultdict(<class 'dict'>, {0: {0.1: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 1: {0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.71: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.72: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.73: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.74: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.75: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.76: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.77: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.78: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.79: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.8: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.81: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.82: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.83: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.84: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.85: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 2: {0.0: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.01: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.02: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.03: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.04: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.05: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.06: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.07: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.08: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.09: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}})]
ipdb> defaultdict(<class 'dict'>, {0: {0.1: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 1: {0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.71: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.72: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.73: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.74: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.75: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.76: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.77: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.78: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.79: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.8: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.81: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.82: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.83: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.84: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.85: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 1, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 2: {0.0: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.01: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.02: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.03: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.04: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.05: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.06: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.07: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.08: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.09: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}})
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(53)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     52 [0;31m[0;34m[0m[0m
[0m[0;32m---> 53 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(54)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     53 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     55 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0miter_result[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(55)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 55 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0miter_result[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     56 [0;31m                    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> 0
ipdb> [[[0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7], [0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85], [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6]], [[0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7], [0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85], [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6]], [[0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7], [0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85], [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6]]]
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(56)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     55 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0miter_result[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 56 [0;31m                    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     57 [0;31m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(53)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     52 [0;31m[0;34m[0m[0m
[0m[0;32m---> 53 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> <module 'torch.nn' from '/home/miyawaki_shumpei/.pyenv/versions/miniconda3-4.0.5/envs/taiwa_Project_2019/lib/python3.6/site-packages/torch/nn/__init__.py'>
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(48)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     47 [0;31m        [0;32mfor[0m [0mit[0m[0;34m,[0m [0mscore[0m [0;32min[0m [0menumerate[0m[0;34m([0m[0mscores[0m[0;34m)[0m[0;34m:[0m [0;31m# iteration æ¯Žã®è§£æž[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 48 [0;31m            [0;32mfor[0m [0mi[0m [0;32min[0m [0mrange[0m[0;34m([0m[0myss[0m[0;34m.[0m[0msize[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     49 [0;31m                [0mys[0m [0;34m=[0m [0myss[0m[0;34m[[0m[0mi[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(49)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     48 [0;31m            [0;32mfor[0m [0mi[0m [0;32min[0m [0mrange[0m[0;34m([0m[0myss[0m[0;34m.[0m[0msize[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 49 [0;31m                [0mys[0m [0;34m=[0m [0myss[0m[0;34m[[0m[0mi[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     50 [0;31m                [0mpredicted[0m [0;34m=[0m [0mtorch[0m[0;34m.[0m[0mt[0m[0;34m([0m[0mscore[0m[0;34m[[0m[0mi[0m[0;34m][0m[0;34m.[0m[0mcpu[0m[0;34m([0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(50)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     49 [0;31m                [0mys[0m [0;34m=[0m [0myss[0m[0;34m[[0m[0mi[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 50 [0;31m                [0mpredicted[0m [0;34m=[0m [0mtorch[0m[0;34m.[0m[0mt[0m[0;34m([0m[0mscore[0m[0;34m[[0m[0mi[0m[0;34m][0m[0;34m.[0m[0mcpu[0m[0;34m([0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     51 [0;31m                [0;31m# print(torch.pow(torch.zeros(predicted.size()) + math.e, predicted.data))[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(53)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     52 [0;31m[0;34m[0m[0m
[0m[0;32m---> 53 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(54)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     53 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     55 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0miter_result[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(55)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 55 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0miter_result[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     56 [0;31m                    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> *** SyntaxError: invalid syntax
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(56)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     55 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0miter_result[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 56 [0;31m                    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     57 [0;31m[0;34m[0m[0m
[0m
ipdb> 3
ipdb> 3
ipdb> tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])
ipdb> tensor(-4.2115, grad_fn=<SelectBackward>)
ipdb> 0
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(53)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     52 [0;31m[0;34m[0m[0m
[0m[0;32m---> 53 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(53)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     52 [0;31m[0;34m[0m[0m
[0m[0;32m---> 53 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(53)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     52 [0;31m[0;34m[0m[0m
[0m[0;32m---> 53 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(53)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     52 [0;31m[0;34m[0m[0m
[0m[0;32m---> 53 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(53)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     52 [0;31m[0;34m[0m[0m
[0m[0;32m---> 53 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> loss: tensor(792.5892) lr: 0.0001 time: 1478.46
pred_count_train 41644

Test...
> [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(53)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     52 [0;31m[0;34m[0m[0m
[0m[0;32m---> 53 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(53)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     52 [0;31m[0;34m[0m[0m
[0m[0;32m---> 53 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(53)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     52 [0;31m[0;34m[0m[0m
[0m[0;32m---> 53 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(53)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     52 [0;31m[0;34m[0m[0m
[0m[0;32m---> 53 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(53)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     52 [0;31m[0;34m[0m[0m
[0m[0;32m---> 53 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(53)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     52 [0;31m[0;34m[0m[0m
[0m[0;32m---> 53 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(53)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     52 [0;31m[0;34m[0m[0m
[0m[0;32m---> 53 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(53)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     52 [0;31m[0;34m[0m[0m
[0m[0;32m---> 53 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(53)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     52 [0;31m[0;34m[0m[0m
[0m[0;32m---> 53 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(53)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     52 [0;31m[0;34m[0m[0m
[0m[0;32m---> 53 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(53)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     52 [0;31m[0;34m[0m[0m
[0m[0;32m---> 53 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(53)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     52 [0;31m[0;34m[0m[0m
[0m[0;32m---> 53 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(53)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     52 [0;31m[0;34m[0m[0m
[0m[0;32m---> 53 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(53)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     52 [0;31m[0;34m[0m[0m
[0m[0;32m---> 53 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(53)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     52 [0;31m[0;34m[0m[0m
[0m[0;32m---> 53 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(53)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     52 [0;31m[0;34m[0m[0m
[0m[0;32m---> 53 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(53)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     52 [0;31m[0;34m[0m[0m
[0m[0;32m---> 53 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(53)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     52 [0;31m[0;34m[0m[0m
[0m[0;32m---> 53 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(53)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     52 [0;31m[0;34m[0m[0m
[0m[0;32m---> 53 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(53)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     52 [0;31m[0;34m[0m[0m
[0m[0;32m---> 53 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(53)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     52 [0;31m[0;34m[0m[0m
[0m[0;32m---> 53 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(53)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     52 [0;31m[0;34m[0m[0m
[0m[0;32m---> 53 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(53)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     52 [0;31m[0;34m[0m[0m
[0m[0;32m---> 53 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(53)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     52 [0;31m[0;34m[0m[0m
[0m[0;32m---> 53 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(53)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     52 [0;31m[0;34m[0m[0m
[0m[0;32m---> 53 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(53)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     52 [0;31m[0;34m[0m[0m
[0m[0;32m---> 53 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(53)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     52 [0;31m[0;34m[0m[0m
[0m[0;32m---> 53 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(53)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     52 [0;31m[0;34m[0m[0m
[0m[0;32m---> 53 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> vocab size:  29396
[34me2e-stack_ve256_vu256_depth2_adam_lr0.0001_du0.1_dh0.0_True_size1_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(199.4973) lr: 0.0001 time: 22.89
pred_count_train 737

Test...
> [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(53)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     52 [0;31m[0;34m[0m[0m
[0m[0;32m---> 53 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     54 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> 3
ipdb> 3
ipdb> 4
ipdb> loss: tensor(792.3431) lr: 0.0001 time: 1448.46
pred_count_train 41644

Test...
vocab size:  29396
[34me2e-stack_ve256_vu256_depth2_adam_lr0.0001_du0.1_dh0.0_True_size1_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
model-e2e-stack_ve256_vu256_depth2_adam_lr0.0001_du0.1_dh0.0_True_size1_sub1030_th0.2_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth2_adam_lr0.0001_du0.1_dh0.0_True_size1_sub1030_th0.2_it3_rs2016_preFalse
gpu: 0
2 3 2
vocab size:  29396
vocab size:  29396
[34me2e-stack_ve256_vu256_depth2_adam_lr0.0001_du0.1_dh0.0_True_size1_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
model-e2e-stack_ve256_vu256_depth2_adam_lr0.0001_du0.1_dh0.0_True_size1_sub1030_th0.2_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth2_adam_lr0.0001_du0.1_dh0.0_True_size1_sub1030_th0.2_it3_rs2016_preFalse
gpu: 0
2 3 2
vocab size:  29396

ga 	p: 81.95 	r: 74.42 	f1: 78.0 	 4590 	 5601 	 6168
wo 	p: 93.49 	r: 85.15 	f1: 89.13 	 2885 	 3086 	 3388
ni 	p: 84.42 	r: 73.72 	f1: 78.71 	 1122 	 1329 	 1522
dev_num_of_high:  0
best_thres [0.63, 0.61, 0.19]
f 0.815113302360861
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 5 	 [0.3, 0.47, 0.14] 	 lr: 0.0001 	 f: 82.160618898508
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 8 [0m
Train...
vocab size:  29396
[34me2e-stack_ve256_vu256_depth2_adam_lr0.0001_du0.1_dh0.0_True_size1_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(199.4973) lr: 0.0001 time: 30.2
pred_count_train 737

Test...
> [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(59)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     58 [0;31m    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 59 [0;31m    [0mbest_thres[0m[0;34m,[0m [0mf[0m [0;34m=[0m [0mcalc_best_thres[0m[0;34m([0m[0mbest_result[0m[0;34m,[0m [0mresults[0m[0;34m,[0m [0mthres_lists[0m[0;34m,[0m [0mlabels[0m[0;34m,[0m [0mmodel_id[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     60 [0;31m[0;34m[0m[0m
[0m
ipdb> Exiting Debugger.
model-e2e-stack_ve256_vu256_depth2_adam_lr0.0001_du0.1_dh0.0_True_size1_sub1030_th0.2_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth2_adam_lr0.0001_du0.1_dh0.0_True_size1_sub1030_th0.2_it3_rs2016_preFalse
gpu: 0
2 3 2
vocab size:  29396

ga 	p: 82.21 	r: 74.68 	f1: 78.26 	 4606 	 5603 	 6168
wo 	p: 93.32 	r: 84.47 	f1: 88.68 	 2862 	 3067 	 3388
ni 	p: 82.85 	r: 76.48 	f1: 79.54 	 1164 	 1405 	 1522
dev_num_of_high:  0
best_thres [0.7, 0.78, 0.13]
f 0.8161490095967475
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 5 	 [0.39, 0.34, 0.17] 	 lr: 0.0001 	 f: 81.89760450915924
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 8 [0m
Train...
vocab size:  29396
[34me2e-stack_ve256_vu256_depth2_adam_lr0.0001_du0.1_dh0.0_True_size1_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(199.4973) lr: 0.0001 time: 28.02
pred_count_train 737

Test...

ga 	p: 20.27 	r: 21.01 	f1: 20.63 	 75 	 370 	 357
wo 	p: 33.33 	r: 4.23 	f1: 7.5 	 9 	 27 	 213
ni 	p: 0.0 	r: 0.0 	f1: 0.0 	 0 	 447 	 78

ga 	p: 20.27 	r: 21.01 	f1: 20.63 	 75 	 370 	 357
wo 	p: 33.33 	r: 4.23 	f1: 7.5 	 9 	 27 	 213
ni 	p: 0.0 	r: 0.0 	f1: 0.0 	 0 	 447 	 78

ga 	p: 20.27 	r: 21.01 	f1: 20.63 	 75 	 370 	 357
wo 	p: 33.33 	r: 4.23 	f1: 7.5 	 9 	 27 	 213
ni 	p: 0.0 	r: 0.0 	f1: 0.0 	 0 	 447 	 78
dev_num_of_high:  0
best_thres [0.1, 0.2, 0.0]
f [0.11260053619302948, 0.11260053619302948, 0.11260053619302948]
save model
e2e-stack_ve256_vu256_depth2_adam_lr0.0001_du0.1_dh0.0_True_size1_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 1 	 [0.1, 0.2, 0.0] 	 lr: 0.0001 	 f: 11.260053619302948
e2e-stack_ve256_vu256_depth2_adam_lr0.0001_du0.1_dh0.0_True_size1_sub1030_th0.2_it3_rs2016_preFalse 	best in epoch 1 	 [0.1, 0.2, 0.0] 	 lr: 0.0001 	 f: 11.260053619302948
model-e2e-stack_ve256_vu256_depth2_adam_lr0.0001_du0.1_dh0.0_True_size1_sub1030_th0.2_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth2_adam_lr0.0001_du0.1_dh0.0_True_size1_sub1030_th0.2_it3_rs2016_preFalse
gpu: 0
2 3 2
vocab size:  29396
prediction mode: model-e2e-stack_ve256_vu256_depth2_adam_lr0.0001_du0.1_dh0.0_True_size1_sub1030_th0.2_it3_rs2016_preFalse test [0.5, 0.5, 0.5]
vocab size:  29396
[34me2e-stack_ve256_vu256_depth10_adam_lr0.0001_du0.1_dh0.0_True_size10_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(990.6341) lr: 0.0001 time: 397.37
pred_count_train 6727

Test...
loss: tensor(609.8294) lr: 0.0001 time: 1591.84
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 47.69 	r: 54.97 	f1: 51.07 	 1725 	 3617 	 3138
[32m iter_0[0m
wo 	p: 76.87 	r: 56.98 	f1: 65.45 	 1077 	 1401 	 1890
[32m iter_0[0m
ni 	p: 60.55 	r: 61.15 	f1: 60.85 	 488 	 806 	 798

[32m iter_1[0m
ga 	p: 47.69 	r: 54.97 	f1: 51.07 	 1725 	 3617 	 3138
[32m iter_1[0m
wo 	p: 76.87 	r: 56.98 	f1: 65.45 	 1077 	 1401 	 1890
[32m iter_1[0m
ni 	p: 60.55 	r: 61.15 	f1: 60.85 	 488 	 806 	 798

[32m iter_2[0m
ga 	p: 47.69 	r: 54.97 	f1: 51.07 	 1725 	 3617 	 3138
[32m iter_2[0m
wo 	p: 76.87 	r: 56.98 	f1: 65.45 	 1077 	 1401 	 1890
[32m iter_2[0m
ni 	p: 60.55 	r: 61.15 	f1: 60.85 	 488 	 806 	 798
dev_num_of_high:  0
best_thres [0.11, 0.25, 0.22]
f [0.5648, 0.5648, 0.5648]
save model
e2e-stack_ve256_vu256_depth10_adam_lr0.0001_du0.1_dh0.0_True_size10_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 1 	 [0.11, 0.25, 0.22] 	 lr: 0.0001 	 f: 56.48068669527897
[34me2e-stack_ve256_vu256_depth10_adam_lr0.0001_du0.1_dh0.0_True_size10_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 2 [0m
Train...
loss: tensor(610.3696) lr: 0.0001 time: 1514.64
pred_count_train 41644

Test...
loss: tensor(620.3004) lr: 0.0001 time: 383.82
pred_count_train 6727

Test...

ga 	p: 79.01 	r: 77.14 	f1: 78.06 	 4758 	 6022 	 6168
wo 	p: 94.27 	r: 84.53 	f1: 89.14 	 2864 	 3038 	 3388
ni 	p: 81.18 	r: 73.98 	f1: 77.41 	 1126 	 1387 	 1522
dev_num_of_high:  0
best_thres [0.46, 0.76, 0.12]
f 0.812822299651568
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 5 	 [0.3, 0.47, 0.14] 	 lr: 0.0001 	 f: 82.160618898508
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 9 [0m
Train...

[32m iter_0[0m
ga 	p: 60.52 	r: 64.28 	f1: 62.34 	 2017 	 3333 	 3138
[32m iter_0[0m
wo 	p: 84.42 	r: 63.92 	f1: 72.75 	 1208 	 1431 	 1890
[32m iter_0[0m
ni 	p: 86.98 	r: 60.28 	f1: 71.21 	 481 	 553 	 798

[32m iter_1[0m
ga 	p: 60.52 	r: 64.28 	f1: 62.34 	 2017 	 3333 	 3138
[32m iter_1[0m
wo 	p: 84.42 	r: 63.92 	f1: 72.75 	 1208 	 1431 	 1890
[32m iter_1[0m
ni 	p: 86.98 	r: 60.28 	f1: 71.21 	 481 	 553 	 798

[32m iter_2[0m
ga 	p: 60.52 	r: 64.28 	f1: 62.34 	 2017 	 3333 	 3138
[32m iter_2[0m
wo 	p: 84.42 	r: 63.92 	f1: 72.75 	 1208 	 1431 	 1890
[32m iter_2[0m
ni 	p: 86.98 	r: 60.28 	f1: 71.21 	 481 	 553 	 798
dev_num_of_high:  0
best_thres [0.13, 0.36, 0.13]
f [0.6652, 0.6652, 0.6652]
save model
e2e-stack_ve256_vu256_depth10_adam_lr0.0001_du0.1_dh0.0_True_size10_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 2 	 [0.13, 0.36, 0.13] 	 lr: 0.0001 	 f: 66.51709593466751
[34me2e-stack_ve256_vu256_depth10_adam_lr0.0001_du0.1_dh0.0_True_size10_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 3 [0m
Train...

ga 	p: 80.28 	r: 73.91 	f1: 76.96 	 4559 	 5679 	 6168
wo 	p: 93.84 	r: 85.04 	f1: 89.22 	 2881 	 3070 	 3388
ni 	p: 82.04 	r: 72.01 	f1: 76.7 	 1096 	 1336 	 1522
dev_num_of_high:  0
best_thres [0.38, 0.69, 0.14]
f 0.8066909228370269
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 5 	 [0.39, 0.34, 0.17] 	 lr: 0.0001 	 f: 81.89760450915924
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 9 [0m
Train...
loss: tensor(472.2958) lr: 0.0001 time: 396.66
pred_count_train 6727

Test...

[32m iter_0[0m
ga 	p: 65.52 	r: 66.32 	f1: 65.92 	 2081 	 3176 	 3138
[32m iter_0[0m
wo 	p: 87.67 	r: 68.47 	f1: 76.89 	 1294 	 1476 	 1890
[32m iter_0[0m
ni 	p: 79.91 	r: 67.79 	f1: 73.36 	 541 	 677 	 798

[32m iter_1[0m
ga 	p: 65.52 	r: 66.32 	f1: 65.92 	 2081 	 3176 	 3138
[32m iter_1[0m
wo 	p: 87.67 	r: 68.47 	f1: 76.89 	 1294 	 1476 	 1890
[32m iter_1[0m
ni 	p: 79.91 	r: 67.79 	f1: 73.36 	 541 	 677 	 798

[32m iter_2[0m
ga 	p: 65.52 	r: 66.32 	f1: 65.92 	 2081 	 3176 	 3138
[32m iter_2[0m
wo 	p: 87.67 	r: 68.47 	f1: 76.89 	 1294 	 1476 	 1890
[32m iter_2[0m
ni 	p: 79.91 	r: 67.79 	f1: 73.36 	 541 	 677 	 798
dev_num_of_high:  0
best_thres [0.4, 0.5, 0.32]
f [0.7021, 0.7021, 0.7021]
save model
e2e-stack_ve256_vu256_depth10_adam_lr0.0001_du0.1_dh0.0_True_size10_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 3 	 [0.4, 0.5, 0.32] 	 lr: 0.0001 	 f: 70.21066786194532
e2e-stack_ve256_vu256_depth10_adam_lr0.0001_du0.1_dh0.0_True_size10_sub1030_th0.2_it3_rs2016_preFalse 	best in epoch 3 	 [0.4, 0.5, 0.32] 	 lr: 0.0001 	 f: 70.21066786194532
model-e2e-stack_ve256_vu256_depth10_adam_lr0.0001_du0.1_dh0.0_True_size10_sub1030_th0.2_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth10_adam_lr0.0001_du0.1_dh0.0_True_size10_sub1030_th0.2_it3_rs2016_preFalse
gpu: 0
10 3 2
vocab size:  29396
prediction mode: model-e2e-stack_ve256_vu256_depth10_adam_lr0.0001_du0.1_dh0.0_True_size10_sub1030_th0.2_it3_rs2016_preFalse test [0.5, 0.5, 0.5]
loss: tensor(478.2227) lr: 0.0001 time: 1593.6
pred_count_train 41644

Test...
loss: tensor(474.9627) lr: 0.0001 time: 1484.64
pred_count_train 41644

Test...

ga 	p: 81.04 	r: 74.76 	f1: 77.77 	 4611 	 5690 	 6168
wo 	p: 93.1 	r: 85.57 	f1: 89.17 	 2899 	 3114 	 3388
ni 	p: 85.1 	r: 70.17 	f1: 76.92 	 1068 	 1255 	 1522
dev_num_of_high:  0
best_thres [0.52, 0.64, 0.1]
f 0.8116572834366278
load model: epoch5
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 5 	 [0.3, 0.47, 0.14] 	 lr: 0.0001 	 f: 82.160618898508
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 10 [0m
Train...

ga 	p: 80.53 	r: 73.88 	f1: 77.06 	 4557 	 5659 	 6168
wo 	p: 94.38 	r: 83.29 	f1: 88.49 	 2822 	 2990 	 3388
ni 	p: 82.89 	r: 72.27 	f1: 77.22 	 1100 	 1327 	 1522
dev_num_of_high:  0
best_thres [0.5, 0.79, 0.09]
f 0.8054526455780373
load model: epoch5
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 5 	 [0.39, 0.34, 0.17] 	 lr: 0.0001 	 f: 81.89760450915924
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 10 [0m
Train...
loss: tensor(853.0122) lr: 5e-05 time: 1411.06
pred_count_train 41644

Test...
loss: tensor(852.0912) lr: 5e-05 time: 1420.45
pred_count_train 41644

Test...

ga 	p: 81.17 	r: 76.3 	f1: 78.66 	 4706 	 5798 	 6168
wo 	p: 92.14 	r: 86.51 	f1: 89.24 	 2931 	 3181 	 3388
ni 	p: 81.59 	r: 78.06 	f1: 79.79 	 1188 	 1456 	 1522
dev_num_of_high:  0
best_thres [0.4, 0.37, 0.16]
f 0.8204341560916656
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 5 	 [0.3, 0.47, 0.14] 	 lr: 0.0001 	 f: 82.160618898508
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 11 [0m
Train...

ga 	p: 81.49 	r: 75.06 	f1: 78.14 	 4630 	 5682 	 6168
wo 	p: 91.62 	r: 86.81 	f1: 89.15 	 2941 	 3210 	 3388
ni 	p: 83.74 	r: 77.46 	f1: 80.48 	 1179 	 1408 	 1522
dev_num_of_high:  0
best_thres [0.51, 0.28, 0.13]
f 0.8185985592665356
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 5 	 [0.39, 0.34, 0.17] 	 lr: 0.0001 	 f: 81.89760450915924
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 11 [0m
Train...
loss: tensor(630.7140) lr: 5e-05 time: 1412.01
pred_count_train 41644

Test...
loss: tensor(632.4666) lr: 5e-05 time: 1408.21
pred_count_train 41644

Test...

ga 	p: 79.03 	r: 76.85 	f1: 77.92 	 4740 	 5998 	 6168
wo 	p: 92.97 	r: 85.89 	f1: 89.29 	 2910 	 3130 	 3388
ni 	p: 81.04 	r: 76.94 	f1: 78.93 	 1171 	 1445 	 1522
dev_num_of_high:  0
best_thres [0.41, 0.71, 0.1]
f 0.8148353424784074
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 5 	 [0.3, 0.47, 0.14] 	 lr: 0.0001 	 f: 82.160618898508
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 12 [0m
Train...

ga 	p: 80.41 	r: 75.73 	f1: 78.0 	 4671 	 5809 	 6168
wo 	p: 91.74 	r: 86.57 	f1: 89.08 	 2933 	 3197 	 3388
ni 	p: 81.74 	r: 77.92 	f1: 79.78 	 1186 	 1451 	 1522
dev_num_of_high:  0
best_thres [0.47, 0.82, 0.08]
f 0.8163454840956582
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 5 	 [0.39, 0.34, 0.17] 	 lr: 0.0001 	 f: 81.89760450915924
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 12 [0m
Train...
loss: tensor(477.8474) lr: 5e-05 time: 1410.76
pred_count_train 41644

Test...
loss: tensor(478.8883) lr: 5e-05 time: 1411.47
pred_count_train 41644

Test...

ga 	p: 82.37 	r: 73.85 	f1: 77.88 	 4555 	 5530 	 6168
wo 	p: 93.13 	r: 84.86 	f1: 88.8 	 2875 	 3087 	 3388
ni 	p: 82.32 	r: 74.05 	f1: 77.97 	 1127 	 1369 	 1522
dev_num_of_high:  0
best_thres [0.51, 0.62, 0.11]
f 0.8124762628180782
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 5 	 [0.3, 0.47, 0.14] 	 lr: 0.0001 	 f: 82.160618898508
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 13 [0m
Train...

ga 	p: 81.98 	r: 73.17 	f1: 77.32 	 4513 	 5505 	 6168
wo 	p: 92.95 	r: 85.27 	f1: 88.95 	 2889 	 3108 	 3388
ni 	p: 83.06 	r: 74.11 	f1: 78.33 	 1128 	 1358 	 1522
dev_num_of_high:  0
best_thres [0.68, 0.74, 0.09]
f 0.8104898094921374
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 5 	 [0.39, 0.34, 0.17] 	 lr: 0.0001 	 f: 81.89760450915924
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 13 [0m
Train...
loss: tensor(364.1954) lr: 5e-05 time: 1417.47
pred_count_train 41644

Test...
loss: tensor(366.2930) lr: 5e-05 time: 1432.19
pred_count_train 41644

Test...

ga 	p: 82.04 	r: 73.9 	f1: 77.76 	 4558 	 5556 	 6168
wo 	p: 91.54 	r: 86.54 	f1: 88.97 	 2932 	 3203 	 3388
ni 	p: 83.52 	r: 71.62 	f1: 77.11 	 1090 	 1305 	 1522
dev_num_of_high:  0
best_thres [0.52, 0.35, 0.14]
f 0.8116545265348595
load model: epoch5
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 5 	 [0.3, 0.47, 0.14] 	 lr: 0.0001 	 f: 82.160618898508
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 14 [0m
Train...

ga 	p: 81.79 	r: 73.18 	f1: 77.25 	 4514 	 5519 	 6168
wo 	p: 93.4 	r: 85.21 	f1: 89.12 	 2887 	 3091 	 3388
ni 	p: 83.54 	r: 73.72 	f1: 78.32 	 1122 	 1343 	 1522
dev_num_of_high:  0
best_thres [0.7, 0.83, 0.09]
f 0.8105178070467406
load model: epoch5
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 5 	 [0.39, 0.34, 0.17] 	 lr: 0.0001 	 f: 81.89760450915924
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 14 [0m
Train...
loss: tensor(791.8895) lr: 2.5e-05 time: 1710.5
pred_count_train 41644

Test...
loss: tensor(789.6508) lr: 2.5e-05 time: 2000.28
pred_count_train 41644

Test...

ga 	p: 81.88 	r: 74.71 	f1: 78.13 	 4608 	 5628 	 6168
wo 	p: 93.65 	r: 85.8 	f1: 89.56 	 2907 	 3104 	 3388
ni 	p: 85.35 	r: 76.94 	f1: 80.93 	 1171 	 1372 	 1522
dev_num_of_high:  0
best_thres [0.4, 0.72, 0.13]
f 0.8201302993107356
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 14 	 [0.4, 0.72, 0.13] 	 lr: 2.5e-05 	 f: 82.01302993107356
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 15 [0m
Train...

ga 	p: 82.18 	r: 75.13 	f1: 78.5 	 4634 	 5639 	 6168
wo 	p: 92.52 	r: 86.89 	f1: 89.62 	 2944 	 3182 	 3388
ni 	p: 86.28 	r: 77.27 	f1: 81.53 	 1176 	 1363 	 1522
dev_num_of_high:  0
best_thres [0.41, 0.51, 0.19]
f 0.8234408804439847
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 14 	 [0.41, 0.51, 0.19] 	 lr: 2.5e-05 	 f: 82.34408804439846
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 15 [0m
Train...
loss: tensor(605.6866) lr: 2.5e-05 time: 1737.83
pred_count_train 41644

Test...

ga 	p: 80.58 	r: 75.75 	f1: 78.09 	 4672 	 5798 	 6168
wo 	p: 93.23 	r: 85.42 	f1: 89.16 	 2894 	 3104 	 3388
ni 	p: 86.63 	r: 73.19 	f1: 79.34 	 1114 	 1286 	 1522
dev_num_of_high:  0
best_thres [0.39, 0.49, 0.23]
f 0.8163265306122449
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 14 	 [0.4, 0.72, 0.13] 	 lr: 2.5e-05 	 f: 82.01302993107356
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 16 [0m
Train...
loss: tensor(611.8685) lr: 2.5e-05 time: 2016.91
pred_count_train 41644

Test...

ga 	p: 80.18 	r: 76.3 	f1: 78.19 	 4706 	 5869 	 6168
wo 	p: 92.58 	r: 86.54 	f1: 89.46 	 2932 	 3167 	 3388
ni 	p: 86.19 	r: 76.68 	f1: 81.15 	 1167 	 1354 	 1522
dev_num_of_high:  0
best_thres [0.38, 0.41, 0.23]
f 0.820290665176076
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 14 	 [0.41, 0.51, 0.19] 	 lr: 2.5e-05 	 f: 82.34408804439846
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 16 [0m
Train...
loss: tensor(479.8684) lr: 2.5e-05 time: 1719.33
pred_count_train 41644

Test...

ga 	p: 79.98 	r: 75.37 	f1: 77.61 	 4649 	 5813 	 6168
wo 	p: 93.24 	r: 85.48 	f1: 89.19 	 2896 	 3106 	 3388
ni 	p: 82.15 	r: 74.38 	f1: 78.07 	 1132 	 1378 	 1522
dev_num_of_high:  0
best_thres [0.26, 0.7, 0.06]
f 0.8118830409356725
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 14 	 [0.4, 0.72, 0.13] 	 lr: 2.5e-05 	 f: 82.01302993107356
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 17 [0m
Train...
loss: tensor(491.7922) lr: 2.5e-05 time: 2040.38
pred_count_train 41644

Test...

ga 	p: 81.94 	r: 74.38 	f1: 77.98 	 4588 	 5599 	 6168
wo 	p: 93.59 	r: 84.86 	f1: 89.01 	 2875 	 3072 	 3388
ni 	p: 82.89 	r: 74.51 	f1: 78.48 	 1134 	 1368 	 1522
dev_num_of_high:  0
best_thres [0.4, 0.63, 0.09]
f 0.8142255055168821
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 14 	 [0.41, 0.51, 0.19] 	 lr: 2.5e-05 	 f: 82.34408804439846
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 17 [0m
Train...
loss: tensor(388.2369) lr: 2.5e-05 time: 1709.06
pred_count_train 41644

Test...

ga 	p: 78.59 	r: 76.04 	f1: 77.29 	 4690 	 5968 	 6168
wo 	p: 92.25 	r: 86.45 	f1: 89.26 	 2929 	 3175 	 3388
ni 	p: 82.4 	r: 75.69 	f1: 78.9 	 1152 	 1398 	 1522
dev_num_of_high:  0
best_thres [0.33, 0.36, 0.09]
f 0.8114158841759563
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 14 	 [0.4, 0.72, 0.13] 	 lr: 2.5e-05 	 f: 82.01302993107356
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 18 [0m
Train...
loss: tensor(388.6299) lr: 2.5e-05 time: 2016.6
pred_count_train 41644

Test...

ga 	p: 82.06 	r: 73.62 	f1: 77.61 	 4541 	 5534 	 6168
wo 	p: 93.23 	r: 85.36 	f1: 89.12 	 2892 	 3102 	 3388
ni 	p: 83.95 	r: 74.24 	f1: 78.8 	 1130 	 1346 	 1522
dev_num_of_high:  0
best_thres [0.59, 0.68, 0.16]
f 0.8132003798670465
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 14 	 [0.41, 0.51, 0.19] 	 lr: 2.5e-05 	 f: 82.34408804439846
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 18 [0m
Train...
loss: tensor(299.9964) lr: 2.5e-05 time: 1714.52
pred_count_train 41644

Test...

ga 	p: 78.75 	r: 75.83 	f1: 77.26 	 4677 	 5939 	 6168
wo 	p: 91.59 	r: 86.1 	f1: 88.76 	 2917 	 3185 	 3388
ni 	p: 82.63 	r: 71.88 	f1: 76.88 	 1094 	 1324 	 1522
dev_num_of_high:  0
best_thres [0.24, 0.21, 0.05]
f 0.807209885719595
load model: epoch14
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 14 	 [0.4, 0.72, 0.13] 	 lr: 2.5e-05 	 f: 82.01302993107356
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 19 [0m
Train...
loss: tensor(307.1559) lr: 2.5e-05 time: 1835.1
pred_count_train 41644

Test...

ga 	p: 79.07 	r: 76.2 	f1: 77.61 	 4700 	 5944 	 6168
wo 	p: 93.57 	r: 84.65 	f1: 88.89 	 2868 	 3065 	 3388
ni 	p: 82.58 	r: 74.44 	f1: 78.3 	 1133 	 1372 	 1522
dev_num_of_high:  0
best_thres [0.35, 0.58, 0.12]
f 0.8109417959830373
load model: epoch14
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 14 	 [0.41, 0.51, 0.19] 	 lr: 2.5e-05 	 f: 82.34408804439846
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 19 [0m
Train...
loss: tensor(569.7694) lr: 1.25e-05 time: 1552.5
pred_count_train 41644

Test...

ga 	p: 79.48 	r: 76.22 	f1: 77.81 	 4701 	 5915 	 6168
wo 	p: 93.36 	r: 85.45 	f1: 89.23 	 2895 	 3101 	 3388
ni 	p: 86.13 	r: 74.24 	f1: 79.75 	 1130 	 1312 	 1522
dev_num_of_high:  0
best_thres [0.32, 0.7, 0.14]
f 0.8152854339904699
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 14 	 [0.4, 0.72, 0.13] 	 lr: 2.5e-05 	 f: 82.01302993107356
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 20 [0m
Train...
loss: tensor(579.3167) lr: 1.25e-05 time: 1418.18
pred_count_train 41644

Test...

ga 	p: 80.84 	r: 75.81 	f1: 78.25 	 4676 	 5784 	 6168
wo 	p: 91.04 	r: 87.6 	f1: 89.29 	 2968 	 3260 	 3388
ni 	p: 81.68 	r: 78.52 	f1: 80.07 	 1195 	 1463 	 1522
dev_num_of_high:  0
best_thres [0.43, 0.29, 0.11]
f 0.8189946722260829
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 14 	 [0.41, 0.51, 0.19] 	 lr: 2.5e-05 	 f: 82.34408804439846
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 20 [0m
Train...
loss: tensor(488.7910) lr: 1.25e-05 time: 1425.7
pred_count_train 41644

Test...

ga 	p: 79.19 	r: 76.44 	f1: 77.79 	 4715 	 5954 	 6168
wo 	p: 92.85 	r: 85.89 	f1: 89.24 	 2910 	 3134 	 3388
ni 	p: 81.18 	r: 75.69 	f1: 78.34 	 1152 	 1419 	 1522
dev_num_of_high:  0
best_thres [0.27, 0.55, 0.08]
f 0.8132499420894139
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 14 	 [0.4, 0.72, 0.13] 	 lr: 2.5e-05 	 f: 82.01302993107356
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 21 [0m
Train...
loss: tensor(497.1150) lr: 1.25e-05 time: 1425.98
pred_count_train 41644

Test...

ga 	p: 78.91 	r: 77.29 	f1: 78.09 	 4767 	 6041 	 6168
wo 	p: 93.11 	r: 85.39 	f1: 89.08 	 2893 	 3107 	 3388
ni 	p: 84.0 	r: 75.56 	f1: 79.56 	 1150 	 1369 	 1522
dev_num_of_high:  0
best_thres [0.32, 0.59, 0.15]
f 0.8159296133364204
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 14 	 [0.41, 0.51, 0.19] 	 lr: 2.5e-05 	 f: 82.34408804439846
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 21 [0m
Train...
loss: tensor(417.3607) lr: 1.25e-05 time: 1414.88
pred_count_train 41644

Test...

ga 	p: 81.62 	r: 72.94 	f1: 77.04 	 4499 	 5512 	 6168
wo 	p: 92.56 	r: 85.54 	f1: 88.91 	 2898 	 3131 	 3388
ni 	p: 82.5 	r: 72.47 	f1: 77.16 	 1103 	 1337 	 1522
dev_num_of_high:  0
best_thres [0.41, 0.72, 0.07]
f 0.8072941399943014
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 14 	 [0.4, 0.72, 0.13] 	 lr: 2.5e-05 	 f: 82.01302993107356
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 22 [0m
Train...
loss: tensor(431.6957) lr: 1.25e-05 time: 1415.17
pred_count_train 41644

Test...

ga 	p: 81.7 	r: 73.9 	f1: 77.6 	 4558 	 5579 	 6168
wo 	p: 92.86 	r: 85.6 	f1: 89.08 	 2900 	 3123 	 3388
ni 	p: 84.5 	r: 72.73 	f1: 78.18 	 1107 	 1310 	 1522
dev_num_of_high:  0
best_thres [0.47, 0.52, 0.11]
f 0.8122332859174964
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 14 	 [0.41, 0.51, 0.19] 	 lr: 2.5e-05 	 f: 82.34408804439846
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 22 [0m
Train...
loss: tensor(359.9886) lr: 1.25e-05 time: 1423.43
pred_count_train 41644

Test...

ga 	p: 80.2 	r: 74.66 	f1: 77.33 	 4605 	 5742 	 6168
wo 	p: 92.92 	r: 85.27 	f1: 88.93 	 2889 	 3109 	 3388
ni 	p: 86.26 	r: 70.96 	f1: 77.87 	 1080 	 1252 	 1522
dev_num_of_high:  0
best_thres [0.24, 0.48, 0.13]
f 0.8095935036117274
load model: epoch14
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 14 	 [0.4, 0.72, 0.13] 	 lr: 2.5e-05 	 f: 82.01302993107356
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 23 [0m
Train...
loss: tensor(368.9752) lr: 1.25e-05 time: 1411.07
pred_count_train 41644

Test...

ga 	p: 82.15 	r: 73.64 	f1: 77.66 	 4542 	 5529 	 6168
wo 	p: 91.36 	r: 86.78 	f1: 89.01 	 2940 	 3218 	 3388
ni 	p: 82.43 	r: 74.57 	f1: 78.3 	 1135 	 1377 	 1522
dev_num_of_high:  0
best_thres [0.42, 0.29, 0.11]
f 0.8128478445429675
load model: epoch14
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 14 	 [0.41, 0.51, 0.19] 	 lr: 2.5e-05 	 f: 82.34408804439846
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 23 [0m
Train...
loss: tensor(557.4673) lr: 6.25e-06 time: 1414.23
pred_count_train 41644

Test...

ga 	p: 78.9 	r: 77.3 	f1: 78.09 	 4768 	 6043 	 6168
wo 	p: 93.1 	r: 86.04 	f1: 89.43 	 2915 	 3131 	 3388
ni 	p: 86.1 	r: 75.3 	f1: 80.34 	 1146 	 1331 	 1522
dev_num_of_high:  0
best_thres [0.24, 0.45, 0.13]
f 0.8181439095584488
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 14 	 [0.4, 0.72, 0.13] 	 lr: 2.5e-05 	 f: 82.01302993107356
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 24 [0m
Train...
loss: tensor(568.4702) lr: 6.25e-06 time: 1420.71
pred_count_train 41644

Test...

ga 	p: 79.53 	r: 77.35 	f1: 78.43 	 4771 	 5999 	 6168
wo 	p: 91.15 	r: 87.87 	f1: 89.48 	 2977 	 3266 	 3388
ni 	p: 86.06 	r: 76.68 	f1: 81.1 	 1167 	 1356 	 1522
dev_num_of_high:  0
best_thres [0.31, 0.3, 0.16]
f 0.8216968523895111
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 14 	 [0.41, 0.51, 0.19] 	 lr: 2.5e-05 	 f: 82.34408804439846
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 24 [0m
Train...
loss: tensor(502.6462) lr: 6.25e-06 time: 1429.0
pred_count_train 41644

Test...

ga 	p: 81.69 	r: 74.08 	f1: 77.7 	 4569 	 5593 	 6168
wo 	p: 93.59 	r: 85.33 	f1: 89.27 	 2891 	 3089 	 3388
ni 	p: 81.92 	r: 77.4 	f1: 79.59 	 1178 	 1438 	 1522
dev_num_of_high:  0
best_thres [0.41, 0.76, 0.08]
f 0.8149825455231625
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 14 	 [0.4, 0.72, 0.13] 	 lr: 2.5e-05 	 f: 82.01302993107356
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 25 [0m
Train...
loss: tensor(519.1648) lr: 6.25e-06 time: 1419.97
pred_count_train 41644

Test...

ga 	p: 80.03 	r: 76.56 	f1: 78.26 	 4722 	 5900 	 6168
wo 	p: 92.33 	r: 86.69 	f1: 89.42 	 2937 	 3181 	 3388
ni 	p: 83.61 	r: 77.4 	f1: 80.38 	 1178 	 1409 	 1522
dev_num_of_high:  0
best_thres [0.34, 0.45, 0.12]
f 0.8194547477744808
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 14 	 [0.41, 0.51, 0.19] 	 lr: 2.5e-05 	 f: 82.34408804439846
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 25 [0m
Train...
loss: tensor(458.8798) lr: 6.25e-06 time: 1423.96
pred_count_train 41644

Test...

ga 	p: 80.96 	r: 74.4 	f1: 77.54 	 4589 	 5668 	 6168
wo 	p: 92.96 	r: 85.8 	f1: 89.24 	 2907 	 3127 	 3388
ni 	p: 83.44 	r: 74.51 	f1: 78.72 	 1134 	 1359 	 1522
dev_num_of_high:  0
best_thres [0.36, 0.42, 0.09]
f 0.8129238884702336
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 14 	 [0.4, 0.72, 0.13] 	 lr: 2.5e-05 	 f: 82.01302993107356
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 26 [0m
Train...
loss: tensor(474.9394) lr: 6.25e-06 time: 1409.12
pred_count_train 41644

Test...

ga 	p: 80.94 	r: 75.11 	f1: 77.92 	 4633 	 5724 	 6168
wo 	p: 90.67 	r: 88.02 	f1: 89.32 	 2982 	 3289 	 3388
ni 	p: 84.51 	r: 75.62 	f1: 79.82 	 1151 	 1362 	 1522
dev_num_of_high:  0
best_thres [0.41, 0.23, 0.12]
f 0.8172283596699762
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 14 	 [0.41, 0.51, 0.19] 	 lr: 2.5e-05 	 f: 82.34408804439846
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 26 [0m
Train...
loss: tensor(419.3452) lr: 6.25e-06 time: 1415.08
pred_count_train 41644

Test...

ga 	p: 78.81 	r: 75.88 	f1: 77.32 	 4680 	 5938 	 6168
wo 	p: 93.05 	r: 85.74 	f1: 89.25 	 2905 	 3122 	 3388
ni 	p: 81.04 	r: 75.82 	f1: 78.34 	 1154 	 1424 	 1522
dev_num_of_high:  0
best_thres [0.22, 0.48, 0.06]
f 0.81059270939616
load model: epoch14
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 14 	 [0.4, 0.72, 0.13] 	 lr: 2.5e-05 	 f: 82.01302993107356
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 27 [0m
Train...
loss: tensor(436.4465) lr: 6.25e-06 time: 1412.96
pred_count_train 41644

Test...

ga 	p: 82.09 	r: 74.32 	f1: 78.01 	 4584 	 5584 	 6168
wo 	p: 91.54 	r: 87.16 	f1: 89.3 	 2953 	 3226 	 3388
ni 	p: 84.4 	r: 74.64 	f1: 79.22 	 1136 	 1346 	 1522
dev_num_of_high:  0
best_thres [0.45, 0.35, 0.12]
f 0.8168974286521615
load model: epoch14
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 14 	 [0.41, 0.51, 0.19] 	 lr: 2.5e-05 	 f: 82.34408804439846
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 27 [0m
Train...
loss: tensor(553.0392) lr: 5e-06 time: 1430.11
pred_count_train 41644

Test...

ga 	p: 79.19 	r: 76.49 	f1: 77.82 	 4718 	 5958 	 6168
wo 	p: 93.32 	r: 85.71 	f1: 89.35 	 2904 	 3112 	 3388
ni 	p: 83.38 	r: 76.81 	f1: 79.96 	 1169 	 1402 	 1522
dev_num_of_high:  0
best_thres [0.25, 0.46, 0.08]
f 0.8158700696055685
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 14 	 [0.4, 0.72, 0.13] 	 lr: 2.5e-05 	 f: 82.01302993107356
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 28 [0m
Train...
loss: tensor(562.2252) lr: 5e-06 time: 1414.33
pred_count_train 41644

Test...

ga 	p: 79.67 	r: 76.99 	f1: 78.31 	 4749 	 5961 	 6168
wo 	p: 91.97 	r: 87.22 	f1: 89.53 	 2955 	 3213 	 3388
ni 	p: 86.25 	r: 75.82 	f1: 80.7 	 1154 	 1338 	 1522
dev_num_of_high:  0
best_thres [0.3, 0.36, 0.14]
f 0.8205650764242706
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 14 	 [0.41, 0.51, 0.19] 	 lr: 2.5e-05 	 f: 82.34408804439846
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 28 [0m
Train...
loss: tensor(509.0623) lr: 5e-06 time: 1408.51
pred_count_train 41644

Test...

ga 	p: 80.09 	r: 75.7 	f1: 77.83 	 4669 	 5830 	 6168
wo 	p: 92.61 	r: 86.25 	f1: 89.32 	 2922 	 3155 	 3388
ni 	p: 83.89 	r: 76.28 	f1: 79.9 	 1161 	 1384 	 1522
dev_num_of_high:  0
best_thres [0.3, 0.37, 0.09]
f 0.816151443092274
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 14 	 [0.4, 0.72, 0.13] 	 lr: 2.5e-05 	 f: 82.01302993107356
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 29 [0m
Train...
loss: tensor(523.7379) lr: 5e-06 time: 1409.84
pred_count_train 41644

Test...

ga 	p: 78.34 	r: 78.0 	f1: 78.17 	 4811 	 6141 	 6168
wo 	p: 91.98 	r: 87.01 	f1: 89.43 	 2948 	 3205 	 3388
ni 	p: 85.3 	r: 75.89 	f1: 80.32 	 1155 	 1354 	 1522
dev_num_of_high:  0
best_thres [0.24, 0.35, 0.13]
f 0.8186242997520433
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 14 	 [0.41, 0.51, 0.19] 	 lr: 2.5e-05 	 f: 82.34408804439846
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 29 [0m
Train...
loss: tensor(472.6327) lr: 5e-06 time: 1419.67
pred_count_train 41644

Test...

ga 	p: 80.36 	r: 74.89 	f1: 77.53 	 4619 	 5748 	 6168
wo 	p: 93.19 	r: 85.63 	f1: 89.25 	 2901 	 3113 	 3388
ni 	p: 82.31 	r: 75.82 	f1: 78.93 	 1154 	 1402 	 1522
dev_num_of_high:  0
best_thres [0.31, 0.41, 0.07]
f 0.8128953657279416
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 14 	 [0.4, 0.72, 0.13] 	 lr: 2.5e-05 	 f: 82.01302993107356
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 30 [0m
Train...
loss: tensor(487.4944) lr: 5e-06 time: 1407.67
pred_count_train 41644

Test...

ga 	p: 82.98 	r: 73.57 	f1: 77.99 	 4538 	 5469 	 6168
wo 	p: 91.33 	r: 87.66 	f1: 89.46 	 2970 	 3252 	 3388
ni 	p: 85.78 	r: 74.51 	f1: 79.75 	 1134 	 1322 	 1522
dev_num_of_high:  0
best_thres [0.51, 0.29, 0.13]
f 0.818332465318877
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 14 	 [0.41, 0.51, 0.19] 	 lr: 2.5e-05 	 f: 82.34408804439846
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 30 [0m
Train...
loss: tensor(439.3271) lr: 5e-06 time: 1427.98
pred_count_train 41644

Test...

ga 	p: 80.62 	r: 74.37 	f1: 77.37 	 4587 	 5690 	 6168
wo 	p: 91.96 	r: 86.72 	f1: 89.26 	 2938 	 3195 	 3388
ni 	p: 80.6 	r: 77.79 	f1: 79.17 	 1184 	 1469 	 1522
dev_num_of_high:  0
best_thres [0.32, 0.3, 0.07]
f 0.8127099664053751
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	best in epoch 14 	 [0.4, 0.72, 0.13] 	 lr: 2.5e-05 	 f: 82.01302993107356
model-e2e-stack_ve256_vu256_depth10_adam_lr0.0001_du0.1_dh0.0_True_size10_sub1030_th0.2_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth10_adam_lr0.0001_du0.1_dh0.0_True_size10_sub1030_th0.2_it3_rs2016_preFalse
gpu: 0
6 3 2
vocab size:  29396
vocab size:  29396
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(456.7536) lr: 5e-06 time: 1408.35
pred_count_train 41644

Test...

ga 	p: 82.1 	r: 74.35 	f1: 78.03 	 4586 	 5586 	 6168
wo 	p: 92.1 	r: 86.66 	f1: 89.29 	 2936 	 3188 	 3388
ni 	p: 85.66 	r: 74.97 	f1: 79.96 	 1141 	 1332 	 1522
dev_num_of_high:  0
best_thres [0.46, 0.42, 0.17]
f 0.8178814199395769
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	best in epoch 14 	 [0.41, 0.51, 0.19] 	 lr: 2.5e-05 	 f: 82.34408804439846
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse
gpu: 0
6 3 2
vocab size:  29396
vocab size:  29396
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(3190.5974) lr: 0.0002 time: 1421.95
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 77.42 	r: 73.48 	f1: 75.4 	 13597 	 17563 	 18504
wo 	p: 92.7 	r: 83.06 	f1: 87.61 	 8442 	 9107 	 10164
ni 	p: 82.71 	r: 81.43 	f1: 82.07 	 3718 	 4495 	 4566

[32m iter_1[0m
ga 	p: 77.42 	r: 73.48 	f1: 75.4 	 13597 	 17563 	 18504
wo 	p: 92.7 	r: 83.06 	f1: 87.61 	 8442 	 9107 	 10164
ni 	p: 82.71 	r: 81.43 	f1: 82.07 	 3718 	 4495 	 4566

[32m iter_2[0m
ga 	p: 77.42 	r: 73.48 	f1: 75.4 	 13597 	 17563 	 18504
wo 	p: 92.7 	r: 83.06 	f1: 87.61 	 8442 	 9107 	 10164
ni 	p: 82.71 	r: 81.43 	f1: 82.07 	 3718 	 4495 	 4566
dev_num_of_high:  0
best_thres [0.41, 0.41, 0.14]
f [0.7999, 0.7999, 0.7999]
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 1 	 [0.41, 0.41, 0.14] 	 lr: 0.0002 	 f: 79.9919253404556
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 2 [0m
Train...
loss: tensor(3189.3999) lr: 0.0002 time: 1382.32
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 78.58 	r: 73.75 	f1: 76.09 	 13646 	 17366 	 18504
wo 	p: 92.71 	r: 83.88 	f1: 88.08 	 8526 	 9196 	 10164
ni 	p: 82.77 	r: 80.57 	f1: 81.66 	 3679 	 4445 	 4566

[32m iter_1[0m
ga 	p: 78.58 	r: 73.75 	f1: 76.09 	 13646 	 17366 	 18504
wo 	p: 92.71 	r: 83.88 	f1: 88.08 	 8526 	 9196 	 10164
ni 	p: 82.77 	r: 80.57 	f1: 81.66 	 3679 	 4445 	 4566

[32m iter_2[0m
ga 	p: 78.58 	r: 73.75 	f1: 76.09 	 13646 	 17366 	 18504
wo 	p: 92.71 	r: 83.88 	f1: 88.08 	 8526 	 9196 	 10164
ni 	p: 82.77 	r: 80.57 	f1: 81.66 	 3679 	 4445 	 4566
dev_num_of_high:  0
best_thres [0.47, 0.36, 0.22]
f [0.8048, 0.8048, 0.8048]
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 1 	 [0.47, 0.36, 0.22] 	 lr: 0.0002 	 f: 80.48131255740103
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 2 [0m
Train...
loss: tensor(2022.0309) lr: 0.0002 time: 1495.93
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.85 	r: 75.4 	f1: 77.56 	 13952 	 17472 	 18504
wo 	p: 90.19 	r: 87.35 	f1: 88.74 	 8878 	 9844 	 10164
ni 	p: 85.07 	r: 81.47 	f1: 83.23 	 3720 	 4373 	 4566

[32m iter_1[0m
ga 	p: 79.85 	r: 75.4 	f1: 77.56 	 13952 	 17472 	 18504
wo 	p: 90.19 	r: 87.35 	f1: 88.74 	 8878 	 9844 	 10164
ni 	p: 85.07 	r: 81.47 	f1: 83.23 	 3720 	 4373 	 4566

[32m iter_2[0m
ga 	p: 79.85 	r: 75.4 	f1: 77.56 	 13952 	 17472 	 18504
wo 	p: 90.19 	r: 87.35 	f1: 88.74 	 8878 	 9844 	 10164
ni 	p: 85.07 	r: 81.47 	f1: 83.23 	 3720 	 4373 	 4566
dev_num_of_high:  0
best_thres [0.4, 0.21, 0.3]
f [0.8179, 0.8179, 0.8179]
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 2 	 [0.4, 0.21, 0.3] 	 lr: 0.0002 	 f: 81.7891964326972
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 3 [0m
Train...
loss: tensor(2019.2153) lr: 0.0002 time: 1386.79
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.52 	r: 76.23 	f1: 77.84 	 14105 	 17738 	 18504
wo 	p: 92.65 	r: 84.69 	f1: 88.49 	 8608 	 9291 	 10164
ni 	p: 84.28 	r: 83.11 	f1: 83.69 	 3795 	 4503 	 4566

[32m iter_1[0m
ga 	p: 79.52 	r: 76.23 	f1: 77.84 	 14105 	 17738 	 18504
wo 	p: 92.65 	r: 84.69 	f1: 88.49 	 8608 	 9291 	 10164
ni 	p: 84.28 	r: 83.11 	f1: 83.69 	 3795 	 4503 	 4566

[32m iter_2[0m
ga 	p: 79.52 	r: 76.23 	f1: 77.84 	 14105 	 17738 	 18504
wo 	p: 92.65 	r: 84.69 	f1: 88.49 	 8608 	 9291 	 10164
ni 	p: 84.28 	r: 83.11 	f1: 83.69 	 3795 	 4503 	 4566
dev_num_of_high:  0
best_thres [0.34, 0.35, 0.18]
f [0.8186, 0.8186, 0.8186]
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 2 	 [0.34, 0.35, 0.18] 	 lr: 0.0002 	 f: 81.8577648766328
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 3 [0m
Train...
loss: tensor(1631.7800) lr: 0.0002 time: 1825.16
pred_count_train 41644

Test...
loss: tensor(1627.4751) lr: 0.0002 time: 1575.5
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.22 	r: 76.27 	f1: 78.67 	 14113 	 17377 	 18504
wo 	p: 91.41 	r: 87.11 	f1: 89.21 	 8854 	 9686 	 10164
ni 	p: 84.96 	r: 79.3 	f1: 82.03 	 3621 	 4262 	 4566

[32m iter_1[0m
ga 	p: 81.22 	r: 76.27 	f1: 78.67 	 14113 	 17377 	 18504
wo 	p: 91.41 	r: 87.11 	f1: 89.21 	 8854 	 9686 	 10164
ni 	p: 84.96 	r: 79.3 	f1: 82.03 	 3621 	 4262 	 4566

[32m iter_2[0m
ga 	p: 81.22 	r: 76.27 	f1: 78.67 	 14113 	 17377 	 18504
wo 	p: 91.41 	r: 87.11 	f1: 89.21 	 8854 	 9686 	 10164
ni 	p: 84.96 	r: 79.3 	f1: 82.03 	 3621 	 4262 	 4566
dev_num_of_high:  0
best_thres [0.41, 0.3, 0.13]
f [0.8237, 0.8237, 0.8237]
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 3 	 [0.41, 0.3, 0.13] 	 lr: 0.0002 	 f: 82.36806641986401
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 4 [0m
Train...
                                                                                                                                                          
[32m iter_0[0m
ga 	p: 83.11 	r: 75.36 	f1: 79.04 	 13944 	 16778 	 18504
wo 	p: 92.79 	r: 85.63 	f1: 89.07 	 8703 	 9379 	 10164
ni 	p: 86.47 	r: 79.39 	f1: 82.78 	 3625 	 4192 	 4566

[32m iter_1[0m
ga 	p: 83.11 	r: 75.36 	f1: 79.04 	 13944 	 16778 	 18504
wo 	p: 92.79 	r: 85.63 	f1: 89.07 	 8703 	 9379 	 10164
ni 	p: 86.47 	r: 79.39 	f1: 82.78 	 3625 	 4192 	 4566

[32m iter_2[0m
ga 	p: 83.11 	r: 75.36 	f1: 79.04 	 13944 	 16778 	 18504
wo 	p: 92.79 	r: 85.63 	f1: 89.07 	 8703 	 9379 	 10164
ni 	p: 86.47 	r: 79.39 	f1: 82.78 	 3625 	 4192 	 4566
dev_num_of_high:  0
best_thres [0.52, 0.44, 0.14]
f [0.8264, 0.8264, 0.8264]
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 3 	 [0.52, 0.44, 0.14] 	 lr: 0.0002 	 f: 82.6384410927449
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 4 [0m
Train...
loss: tensor(1004.0588) lr: 0.0001 time: 346.82
pred_count_train 6727

Test...

[32m iter_0[0m
ga 	p: 48.93 	r: 51.75 	f1: 50.3 	 1624 	 3319 	 3138
wo 	p: 82.83 	r: 49.79 	f1: 62.19 	 941 	 1136 	 1890
ni 	p: 62.2 	r: 55.89 	f1: 58.88 	 446 	 717 	 798

[32m iter_1[0m
ga 	p: 48.93 	r: 51.75 	f1: 50.3 	 1624 	 3319 	 3138
wo 	p: 82.83 	r: 49.79 	f1: 62.19 	 941 	 1136 	 1890
ni 	p: 62.2 	r: 55.89 	f1: 58.88 	 446 	 717 	 798

[32m iter_2[0m
ga 	p: 48.93 	r: 51.75 	f1: 50.3 	 1624 	 3319 	 3138
wo 	p: 82.83 	r: 49.79 	f1: 62.19 	 941 	 1136 	 1890
ni 	p: 62.2 	r: 55.89 	f1: 58.88 	 446 	 717 	 798
dev_num_of_high:  0
best_thres [0.12, 0.2, 0.19]
f [0.5476, 0.5476, 0.5476]
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size10_sub0_th0.2_it3_rs2016_preFalse 	current best epoch 1 	 [0.12, 0.2, 0.19] 	 lr: 0.0001 	 f: 54.755410074559016
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size10_sub0_th0.2_it3_rs2016_preFalse[0m [33m epoch 2 [0m
Train...
loss: tensor(641.0156) lr: 0.0001 time: 397.61
pred_count_train 6727

Test...

[32m iter_0[0m
ga 	p: 57.08 	r: 64.37 	f1: 60.51 	 2020 	 3539 	 3138
wo 	p: 81.25 	r: 64.44 	f1: 71.88 	 1218 	 1499 	 1890
ni 	p: 79.37 	r: 62.66 	f1: 70.03 	 500 	 630 	 798

[32m iter_1[0m
ga 	p: 57.08 	r: 64.37 	f1: 60.51 	 2020 	 3539 	 3138
wo 	p: 81.25 	r: 64.44 	f1: 71.88 	 1218 	 1499 	 1890
ni 	p: 79.37 	r: 62.66 	f1: 70.03 	 500 	 630 	 798

[32m iter_2[0m
ga 	p: 57.08 	r: 64.37 	f1: 60.51 	 2020 	 3539 	 3138
wo 	p: 81.25 	r: 64.44 	f1: 71.88 	 1218 	 1499 	 1890
ni 	p: 79.37 	r: 62.66 	f1: 70.03 	 500 	 630 	 798
dev_num_of_high:  0
best_thres [0.11, 0.26, 0.19]
f [0.6504, 0.6504, 0.6504]
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size10_sub0_th0.2_it3_rs2016_preFalse 	current best epoch 2 	 [0.11, 0.26, 0.19] 	 lr: 0.0001 	 f: 65.04263093788063
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size10_sub0_th0.2_it3_rs2016_preFalse[0m [33m epoch 3 [0m
Train...
loss: tensor(497.7917) lr: 0.0001 time: 324.51
pred_count_train 6727

Test...
                                                                                 
[32m iter_0[0m
ga 	p: 64.13 	r: 63.19 	f1: 63.66 	 1983 	 3092 	 3138
wo 	p: 83.98 	r: 71.27 	f1: 77.1 	 1347 	 1604 	 1890
ni 	p: 78.47 	r: 70.8 	f1: 74.44 	 565 	 720 	 798

[32m iter_1[0m
ga 	p: 64.13 	r: 63.19 	f1: 63.66 	 1983 	 3092 	 3138
wo 	p: 83.98 	r: 71.27 	f1: 77.1 	 1347 	 1604 	 1890
ni 	p: 78.47 	r: 70.8 	f1: 74.44 	 565 	 720 	 798

[32m iter_2[0m
ga 	p: 64.13 	r: 63.19 	f1: 63.66 	 1983 	 3092 	 3138
wo 	p: 83.98 	r: 71.27 	f1: 77.1 	 1347 	 1604 	 1890
ni 	p: 78.47 	r: 70.8 	f1: 74.44 	 565 	 720 	 798
dev_num_of_high:  0
best_thres [0.55, 0.25, 0.47]
f [0.6929, 0.6929, 0.6929]
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size10_sub0_th0.2_it3_rs2016_preFalse 	current best epoch 3 	 [0.55, 0.25, 0.47] 	 lr: 0.0001 	 f: 69.29371997865148
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size10_sub0_th0.2_it3_rs2016_preFalse 	best in epoch 3 	 [0.55, 0.25, 0.47] 	 lr: 0.0001 	 f: 69.29371997865148
loss: tensor(1294.3757) lr: 0.0002 time: 1575.41
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.71 	r: 75.14 	f1: 77.36 	 13904 	 17444 	 18504
wo 	p: 93.02 	r: 85.62 	f1: 89.16 	 8702 	 9355 	 10164
ni 	p: 85.4 	r: 80.46 	f1: 82.86 	 3674 	 4302 	 4566

[32m iter_1[0m
ga 	p: 79.71 	r: 75.14 	f1: 77.36 	 13904 	 17444 	 18504
wo 	p: 93.02 	r: 85.62 	f1: 89.16 	 8702 	 9355 	 10164
ni 	p: 85.4 	r: 80.46 	f1: 82.86 	 3674 	 4302 	 4566

[32m iter_2[0m
ga 	p: 79.71 	r: 75.14 	f1: 77.36 	 13904 	 17444 	 18504
wo 	p: 93.02 	r: 85.62 	f1: 89.16 	 8702 	 9355 	 10164
ni 	p: 85.4 	r: 80.46 	f1: 82.86 	 3674 	 4302 	 4566
dev_num_of_high:  0
best_thres [0.27, 0.64, 0.26]
f [0.817, 0.817, 0.817]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 3 	 [0.41, 0.3, 0.13] 	 lr: 0.0002 	 f: 82.36806641986401
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 5 [0m
Train...

[32m iter_0[0m
ga 	p: 79.8 	r: 76.29 	f1: 78.01 	 14117 	 17690 	 18504
wo 	p: 92.6 	r: 86.44 	f1: 89.42 	 8786 	 9488 	 10164
ni 	p: 86.57 	r: 81.03 	f1: 83.71 	 3700 	 4274 	 4566

[32m iter_1[0m
ga 	p: 79.8 	r: 76.29 	f1: 78.01 	 14117 	 17690 	 18504
wo 	p: 92.6 	r: 86.44 	f1: 89.42 	 8786 	 9488 	 10164
ni 	p: 86.57 	r: 81.03 	f1: 83.71 	 3700 	 4274 	 4566

[32m iter_2[0m
ga 	p: 79.8 	r: 76.29 	f1: 78.01 	 14117 	 17690 	 18504
wo 	p: 92.6 	r: 86.44 	f1: 89.42 	 8786 	 9488 	 10164
ni 	p: 86.57 	r: 81.03 	f1: 83.71 	 3700 	 4274 	 4566
dev_num_of_high:  0
best_thres [0.23, 0.57, 0.24]
f [0.8225, 0.8225, 0.8225]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 3 	 [0.52, 0.44, 0.14] 	 lr: 0.0002 	 f: 82.6384410927449
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 5 [0m
Train...
loss: tensor(980.0665) lr: 0.0002 time: 1582.51
pred_count_train 41644

Test...
loss: tensor(988.0747) lr: 0.0002 time: 1929.39
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.23 	r: 74.73 	f1: 77.85 	 13828 	 17023 	 18504
wo 	p: 93.9 	r: 85.52 	f1: 89.51 	 8692 	 9257 	 10164
ni 	p: 82.75 	r: 71.55 	f1: 76.74 	 3267 	 3948 	 4566

[32m iter_1[0m
ga 	p: 81.23 	r: 74.73 	f1: 77.85 	 13828 	 17023 	 18504
wo 	p: 93.9 	r: 85.52 	f1: 89.51 	 8692 	 9257 	 10164
ni 	p: 82.75 	r: 71.55 	f1: 76.74 	 3267 	 3948 	 4566

[32m iter_2[0m
ga 	p: 81.23 	r: 74.73 	f1: 77.85 	 13828 	 17023 	 18504
wo 	p: 93.9 	r: 85.52 	f1: 89.51 	 8692 	 9257 	 10164
ni 	p: 82.75 	r: 71.55 	f1: 76.74 	 3267 	 3948 	 4566
dev_num_of_high:  0
best_thres [0.54, 0.73, 0.16]
f [0.8127, 0.8127, 0.8127]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 3 	 [0.52, 0.44, 0.14] 	 lr: 0.0002 	 f: 82.6384410927449
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 6 [0m
Train...

[32m iter_0[0m
ga 	p: 80.49 	r: 76.5 	f1: 78.44 	 14155 	 17586 	 18504
wo 	p: 93.53 	r: 84.99 	f1: 89.05 	 8638 	 9236 	 10164
ni 	p: 83.4 	r: 75.03 	f1: 78.99 	 3426 	 4108 	 4566

[32m iter_1[0m
ga 	p: 80.49 	r: 76.5 	f1: 78.44 	 14155 	 17586 	 18504
wo 	p: 93.53 	r: 84.99 	f1: 89.05 	 8638 	 9236 	 10164
ni 	p: 83.4 	r: 75.03 	f1: 78.99 	 3426 	 4108 	 4566

[32m iter_2[0m
ga 	p: 80.49 	r: 76.5 	f1: 78.44 	 14155 	 17586 	 18504
wo 	p: 93.53 	r: 84.99 	f1: 89.05 	 8638 	 9236 	 10164
ni 	p: 83.4 	r: 75.03 	f1: 78.99 	 3426 	 4108 	 4566
dev_num_of_high:  0
best_thres [0.35, 0.58, 0.17]
f [0.8172, 0.8172, 0.8172]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 3 	 [0.41, 0.3, 0.13] 	 lr: 0.0002 	 f: 82.36806641986401
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 6 [0m
Train...
                                                                                                                                                                                                                                                                                                                                                                                               loss: tensor(744.0271) lr: 0.0002 time: 1587.1
pred_count_train 41644

Test...
vocab size:  29396
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(176.5129) lr: 0.0001 time: 26.97
pred_count_train 737

Test...
{0: [tensor([[-1.6279e+00, -3.0432e+00, -2.8770e+00, -3.5713e-01],
        [-5.1149e+00, -6.3549e+00, -5.9598e+00, -1.0379e-02],
        [-3.7676e+00, -5.1799e+00, -4.6362e+00, -3.9187e-02],
        [-2.6225e+00, -3.4358e+00, -2.9382e+00, -1.7172e-01],
        [-1.3992e+00, -2.7557e+00, -2.0227e+00, -5.8457e-01],
        [-3.6872e+00, -5.5757e+00, -4.7682e+00, -3.8041e-02],
        [-5.0489e+00, -7.2929e+00, -6.1866e+00, -9.1958e-03],
        [-3.1287e+00, -5.2316e+00, -4.1738e+00, -6.6690e-02],
        [-2.7268e+00, -4.2339e+00, -3.3431e+00, -1.2245e-01],
        [-5.1200e+00, -6.7472e+00, -5.7614e+00, -1.0351e-02],
        [-3.2932e+00, -5.0008e+00, -4.0293e+00, -6.3638e-02],
        [-2.2741e+00, -3.2506e+00, -3.0623e+00, -2.0877e-01],
        [-4.4493e+00, -5.5619e+00, -5.1910e+00, -2.1320e-02],
        [-2.9334e+00, -4.4235e+00, -4.0198e+00, -8.6830e-02],
        [-5.3775e+00, -6.8527e+00, -6.4980e+00, -7.2083e-03],
        [-5.9058e+00, -7.6885e+00, -7.1842e+00, -3.9482e-03],
        [-3.7938e+00, -6.0074e+00, -5.6972e+00, -2.8735e-02],
        [-5.7491e+00, -7.7479e+00, -7.5045e+00, -4.1766e-03],
        [-3.3108e+00, -5.1926e+00, -5.1474e+00, -4.9043e-02],
        [-3.3511e+00, -4.4378e+00, -4.9772e+00, -5.5261e-02],
        [-5.5593e+00, -6.5757e+00, -6.7762e+00, -6.4068e-03],
        [-3.3836e+00, -3.7222e+00, -4.2949e+00, -7.4446e-02],
        [-5.4550e+00, -5.8780e+00, -6.1546e+00, -9.2413e-03],
        [-4.1192e+00, -4.4704e+00, -4.7322e+00, -3.7190e-02],
        [-2.8477e+00, -3.5276e+00, -3.3721e+00, -1.2973e-01],
        [-1.6075e+00, -2.6256e+00, -2.1920e+00, -4.8528e-01],
        [-4.4819e+00, -6.4514e+00, -5.5548e+00, -1.6901e-02],
        [-3.9804e+00, -6.1800e+00, -5.0263e+00, -2.7692e-02],
        [-3.9814e+00, -5.3273e+00, -4.5206e+00, -3.5005e-02],
        [-2.9331e+00, -3.5193e+00, -3.7162e+00, -1.1336e-01],
        [-6.6582e+00, -6.3439e+00, -6.8913e+00, -4.0655e-03],
        [-3.5643e+00, -4.1593e+00, -4.2710e+00, -5.9649e-02],
        [-6.3738e+00, -6.5114e+00, -6.8505e+00, -4.2601e-03],
        [-4.0655e+00, -4.2679e+00, -4.7695e+00, -4.0457e-02],
        [-7.8144e+00, -6.9636e+00, -7.7157e+00, -1.7967e-03],
        [-5.8542e+00, -4.7265e+00, -5.5213e+00, -1.5851e-02],
        [-2.6855e+00, -2.6950e+00, -3.0901e+00, -1.9995e-01],
        [-1.3059e+00, -2.3101e+00, -2.2915e+00, -6.3730e-01],
        [-4.8337e+00, -5.8538e+00, -5.7554e+00, -1.4091e-02],
        [-3.4176e+00, -4.7542e+00, -4.6175e+00, -5.2644e-02],
        [-3.5912e+00, -4.6756e+00, -4.6214e+00, -4.7853e-02],
        [-6.4808e+00, -6.5778e+00, -6.6264e+00, -4.2572e-03]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>), tensor([[-1.6290e+00, -3.0477e+00, -2.8814e+00, -3.5618e-01],
        [-5.1193e+00, -6.3634e+00, -5.9685e+00, -1.0315e-02],
        [-3.7762e+00, -5.1931e+00, -4.6500e+00, -3.8769e-02],
        [-2.6361e+00, -3.4534e+00, -2.9573e+00, -1.6870e-01],
        [-1.4148e+00, -2.7721e+00, -2.0423e+00, -5.7135e-01],
        [-3.7327e+00, -5.6119e+00, -4.8113e+00, -3.6373e-02],
        [-5.0346e+00, -7.2786e+00, -6.1912e+00, -9.2893e-03],
        [-3.1244e+00, -5.2214e+00, -4.1782e+00, -6.6877e-02],
        [-2.7275e+00, -4.2324e+00, -3.3477e+00, -1.2225e-01],
        [-5.1222e+00, -6.7501e+00, -5.7663e+00, -1.0318e-02],
        [-3.2948e+00, -5.0039e+00, -4.0327e+00, -6.3487e-02],
        [-2.2745e+00, -3.2530e+00, -3.0639e+00, -2.0851e-01],
        [-4.4489e+00, -5.5630e+00, -5.1914e+00, -2.1319e-02],
        [-2.9326e+00, -4.4236e+00, -4.0194e+00, -8.6879e-02],
        [-5.3763e+00, -6.8516e+00, -6.4968e+00, -7.2174e-03],
        [-5.9042e+00, -7.6863e+00, -7.1821e+00, -3.9549e-03],
        [-3.7918e+00, -6.0039e+00, -5.6941e+00, -2.8801e-02],
        [-5.7460e+00, -7.7425e+00, -7.4997e+00, -4.1914e-03],
        [-3.3063e+00, -5.1844e+00, -5.1400e+00, -4.9310e-02],
        [-3.3436e+00, -4.4257e+00, -4.9660e+00, -5.5775e-02],
        [-5.5449e+00, -6.5568e+00, -6.7579e+00, -6.5103e-03],
        [-3.3600e+00, -3.6953e+00, -4.2678e+00, -7.6434e-02],
        [-5.4136e+00, -5.8384e+00, -6.1137e+00, -9.6273e-03],
        [-4.1259e+00, -4.4692e+00, -4.7076e+00, -3.7319e-02],
        [-2.8500e+00, -3.5204e+00, -3.3565e+00, -1.3044e-01],
        [-1.6084e+00, -2.6197e+00, -2.1822e+00, -4.8748e-01],
        [-4.4785e+00, -6.4427e+00, -5.5457e+00, -1.6990e-02],
        [-3.9778e+00, -6.1732e+00, -5.0202e+00, -2.7798e-02],
        [-3.9796e+00, -5.3226e+00, -4.5169e+00, -3.5105e-02],
        [-2.9326e+00, -3.5164e+00, -3.7145e+00, -1.1354e-01],
        [-6.6583e+00, -6.3423e+00, -6.8905e+00, -4.0693e-03],
        [-3.5646e+00, -4.1583e+00, -4.2707e+00, -5.9659e-02],
        [-6.3743e+00, -6.5110e+00, -6.8505e+00, -4.2596e-03],
        [-4.0660e+00, -4.2677e+00, -4.7697e+00, -4.0450e-02],
        [-7.8148e+00, -6.9636e+00, -7.7159e+00, -1.7967e-03],
        [-5.8545e+00, -4.7266e+00, -5.5215e+00, -1.5849e-02],
        [-2.6857e+00, -2.6951e+00, -3.0903e+00, -1.9992e-01],
        [-1.3060e+00, -2.3101e+00, -2.2916e+00, -6.3723e-01],
        [-4.8339e+00, -5.8539e+00, -5.7555e+00, -1.4088e-02],
        [-3.4178e+00, -4.7543e+00, -4.6177e+00, -5.2637e-02],
        [-3.5913e+00, -4.6757e+00, -4.6215e+00, -4.7847e-02],
        [-6.4809e+00, -6.5779e+00, -6.6265e+00, -4.2567e-03]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>), tensor([[-1.6290e+00, -3.0477e+00, -2.8814e+00, -3.5617e-01],
        [-5.1193e+00, -6.3635e+00, -5.9685e+00, -1.0315e-02],
        [-3.7762e+00, -5.1931e+00, -4.6500e+00, -3.8768e-02],
        [-2.6362e+00, -3.4535e+00, -2.9573e+00, -1.6869e-01],
        [-1.4148e+00, -2.7722e+00, -2.0423e+00, -5.7132e-01],
        [-3.7328e+00, -5.6121e+00, -4.8114e+00, -3.6370e-02],
        [-5.0348e+00, -7.2787e+00, -6.1912e+00, -9.2883e-03],
        [-3.1245e+00, -5.2216e+00, -4.1783e+00, -6.6869e-02],
        [-2.7276e+00, -4.2326e+00, -3.3478e+00, -1.2223e-01],
        [-5.1224e+00, -6.7504e+00, -5.7665e+00, -1.0316e-02],
        [-3.2951e+00, -5.0043e+00, -4.0329e+00, -6.3471e-02],
        [-2.2748e+00, -3.2535e+00, -3.0641e+00, -2.0845e-01],
        [-4.4493e+00, -5.5638e+00, -5.1919e+00, -2.1308e-02],
        [-2.9331e+00, -4.4246e+00, -4.0201e+00, -8.6823e-02],
        [-5.3770e+00, -6.8531e+00, -6.4979e+00, -7.2103e-03],
        [-5.9052e+00, -7.6885e+00, -7.1838e+00, -3.9496e-03],
        [-3.7932e+00, -6.0072e+00, -5.6966e+00, -2.8750e-02],
        [-5.7485e+00, -7.7474e+00, -7.5037e+00, -4.1795e-03],
        [-3.3102e+00, -5.1918e+00, -5.1464e+00, -4.9075e-02],
        [-3.3505e+00, -4.4366e+00, -4.9759e+00, -5.5306e-02],
        [-5.5585e+00, -6.5740e+00, -6.7743e+00, -6.4139e-03],
        [-3.3828e+00, -3.7199e+00, -4.2924e+00, -7.4575e-02],
        [-5.4535e+00, -5.8743e+00, -6.1505e+00, -9.2676e-03],
        [-4.1162e+00, -4.4646e+00, -4.7257e+00, -3.7369e-02],
        [-2.8424e+00, -3.5188e+00, -3.3623e+00, -1.3077e-01],
        [-1.6005e+00, -2.6157e+00, -2.1798e+00, -4.9099e-01],
        [-4.4581e+00, -6.4280e+00, -5.5269e+00, -1.7327e-02],
        [-3.9423e+00, -6.1487e+00, -4.9882e+00, -2.8767e-02],
        [-3.9999e+00, -5.3395e+00, -4.5234e+00, -3.4557e-02],
        [-2.9359e+00, -3.5173e+00, -3.7120e+00, -1.1339e-01],
        [-6.6596e+00, -6.3396e+00, -6.8856e+00, -4.0774e-03],
        [-3.5639e+00, -4.1545e+00, -4.2657e+00, -5.9819e-02],
        [-6.3733e+00, -6.5079e+00, -6.8471e+00, -4.2696e-03],
        [-4.0651e+00, -4.2653e+00, -4.7674e+00, -4.0522e-02],
        [-7.8144e+00, -6.9621e+00, -7.7148e+00, -1.7986e-03],
        [-5.8542e+00, -4.7254e+00, -5.5210e+00, -1.5862e-02],
        [-2.6856e+00, -2.6944e+00, -3.0902e+00, -1.9998e-01],
        [-1.3060e+00, -2.3098e+00, -2.2916e+00, -6.3731e-01],
        [-4.8338e+00, -5.8537e+00, -5.7556e+00, -1.4089e-02],
        [-3.4178e+00, -4.7542e+00, -4.6177e+00, -5.2637e-02],
        [-3.5913e+00, -4.6755e+00, -4.6216e+00, -4.7846e-02],
        [-6.4810e+00, -6.5779e+00, -6.6266e+00, -4.2567e-03]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>), tensor([[-1.6290e+00, -3.0477e+00, -2.8814e+00, -3.5617e-01],
        [-5.1193e+00, -6.3635e+00, -5.9685e+00, -1.0315e-02],
        [-3.7762e+00, -5.1931e+00, -4.6500e+00, -3.8767e-02],
        [-2.6362e+00, -3.4536e+00, -2.9573e+00, -1.6868e-01],
        [-1.4148e+00, -2.7722e+00, -2.0423e+00, -5.7130e-01],
        [-3.7328e+00, -5.6121e+00, -4.8114e+00, -3.6369e-02],
        [-5.0348e+00, -7.2788e+00, -6.1912e+00, -9.2878e-03],
        [-3.1246e+00, -5.2217e+00, -4.1783e+00, -6.6865e-02],
        [-2.7277e+00, -4.2327e+00, -3.3478e+00, -1.2222e-01],
        [-5.1225e+00, -6.7505e+00, -5.7665e+00, -1.0315e-02],
        [-3.2951e+00, -5.0044e+00, -4.0329e+00, -6.3465e-02],
        [-2.2748e+00, -3.2536e+00, -3.0642e+00, -2.0843e-01],
        [-4.4495e+00, -5.5639e+00, -5.1920e+00, -2.1305e-02],
        [-2.9333e+00, -4.4247e+00, -4.0203e+00, -8.6810e-02],
        [-5.3772e+00, -6.8533e+00, -6.4981e+00, -7.2088e-03],
        [-5.9054e+00, -7.6888e+00, -7.1841e+00, -3.9492e-03],
        [-3.7935e+00, -6.0075e+00, -5.6970e+00, -2.8743e-02],
        [-5.7487e+00, -7.7479e+00, -7.5042e+00, -4.1780e-03],
        [-3.3105e+00, -5.1925e+00, -5.1472e+00, -4.9055e-02],
        [-3.3508e+00, -4.4377e+00, -4.9770e+00, -5.5273e-02],
        [-5.5590e+00, -6.5755e+00, -6.7760e+00, -6.4077e-03],
        [-3.3834e+00, -3.7221e+00, -4.2948e+00, -7.4460e-02],
        [-5.4548e+00, -5.8778e+00, -6.1544e+00, -9.2432e-03],
        [-4.1190e+00, -4.4703e+00, -4.7320e+00, -3.7198e-02],
        [-2.8475e+00, -3.5274e+00, -3.3719e+00, -1.2976e-01],
        [-1.6074e+00, -2.6255e+00, -2.1918e+00, -4.8537e-01],
        [-4.4816e+00, -6.4511e+00, -5.5545e+00, -1.6906e-02],
        [-3.9801e+00, -6.1797e+00, -5.0259e+00, -2.7700e-02],
        [-3.9811e+00, -5.3270e+00, -4.5202e+00, -3.5018e-02],
        [-2.9328e+00, -3.5190e+00, -3.7157e+00, -1.1341e-01],
        [-6.6578e+00, -6.3434e+00, -6.8905e+00, -4.0679e-03],
        [-3.5640e+00, -4.1585e+00, -4.2700e+00, -5.9685e-02],
        [-6.3734e+00, -6.5103e+00, -6.8490e+00, -4.2639e-03],
        [-4.0650e+00, -4.2663e+00, -4.7675e+00, -4.0506e-02],
        [-7.8134e+00, -6.9609e+00, -7.7126e+00, -1.8015e-03],
        [-5.8520e+00, -4.7219e+00, -5.5160e+00, -1.5920e-02],
        [-2.6819e+00, -2.6883e+00, -3.0826e+00, -2.0122e-01],
        [-1.3015e+00, -2.3022e+00, -2.2824e+00, -6.4282e-01],
        [-4.8134e+00, -5.8332e+00, -5.7315e+00, -1.4394e-02],
        [-3.3871e+00, -4.7287e+00, -4.5866e+00, -5.4277e-02],
        [-3.6113e+00, -4.6797e+00, -4.6134e+00, -4.7318e-02],
        [-6.4939e+00, -6.5870e+00, -6.6308e+00, -4.2191e-03]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)], 1: [tensor([[-2.2727e+00, -3.8671e+00, -3.6158e+00, -1.6352e-01],
        [-5.4998e+00, -6.6626e+00, -6.3869e+00, -7.0739e-03],
        [-4.3271e+00, -5.4675e+00, -5.1839e+00, -2.3303e-02],
        [-3.9121e+00, -4.1476e+00, -3.9976e+00, -5.5683e-02],
        [-2.1320e+00, -2.7932e+00, -2.4549e+00, -3.0882e-01],
        [-4.0855e+00, -5.3392e+00, -4.9235e+00, -2.9314e-02],
        [-5.4460e+00, -7.1434e+00, -6.4162e+00, -6.7611e-03],
        [-3.7450e+00, -5.4208e+00, -4.6680e+00, -3.8169e-02],
        [-3.0892e+00, -4.2625e+00, -3.6629e+00, -8.9143e-02],
        [-5.4137e+00, -6.7703e+00, -6.0366e+00, -8.0242e-03],
        [-3.3329e+00, -4.8932e+00, -4.1262e+00, -6.1163e-02],
        [-2.5401e+00, -3.5414e+00, -3.4448e+00, -1.5052e-01],
        [-4.4749e+00, -5.7516e+00, -5.3669e+00, -1.9425e-02],
        [-3.0315e+00, -4.6871e+00, -4.2457e+00, -7.4488e-02],
        [-5.5144e+00, -6.9911e+00, -6.6474e+00, -6.2652e-03],
        [-6.1437e+00, -7.8090e+00, -7.3379e+00, -3.2086e-03],
        [-3.9663e+00, -6.0276e+00, -5.7301e+00, -2.4908e-02],
        [-5.8431e+00, -7.7351e+00, -7.4951e+00, -3.9005e-03],
        [-3.3418e+00, -5.2328e+00, -5.1273e+00, -4.7767e-02],
        [-3.5047e+00, -4.6944e+00, -5.1466e+00, -4.6066e-02],
        [-5.5362e+00, -6.6709e+00, -6.8517e+00, -6.2857e-03],
        [-3.6543e+00, -4.1504e+00, -4.7470e+00, -5.1625e-02],
        [-5.5921e+00, -6.1224e+00, -6.4630e+00, -7.5088e-03],
        [-4.2187e+00, -4.6805e+00, -5.0266e+00, -3.1030e-02],
        [-3.0961e+00, -3.8351e+00, -3.8191e+00, -9.2963e-02],
        [-1.7569e+00, -2.7971e+00, -2.5491e+00, -3.7355e-01],
        [-4.6613e+00, -6.4701e+00, -5.7461e+00, -1.4300e-02],
        [-4.2553e+00, -6.2064e+00, -5.2300e+00, -2.1795e-02],
        [-4.1106e+00, -5.2997e+00, -4.6135e+00, -3.1808e-02],
        [-3.0728e+00, -3.7082e+00, -3.8965e+00, -9.5547e-02],
        [-6.5058e+00, -6.3585e+00, -6.8400e+00, -4.3063e-03],
        [-3.4239e+00, -4.2807e+00, -4.2837e+00, -6.2100e-02],
        [-6.1460e+00, -6.5708e+00, -6.7795e+00, -4.6906e-03],
        [-3.7783e+00, -4.2914e+00, -4.6782e+00, -4.6928e-02],
        [-7.2661e+00, -6.8101e+00, -7.4465e+00, -2.3880e-03],
        [-5.4829e+00, -4.8458e+00, -5.4775e+00, -1.6331e-02],
        [-2.7546e+00, -3.0456e+00, -3.3259e+00, -1.5916e-01],
        [-1.3761e+00, -2.3887e+00, -2.3632e+00, -5.7700e-01],
        [-4.8039e+00, -5.6611e+00, -5.6606e+00, -1.5273e-02],
        [-3.5090e+00, -4.6498e+00, -4.6511e+00, -5.0286e-02],
        [-3.8415e+00, -4.7435e+00, -4.8000e+00, -3.9155e-02],
        [-6.2011e+00, -6.1838e+00, -6.3545e+00, -5.8455e-03]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>), tensor([[-2.2739e+00, -3.8705e+00, -3.6193e+00, -1.6318e-01],
        [-5.5034e+00, -6.6690e+00, -6.3936e+00, -7.0395e-03],
        [-4.3344e+00, -5.4777e+00, -5.1947e+00, -2.3100e-02],
        [-3.9255e+00, -4.1626e+00, -4.0142e+00, -5.4835e-02],
        [-2.1499e+00, -2.8094e+00, -2.4746e+00, -3.0236e-01],
        [-4.1262e+00, -5.3696e+00, -4.9604e+00, -2.8205e-02],
        [-5.4290e+00, -7.1246e+00, -6.4154e+00, -6.8521e-03],
        [-3.7400e+00, -5.4090e+00, -4.6699e+00, -3.8328e-02],
        [-3.0900e+00, -4.2599e+00, -3.6657e+00, -8.9062e-02],
        [-5.4157e+00, -6.7717e+00, -6.0398e+00, -8.0061e-03],
        [-3.3343e+00, -4.8949e+00, -4.1281e+00, -6.1064e-02],
        [-2.5403e+00, -3.5422e+00, -3.4449e+00, -1.5047e-01],
        [-4.4739e+00, -5.7508e+00, -5.3657e+00, -1.9444e-02],
        [-3.0299e+00, -4.6850e+00, -4.2435e+00, -7.4628e-02],
        [-5.5117e+00, -6.9875e+00, -6.6437e+00, -6.2842e-03],
        [-6.1396e+00, -7.8039e+00, -7.3328e+00, -3.2229e-03],
        [-3.9612e+00, -6.0208e+00, -5.7236e+00, -2.5047e-02],
        [-5.8367e+00, -7.7263e+00, -7.4868e+00, -3.9277e-03],
        [-3.3369e+00, -5.2232e+00, -5.1191e+00, -4.8055e-02],
        [-3.4966e+00, -4.6813e+00, -5.1347e+00, -4.6523e-02],
        [-5.5230e+00, -6.6531e+00, -6.8344e+00, -6.3801e-03],
        [-3.6327e+00, -4.1258e+00, -4.7220e+00, -5.2866e-02],
        [-5.5552e+00, -6.0874e+00, -6.4265e+00, -7.7872e-03],
        [-4.2296e+00, -4.6834e+00, -5.0066e+00, -3.0975e-02],
        [-3.1003e+00, -3.8299e+00, -3.8059e+00, -9.3197e-02],
        [-1.7578e+00, -2.7922e+00, -2.5402e+00, -3.7478e-01],
        [-4.6586e+00, -6.4636e+00, -5.7388e+00, -1.4361e-02],
        [-4.2532e+00, -6.2016e+00, -5.2256e+00, -2.1860e-02],
        [-4.1094e+00, -5.2969e+00, -4.6115e+00, -3.1865e-02],
        [-3.0729e+00, -3.7073e+00, -3.8966e+00, -9.5565e-02],
        [-6.5068e+00, -6.3593e+00, -6.8413e+00, -4.3020e-03],
        [-3.4255e+00, -4.2829e+00, -4.2862e+00, -6.1974e-02],
        [-6.1486e+00, -6.5742e+00, -6.7832e+00, -4.6759e-03],
        [-3.7808e+00, -4.2954e+00, -4.6820e+00, -4.6774e-02],
        [-7.2676e+00, -6.8132e+00, -7.4498e+00, -2.3813e-03],
        [-5.4835e+00, -4.8498e+00, -5.4806e+00, -1.6283e-02],
        [-2.7563e+00, -3.0482e+00, -3.3287e+00, -1.5877e-01],
        [-1.3768e+00, -2.3901e+00, -2.3648e+00, -5.7619e-01],
        [-4.8044e+00, -5.6619e+00, -5.6617e+00, -1.5262e-02],
        [-3.5094e+00, -4.6504e+00, -4.6520e+00, -5.0258e-02],
        [-3.8418e+00, -4.7439e+00, -4.8006e+00, -3.9141e-02],
        [-6.2012e+00, -6.1839e+00, -6.3548e+00, -5.8446e-03]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>), tensor([[-2.2738e+00, -3.8705e+00, -3.6192e+00, -1.6319e-01],
        [-5.5033e+00, -6.6689e+00, -6.3934e+00, -7.0400e-03],
        [-4.3343e+00, -5.4775e+00, -5.1944e+00, -2.3104e-02],
        [-3.9253e+00, -4.1623e+00, -4.0138e+00, -5.4852e-02],
        [-2.1498e+00, -2.8091e+00, -2.4742e+00, -3.0245e-01],
        [-4.1259e+00, -5.3692e+00, -4.9598e+00, -2.8215e-02],
        [-5.4286e+00, -7.1240e+00, -6.4145e+00, -6.8560e-03],
        [-3.7393e+00, -5.4081e+00, -4.6685e+00, -3.8363e-02],
        [-3.0891e+00, -4.2590e+00, -3.6641e+00, -8.9166e-02],
        [-5.4144e+00, -6.7704e+00, -6.0376e+00, -8.0185e-03],
        [-3.3332e+00, -4.8937e+00, -4.1258e+00, -6.1154e-02],
        [-2.5384e+00, -3.5401e+00, -3.4419e+00, -1.5084e-01],
        [-4.4727e+00, -5.7494e+00, -5.3637e+00, -1.9473e-02],
        [-3.0315e+00, -4.6858e+00, -4.2444e+00, -7.4523e-02],
        [-5.5130e+00, -6.9900e+00, -6.6458e+00, -6.2742e-03],
        [-6.1428e+00, -7.8084e+00, -7.3367e+00, -3.2115e-03],
        [-3.9657e+00, -6.0271e+00, -5.7291e+00, -2.4926e-02],
        [-5.8426e+00, -7.7345e+00, -7.4942e+00, -3.9024e-03],
        [-3.3413e+00, -5.2320e+00, -5.1263e+00, -4.7797e-02],
        [-3.5042e+00, -4.6933e+00, -5.1454e+00, -4.6099e-02],
        [-5.5356e+00, -6.6695e+00, -6.8500e+00, -6.2919e-03],
        [-3.6535e+00, -4.1484e+00, -4.7448e+00, -5.1700e-02],
        [-5.5906e+00, -6.1192e+00, -6.4595e+00, -7.5264e-03],
        [-4.2160e+00, -4.6756e+00, -5.0211e+00, -3.1154e-02],
        [-3.0911e+00, -3.8276e+00, -3.8107e+00, -9.3592e-02],
        [-1.7496e+00, -2.7880e+00, -2.5381e+00, -3.7746e-01],
        [-4.6396e+00, -6.4498e+00, -5.7220e+00, -1.4622e-02],
        [-4.2198e+00, -6.1787e+00, -5.1963e+00, -2.2564e-02],
        [-4.1299e+00, -5.3152e+00, -4.6197e+00, -3.1342e-02],
        [-3.0771e+00, -3.7083e+00, -3.8952e+00, -9.5357e-02],
        [-6.5096e+00, -6.3574e+00, -6.8382e+00, -4.3044e-03],
        [-3.4256e+00, -4.2794e+00, -4.2824e+00, -6.2080e-02],
        [-6.1483e+00, -6.5715e+00, -6.7807e+00, -4.6830e-03],
        [-3.7802e+00, -4.2932e+00, -4.6803e+00, -4.6835e-02],
        [-7.2674e+00, -6.8119e+00, -7.4491e+00, -2.3832e-03],
        [-5.4834e+00, -4.8488e+00, -5.4803e+00, -1.6293e-02],
        [-2.7563e+00, -3.0476e+00, -3.3287e+00, -1.5881e-01],
        [-1.3768e+00, -2.3898e+00, -2.3649e+00, -5.7624e-01],
        [-4.8044e+00, -5.6617e+00, -5.6618e+00, -1.5263e-02],
        [-3.5094e+00, -4.6503e+00, -4.6521e+00, -5.0257e-02],
        [-3.8418e+00, -4.7438e+00, -4.8007e+00, -3.9140e-02],
        [-6.2013e+00, -6.1839e+00, -6.3549e+00, -5.8446e-03]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>), tensor([[-2.2698e+00, -3.8679e+00, -3.6137e+00, -1.6392e-01],
        [-5.5027e+00, -6.6679e+00, -6.3906e+00, -7.0491e-03],
        [-4.3323e+00, -5.4745e+00, -5.1912e+00, -2.3162e-02],
        [-3.9236e+00, -4.1596e+00, -4.0110e+00, -5.4983e-02],
        [-2.1486e+00, -2.8072e+00, -2.4725e+00, -3.0299e-01],
        [-4.1256e+00, -5.3688e+00, -4.9593e+00, -2.8227e-02],
        [-5.4290e+00, -7.1246e+00, -6.4151e+00, -6.8526e-03],
        [-3.7403e+00, -5.4094e+00, -4.6699e+00, -3.8320e-02],
        [-3.0903e+00, -4.2606e+00, -3.6661e+00, -8.9027e-02],
        [-5.4162e+00, -6.7727e+00, -6.0404e+00, -8.0013e-03],
        [-3.3349e+00, -4.8962e+00, -4.1290e+00, -6.1018e-02],
        [-2.5411e+00, -3.5441e+00, -3.4462e+00, -1.5029e-01],
        [-4.4751e+00, -5.7533e+00, -5.3676e+00, -1.9413e-02],
        [-3.0315e+00, -4.6883e+00, -4.2461e+00, -7.4469e-02],
        [-5.5143e+00, -6.9916e+00, -6.6474e+00, -6.2656e-03],
        [-6.1434e+00, -7.8093e+00, -7.3378e+00, -3.2091e-03],
        [-3.9661e+00, -6.0278e+00, -5.7300e+00, -2.4912e-02],
        [-5.8429e+00, -7.7352e+00, -7.4949e+00, -3.9010e-03],
        [-3.3416e+00, -5.2328e+00, -5.1272e+00, -4.7775e-02],
        [-3.5046e+00, -4.6944e+00, -5.1465e+00, -4.6071e-02],
        [-5.5361e+00, -6.6709e+00, -6.8516e+00, -6.2866e-03],
        [-3.6542e+00, -4.1504e+00, -4.7470e+00, -5.1628e-02],
        [-5.5920e+00, -6.1225e+00, -6.4630e+00, -7.5088e-03],
        [-4.2186e+00, -4.6806e+00, -5.0266e+00, -3.1029e-02],
        [-3.0961e+00, -3.8353e+00, -3.8193e+00, -9.2956e-02],
        [-1.7569e+00, -2.7974e+00, -2.5493e+00, -3.7350e-01],
        [-4.6614e+00, -6.4705e+00, -5.7465e+00, -1.4298e-02],
        [-4.2555e+00, -6.2069e+00, -5.2306e+00, -2.1788e-02],
        [-4.1109e+00, -5.3005e+00, -4.6144e+00, -3.1790e-02],
        [-3.0734e+00, -3.7095e+00, -3.8977e+00, -9.5459e-02],
        [-6.5067e+00, -6.3603e+00, -6.8415e+00, -4.3001e-03],
        [-3.4252e+00, -4.2831e+00, -4.2857e+00, -6.1988e-02],
        [-6.1480e+00, -6.5737e+00, -6.7820e+00, -4.6792e-03],
        [-3.7802e+00, -4.2941e+00, -4.6803e+00, -4.6823e-02],
        [-7.2665e+00, -6.8108e+00, -7.4470e+00, -2.3866e-03],
        [-5.4814e+00, -4.8457e+00, -5.4760e+00, -1.6344e-02],
        [-2.7528e+00, -3.0420e+00, -3.3220e+00, -1.5966e-01],
        [-1.3725e+00, -2.3828e+00, -2.3567e+00, -5.8070e-01],
        [-4.7865e+00, -5.6437e+00, -5.6410e+00, -1.5551e-02],
        [-3.4833e+00, -4.6282e+00, -4.6257e+00, -5.1583e-02],
        [-3.8624e+00, -4.7481e+00, -4.7945e+00, -3.8700e-02],
        [-6.2152e+00, -6.1947e+00, -6.3612e+00, -5.7831e-03]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)], 2: [tensor([[-2.2840e+00, -3.8484e+00, -3.6057e+00, -1.6294e-01],
        [-5.5063e+00, -6.6544e+00, -6.3854e+00, -7.0605e-03],
        [-4.3323e+00, -5.4588e+00, -5.1802e+00, -2.3293e-02],
        [-3.8933e+00, -4.1191e+00, -3.9763e+00, -5.6986e-02],
        [-2.1172e+00, -2.7970e+00, -2.4549e+00, -3.1094e-01],
        [-4.0953e+00, -5.3866e+00, -4.9594e+00, -2.8653e-02],
        [-5.4466e+00, -7.1707e+00, -6.4317e+00, -6.7120e-03],
        [-3.7272e+00, -5.4178e+00, -4.6555e+00, -3.8748e-02],
        [-3.0789e+00, -4.2697e+00, -3.6548e+00, -8.9775e-02],
        [-5.4145e+00, -6.7816e+00, -6.0359e+00, -8.0094e-03],
        [-3.3396e+00, -4.9079e+00, -4.1301e+00, -6.0729e-02],
        [-2.5342e+00, -3.5377e+00, -3.4331e+00, -1.5163e-01],
        [-4.4783e+00, -5.7504e+00, -5.3646e+00, -1.9401e-02],
        [-3.0268e+00, -4.6721e+00, -4.2356e+00, -7.5039e-02],
        [-5.5151e+00, -6.9840e+00, -6.6462e+00, -6.2709e-03],
        [-6.1469e+00, -7.8055e+00, -7.3413e+00, -3.2010e-03],
        [-3.9682e+00, -6.0235e+00, -5.7350e+00, -2.4866e-02],
        [-5.8541e+00, -7.7387e+00, -7.5057e+00, -3.8614e-03],
        [-3.3494e+00, -5.2323e+00, -5.1326e+00, -4.7455e-02],
        [-3.5108e+00, -4.6940e+00, -5.1472e+00, -4.5876e-02],
        [-5.5488e+00, -6.6758e+00, -6.8576e+00, -6.2237e-03],
        [-3.6538e+00, -4.1398e+00, -4.7356e+00, -5.1921e-02],
        [-5.5945e+00, -6.1155e+00, -6.4548e+00, -7.5278e-03],
        [-4.2103e+00, -4.6650e+00, -5.0094e+00, -3.1425e-02],
        [-3.0829e+00, -3.8171e+00, -3.7992e+00, -9.4537e-02],
        [-1.7490e+00, -2.7880e+00, -2.5349e+00, -3.7798e-01],
        [-4.6560e+00, -6.4666e+00, -5.7388e+00, -1.4380e-02],
        [-4.2517e+00, -6.2051e+00, -5.2261e+00, -2.1871e-02],
        [-4.1110e+00, -5.3036e+00, -4.6145e+00, -3.1772e-02],
        [-3.0705e+00, -3.7029e+00, -3.8921e+00, -9.5908e-02],
        [-6.5131e+00, -6.3592e+00, -6.8421e+00, -4.2915e-03],
        [-3.4140e+00, -4.2658e+00, -4.2678e+00, -6.2899e-02],
        [-6.1446e+00, -6.5667e+00, -6.7711e+00, -4.7088e-03],
        [-3.7755e+00, -4.2862e+00, -4.6677e+00, -4.7172e-02],
        [-7.2752e+00, -6.8211e+00, -7.4478e+00, -2.3685e-03],
        [-5.4955e+00, -4.8538e+00, -5.4749e+00, -1.6226e-02],
        [-2.7903e+00, -3.0694e+00, -3.3317e+00, -1.5500e-01],
        [-1.3920e+00, -2.4072e+00, -2.3433e+00, -5.7032e-01],
        [-4.8086e+00, -5.6783e+00, -5.6531e+00, -1.5200e-02],
        [-3.4893e+00, -4.6363e+00, -4.6279e+00, -5.1284e-02],
        [-3.7996e+00, -4.7117e+00, -4.7632e+00, -4.0727e-02],
        [-6.2137e+00, -6.2159e+00, -6.3740e+00, -5.7211e-03]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>), tensor([[-2.2852e+00, -3.8520e+00, -3.6093e+00, -1.6259e-01],
        [-5.5099e+00, -6.6610e+00, -6.3922e+00, -7.0257e-03],
        [-4.3395e+00, -5.4691e+00, -5.1912e+00, -2.3088e-02],
        [-3.9066e+00, -4.1344e+00, -3.9931e+00, -5.6109e-02],
        [-2.1350e+00, -2.8137e+00, -2.4750e+00, -3.0434e-01],
        [-4.1364e+00, -5.4179e+00, -4.9970e+00, -2.7552e-02],
        [-5.4297e+00, -7.1525e+00, -6.4314e+00, -6.8007e-03],
        [-3.7224e+00, -5.4065e+00, -4.6578e+00, -3.8898e-02],
        [-3.0799e+00, -4.2676e+00, -3.6580e+00, -8.9665e-02],
        [-5.4166e+00, -6.7834e+00, -6.0393e+00, -7.9899e-03],
        [-3.3411e+00, -4.9098e+00, -4.1322e+00, -6.0622e-02],
        [-2.5343e+00, -3.5387e+00, -3.4333e+00, -1.5157e-01],
        [-4.4773e+00, -5.7498e+00, -5.3634e+00, -1.9419e-02],
        [-3.0252e+00, -4.6701e+00, -4.2334e+00, -7.5177e-02],
        [-5.5124e+00, -6.9805e+00, -6.6426e+00, -6.2900e-03],
        [-6.1429e+00, -7.8004e+00, -7.3361e+00, -3.2148e-03],
        [-3.9631e+00, -6.0167e+00, -5.7284e+00, -2.5003e-02],
        [-5.8476e+00, -7.7299e+00, -7.4973e+00, -3.8886e-03],
        [-3.3446e+00, -5.2230e+00, -5.1245e+00, -4.7734e-02],
        [-3.5028e+00, -4.6812e+00, -5.1355e+00, -4.6322e-02],
        [-5.5356e+00, -6.6581e+00, -6.8403e+00, -6.3171e-03],
        [-3.6320e+00, -4.1151e+00, -4.7105e+00, -5.3175e-02],
        [-5.5574e+00, -6.0802e+00, -6.4181e+00, -7.8087e-03],
        [-4.2211e+00, -4.6677e+00, -4.9893e+00, -3.1374e-02],
        [-3.0870e+00, -3.8117e+00, -3.7858e+00, -9.4795e-02],
        [-1.7498e+00, -2.7828e+00, -2.5259e+00, -3.7930e-01],
        [-4.6533e+00, -6.4599e+00, -5.7313e+00, -1.4442e-02],
        [-4.2496e+00, -6.2003e+00, -5.2216e+00, -2.1937e-02],
        [-4.1098e+00, -5.3008e+00, -4.6123e+00, -3.1830e-02],
        [-3.0706e+00, -3.7019e+00, -3.8920e+00, -9.5931e-02],
        [-6.5140e+00, -6.3599e+00, -6.8434e+00, -4.2877e-03],
        [-3.4157e+00, -4.2678e+00, -4.2701e+00, -6.2777e-02],
        [-6.1472e+00, -6.5699e+00, -6.7746e+00, -4.6949e-03],
        [-3.7780e+00, -4.2899e+00, -4.6712e+00, -4.7024e-02],
        [-7.2767e+00, -6.8240e+00, -7.4509e+00, -2.3627e-03],
        [-5.4960e+00, -4.8573e+00, -5.4777e+00, -1.6184e-02],
        [-2.7918e+00, -3.0716e+00, -3.3341e+00, -1.5467e-01],
        [-1.3926e+00, -2.4082e+00, -2.3445e+00, -5.6967e-01],
        [-4.8090e+00, -5.6789e+00, -5.6540e+00, -1.5192e-02],
        [-3.4895e+00, -4.6367e+00, -4.6286e+00, -5.1265e-02],
        [-3.7997e+00, -4.7119e+00, -4.7637e+00, -4.0716e-02],
        [-6.2138e+00, -6.2160e+00, -6.3742e+00, -5.7201e-03]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>), tensor([[-2.2851e+00, -3.8519e+00, -3.6092e+00, -1.6260e-01],
        [-5.5099e+00, -6.6609e+00, -6.3920e+00, -7.0262e-03],
        [-4.3394e+00, -5.4689e+00, -5.1909e+00, -2.3092e-02],
        [-3.9064e+00, -4.1341e+00, -3.9927e+00, -5.6125e-02],
        [-2.1349e+00, -2.8135e+00, -2.4746e+00, -3.0443e-01],
        [-4.1361e+00, -5.4175e+00, -4.9964e+00, -2.7561e-02],
        [-5.4293e+00, -7.1519e+00, -6.4305e+00, -6.8040e-03],
        [-3.7217e+00, -5.4057e+00, -4.6565e+00, -3.8933e-02],
        [-3.0790e+00, -4.2666e+00, -3.6564e+00, -8.9769e-02],
        [-5.4152e+00, -6.7821e+00, -6.0372e+00, -8.0023e-03],
        [-3.3399e+00, -4.9086e+00, -4.1299e+00, -6.0714e-02],
        [-2.5326e+00, -3.5367e+00, -3.4305e+00, -1.5191e-01],
        [-4.4763e+00, -5.7486e+00, -5.3617e+00, -1.9444e-02],
        [-3.0271e+00, -4.6713e+00, -4.2347e+00, -7.5046e-02],
        [-5.5138e+00, -6.9832e+00, -6.6449e+00, -6.2780e-03],
        [-6.1461e+00, -7.8050e+00, -7.3403e+00, -3.2034e-03],
        [-3.9676e+00, -6.0231e+00, -5.7341e+00, -2.4882e-02],
        [-5.8536e+00, -7.7381e+00, -7.5048e+00, -3.8633e-03],
        [-3.3490e+00, -5.2315e+00, -5.1316e+00, -4.7484e-02],
        [-3.5103e+00, -4.6930e+00, -5.1460e+00, -4.5908e-02],
        [-5.5482e+00, -6.6744e+00, -6.8559e+00, -6.2299e-03],
        [-3.6530e+00, -4.1377e+00, -4.7334e+00, -5.1996e-02],
        [-5.5931e+00, -6.1123e+00, -6.4513e+00, -7.5459e-03],
        [-4.2077e+00, -4.6601e+00, -5.0040e+00, -3.1551e-02],
        [-3.0779e+00, -3.8096e+00, -3.7909e+00, -9.5177e-02],
        [-1.7417e+00, -2.7789e+00, -2.5240e+00, -3.8195e-01],
        [-4.6343e+00, -6.4463e+00, -5.7147e+00, -1.4704e-02],
        [-4.2162e+00, -6.1773e+00, -5.1924e+00, -2.2644e-02],
        [-4.1301e+00, -5.3187e+00, -4.6204e+00, -3.1315e-02],
        [-3.0745e+00, -3.7026e+00, -3.8905e+00, -9.5744e-02],
        [-6.5167e+00, -6.3579e+00, -6.8401e+00, -4.2911e-03],
        [-3.4155e+00, -4.2640e+00, -4.2661e+00, -6.2897e-02],
        [-6.1469e+00, -6.5671e+00, -6.7720e+00, -4.7026e-03],
        [-3.7774e+00, -4.2876e+00, -4.6695e+00, -4.7090e-02],
        [-7.2765e+00, -6.8227e+00, -7.4502e+00, -2.3646e-03],
        [-5.4959e+00, -4.8563e+00, -5.4773e+00, -1.6194e-02],
        [-2.7918e+00, -3.0709e+00, -3.3341e+00, -1.5471e-01],
        [-1.3926e+00, -2.4079e+00, -2.3446e+00, -5.6973e-01],
        [-4.8090e+00, -5.6787e+00, -5.6541e+00, -1.5192e-02],
        [-3.4896e+00, -4.6366e+00, -4.6287e+00, -5.1263e-02],
        [-3.7998e+00, -4.7119e+00, -4.7638e+00, -4.0715e-02],
        [-6.2138e+00, -6.2159e+00, -6.3743e+00, -5.7201e-03]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>), tensor([[-2.2813e+00, -3.8495e+00, -3.6041e+00, -1.6329e-01],
        [-5.5093e+00, -6.6600e+00, -6.3894e+00, -7.0338e-03],
        [-4.3378e+00, -5.4664e+00, -5.1882e+00, -2.3139e-02],
        [-3.9051e+00, -4.1319e+00, -3.9905e+00, -5.6231e-02],
        [-2.1340e+00, -2.8120e+00, -2.4733e+00, -3.0483e-01],
        [-4.1360e+00, -5.4173e+00, -4.9962e+00, -2.7565e-02],
        [-5.4298e+00, -7.1527e+00, -6.4313e+00, -6.8002e-03],
        [-3.7226e+00, -5.4070e+00, -4.6579e+00, -3.8888e-02],
        [-3.0802e+00, -4.2683e+00, -3.6584e+00, -8.9627e-02],
        [-5.4171e+00, -6.7844e+00, -6.0400e+00, -7.9846e-03],
        [-3.3417e+00, -4.9111e+00, -4.1331e+00, -6.0575e-02],
        [-2.5351e+00, -3.5405e+00, -3.4346e+00, -1.5140e-01],
        [-4.4785e+00, -5.7522e+00, -5.3654e+00, -1.9388e-02],
        [-3.0269e+00, -4.6733e+00, -4.2360e+00, -7.5020e-02],
        [-5.5149e+00, -6.9846e+00, -6.6462e+00, -6.2709e-03],
        [-6.1467e+00, -7.8058e+00, -7.3412e+00, -3.2015e-03],
        [-3.9680e+00, -6.0237e+00, -5.7349e+00, -2.4870e-02],
        [-5.8538e+00, -7.7387e+00, -7.5055e+00, -3.8619e-03],
        [-3.3493e+00, -5.2323e+00, -5.1324e+00, -4.7462e-02],
        [-3.5106e+00, -4.6940e+00, -5.1471e+00, -4.5881e-02],
        [-5.5487e+00, -6.6758e+00, -6.8575e+00, -6.2246e-03],
        [-3.6537e+00, -4.1398e+00, -4.7356e+00, -5.1924e-02],
        [-5.5944e+00, -6.1155e+00, -6.4548e+00, -7.5278e-03],
        [-4.2102e+00, -4.6651e+00, -5.0095e+00, -3.1424e-02],
        [-3.0828e+00, -3.8173e+00, -3.7994e+00, -9.4532e-02],
        [-1.7490e+00, -2.7882e+00, -2.5352e+00, -3.7794e-01],
        [-4.6561e+00, -6.4669e+00, -5.7392e+00, -1.4378e-02],
        [-4.2518e+00, -6.2056e+00, -5.2267e+00, -2.1865e-02],
        [-4.1113e+00, -5.3044e+00, -4.6152e+00, -3.1755e-02],
        [-3.0710e+00, -3.7041e+00, -3.8932e+00, -9.5824e-02],
        [-6.5139e+00, -6.3610e+00, -6.8436e+00, -4.2858e-03],
        [-3.4153e+00, -4.2680e+00, -4.2696e+00, -6.2793e-02],
        [-6.1466e+00, -6.5694e+00, -6.7734e+00, -4.6983e-03],
        [-3.7773e+00, -4.2887e+00, -4.6695e+00, -4.7075e-02],
        [-7.2756e+00, -6.8217e+00, -7.4481e+00, -2.3675e-03],
        [-5.4940e+00, -4.8532e+00, -5.4730e+00, -1.6245e-02],
        [-2.7882e+00, -3.0653e+00, -3.3272e+00, -1.5556e-01],
        [-1.3882e+00, -2.4009e+00, -2.3362e+00, -5.7422e-01],
        [-4.7909e+00, -5.6606e+00, -5.6331e+00, -1.5482e-02],
        [-3.4631e+00, -4.6143e+00, -4.6018e+00, -5.2638e-02],
        [-3.8203e+00, -4.7159e+00, -4.7574e+00, -4.0260e-02],
        [-6.2275e+00, -6.2262e+00, -6.3802e+00, -5.6624e-03]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)]}
vocab size:  29396
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(176.5129) lr: 0.0001 time: 28.0
pred_count_train 737

Test...
[[tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(0, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(0, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(0, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(0, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(0, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0')], [tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0')], [tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(0, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(0, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(0, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(0, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(0, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0')]]
[[tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(0, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(0, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(0, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(0, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0')], [tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0')], [tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0')]]
[[tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(0, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(0, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0')], [tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0')], [tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0')]]
[[tensor(3, device='cuda:0'), tensor(0, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(0, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(0, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(0, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0')], [tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0')], [tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0')]]loss: tensor(746.2275) lr: 0.0002 time: 1913.87
pred_count_train 41644

Test...
rue_size1_sub0_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(176.5129) lr: 0.0001 time: 27.67
pred_count_train 737

Test...
[[3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]]
[[3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]]
[[3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]]
[[3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]]
[[0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]]
[[3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]]

[32m iter_0[0m
ga 	p: 36.94 	r: 27.73 	f1: 31.68 	 99 	 268 	 357
wo 	p: 26.09 	r: 8.45 	f1: 12.77 	 18 	 69 	 213
ni 	p: 3.36 	r: 19.23 	f1: 5.71 	 15 	 447 	 78

[32m iter_1[0m
ga 	p: 36.94 	r: 27.73 	f1: 31.68 	 99 	 268 	 357
wo 	p: 26.09 	r: 8.45 	f1: 12.77 	 18 	 69 	 213
ni 	p: 3.36 	r: 19.23 	f1: 5.71 	 15 	 447 	 78

[32m iter_2[0m
ga 	p: 36.94 	r: 27.73 	f1: 31.68 	 99 	 268 	 357
wo 	p: 26.09 	r: 8.45 	f1: 12.77 	 18 	 69 	 213
ni 	p: 3.36 	r: 19.23 	f1: 5.71 	 15 	 447 	 78
dev_num_of_high:  0
best_thres [0.16, 0.2, 0.0]
f [0.1844, 0.1844, 0.1844]
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse 	current best epoch 1 	 [0.16, 0.2, 0.0] 	 lr: 0.0001 	 f: 18.43575418994413
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse[0m [33m epoch 2 [0m
Train...

[32m iter_0[0m
ga 	p: 82.83 	r: 74.12 	f1: 78.23 	 13716 	 16560 	 18504
wo 	p: 92.62 	r: 85.81 	f1: 89.09 	 8722 	 9417 	 10164
ni 	p: 83.94 	r: 74.77 	f1: 79.09 	 3414 	 4067 	 4566

[32m iter_1[0m
ga 	p: 82.83 	r: 74.12 	f1: 78.23 	 13716 	 16560 	 18504
wo 	p: 92.62 	r: 85.81 	f1: 89.09 	 8722 	 9417 	 10164
ni 	p: 83.94 	r: 74.77 	f1: 79.09 	 3414 	 4067 	 4566

[32m iter_2[0m
ga 	p: 82.83 	r: 74.12 	f1: 78.23 	 13716 	 16560 	 18504
wo 	p: 92.62 	r: 85.81 	f1: 89.09 	 8722 	 9417 	 10164
ni 	p: 83.94 	r: 74.77 	f1: 79.09 	 3414 	 4067 	 4566
dev_num_of_high:  0
best_thres [0.65, 0.79, 0.09]
f [0.8171, 0.8171, 0.8171]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 3 	 [0.52, 0.44, 0.14] 	 lr: 0.0002 	 f: 82.6384410927449
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 7 [0m
Train...
vocab size:  29396
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(176.5129) lr: 0.0001 time: 32.04
pred_count_train 737

Test...

[32m iter_0[0m
ga 	p: 36.94 	r: 27.73 	f1: 31.68 	 99 	 268 	 357
wo 	p: 26.09 	r: 8.45 	f1: 12.77 	 18 	 69 	 213
ni 	p: 3.36 	r: 19.23 	f1: 5.71 	 15 	 447 	 78

[32m iter_1[0m
ga 	p: 36.94 	r: 27.73 	f1: 31.68 	 99 	 268 	 357
wo 	p: 26.09 	r: 8.45 	f1: 12.77 	 18 	 69 	 213
ni 	p: 3.36 	r: 19.23 	f1: 5.71 	 15 	 447 	 78

[32m iter_2[0m
ga 	p: 36.94 	r: 27.73 	f1: 31.68 	 99 	 268 	 357
wo 	p: 26.09 	r: 8.45 	f1: 12.77 	 18 	 69 	 213
ni 	p: 3.36 	r: 19.23 	f1: 5.71 	 15 	 447 	 78

[32m iter_0[0m
ga 	p: 80.78 	r: 75.83 	f1: 78.23 	 14032 	 17371 	 18504
wo 	p: 93.94 	r: 85.23 	f1: 89.37 	 8663 	 9222 	 10164
ni 	p: 81.03 	r: 78.3 	f1: 79.64 	 3575 	 4412 	 4566

[32m iter_1[0m
ga 	p: 80.78 	r: 75.83 	f1: 78.23 	 14032 	 17371 	 18504
wo 	p: 93.94 	r: 85.23 	f1: 89.37 	 8663 	 9222 	 10164
ni 	p: 81.03 	r: 78.3 	f1: 79.64 	 3575 	 4412 	 4566

[32m iter_2[0m
ga 	p: 80.78 	r: 75.83 	f1: 78.23 	 14032 	 17371 	 18504
wo 	p: 93.94 	r: 85.23 	f1: 89.37 	 8663 	 9222 	 10164
ni 	p: 81.03 	r: 78.3 	f1: 79.64 	 3575 	 4412 	 4566
dev_num_of_high:  0
best_thres [0.56, 0.64, 0.06]
f [0.8179, 0.8179, 0.8179]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 3 	 [0.41, 0.3, 0.13] 	 lr: 0.0002 	 f: 82.36806641986401
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 7 [0m
Train...
vocab size:  29396
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(176.5129) lr: 0.0001 time: 31.34
pred_count_train 737

Test...

[32m iter_0[0m
ga 	p: 36.94 	r: 27.73 	f1: 31.68 	 99 	 268 	 357
wo 	p: 26.09 	r: 8.45 	f1: 12.77 	 18 	 69 	 213
ni 	p: 3.36 	r: 19.23 	f1: 5.71 	 15 	 447 	 78

[32m iter_1[0m
ga 	p: 36.94 	r: 27.73 	f1: 31.68 	 99 	 268 	 357
wo 	p: 26.09 	r: 8.45 	f1: 12.77 	 18 	 69 	 213
ni 	p: 3.36 	r: 19.23 	f1: 5.71 	 15 	 447 	 78

[32m iter_2[0m
ga 	p: 36.94 	r: 27.73 	f1: 31.68 	 99 	 268 	 357
wo 	p: 26.09 	r: 8.45 	f1: 12.77 	 18 	 69 	 213
ni 	p: 3.36 	r: 19.23 	f1: 5.71 	 15 	 447 	 78
vocab size:  29396
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(176.5129) lr: 0.0001 time: 31.58
pred_count_train 737

Test...

[32m iter_0[0m
ga 	p: 36.94 	r: 27.73 	f1: 31.68 	 99 	 268 	 357
wo 	p: 26.09 	r: 8.45 	f1: 12.77 	 18 	 69 	 213
ni 	p: 3.36 	r: 19.23 	f1: 5.71 	 15 	 447 	 78

[32m iter_1[0m
ga 	p: 36.94 	r: 27.73 	f1: 31.68 	 99 	 268 	 357
wo 	p: 26.09 	r: 8.45 	f1: 12.77 	 18 	 69 	 213
ni 	p: 3.36 	r: 19.23 	f1: 5.71 	 15 	 447 	 78

[32m iter_2[0m
ga 	p: 36.94 	r: 27.73 	f1: 31.68 	 99 	 268 	 357
wo 	p: 26.09 	r: 8.45 	f1: 12.77 	 18 	 69 	 213
ni 	p: 3.36 	r: 19.23 	f1: 5.71 	 15 	 447 	 78
vocab size:  29396
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(176.5129) lr: 0.0001 time: 27.48
pred_count_train 737

Test...

[32m iter_0[0m
vocab size:  29396
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(176.5129) lr: 0.0001 time: 26.59
pred_count_train 737

Test...

vocab size:  29396
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(176.5129) lr: 0.0001 time: 27.01
pred_count_train 737

Test...

[32m iter_0[0m
ga 	p: 36.94 	r: 27.73 	f1: 31.68 	 99 	 268 	 357
wo 	p: 26.09 	r: 8.45 	f1: 12.77 	 18 	 69 	 213
ni 	p: 3.36 	r: 19.23 	f1: 5.71 	 15 	 447 	 78

[32m iter_1[0m
ga 	p: 36.94 	r: 27.73 	f1: 31.68 	 99 	 268 	 357
wo 	p: 26.09 	r: 8.45 	f1: 12.77 	 18 	 69 	 213
ni 	p: 3.36 	r: 19.23 	f1: 5.71 	 15 	 447 	 78

[32m iter_2[0m
ga 	p: 36.94 	r: 27.73 	f1: 31.68 	 99 	 268 	 357
wo 	p: 26.09 	r: 8.45 	f1: 12.77 	 18 	 69 	 213
ni 	p: 3.36 	r: 19.23 	f1: 5.71 	 15 	 447 	 78
best_thres [[0.16, 0.2, 0.0], [0.16, 0.2, 0.0], [0.16, 0.2, 0.0]]
f [0.1844, 0.1844, 0.1844]
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse 	current best epoch 1 	 [0.16, 0.2, 0.0] 	 lr: 0.0001 	 f: 18.43575418994413
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse[0m [33m epoch 2 [0m
Train...
loss: tensor(584.7490) lr: 0.0002 time: 1927.38
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.78 	r: 75.58 	f1: 78.56 	 13985 	 17100 	 18504
wo 	p: 92.39 	r: 86.49 	f1: 89.34 	 8791 	 9515 	 10164
ni 	p: 83.48 	r: 71.16 	f1: 76.83 	 3249 	 3892 	 4566

[32m iter_1[0m
ga 	p: 81.78 	r: 75.58 	f1: 78.56 	 13985 	 17100 	 18504
wo 	p: 92.39 	r: 86.49 	f1: 89.34 	 8791 	 9515 	 10164
ni 	p: 83.48 	r: 71.16 	f1: 76.83 	 3249 	 3892 	 4566

[32m iter_2[0m
ga 	p: 81.78 	r: 75.58 	f1: 78.56 	 13985 	 17100 	 18504
wo 	p: 92.39 	r: 86.49 	f1: 89.34 	 8791 	 9515 	 10164
ni 	p: 83.48 	r: 71.16 	f1: 76.83 	 3249 	 3892 	 4566
dev_num_of_high:  0
best_thres [0.6, 0.55, 0.07]
f [0.8166, 0.8166, 0.8166]
load model: epoch3
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 3 	 [0.52, 0.44, 0.14] 	 lr: 0.0002 	 f: 82.6384410927449
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 8 [0m
Train...

[32m iter_0[0m
ga 	p: 81.88 	r: 74.7 	f1: 78.13 	 13823 	 16881 	 18504
wo 	p: 93.04 	r: 85.76 	f1: 89.25 	 8717 	 9369 	 10164
ni 	p: 82.12 	r: 74.16 	f1: 77.94 	 3386 	 4123 	 4566

[32m iter_1[0m
ga 	p: 81.88 	r: 74.7 	f1: 78.13 	 13823 	 16881 	 18504
wo 	p: 93.04 	r: 85.76 	f1: 89.25 	 8717 	 9369 	 10164
ni 	p: 82.12 	r: 74.16 	f1: 77.94 	 3386 	 4123 	 4566

[32m iter_2[0m
ga 	p: 81.88 	r: 74.7 	f1: 78.13 	 13823 	 16881 	 18504
wo 	p: 93.04 	r: 85.76 	f1: 89.25 	 8717 	 9369 	 10164
ni 	p: 82.12 	r: 74.16 	f1: 77.94 	 3386 	 4123 	 4566
dev_num_of_high:  0
best_thres [0.58, 0.63, 0.13]
f [0.8152, 0.8152, 0.8152]
load model: epoch3
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 3 	 [0.41, 0.3, 0.13] 	 lr: 0.0002 	 f: 82.36806641986401
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 8 [0m
Train...
loss: tensor(1085.1367) lr: 0.0001 time: 1567.67
pred_count_train 41644

Test...
vocab size:  29396
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(176.5129) lr: 0.0001 time: 32.7
pred_count_train 737

Test...
> [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(54)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     53 [0;31m[0;34m[0m[0m
[0m[0;32m---> 54 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     55 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> [defaultdict(<class 'dict'>, {0: {0.1: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 1: {0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.71: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.72: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.73: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.74: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.75: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.76: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.77: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.78: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.79: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.8: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.81: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.82: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.83: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.84: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.85: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 2: {0.0: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.01: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.02: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.03: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.04: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.05: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.06: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.07: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.08: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.09: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}}), defaultdict(<class 'dict'>, {0: {0.1: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 1: {0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.71: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.72: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.73: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.74: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.75: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.76: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.77: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.78: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.79: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.8: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.81: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.82: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.83: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.84: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.85: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 2: {0.0: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.01: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.02: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.03: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.04: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.05: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.06: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.07: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.08: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.09: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}}), defaultdict(<class 'dict'>, {0: {0.1: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 1: {0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.71: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.72: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.73: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.74: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.75: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.76: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.77: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.78: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.79: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.8: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.81: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.82: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.83: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.84: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.85: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 2: {0.0: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.01: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.02: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.03: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.04: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.05: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.06: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.07: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.08: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.09: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}})]
ipdb> defaultdict(<class 'dict'>, {0: {0.1: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 1: {0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.71: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.72: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.73: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.74: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.75: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.76: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.77: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.78: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.79: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.8: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.81: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.82: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.83: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.84: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.85: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 2: {0.0: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.01: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.02: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.03: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.04: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.05: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.06: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.07: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.08: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.09: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}})
ipdb> 
[32m iter_0[0m
ga 	p: 83.22 	r: 75.92 	f1: 79.4 	 14049 	 16882 	 18504
wo 	p: 93.55 	r: 86.84 	f1: 90.07 	 8826 	 9435 	 10164
ni 	p: 87.43 	r: 80.57 	f1: 83.86 	 3679 	 4208 	 4566

[32m iter_1[0m
ga 	p: 83.22 	r: 75.92 	f1: 79.4 	 14049 	 16882 	 18504
wo 	p: 93.55 	r: 86.84 	f1: 90.07 	 8826 	 9435 	 10164
ni 	p: 87.43 	r: 80.57 	f1: 83.86 	 3679 	 4208 	 4566

[32m iter_2[0m
ga 	p: 83.22 	r: 75.92 	f1: 79.4 	 14049 	 16882 	 18504
wo 	p: 93.55 	r: 86.84 	f1: 90.07 	 8826 	 9435 	 10164
ni 	p: 87.43 	r: 80.57 	f1: 83.86 	 3679 	 4208 	 4566
dev_num_of_high:  0
best_thres [0.47, 0.59, 0.17]
f [0.8329, 0.8329, 0.8329]
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 8 	 [0.47, 0.59, 0.17] 	 lr: 0.0001 	 f: 83.29490738562399
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 9 [0m
Train...
defaultdict(<class 'dict'>, {0: {0.1: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 1: {0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.71: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.72: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.73: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.74: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.75: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.76: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.77: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.78: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.79: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.8: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.81: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.82: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.83: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.84: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.85: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 2: {0.0: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.01: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.02: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.03: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.04: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.05: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.06: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.07: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.08: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.09: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}})
ipdb> <class 'list'>
ipdb> 3
ipdb> Exiting Debugger.
vocab size:  29396
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(176.5129) lr: 0.0001 time: 37.42
pred_count_train 737

Test...
> [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(57)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     56 [0;31m                    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 57 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0miter_result[0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m,[0m [0mit[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     58 [0;31m                    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> [defaultdict(<class 'dict'>, {0: {0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 1: {0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.71: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.72: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.73: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.74: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.75: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.76: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.77: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.78: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.79: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.8: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.81: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.82: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.83: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.84: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.85: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 2: {0.0: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.01: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.02: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.03: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.04: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.05: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.06: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.07: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.08: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.09: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}}), defaultdict(<class 'dict'>, {0: {0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 1: {0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.71: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.72: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.73: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.74: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.75: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.76: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.77: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.78: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.79: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.8: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.81: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.82: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.83: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.84: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.85: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 2: {0.0: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.01: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.02: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.03: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.04: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.05: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.06: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.07: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.08: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.09: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}}), defaultdict(<class 'dict'>, {0: {0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 1: {0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.71: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.72: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.73: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.74: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.75: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.76: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.77: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.78: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.79: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.8: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.81: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.82: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.83: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.84: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.85: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 2: {0.0: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.01: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.02: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.03: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.04: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.05: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.06: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.07: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.08: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.09: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}})]
ipdb> defaultdict(<class 'dict'>, {0: {0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 1: {0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.71: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.72: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.73: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.74: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.75: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.76: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.77: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.78: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.79: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.8: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.81: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.82: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.83: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.84: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.85: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 2: {0.0: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.01: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.02: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.03: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.04: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.05: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.06: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.07: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.08: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.09: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}})
ipdb> {0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}
ipdb> {0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(58)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     57 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0miter_result[0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m,[0m [0mit[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 58 [0;31m                    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     59 [0;31m    [0miter_best_thres[0m[0;34m,[0m [0miter_f[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m,[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> {0.1: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}
ipdb> {0.1: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}
ipdb> True
ipdb> *** SyntaxError: unexpected EOF while parsing
ipdb> loss: tensor(1082.8467) lr: 0.0001 time: 1907.27
pred_count_train 41644

Test...
True
ipdb> Exiting Debugger.
vocab size:  29396
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
vocab size:  29396
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(176.5129) lr: 0.0001 time: 38.31
pred_count_train 737

Test...
vocab size:  29396
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(176.5129) lr: 0.0001 time: 36.81
pred_count_train 737

Test...
> [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(88)[0;36madd_results_foreach_thres_without_none[0;34m()[0m
[0;32m     87 [0;31m    [0massignment[0m [0;34m=[0m [0massignments[0m[0;34m.[0m[0mitem[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 88 [0;31m    [0;32mfor[0m [0mthres[0m [0;32min[0m [0mthres_list[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     89 [0;31m        [0;31m# prob = math.pow(math.e, values.data[0]) - thres[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> [{0: {0.1: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 1: {0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.71: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.72: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.73: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.74: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.75: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.76: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.77: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.78: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.79: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.8: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.81: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.82: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.83: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.84: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.85: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 2: {0.0: {'pp': 1, 'np': 1, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.01: {'pp': 1, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.02: {'pp': 1, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.03: {'pp': 1, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.04: {'pp': 1, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.05: {'pp': 1, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.06: {'pp': 1, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.07: {'pp': 1, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.08: {'pp': 1, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.09: {'pp': 1, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1: {'pp': 1, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 1, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 1, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 1, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}}, {0: {0.1: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 1: {0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.71: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.72: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.73: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.74: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.75: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.76: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.77: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.78: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.79: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.8: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.81: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.82: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.83: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.84: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.85: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 2: {0.0: {'pp': 1, 'np': 1, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.01: {'pp': 1, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.02: {'pp': 1, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.03: {'pp': 1, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.04: {'pp': 1, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.05: {'pp': 1, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.06: {'pp': 1, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.07: {'pp': 1, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.08: {'pp': 1, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.09: {'pp': 1, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1: {'pp': 1, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 1, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 1, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 1, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}}, {0: {0.1: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 1: {0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.71: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.72: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.73: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.74: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.75: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.76: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.77: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.78: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.79: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.8: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.81: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.82: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.83: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.84: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.85: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 2: {0.0: {'pp': 1, 'np': 1, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.01: {'pp': 1, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.02: {'pp': 1, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.03: {'pp': 1, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.04: {'pp': 1, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.05: {'pp': 1, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.06: {'pp': 1, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.07: {'pp': 1, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.08: {'pp': 1, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.09: {'pp': 1, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1: {'pp': 1, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 1, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 1, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 1, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}}]
ipdb> {0.1: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(90)[0;36madd_results_foreach_thres_without_none[0;34m()[0m
[0;32m     89 [0;31m        [0;31m# prob = math.pow(math.e, values.data[0]) - thres[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 90 [0;31m        [0mprob[0m [0;34m=[0m [0mmath[0m[0;34m.[0m[0mpow[0m[0;34m([0m[0mmath[0m[0;34m.[0m[0me[0m[0;34m,[0m [0mvalues[0m[0;34m.[0m[0mdata[0m[0;34m.[0m[0mitem[0m[0;34m([0m[0;34m)[0m[0;34m)[0m [0;34m-[0m [0mthres[0m  [0;31m# fix[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     91 [0;31m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(92)[0;36madd_results_foreach_thres_without_none[0;34m()[0m
[0;32m     91 [0;31m[0;34m[0m[0m
[0m[0;32m---> 92 [0;31m        [0;32mif[0m [0;32mnot[0m [0many[0m[0;34m([0m[0my[0m [0;34m==[0m [0mlabel[0m [0;32mfor[0m [0my[0m [0;32min[0m [0mys[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     93 [0;31m            [0;32mif[0m [0mprob[0m [0;34m<[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(93)[0;36madd_results_foreach_thres_without_none[0;34m()[0m
[0;32m     92 [0;31m        [0;32mif[0m [0;32mnot[0m [0many[0m[0;34m([0m[0my[0m [0;34m==[0m [0mlabel[0m [0;32mfor[0m [0my[0m [0;32min[0m [0mys[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 93 [0;31m            [0;32mif[0m [0mprob[0m [0;34m<[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     94 [0;31m                [0miter_result[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[[0m[0mthres[0m[0;34m][0m[0;34m[[0m[0;34m"nn"[0m[0;34m][0m [0;34m+=[0m [0;36m1[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(96)[0;36madd_results_foreach_thres_without_none[0;34m()[0m
[0;32m     95 [0;31m            [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 96 [0;31m                [0miter_result[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[[0m[0mthres[0m[0;34m][0m[0;34m[[0m[0;34m"np"[0m[0;34m][0m [0;34m+=[0m [0;36m1[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     97 [0;31m                [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(97)[0;36madd_results_foreach_thres_without_none[0;34m()[0m
[0;32m     96 [0;31m                [0miter_result[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[[0m[0mthres[0m[0;34m][0m[0;34m[[0m[0;34m"np"[0m[0;34m][0m [0;34m+=[0m [0;36m1[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 97 [0;31m                [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     98 [0;31m        [0;32melif[0m [0mprob[0m [0;34m>=[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> Exiting Debugger.
vocab size:  29396
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(176.5129) lr: 0.0001 time: 36.87
pred_count_train 737

Test...

[32m iter_0[0m
ga 	p: 36.94 	r: 27.73 	f1: 31.68 	 99 	 268 	 357
wo 	p: 26.09 	r: 8.45 	f1: 12.77 	 18 	 69 	 213
ni 	p: 3.36 	r: 19.23 	f1: 5.71 	 15 	 447 	 78

[32m iter_1[0m
ga 	p: 36.94 	r: 27.73 	f1: 31.68 	 99 	 268 	 357
wo 	p: 26.09 	r: 8.45 	f1: 12.77 	 18 	 69 	 213
ni 	p: 3.36 	r: 19.23 	f1: 5.71 	 15 	 447 	 78

[32m iter_2[0m
ga 	p: 36.94 	r: 27.73 	f1: 31.68 	 99 	 268 	 357
wo 	p: 26.09 	r: 8.45 	f1: 12.77 	 18 	 69 	 213
ni 	p: 3.36 	r: 19.23 	f1: 5.71 	 15 	 447 	 78
best_thres [[0.16, 0.2, 0.0], [0.16, 0.2, 0.0], [0.16, 0.2, 0.0]]
f [0.1844, 0.1844, 0.1844]
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse 	current best epoch 1 	 [0.16, 0.2, 0.0] 	 lr: 0.0001 	 f: 18.43575418994413
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse[0m [33m epoch 2 [0m
Train...
loss: tensor(117.2073) lr: 0.0001 time: 37.64
pred_count_train 737

Test...

[32m iter_0[0m
ga 	p: 33.09 	r: 38.66 	f1: 35.66 	 138 	 417 	 357
wo 	p: 28.51 	r: 30.52 	f1: 29.48 	 65 	 228 	 213
ni 	p: 8.47 	r: 20.51 	f1: 11.99 	 16 	 189 	 78

[32m iter_1[0m
ga 	p: 33.09 	r: 38.66 	f1: 35.66 	 138 	 417 	 357
wo 	p: 28.51 	r: 30.52 	f1: 29.48 	 65 	 228 	 213
ni 	p: 8.47 	r: 20.51 	f1: 11.99 	 16 	 189 	 78

[32m iter_2[0m
ga 	p: 33.09 	r: 38.66 	f1: 35.66 	 138 	 417 	 357
wo 	p: 28.51 	r: 30.52 	f1: 29.48 	 65 	 228 	 213
ni 	p: 8.47 	r: 20.51 	f1: 11.99 	 16 	 189 	 78
best_thres [[0.12, 0.2, 0.19], [0.12, 0.2, 0.19], [0.12, 0.2, 0.19]]
f [0.2955, 0.2955, 0.2955]
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse 	current best epoch 2 	 [0.12, 0.2, 0.19] 	 lr: 0.0001 	 f: 29.554655870445345
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse[0m [33m epoch 3 [0m
Train...
loss: tensor(100.6199) lr: 0.0001 time: 38.54
pred_count_train 737

Test...

[32m iter_0[0m
ga 	p: 40.13 	r: 35.85 	f1: 37.87 	 128 	 319 	 357
wo 	p: 40.95 	r: 20.19 	f1: 27.04 	 43 	 105 	 213
ni 	p: 10.14 	r: 26.92 	f1: 14.74 	 21 	 207 	 78

[32m iter_1[0m
ga 	p: 40.13 	r: 35.85 	f1: 37.87 	 128 	 319 	 357
wo 	p: 40.95 	r: 20.19 	f1: 27.04 	 43 	 105 	 213
ni 	p: 10.14 	r: 26.92 	f1: 14.74 	 21 	 207 	 78

[32m iter_2[0m
ga 	p: 40.13 	r: 35.85 	f1: 37.87 	 128 	 319 	 357
wo 	p: 40.95 	r: 20.19 	f1: 27.04 	 43 	 105 	 213
ni 	p: 10.14 	r: 26.92 	f1: 14.74 	 21 	 207 	 78
best_thres [[0.14, 0.21, 0.03], [0.14, 0.21, 0.03], [0.14, 0.21, 0.03]]
f [0.3002, 0.3002, 0.3002]
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse 	current best epoch 3 	 [0.14, 0.21, 0.03] 	 lr: 0.0001 	 f: 30.023455824863177
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse 	best in epoch 3 	 [0.14, 0.21, 0.03] 	 lr: 0.0001 	 f: 30.023455824863177
vocab size:  29396
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(176.5129) lr: 0.0001 time: 40.3
pred_count_train 737

Test...
vocab size:  29396
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(176.5129) lr: 0.0001 time: 39.04
pred_count_train 737

Test...
> [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(54)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     53 [0;31m                [0;31m# print(torch.pow(torch.zeros(predicted.size()) + math.e, predicted.data))[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 54 [0;31m[0;34m[0m[0m
[0m[0;32m     55 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> {0: {0.1: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 1: {0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.71: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.72: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.73: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.74: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.75: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.76: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.77: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.78: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.79: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.8: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.81: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.82: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.83: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.84: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.85: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 2: {0.0: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.01: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.02: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.03: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.04: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.05: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.06: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.07: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.08: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.09: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}}
ipdb> *** KeyError: 0
ipdb> {0.1: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}
ipdb> *** NameError: name 'iter_num' is not defined
ipdb> 
[32m iter_0[0m
ga 	p: 81.75 	r: 77.11 	f1: 79.36 	 14268 	 17453 	 18504
wo 	p: 93.46 	r: 87.26 	f1: 90.25 	 8869 	 9490 	 10164
ni 	p: 85.25 	r: 80.49 	f1: 82.8 	 3675 	 4311 	 4566

[32m iter_1[0m
ga 	p: 81.75 	r: 77.11 	f1: 79.36 	 14268 	 17453 	 18504
wo 	p: 93.46 	r: 87.26 	f1: 90.25 	 8869 	 9490 	 10164
ni 	p: 85.25 	r: 80.49 	f1: 82.8 	 3675 	 4311 	 4566

[32m iter_2[0m
ga 	p: 81.75 	r: 77.11 	f1: 79.36 	 14268 	 17453 	 18504
wo 	p: 93.46 	r: 87.26 	f1: 90.25 	 8869 	 9490 	 10164
ni 	p: 85.25 	r: 80.49 	f1: 82.8 	 3675 	 4311 	 4566
dev_num_of_high:  0
best_thres [0.47, 0.68, 0.17]
f [0.8315, 0.8315, 0.8315]
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 8 	 [0.47, 0.68, 0.17] 	 lr: 0.0001 	 f: 83.15345490633916
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 9 [0m
Train...
{0: {0.1: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 1: {0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.71: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.72: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.73: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.74: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.75: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.76: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.77: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.78: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.79: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.8: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.81: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.82: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.83: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.84: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.85: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 2: {0.0: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.01: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.02: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.03: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.04: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.05: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.06: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.07: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.08: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.09: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}}
ipdb> {0.1: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}
ipdb> 0
ipdb> 0
ipdb> loss: tensor(751.7296) lr: 0.0001 time: 1669.23
pred_count_train 41644

Test...
tensor([-1.6279, -5.1149, -3.7676, -2.6225, -1.3992, -3.6872, -5.0489, -3.1287,
        -2.7268, -5.1200, -3.2932, -2.2741, -4.4493, -2.9334, -5.3775, -5.9058,
        -3.7938, -5.7491, -3.3108, -3.3511, -5.5593, -3.3836, -5.4550, -4.1192,
        -2.8477, -1.6075, -4.4819, -3.9804, -3.9814, -2.9331, -6.6582, -3.5643,
        -6.3738, -4.0655, -7.8144, -5.8542, -2.6855, -1.3059, -4.8337, -3.4176,
        -3.5912, -6.4808], grad_fn=<SelectBackward>)
ipdb> [[[0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7], [0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85], [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6]], [[0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7], [0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85], [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6]], [[0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7], [0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85], [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6]]]
ipdb> [[0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7], [0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85], [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6]]
ipdb> 0
ipdb> 'e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse'
ipdb> ['ga', 'wo', 'ni', 'all']
ipdb> Exiting Debugger.
vocab size:  29396
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(176.5129) lr: 0.0001 time: 33.89
pred_count_train 737

Test...
> [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(59)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     58 [0;31m                    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 59 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mhoge[0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m,[0m [0mit[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     60 [0;31m                    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> {0: {0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 1: {0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.71: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.72: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.73: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.74: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.75: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.76: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.77: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.78: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.79: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.8: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.81: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.82: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.83: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.84: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.85: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 2: {0.0: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.01: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.02: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.03: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.04: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.05: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.06: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.07: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.08: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.09: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}}
ipdb> <class 'dict'>
ipdb> <class 'list'>
ipdb> {0: {0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 1: {0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.71: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.72: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.73: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.74: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.75: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.76: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.77: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.78: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.79: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.8: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.81: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.82: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.83: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.84: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.85: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 2: {0.0: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.01: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.02: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.03: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.04: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.05: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.06: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.07: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.08: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.09: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}}
ipdb> {0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}
ipdb> {0: {0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 1: {0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.71: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.72: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.73: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.74: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.75: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.76: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.77: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.78: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.79: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.8: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.81: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.82: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.83: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.84: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.85: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 2: {0.0: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.01: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.02: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.03: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.04: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.05: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.06: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.07: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.08: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.09: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}}
ipdb> {0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(60)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     59 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mhoge[0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m,[0m [0mit[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 60 [0;31m                    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     61 [0;31m[0;34m[0m[0m
[0m
ipdb> {0.1: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}
ipdb> {0.1: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}
ipdb> *** NameError: name 'qq' is not defined
ipdb> Exiting Debugger.
vocab size:  29396
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(176.5129) lr: 0.0001 time: 32.25
pred_count_train 737

Test...
> [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(59)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     58 [0;31m                    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 59 [0;31m                    [0mresult[0m [0;34m=[0m [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mhoge[0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m,[0m [0mit[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     60 [0;31m                    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> {0: {0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 1: {0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.71: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.72: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.73: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.74: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.75: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.76: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.77: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.78: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.79: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.8: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.81: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.82: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.83: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.84: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.85: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 2: {0.0: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.01: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.02: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.03: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.04: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.05: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.06: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.07: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.08: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.09: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}}
ipdb> {0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}
ipdb> {0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}
ipdb> *** NameError: name 'result' is not defined
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(95)[0;36madd_results_foreach_thres_without_none[0;34m()[0m
[0;32m     94 [0;31m    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 95 [0;31m    [0;32mfor[0m [0mthres[0m [0;32min[0m [0mthres_list[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     96 [0;31m        [0;31m# prob = math.pow(math.e, values.data[0]) - thres[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> *** NameError: name 'assignmen' is not defined
ipdb> 37
ipdb> tensor(37)
ipdb> *** NameError: name 'ps_ys' is not defined
ipdb> tensor([-1.6279, -5.1149, -3.7676, -2.6225, -1.3992, -3.6872, -5.0489, -3.1287,
        -2.7268, -5.1200, -3.2932, -2.2741, -4.4493, -2.9334, -5.3775, -5.9058,
        -3.7938, -5.7491, -3.3108, -3.3511, -5.5593, -3.3836, -5.4550, -4.1192,
        -2.8477, -1.6075, -4.4819, -3.9804, -3.9814, -2.9331, -6.6582, -3.5643,
        -6.3738, -4.0655, -7.8144, -5.8542, -2.6855, -1.3059, -4.8337, -3.4176,
        -3.5912, -6.4808], grad_fn=<SelectBackward>)
ipdb> torch.return_types.max(
values=tensor(-1.3059, grad_fn=<MaxBackward0>),
indices=tensor(37))
ipdb> tensor(-1.3059, grad_fn=<MaxBackward1>)
ipdb> tensor(37)
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(97)[0;36madd_results_foreach_thres_without_none[0;34m()[0m
[0;32m     96 [0;31m        [0;31m# prob = math.pow(math.e, values.data[0]) - thres[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 97 [0;31m        [0mprob[0m [0;34m=[0m [0mmath[0m[0;34m.[0m[0mpow[0m[0;34m([0m[0mmath[0m[0;34m.[0m[0me[0m[0;34m,[0m [0mvalues[0m[0;34m.[0m[0mdata[0m[0;34m.[0m[0mitem[0m[0;34m([0m[0;34m)[0m[0;34m)[0m [0;34m-[0m [0mthres[0m  [0;31m# fix[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     98 [0;31m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(99)[0;36madd_results_foreach_thres_without_none[0;34m()[0m
[0;32m     98 [0;31m[0;34m[0m[0m
[0m[0;32m---> 99 [0;31m        [0;32mif[0m [0;32mnot[0m [0many[0m[0;34m([0m[0my[0m [0;34m==[0m [0mlabel[0m [0;32mfor[0m [0my[0m [0;32min[0m [0mys[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    100 [0;31m            [0;32mif[0m [0mprob[0m [0;34m<[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> 0.17091897120577418
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(104)[0;36madd_results_foreach_thres_without_none[0;34m()[0m
[0;32m    103 [0;31m                [0miter_result[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[[0m[0mthres[0m[0;34m][0m[0;34m[[0m[0;34m"np"[0m[0;34m][0m [0;34m+=[0m [0;36m1[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 104 [0;31m        [0;32melif[0m [0mprob[0m [0;34m>=[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    105 [0;31m            [0;32mif[0m [0mys[0m[0;34m[[0m[0massignment[0m[0;34m][0m [0;34m==[0m [0mlabel[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> tensor([3, 3, 3, 3, 2, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])
ipdb> 0
ipdb> 
[32m iter_0[0m
ga 	p: 81.04 	r: 76.53 	f1: 78.72 	 14162 	 17475 	 18504
wo 	p: 92.82 	r: 86.11 	f1: 89.34 	 8752 	 9429 	 10164
ni 	p: 86.99 	r: 75.87 	f1: 81.05 	 3464 	 3982 	 4566

[32m iter_1[0m
ga 	p: 81.04 	r: 76.53 	f1: 78.72 	 14162 	 17475 	 18504
wo 	p: 92.82 	r: 86.11 	f1: 89.34 	 8752 	 9429 	 10164
ni 	p: 86.99 	r: 75.87 	f1: 81.05 	 3464 	 3982 	 4566

[32m iter_2[0m
ga 	p: 81.04 	r: 76.53 	f1: 78.72 	 14162 	 17475 	 18504
wo 	p: 92.82 	r: 86.11 	f1: 89.34 	 8752 	 9429 	 10164
ni 	p: 86.99 	r: 75.87 	f1: 81.05 	 3464 	 3982 	 4566
dev_num_of_high:  0
best_thres [0.38, 0.59, 0.19]
f [0.8228, 0.8228, 0.8228]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 8 	 [0.47, 0.59, 0.17] 	 lr: 0.0001 	 f: 83.29490738562399
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 10 [0m
Train...
> [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(105)[0;36madd_results_foreach_thres_without_none[0;34m()[0m
[0;32m    104 [0;31m        [0;32melif[0m [0mprob[0m [0;34m>=[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 105 [0;31m            [0;32mif[0m [0mys[0m[0;34m[[0m[0massignment[0m[0;34m][0m [0;34m==[0m [0mlabel[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    106 [0;31m                [0miter_result[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[[0m[0mthres[0m[0;34m][0m[0;34m[[0m[0;34m"pp"[0m[0;34m][0m [0;34m+=[0m [0;36m1[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(108)[0;36madd_results_foreach_thres_without_none[0;34m()[0m
[0;32m    107 [0;31m            [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 108 [0;31m                [0miter_result[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[[0m[0mthres[0m[0;34m][0m[0;34m[[0m[0;34m"np"[0m[0;34m][0m [0;34m+=[0m [0;36m1[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    109 [0;31m                [0miter_result[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[[0m[0mthres[0m[0;34m][0m[0;34m[[0m[0;34m"pn"[0m[0;34m][0m [0;34m+=[0m [0;36m1[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(109)[0;36madd_results_foreach_thres_without_none[0;34m()[0m
[0;32m    108 [0;31m                [0miter_result[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[[0m[0mthres[0m[0;34m][0m[0;34m[[0m[0;34m"np"[0m[0;34m][0m [0;34m+=[0m [0;36m1[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 109 [0;31m                [0miter_result[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[[0m[0mthres[0m[0;34m][0m[0;34m[[0m[0;34m"pn"[0m[0;34m][0m [0;34m+=[0m [0;36m1[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    110 [0;31m        [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(95)[0;36madd_results_foreach_thres_without_none[0;34m()[0m
[0;32m     94 [0;31m    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 95 [0;31m    [0;32mfor[0m [0mthres[0m [0;32min[0m [0mthres_list[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     96 [0;31m        [0;31m# prob = math.pow(math.e, values.data[0]) - thres[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(97)[0;36madd_results_foreach_thres_without_none[0;34m()[0m
[0;32m     96 [0;31m        [0;31m# prob = math.pow(math.e, values.data[0]) - thres[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 97 [0;31m        [0mprob[0m [0;34m=[0m [0mmath[0m[0;34m.[0m[0mpow[0m[0;34m([0m[0mmath[0m[0;34m.[0m[0me[0m[0;34m,[0m [0mvalues[0m[0;34m.[0m[0mdata[0m[0;34m.[0m[0mitem[0m[0;34m([0m[0;34m)[0m[0;34m)[0m [0;34m-[0m [0mthres[0m  [0;31m# fix[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     98 [0;31m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(99)[0;36madd_results_foreach_thres_without_none[0;34m()[0m
[0;32m     98 [0;31m[0;34m[0m[0m
[0m[0;32m---> 99 [0;31m        [0;32mif[0m [0;32mnot[0m [0many[0m[0;34m([0m[0my[0m [0;34m==[0m [0mlabel[0m [0;32mfor[0m [0my[0m [0;32min[0m [0mys[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    100 [0;31m            [0;32mif[0m [0mprob[0m [0;34m<[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(104)[0;36madd_results_foreach_thres_without_none[0;34m()[0m
[0;32m    103 [0;31m                [0miter_result[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[[0m[0mthres[0m[0;34m][0m[0;34m[[0m[0;34m"np"[0m[0;34m][0m [0;34m+=[0m [0;36m1[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 104 [0;31m        [0;32melif[0m [0mprob[0m [0;34m>=[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    105 [0;31m            [0;32mif[0m [0mys[0m[0;34m[[0m[0massignment[0m[0;34m][0m [0;34m==[0m [0mlabel[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> Exiting Debugger.
vocab size:  29396
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(176.5129) lr: 0.0001 time: 38.01
pred_count_train 737

Test...
> [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(59)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     58 [0;31m                    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 59 [0;31m                    [0mresult[0m [0;34m=[0m [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mhoge[0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m,[0m [0mit[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     60 [0;31m                    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(60)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     59 [0;31m                    [0mresult[0m [0;34m=[0m [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mhoge[0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m,[0m [0mit[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 60 [0;31m                    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     61 [0;31m[0;34m[0m[0m
[0m
ipdb> {0: {0.1: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 1: {0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.71: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.72: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.73: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.74: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.75: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.76: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.77: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.78: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.79: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.8: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.81: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.82: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.83: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.84: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.85: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 2: {0.0: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.01: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.02: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.03: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.04: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.05: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.06: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.07: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.08: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.09: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}}
ipdb> {0.1: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}
ipdb> {0: {0.1: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 1: {0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.71: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.72: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.73: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.74: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.75: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.76: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.77: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.78: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.79: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.8: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.81: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.82: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.83: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.84: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.85: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 2: {0.0: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.01: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.02: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.03: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.04: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.05: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.06: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.07: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.08: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.09: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}}
ipdb> {0.1: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}
ipdb> {0.1: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}
ipdb> Exiting Debugger.
vocab size:  29396
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(176.5129) lr: 0.0001 time: 37.22
pred_count_train 737

Test...
> [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(55)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     54 [0;31m[0;34m[0m[0m
[0m[0;32m---> 55 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     56 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> defaultdict(<class 'dict'>, {0: {0.1: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 1: {0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.71: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.72: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.73: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.74: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.75: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.76: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.77: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.78: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.79: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.8: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.81: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.82: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.83: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.84: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.85: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 2: {0.0: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.01: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.02: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.03: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.04: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.05: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.06: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.07: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.08: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.09: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}})
ipdb> {0.1: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}
ipdb> *** NameError: name 'iter_results' is not defined
ipdb> []
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(56)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     55 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 56 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     57 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m,[0m [0mit[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(57)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     56 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 57 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m,[0m [0mit[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     58 [0;31m                    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(58)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     57 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m,[0m [0mit[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 58 [0;31m                    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     59 [0;31m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(55)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     54 [0;31m[0;34m[0m[0m
[0m[0;32m---> 55 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     56 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(56)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     55 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 56 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     57 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m,[0m [0mit[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(57)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     56 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 57 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m,[0m [0mit[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     58 [0;31m                    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(58)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     57 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m,[0m [0mit[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 58 [0;31m                    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     59 [0;31m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(55)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     54 [0;31m[0;34m[0m[0m
[0m[0;32m---> 55 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     56 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(50)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     49 [0;31m            [0miter_best_result[0m[0;34m.[0m[0mappend[0m[0;34m([0m[0mbest_result[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 50 [0;31m            [0;32mfor[0m [0mi[0m [0;32min[0m [0mrange[0m[0;34m([0m[0myss[0m[0;34m.[0m[0msize[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     51 [0;31m                [0mys[0m [0;34m=[0m [0myss[0m[0;34m[[0m[0mi[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(51)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     50 [0;31m            [0;32mfor[0m [0mi[0m [0;32min[0m [0mrange[0m[0;34m([0m[0myss[0m[0;34m.[0m[0msize[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 51 [0;31m                [0mys[0m [0;34m=[0m [0myss[0m[0;34m[[0m[0mi[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     52 [0;31m                [0mpredicted[0m [0;34m=[0m [0mtorch[0m[0;34m.[0m[0mt[0m[0;34m([0m[0mscore[0m[0;34m[[0m[0mi[0m[0;34m][0m[0;34m.[0m[0mcpu[0m[0;34m([0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(52)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     51 [0;31m                [0mys[0m [0;34m=[0m [0myss[0m[0;34m[[0m[0mi[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 52 [0;31m                [0mpredicted[0m [0;34m=[0m [0mtorch[0m[0;34m.[0m[0mt[0m[0;34m([0m[0mscore[0m[0;34m[[0m[0mi[0m[0;34m][0m[0;34m.[0m[0mcpu[0m[0;34m([0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     53 [0;31m                [0;31m# print(torch.pow(torch.zeros(predicted.size()) + math.e, predicted.data))[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(55)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     54 [0;31m[0;34m[0m[0m
[0m[0;32m---> 55 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     56 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(56)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     55 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 56 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     57 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m,[0m [0mit[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(57)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     56 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 57 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m,[0m [0mit[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     58 [0;31m                    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> []
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(58)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     57 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m,[0m [0mit[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 58 [0;31m                    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     59 [0;31m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(55)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     54 [0;31m[0;34m[0m[0m
[0m[0;32m---> 55 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     56 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(56)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     55 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 56 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     57 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m,[0m [0mit[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(57)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     56 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 57 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m,[0m [0mit[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     58 [0;31m                    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(58)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     57 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m,[0m [0mit[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 58 [0;31m                    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     59 [0;31m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(55)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     54 [0;31m[0;34m[0m[0m
[0m[0;32m---> 55 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     56 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(56)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     55 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 56 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     57 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m,[0m [0mit[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(57)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     56 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 57 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m,[0m [0mit[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     58 [0;31m                    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(58)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     57 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m,[0m [0mit[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 58 [0;31m                    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     59 [0;31m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(55)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     54 [0;31m[0;34m[0m[0m
[0m[0;32m---> 55 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     56 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(50)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     49 [0;31m            [0miter_best_result[0m[0;34m.[0m[0mappend[0m[0;34m([0m[0mbest_result[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 50 [0;31m            [0;32mfor[0m [0mi[0m [0;32min[0m [0mrange[0m[0;34m([0m[0myss[0m[0;34m.[0m[0msize[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     51 [0;31m                [0mys[0m [0;34m=[0m [0myss[0m[0;34m[[0m[0mi[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(51)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     50 [0;31m            [0;32mfor[0m [0mi[0m [0;32min[0m [0mrange[0m[0;34m([0m[0myss[0m[0;34m.[0m[0msize[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 51 [0;31m                [0mys[0m [0;34m=[0m [0myss[0m[0;34m[[0m[0mi[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     52 [0;31m                [0mpredicted[0m [0;34m=[0m [0mtorch[0m[0;34m.[0m[0mt[0m[0;34m([0m[0mscore[0m[0;34m[[0m[0mi[0m[0;34m][0m[0;34m.[0m[0mcpu[0m[0;34m([0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(52)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     51 [0;31m                [0mys[0m [0;34m=[0m [0myss[0m[0;34m[[0m[0mi[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 52 [0;31m                [0mpredicted[0m [0;34m=[0m [0mtorch[0m[0;34m.[0m[0mt[0m[0;34m([0m[0mscore[0m[0;34m[[0m[0mi[0m[0;34m][0m[0;34m.[0m[0mcpu[0m[0;34m([0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     53 [0;31m                [0;31m# print(torch.pow(torch.zeros(predicted.size()) + math.e, predicted.data))[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(55)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     54 [0;31m[0;34m[0m[0m
[0m[0;32m---> 55 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     56 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(56)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     55 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 56 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     57 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m,[0m [0mit[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(57)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     56 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 57 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m,[0m [0mit[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     58 [0;31m                    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(58)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     57 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m,[0m [0mit[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 58 [0;31m                    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     59 [0;31m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(55)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     54 [0;31m[0;34m[0m[0m
[0m[0;32m---> 55 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     56 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(56)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     55 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 56 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     57 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m,[0m [0mit[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(57)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     56 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 57 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m,[0m [0mit[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     58 [0;31m                    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(58)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     57 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m,[0m [0mit[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 58 [0;31m                    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     59 [0;31m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(55)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     54 [0;31m[0;34m[0m[0m
[0m[0;32m---> 55 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     56 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(56)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     55 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 56 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     57 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m,[0m [0mit[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(57)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     56 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 57 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m,[0m [0mit[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     58 [0;31m                    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(58)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     57 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m,[0m [0mit[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 58 [0;31m                    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     59 [0;31m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(55)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     54 [0;31m[0;34m[0m[0m
[0m[0;32m---> 55 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     56 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(50)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     49 [0;31m            [0miter_best_result[0m[0;34m.[0m[0mappend[0m[0;34m([0m[0mbest_result[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 50 [0;31m            [0;32mfor[0m [0mi[0m [0;32min[0m [0mrange[0m[0;34m([0m[0myss[0m[0;34m.[0m[0msize[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     51 [0;31m                [0mys[0m [0;34m=[0m [0myss[0m[0;34m[[0m[0mi[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(51)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     50 [0;31m            [0;32mfor[0m [0mi[0m [0;32min[0m [0mrange[0m[0;34m([0m[0myss[0m[0;34m.[0m[0msize[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 51 [0;31m                [0mys[0m [0;34m=[0m [0myss[0m[0;34m[[0m[0mi[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     52 [0;31m                [0mpredicted[0m [0;34m=[0m [0mtorch[0m[0;34m.[0m[0mt[0m[0;34m([0m[0mscore[0m[0;34m[[0m[0mi[0m[0;34m][0m[0;34m.[0m[0mcpu[0m[0;34m([0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(52)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     51 [0;31m                [0mys[0m [0;34m=[0m [0myss[0m[0;34m[[0m[0mi[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 52 [0;31m                [0mpredicted[0m [0;34m=[0m [0mtorch[0m[0;34m.[0m[0mt[0m[0;34m([0m[0mscore[0m[0;34m[[0m[0mi[0m[0;34m][0m[0;34m.[0m[0mcpu[0m[0;34m([0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     53 [0;31m                [0;31m# print(torch.pow(torch.zeros(predicted.size()) + math.e, predicted.data))[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(55)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     54 [0;31m[0;34m[0m[0m
[0m[0;32m---> 55 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     56 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(56)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     55 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 56 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     57 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m,[0m [0mit[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(57)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     56 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 57 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m,[0m [0mit[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     58 [0;31m                    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(58)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     57 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m,[0m [0mit[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 58 [0;31m                    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     59 [0;31m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(55)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     54 [0;31m[0;34m[0m[0m
[0m[0;32m---> 55 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     56 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(56)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     55 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 56 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     57 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m,[0m [0mit[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(57)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     56 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 57 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m,[0m [0mit[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     58 [0;31m                    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(58)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     57 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m,[0m [0mit[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 58 [0;31m                    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     59 [0;31m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(55)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     54 [0;31m[0;34m[0m[0m
[0m[0;32m---> 55 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     56 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(56)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     55 [0;31m                [0;32mfor[0m [0mlabel[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mlen[0m[0;34m([0m[0mthres_lists[0m[0;34m)[0m[0;34m)[0m[0;34m:[0m  [0;31m# case label index[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 56 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     57 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m,[0m [0mit[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(57)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     56 [0;31m                    [0mp_ys[0m [0;34m=[0m [0mpredicted[0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 57 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m,[0m [0mit[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     58 [0;31m                    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> Exiting Debugger.
vocab size:  29396
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(754.3307) lr: 0.0001 time: 1911.81
pred_count_train 41644

Test...
loss: tensor(176.5129) lr: 0.0001 time: 37.4
pred_count_train 737

Test...
> [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(60)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     59 [0;31m[0;34m[0m[0m
[0m[0;32m---> 60 [0;31m            [0miter_result[0m[0;34m.[0m[0mappend[0m[0;34m([0m[0mresults[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     61 [0;31m[0;34m[0m[0m
[0m
ipdb> []
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(46)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     45 [0;31m[0;34m[0m[0m
[0m[0;32m---> 46 [0;31m        [0;32mfor[0m [0mit[0m[0;34m,[0m [0mscore[0m [0;32min[0m [0menumerate[0m[0;34m([0m[0mscores[0m[0;34m)[0m[0;34m:[0m [0;31m# iteration æ¯Žã®è§£æž[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     47 [0;31m            [0mresults[0m [0;34m=[0m [0mdefaultdict[0m[0;34m([0m[0mdict[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> [defaultdict(<class 'dict'>, {0: {0.1: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 1: {0.2: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.71: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.72: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.73: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.74: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.75: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.76: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.77: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.78: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.79: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.8: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.81: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.82: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.83: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.84: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.85: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 2: {0.0: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.01: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.02: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.03: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.04: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.05: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.06: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.07: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.08: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.09: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 1, 'np': 0, 'pn': 1, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}})]
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(60)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     59 [0;31m[0;34m[0m[0m
[0m[0;32m---> 60 [0;31m            [0miter_result[0m[0;34m.[0m[0mappend[0m[0;34m([0m[0mresults[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     61 [0;31m[0;34m[0m[0m
[0m
ipdb> defaultdict(<class 'dict'>, {0: {0.1: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 1: {0.2: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.71: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.72: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.73: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.74: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.75: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.76: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.77: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.78: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.79: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.8: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.81: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.82: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.83: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.84: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.85: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 2: {0.0: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.01: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.02: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.03: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.04: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.05: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.06: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.07: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.08: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.09: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 1, 'np': 0, 'pn': 1, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}})
ipdb> 1
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(46)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     45 [0;31m[0;34m[0m[0m
[0m[0;32m---> 46 [0;31m        [0;32mfor[0m [0mit[0m[0;34m,[0m [0mscore[0m [0;32min[0m [0menumerate[0m[0;34m([0m[0mscores[0m[0;34m)[0m[0;34m:[0m [0;31m# iteration æ¯Žã®è§£æž[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     47 [0;31m            [0mresults[0m [0;34m=[0m [0mdefaultdict[0m[0;34m([0m[0mdict[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> 2
ipdb> False
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(60)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     59 [0;31m[0;34m[0m[0m
[0m[0;32m---> 60 [0;31m            [0miter_result[0m[0;34m.[0m[0mappend[0m[0;34m([0m[0mresults[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     61 [0;31m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(58)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     57 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m,[0m [0mit[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 58 [0;31m            [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     59 [0;31m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(60)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     59 [0;31m[0;34m[0m[0m
[0m[0;32m---> 60 [0;31m            [0miter_result[0m[0;34m.[0m[0mappend[0m[0;34m([0m[0mresults[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     61 [0;31m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(58)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     57 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m,[0m [0mit[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 58 [0;31m            [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     59 [0;31m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(60)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     59 [0;31m[0;34m[0m[0m
[0m[0;32m---> 60 [0;31m            [0miter_result[0m[0;34m.[0m[0mappend[0m[0;34m([0m[0mresults[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     61 [0;31m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(58)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     57 [0;31m                    [0madd_results_foreach_thres_without_none[0m[0;34m([0m[0mlabel[0m[0;34m,[0m [0mp_ys[0m[0;34m,[0m [0mresults[0m[0;34m,[0m [0miter_thres_lists[0m[0;34m[[0m[0mit[0m[0;34m][0m[0;34m[[0m[0mlabel[0m[0;34m][0m[0;34m,[0m [0mys[0m[0;34m,[0m [0mit[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 58 [0;31m            [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     59 [0;31m[0;34m[0m[0m
[0m
ipdb> *** NameError: name 'e' is not defined
ipdb> Exiting Debugger.
loss: tensor(529.6319) lr: 0.0001 time: 1583.11
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.37 	r: 76.92 	f1: 78.13 	 14234 	 17934 	 18504
wo 	p: 92.51 	r: 86.3 	f1: 89.3 	 8772 	 9482 	 10164
ni 	p: 84.36 	r: 75.71 	f1: 79.8 	 3457 	 4098 	 4566

[32m iter_1[0m
ga 	p: 79.37 	r: 76.92 	f1: 78.13 	 14234 	 17934 	 18504
wo 	p: 92.51 	r: 86.3 	f1: 89.3 	 8772 	 9482 	 10164
ni 	p: 84.36 	r: 75.71 	f1: 79.8 	 3457 	 4098 	 4566

[32m iter_2[0m
ga 	p: 79.37 	r: 76.92 	f1: 78.13 	 14234 	 17934 	 18504
wo 	p: 92.51 	r: 86.3 	f1: 89.3 	 8772 	 9482 	 10164
ni 	p: 84.36 	r: 75.71 	f1: 79.8 	 3457 	 4098 	 4566
dev_num_of_high:  0
best_thres [0.24, 0.64, 0.12]
f [0.8174, 0.8174, 0.8174]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 8 	 [0.47, 0.68, 0.17] 	 lr: 0.0001 	 f: 83.15345490633916
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 10 [0m
Train...
vocab size:  29396
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(176.5129) lr: 0.0001 time: 34.0
pred_count_train 737

Test...
vocab size:  29396
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(176.5129) lr: 0.0001 time: 32.58
pred_count_train 737

Test...
> [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(63)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     62 [0;31m        [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 63 [0;31m        [0mlist_result[0m [0;34m=[0m [0;34m[[0m[0mmerge_dict[0m[0;34m([0m[0mlres[0m[0;34m,[0m [0mires[0m[0;34m,[0m [0;32mlambda[0m [0mx[0m[0;34m,[0m [0my[0m[0;34m:[0m [0mx[0m [0;34m+[0m [0my[0m[0;34m)[0m [0;32mfor[0m [0mlres[0m[0;34m,[0m [0mires[0m [0;32min[0m [0mzip[0m[0;34m([0m[0mlist_result[0m[0;34m,[0m [0miter_result[0m[0;34m)[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     64 [0;31m[0;34m[0m[0m
[0m
ipdb> [defaultdict(<class 'dict'>, {}), defaultdict(<class 'dict'>, {}), defaultdict(<class 'dict'>, {})]
ipdb> [defaultdict(<class 'dict'>, {0: {0.1: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 1: {0.2: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.71: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.72: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.73: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.74: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.75: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.76: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.77: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.78: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.79: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.8: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.81: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.82: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.83: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.84: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.85: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 2: {0.0: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.01: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.02: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.03: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.04: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.05: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.06: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.07: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.08: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.09: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 1, 'np': 0, 'pn': 1, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}}), defaultdict(<class 'dict'>, {0: {0.1: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 1: {0.2: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.71: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.72: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.73: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.74: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.75: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.76: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.77: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.78: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.79: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.8: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.81: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.82: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.83: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.84: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.85: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 2: {0.0: {'pp': 0, 'np': 4, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.01: {'pp': 0, 'np': 4, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.02: {'pp': 0, 'np': 4, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.03: {'pp': 0, 'np': 4, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.04: {'pp': 0, 'np': 4, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.05: {'pp': 0, 'np': 4, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.06: {'pp': 0, 'np': 4, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.07: {'pp': 0, 'np': 4, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.08: {'pp': 0, 'np': 4, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.09: {'pp': 0, 'np': 4, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}}), defaultdict(<class 'dict'>, {0: {0.1: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 1: {0.2: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.71: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.72: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.73: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.74: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.75: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.76: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.77: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.78: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.79: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.8: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.81: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.82: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.83: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.84: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.85: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 2: {0.0: {'pp': 0, 'np': 4, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.01: {'pp': 0, 'np': 4, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.02: {'pp': 0, 'np': 4, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.03: {'pp': 0, 'np': 4, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.04: {'pp': 0, 'np': 4, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.05: {'pp': 0, 'np': 4, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.06: {'pp': 0, 'np': 4, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.07: {'pp': 0, 'np': 4, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.08: {'pp': 0, 'np': 4, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.09: {'pp': 0, 'np': 4, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}})]
ipdb> 3
ipdb> defaultdict(<class 'dict'>, {0: {0.1: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 1: {0.2: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.71: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.72: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.73: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.74: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.75: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.76: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.77: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.78: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.79: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.8: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.81: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.82: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.83: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.84: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.85: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 2: {0.0: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.01: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.02: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.03: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.04: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.05: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.06: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.07: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.08: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.09: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 1, 'np': 0, 'pn': 1, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}})
ipdb> AttributeError: 'collections.defaultdict' object has no attribute 'iteritems'
> [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(63)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     62 [0;31m        [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 63 [0;31m        [0mlist_result[0m [0;34m=[0m [0;34m[[0m[0mmerge_dict[0m[0;34m([0m[0mlres[0m[0;34m,[0m [0mires[0m[0;34m,[0m [0;32mlambda[0m [0mx[0m[0;34m,[0m [0my[0m[0;34m:[0m [0mx[0m [0;34m+[0m [0my[0m[0;34m)[0m [0;32mfor[0m [0mlres[0m[0;34m,[0m [0mires[0m [0;32min[0m [0mzip[0m[0;34m([0m[0mlist_result[0m[0;34m,[0m [0miter_result[0m[0;34m)[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     64 [0;31m[0;34m[0m[0m
[0m
ipdb> *** NameError: name 'lres' is not defined
ipdb> --Return--
None
> [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(63)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     62 [0;31m        [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 63 [0;31m        [0mlist_result[0m [0;34m=[0m [0;34m[[0m[0mmerge_dict[0m[0;34m([0m[0mlres[0m[0;34m,[0m [0mires[0m[0;34m,[0m [0;32mlambda[0m [0mx[0m[0;34m,[0m [0my[0m[0;34m:[0m [0mx[0m [0;34m+[0m [0my[0m[0;34m)[0m [0;32mfor[0m [0mlres[0m[0;34m,[0m [0mires[0m [0;32min[0m [0mzip[0m[0;34m([0m[0mlist_result[0m[0;34m,[0m [0miter_result[0m[0;34m)[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     64 [0;31m[0;34m[0m[0m
[0m
ipdb> AttributeError: 'collections.defaultdict' object has no attribute 'iteritems'
> [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/train_edit.py[0m(148)[0;36mtrain[0;34m()[0m
[0;32m    147 [0;31m        [0mmodel[0m[0;34m.[0m[0meval[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 148 [0;31m        [0mthres[0m[0;34m,[0m [0mobj_score[0m[0;34m,[0m [0mnum_test_batch_instance[0m [0;34m=[0m [0mevaluate_multiclass_without_none[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata_dev[0m[0;34m,[0m [0mlen_dev[0m[0;34m,[0m [0mlabels[0m[0;34m,[0m[0mthres_lists[0m[0;34m,[0m [0mmodel_id[0m[0;34m,[0m [0mthreshold[0m[0;34m,[0m [0miterate_num[0m[0;34m,[0m [0mep[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    149 [0;31m        [0mf[0m [0;34m=[0m [0mobj_score[0m [0;34m*[0m [0;36m100[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> *** NameError: name 'list_result' is not defined
ipdb> --Return--
None
> [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/train_edit.py[0m(148)[0;36mtrain[0;34m()[0m
[0;32m    147 [0;31m        [0mmodel[0m[0;34m.[0m[0meval[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 148 [0;31m        [0mthres[0m[0;34m,[0m [0mobj_score[0m[0;34m,[0m [0mnum_test_batch_instance[0m [0;34m=[0m [0mevaluate_multiclass_without_none[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata_dev[0m[0;34m,[0m [0mlen_dev[0m[0;34m,[0m [0mlabels[0m[0;34m,[0m[0mthres_lists[0m[0;34m,[0m [0mmodel_id[0m[0;34m,[0m [0mthreshold[0m[0;34m,[0m [0miterate_num[0m[0;34m,[0m [0mep[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    149 [0;31m        [0mf[0m [0;34m=[0m [0mobj_score[0m [0;34m*[0m [0;36m100[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> AttributeError: 'collections.defaultdict' object has no attribute 'iteritems'
> [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/train_edit.py[0m(321)[0;36mrun[0;34m()[0m
[0;32m    320 [0;31m        [0;32mwith[0m [0mtorch[0m[0;34m.[0m[0mcuda[0m[0;34m.[0m[0mdevice[0m[0;34m([0m[0mgpu_id[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 321 [0;31m            [0mtrain[0m[0;34m([0m[0margs[0m[0;34m.[0m[0mout_dir[0m[0;34m,[0m [0mdata_train[0m[0;34m,[0m [0mdata_dev[0m[0;34m,[0m [0mmodel[0m[0;34m,[0m [0mmodel_id[0m[0;34m,[0m [0margs[0m[0;34m.[0m[0mmax_epoch[0m[0;34m,[0m [0margs[0m[0;34m.[0m[0mlr[0m[0;34m,[0m [0margs[0m[0;34m.[0m[0mlr[0m [0;34m/[0m [0;36m20[0m[0;34m,[0m [0margs[0m[0;34m.[0m[0mthreshold[0m[0;34m,[0m [0margs[0m[0;34m.[0m[0miter[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    322 [0;31m    [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> --Return--
None
> [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/train_edit.py[0m(321)[0;36mrun[0;34m()[0m
[0;32m    320 [0;31m        [0;32mwith[0m [0mtorch[0m[0;34m.[0m[0mcuda[0m[0;34m.[0m[0mdevice[0m[0;34m([0m[0mgpu_id[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 321 [0;31m            [0mtrain[0m[0;34m([0m[0margs[0m[0;34m.[0m[0mout_dir[0m[0;34m,[0m [0mdata_train[0m[0;34m,[0m [0mdata_dev[0m[0;34m,[0m [0mmodel[0m[0;34m,[0m [0mmodel_id[0m[0;34m,[0m [0margs[0m[0;34m.[0m[0mmax_epoch[0m[0;34m,[0m [0margs[0m[0;34m.[0m[0mlr[0m[0;34m,[0m [0margs[0m[0;34m.[0m[0mlr[0m [0;34m/[0m [0;36m20[0m[0;34m,[0m [0margs[0m[0;34m.[0m[0mthreshold[0m[0;34m,[0m [0margs[0m[0;34m.[0m[0miter[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    322 [0;31m    [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> AttributeError: 'collections.defaultdict' object has no attribute 'iteritems'
> [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/train_edit.py[0m(327)[0;36m<module>[0;34m()[0m
[0;32m    325 [0;31m[0;34m[0m[0m
[0m[0;32m    326 [0;31m[0;32mif[0m [0m__name__[0m [0;34m==[0m [0;34m'__main__'[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 327 [0;31m    [0mrun[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> 
[32m iter_0[0m
ga 	p: 82.44 	r: 75.29 	f1: 78.7 	 13932 	 16900 	 18504
wo 	p: 93.42 	r: 85.91 	f1: 89.51 	 8732 	 9347 	 10164
ni 	p: 85.85 	r: 75.76 	f1: 80.49 	 3459 	 4029 	 4566

[32m iter_1[0m
ga 	p: 82.44 	r: 75.29 	f1: 78.7 	 13932 	 16900 	 18504
wo 	p: 93.42 	r: 85.91 	f1: 89.51 	 8732 	 9347 	 10164
ni 	p: 85.85 	r: 75.76 	f1: 80.49 	 3459 	 4029 	 4566

[32m iter_2[0m
ga 	p: 82.44 	r: 75.29 	f1: 78.7 	 13932 	 16900 	 18504
wo 	p: 93.42 	r: 85.91 	f1: 89.51 	 8732 	 9347 	 10164
ni 	p: 85.85 	r: 75.76 	f1: 80.49 	 3459 	 4029 	 4566
dev_num_of_high:  0
best_thres [0.57, 0.59, 0.28]
f [0.8226, 0.8226, 0.8226]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 8 	 [0.47, 0.59, 0.17] 	 lr: 0.0001 	 f: 83.29490738562399
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 11 [0m
Train...
vocab size:  29396
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(176.5129) lr: 0.0001 time: 37.17
pred_count_train 737

Test...
> [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(64)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     63 [0;31m        [0;31m# {0: {0.1: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}},[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 64 [0;31m        [0;32mif[0m [0mlist_result[0m[0;34m[[0m[0;36m0[0m[0;34m][0m [0;34m==[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     65 [0;31m            [0mlist_result[0m [0;34m=[0m [0miter_result[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(67)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     66 [0;31m        [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 67 [0;31m            [0mlist_result[0m [0;34m=[0m [0;34m[[0m[0mmerge_dict[0m[0;34m([0m[0mlres[0m[0;34m,[0m [0mires[0m[0;34m,[0m [0;32mlambda[0m [0mx[0m[0;34m,[0m [0my[0m[0;34m:[0m [0mx[0m [0;34m+[0m [0my[0m[0;34m)[0m [0;32mfor[0m [0mlres[0m[0;34m,[0m [0mires[0m [0;32min[0m [0mzip[0m[0;34m([0m[0mlist_result[0m[0;34m,[0m [0miter_result[0m[0;34m)[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     68 [0;31m[0;34m[0m[0m
[0m
ipdb> Exiting Debugger.
vocab size:  29396
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(176.5129) lr: 0.0001 time: 37.75
pred_count_train 737

Test...
> [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(64)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     63 [0;31m        [0;31m# {0: {0.1: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}},[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 64 [0;31m        [0;32mif[0m [0mlen[0m[0;34m([0m[0mlist_result[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m)[0m [0;34m==[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     65 [0;31m            [0mlist_result[0m [0;34m=[0m [0miter_result[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(65)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     64 [0;31m        [0;32mif[0m [0mlen[0m[0;34m([0m[0mlist_result[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m)[0m [0;34m==[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 65 [0;31m            [0mlist_result[0m [0;34m=[0m [0miter_result[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     66 [0;31m        [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(29)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     28 [0;31m[0;34m[0m[0m
[0m[0;32m---> 29 [0;31m    [0;32mfor[0m [0mxss[0m[0;34m,[0m [0myss[0m[0;34m,[0m [0msent_id[0m[0;34m,[0m [0mfile_name[0m [0;32min[0m [0mtqdm[0m[0;34m([0m[0mdata_test[0m[0;34m,[0m [0mtotal[0m[0;34m=[0m[0mlen_test[0m[0;34m,[0m [0mmininterval[0m[0;34m=[0m[0;36m5[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     30 [0;31m[0;34m[0m[0m
[0m
ipdb> [defaultdict(<class 'dict'>, {0: {0.1: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 1: {0.2: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.71: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.72: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.73: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.74: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.75: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.76: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.77: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.78: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.79: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.8: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.81: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.82: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.83: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.84: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.85: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 2: {0.0: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.01: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.02: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.03: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.04: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.05: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.06: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.07: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.08: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.09: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 1, 'np': 3, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 1, 'np': 0, 'pn': 1, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}}), defaultdict(<class 'dict'>, {0: {0.1: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 1: {0.2: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.71: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.72: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.73: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.74: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.75: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.76: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.77: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.78: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.79: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.8: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.81: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.82: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.83: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.84: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.85: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 2: {0.0: {'pp': 0, 'np': 4, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.01: {'pp': 0, 'np': 4, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.02: {'pp': 0, 'np': 4, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.03: {'pp': 0, 'np': 4, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.04: {'pp': 0, 'np': 4, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.05: {'pp': 0, 'np': 4, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.06: {'pp': 0, 'np': 4, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.07: {'pp': 0, 'np': 4, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.08: {'pp': 0, 'np': 4, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.09: {'pp': 0, 'np': 4, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}}), defaultdict(<class 'dict'>, {0: {0.1: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 4, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 1: {0.2: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.71: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.72: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.73: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.74: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.75: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.76: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.77: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.78: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.79: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.8: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.81: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.82: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.83: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.84: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.85: {'pp': 0, 'np': 0, 'pn': 1, 'nn': 3, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 2: {0.0: {'pp': 0, 'np': 4, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.01: {'pp': 0, 'np': 4, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.02: {'pp': 0, 'np': 4, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.03: {'pp': 0, 'np': 4, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.04: {'pp': 0, 'np': 4, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.05: {'pp': 0, 'np': 4, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.06: {'pp': 0, 'np': 4, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.07: {'pp': 0, 'np': 4, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.08: {'pp': 0, 'np': 4, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.09: {'pp': 0, 'np': 4, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}})]
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(31)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     30 [0;31m[0;34m[0m[0m
[0m[0;32m---> 31 [0;31m        [0mtemp[0m [0;34m=[0m [0mtorch[0m[0;34m.[0m[0mzeros[0m[0;34m([0m[0myss[0m[0;34m.[0m[0msize[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m,[0m [0myss[0m[0;34m.[0m[0msize[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0;36m1[0m[0;34m][0m[0;34m,[0m [0;36m4[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     32 [0;31m        [0;32mif[0m [0mtorch[0m[0;34m.[0m[0mcuda[0m[0;34m.[0m[0mis_available[0m[0;34m([0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(64)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     63 [0;31m        [0;31m# {0: {0.1: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}},[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 64 [0;31m        [0;32mif[0m [0mlen[0m[0;34m([0m[0mlist_result[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m)[0m [0;34m==[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     65 [0;31m            [0mlist_result[0m [0;34m=[0m [0miter_result[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(67)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     66 [0;31m        [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 67 [0;31m            [0mlist_result[0m [0;34m=[0m [0;34m[[0m[0mmerge_dict[0m[0;34m([0m[0mlres[0m[0;34m,[0m [0mires[0m[0;34m,[0m [0;32mlambda[0m [0mx[0m[0;34m,[0m [0my[0m[0;34m:[0m [0mx[0m [0;34m+[0m [0my[0m[0;34m)[0m [0;32mfor[0m [0mlres[0m[0;34m,[0m [0mires[0m [0;32min[0m [0mzip[0m[0;34m([0m[0mlist_result[0m[0;34m,[0m [0miter_result[0m[0;34m)[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     68 [0;31m[0;34m[0m[0m
[0m
ipdb> AttributeError: 'collections.defaultdict' object has no attribute 'iteritems'
> [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(67)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     66 [0;31m        [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 67 [0;31m            [0mlist_result[0m [0;34m=[0m [0;34m[[0m[0mmerge_dict[0m[0;34m([0m[0mlres[0m[0;34m,[0m [0mires[0m[0;34m,[0m [0;32mlambda[0m [0mx[0m[0;34m,[0m [0my[0m[0;34m:[0m [0mx[0m [0;34m+[0m [0my[0m[0;34m)[0m [0;32mfor[0m [0mlres[0m[0;34m,[0m [0mires[0m [0;32min[0m [0mzip[0m[0;34m([0m[0mlist_result[0m[0;34m,[0m [0miter_result[0m[0;34m)[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     68 [0;31m[0;34m[0m[0m
[0m
ipdb> *** SyntaxError: invalid syntax
ipdb> Exiting Debugger.
loss: tensor(528.5531) lr: 0.0001 time: 1943.96
pred_count_train 41644

Test...
vocab size:  29396
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(176.5129) lr: 0.0001 time: 38.17
pred_count_train 737

Test...
> [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(64)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     63 [0;31m        [0;31m# {0: {0.1: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1:{}}, 1:{}}[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 64 [0;31m        [0;32mif[0m [0mlen[0m[0;34m([0m[0mlist_result[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m)[0m [0;34m==[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     65 [0;31m            [0mlist_result[0m [0;34m=[0m [0miter_result[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(62)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     61 [0;31m[0;34m[0m[0m
[0m[0;32m---> 62 [0;31m        [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     63 [0;31m        [0;31m# {0: {0.1: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1:{}}, 1:{}}[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> vocab size:  29396
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(176.5129) lr: 0.0001 time: 38.11
pred_count_train 737

Test...
> [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(64)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     63 [0;31m        [0;31m# {0: {0.1: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1:{}}, 1:{}}[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 64 [0;31m        [0;32mif[0m [0mlen[0m[0;34m([0m[0mlist_result[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m)[0m [0;34m==[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     65 [0;31m            [0mlist_result[0m [0;34m=[0m [0miter_result[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(62)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     61 [0;31m[0;34m[0m[0m
[0m[0;32m---> 62 [0;31m        [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     63 [0;31m        [0;31m# {0: {0.1: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1:{}}, 1:{}}[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> vocab size:  29396
[34me2e-stack_ve256_vu256_depth2_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(194.8762) lr: 0.0001 time: 29.34
pred_count_train 737

Test...
> [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(64)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     63 [0;31m        [0;31m# {0: {0.1: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1:{}}, 1:{}}[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 64 [0;31m        [0;32mif[0m [0mlen[0m[0;34m([0m[0mlist_result[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m)[0m [0;34m==[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     65 [0;31m            [0mlist_result[0m [0;34m=[0m [0miter_result[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(62)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     61 [0;31m[0;34m[0m[0m
[0m[0;32m---> 62 [0;31m        [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     63 [0;31m        [0;31m# {0: {0.1: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1:{}}, 1:{}}[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(64)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     63 [0;31m        [0;31m# {0: {0.1: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1:{}}, 1:{}}[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 64 [0;31m        [0;32mif[0m [0mlen[0m[0;34m([0m[0mlist_result[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m)[0m [0;34m==[0m [0;36m0[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     65 [0;31m            [0mlist_result[0m [0;34m=[0m [0miter_result[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/eval_edit.py[0m(62)[0;36mevaluate_multiclass_without_none[0;34m()[0m
[0;32m     61 [0;31m[0;34m[0m[0m
[0m[0;32m---> 62 [0;31m        [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     63 [0;31m        [0;31m# {0: {0.1: {'pp': 1, 'np': 3, 'pn': 3, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1:{}}, 1:{}}[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> Exiting Debugger.
vocab size:  29396
[34me2e-stack_ve256_vu256_depth2_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(194.8762) lr: 0.0001 time: 28.6
pred_count_train 737

Test...
[defaultdict(<class 'dict'>, {0: {0.1: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 1: {0.2: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.71: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.72: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.73: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.74: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.75: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.76: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.77: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.78: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.79: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.8: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.81: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.82: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.83: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.84: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.85: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 2: {0.0: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.01: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.02: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.03: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.04: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.05: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.06: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.07: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.08: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.09: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}}), defaultdict(<class 'dict'>, {0: {0.1: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 1: {0.2: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.71: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.72: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.73: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.74: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.75: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.76: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.77: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.78: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.79: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.8: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.81: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.82: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.83: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.84: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.85: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 2: {0.0: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.01: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.02: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.03: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.04: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.05: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.06: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.07: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.08: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.09: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}}), defaultdict(<class 'dict'>, {0: {0.1: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 1: {0.2: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.71: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.72: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.73: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.74: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.75: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.76: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.77: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.78: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.79: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.8: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.81: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.82: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.83: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.84: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.85: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 2: {0.0: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.01: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.02: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.03: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.04: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.05: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.06: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.07: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.08: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.09: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}})]

[32m iter_0[0m
ga 	p: 21.21 	r: 23.53 	f1: 22.31 	 28 	 132 	 119
wo 	p: 33.33 	r: 5.63 	f1: 9.64 	 4 	 12 	 71
ni 	p: 0.0 	r: 0.0 	f1: 0.0 	 0 	 149 	 26

[32m iter_1[0m
ga 	p: 19.13 	r: 18.49 	f1: 18.8 	 22 	 115 	 119
wo 	p: 25.0 	r: 1.41 	f1: 2.67 	 1 	 4 	 71
ni 	p: 1.39 	r: 7.69 	f1: 2.35 	 2 	 144 	 26

[32m iter_2[0m
ga 	p: 23.53 	r: 13.45 	f1: 17.11 	 16 	 68 	 119
wo 	p: 25.0 	r: 1.41 	f1: 2.67 	 1 	 4 	 71
ni 	p: 1.39 	r: 7.69 	f1: 2.35 	 2 	 144 	 26
best_thres [[0.1, 0.2, 0.0], [0.1, 0.2, 0.02], [0.14, 0.2, 0.02]]
f [0.1257, 0.1154, 0.107]
save model
e2e-stack_ve256_vu256_depth2_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse 	current best epoch 1 	 [0.14, 0.2, 0.02] 	 lr: 0.0001 	 f: 10.704225352112676
[34me2e-stack_ve256_vu256_depth2_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse[0m [33m epoch 2 [0m
Train...
loss: tensor(139.9196) lr: 0.0001 time: 29.12
pred_count_train 737

Test...
[defaultdict(<class 'dict'>, {0: {0.1: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 2, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 1: {0.2: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.71: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.72: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.73: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.74: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.75: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.76: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.77: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.78: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.79: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.8: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.81: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.82: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.83: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.84: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.85: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 2: {0.0: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.01: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.02: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.03: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.04: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.05: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.06: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.07: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.08: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.09: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}}), defaultdict(<class 'dict'>, {0: {0.1: {'pp': 1, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 1, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 1, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 1, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 1, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 1, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 1, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 1, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 1, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 1, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 1, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 1, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 1, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 1, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 1, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 1, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 1, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 1: {0.2: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.71: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.72: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.73: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.74: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.75: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.76: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.77: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.78: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.79: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.8: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.81: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.82: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.83: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.84: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.85: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 2: {0.0: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.01: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.02: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.03: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.04: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.05: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.06: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.07: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.08: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.09: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}}), defaultdict(<class 'dict'>, {0: {0.1: {'pp': 1, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 1, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 1, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 1, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 1, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 1, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 1, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 1, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 1, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 1, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 1, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 1, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 1, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 1, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 1, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 1, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 1, 'np': 1, 'pn': 1, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 1: {0.2: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.61: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.62: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.63: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.64: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.65: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.66: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.67: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.68: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.69: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.7: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.71: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.72: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.73: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.74: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.75: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.76: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.77: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.78: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.79: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.8: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.81: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.82: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.83: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.84: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.85: {'pp': 0, 'np': 0, 'pn': 2, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}, 2: {0.0: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.01: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.02: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.03: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.04: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.05: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.06: {'pp': 0, 'np': 2, 'pn': 0, 'nn': 0, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.07: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.08: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.09: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.1: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.11: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.12: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.13: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.14: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.15: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.16: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.17: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.18: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.19: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.2: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.21: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.22: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.23: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.24: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.25: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.26: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.27: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.28: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.29: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.3: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.31: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.32: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.33: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.34: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.35: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.36: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.37: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.38: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.39: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.4: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.41: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.42: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.43: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.44: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.45: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.46: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.47: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.48: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.49: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.5: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.51: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.52: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.53: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.54: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.55: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.56: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.57: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.58: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.59: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}, 0.6: {'pp': 0, 'np': 0, 'pn': 0, 'nn': 2, 'prf': {'pp': 0, 'ppnp': 0, 'pppn': 0}}}})]

[32m iter_0[0m
ga 	p: 34.74 	r: 27.73 	f1: 30.84 	 33 	 95 	 119
wo 	p: 28.57 	r: 16.9 	f1: 21.24 	 12 	 42 	 71
ni 	p: 2.44 	r: 11.54 	f1: 4.03 	 3 	 123 	 26

[32m iter_1[0m
ga 	p: 30.17 	r: 29.41 	f1: 29.79 	 35 	 116 	 119
wo 	p: 28.57 	r: 8.45 	f1: 13.04 	 6 	 21 	 71
ni 	p: 2.68 	r: 15.38 	f1: 4.57 	 4 	 149 	 26

[32m iter_2[0m
ga 	p: 31.9 	r: 31.09 	f1: 31.49 	 37 	 116 	 119
wo 	p: 28.57 	r: 8.45 	f1: 13.04 	 6 	 21 	 71
ni 	p: 3.33 	r: 7.69 	f1: 4.65 	 2 	 60 	 26
best_thres [[0.3, 0.2, 0.05], [0.18, 0.2, 0.0], [0.18, 0.2, 0.08]]
f [0.2017, 0.1902, 0.1984]
save model
e2e-stack_ve256_vu256_depth2_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse 	current best epoch 2 	 [0.18, 0.2, 0.08] 	 lr: 0.0001 	 f: 19.84184040258807
[34me2e-stack_ve256_vu256_depth2_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it3_rs2016_preFalse[0m [33m epoch 3 [0m
Train...
loss: tensor(361.3835) lr: 0.0001 time: 1640.47
pred_count_train 41644

Test...
                                                                                                                                                             
[32m iter_0[0m
ga 	p: 81.78 	r: 75.59 	f1: 78.56 	 13988 	 17105 	 18504
wo 	p: 92.59 	r: 86.26 	f1: 89.31 	 8767 	 9469 	 10164
ni 	p: 86.61 	r: 72.54 	f1: 78.95 	 3312 	 3824 	 4566

[32m iter_1[0m
ga 	p: 81.78 	r: 75.59 	f1: 78.56 	 13988 	 17105 	 18504
wo 	p: 92.59 	r: 86.26 	f1: 89.31 	 8767 	 9469 	 10164
ni 	p: 86.61 	r: 72.54 	f1: 78.95 	 3312 	 3824 	 4566

[32m iter_2[0m
ga 	p: 81.78 	r: 75.59 	f1: 78.56 	 13988 	 17105 	 18504
wo 	p: 92.59 	r: 86.26 	f1: 89.31 	 8767 	 9469 	 10164
ni 	p: 86.61 	r: 72.54 	f1: 78.95 	 3312 	 3824 	 4566
dev_num_of_high:  0
best_thres [0.44, 0.38, 0.2]
f [0.8193, 0.8193, 0.8193]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 8 	 [0.47, 0.68, 0.17] 	 lr: 0.0001 	 f: 83.15345490633916
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 11 [0m
Train...

[32m iter_0[0m
ga 	p: 83.04 	r: 74.69 	f1: 78.64 	 13820 	 16642 	 18504
wo 	p: 91.9 	r: 86.88 	f1: 89.32 	 8830 	 9608 	 10164
ni 	p: 84.82 	r: 76.5 	f1: 80.45 	 3493 	 4118 	 4566

[32m iter_1[0m
ga 	p: 83.04 	r: 74.69 	f1: 78.64 	 13820 	 16642 	 18504
wo 	p: 91.9 	r: 86.88 	f1: 89.32 	 8830 	 9608 	 10164
ni 	p: 84.82 	r: 76.5 	f1: 80.45 	 3493 	 4118 	 4566

[32m iter_2[0m
ga 	p: 83.04 	r: 74.69 	f1: 78.64 	 13820 	 16642 	 18504
wo 	p: 91.9 	r: 86.88 	f1: 89.32 	 8830 	 9608 	 10164
ni 	p: 84.82 	r: 76.5 	f1: 80.45 	 3493 	 4118 	 4566
dev_num_of_high:  0
best_thres [0.65, 0.43, 0.23]
f [0.8221, 0.8221, 0.8221]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 8 	 [0.47, 0.59, 0.17] 	 lr: 0.0001 	 f: 83.29490738562399
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 12 [0m
Train...
                                                                                                                                                             loss: tensor(277.8336) lr: 0.0001 time: 1569.37
pred_count_train 41644

Test...
loss: tensor(370.8697) lr: 0.0001 time: 1947.7
pred_count_train 41644

Test...
                                                                                 
[32m iter_0[0m
ga 	p: 81.59 	r: 75.09 	f1: 78.21 	 13895 	 17030 	 18504
wo 	p: 93.02 	r: 85.46 	f1: 89.08 	 8686 	 9338 	 10164
ni 	p: 82.02 	r: 71.16 	f1: 76.2 	 3249 	 3961 	 4566

[32m iter_1[0m
ga 	p: 81.59 	r: 75.09 	f1: 78.21 	 13895 	 17030 	 18504
wo 	p: 93.02 	r: 85.46 	f1: 89.08 	 8686 	 9338 	 10164
ni 	p: 82.02 	r: 71.16 	f1: 76.2 	 3249 	 3961 	 4566

[32m iter_2[0m
ga 	p: 81.59 	r: 75.09 	f1: 78.21 	 13895 	 17030 	 18504
wo 	p: 93.02 	r: 85.46 	f1: 89.08 	 8686 	 9338 	 10164
ni 	p: 82.02 	r: 71.16 	f1: 76.2 	 3249 	 3961 	 4566
dev_num_of_high:  0
best_thres [0.67, 0.76, 0.1]
f [0.8127, 0.8127, 0.8127]
load model: epoch8
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 8 	 [0.47, 0.59, 0.17] 	 lr: 0.0001 	 f: 83.29490738562399
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 13 [0m
Train...

[32m iter_0[0m
ga 	p: 80.89 	r: 75.35 	f1: 78.02 	 13942 	 17236 	 18504
wo 	p: 92.95 	r: 86.06 	f1: 89.37 	 8747 	 9410 	 10164
ni 	p: 80.82 	r: 75.41 	f1: 78.02 	 3443 	 4260 	 4566

[32m iter_1[0m
ga 	p: 80.89 	r: 75.35 	f1: 78.02 	 13942 	 17236 	 18504
wo 	p: 92.95 	r: 86.06 	f1: 89.37 	 8747 	 9410 	 10164
ni 	p: 80.82 	r: 75.41 	f1: 78.02 	 3443 	 4260 	 4566

[32m iter_2[0m
ga 	p: 80.89 	r: 75.35 	f1: 78.02 	 13942 	 17236 	 18504
wo 	p: 92.95 	r: 86.06 	f1: 89.37 	 8747 	 9410 	 10164
ni 	p: 80.82 	r: 75.41 	f1: 78.02 	 3443 	 4260 	 4566
dev_num_of_high:  0
best_thres [0.49, 0.54, 0.1]
f [0.8148, 0.8148, 0.8148]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 8 	 [0.47, 0.68, 0.17] 	 lr: 0.0001 	 f: 83.15345490633916
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 12 [0m
Train...

[32m iter_0[0m
ga 	p: 72.2 	r: 71.3 	f1: 71.75 	 4398 	 6091 	 6168
wo 	p: 90.95 	r: 81.23 	f1: 85.81 	 2752 	 3026 	 3388
ni 	p: 82.0 	r: 76.61 	f1: 79.21 	 1166 	 1422 	 1522

[32m iter_1[0m
ga 	p: 74.03 	r: 69.68 	f1: 71.79 	 4298 	 5806 	 6168
wo 	p: 90.68 	r: 80.67 	f1: 85.38 	 2733 	 3014 	 3388
ni 	p: 80.66 	r: 77.0 	f1: 78.79 	 1172 	 1453 	 1522

[32m iter_2[0m
ga 	p: 74.4 	r: 69.36 	f1: 71.79 	 4278 	 5750 	 6168
wo 	p: 90.68 	r: 80.7 	f1: 85.4 	 2734 	 3015 	 3388
ni 	p: 80.66 	r: 77.53 	f1: 79.06 	 1180 	 1463 	 1522
best_thres [[0.4, 0.28, 0.25], [0.46, 0.26, 0.21], [0.48, 0.26, 0.21]]
f [0.7694, 0.7689, 0.7689]
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1032_th0.2_it3_rs2016_preFalse 	current best epoch 1 	 [0.48, 0.26, 0.21] 	 lr: 0.0001 	 f: 76.89267822136479
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1032_th0.2_it3_rs2016_preFalse[0m [33m epoch 2 [0m
Train...
loss: tensor(650.8281) lr: 5e-05 time: 1571.37
pred_count_train 41644

Test...
loss: tensor(2238.4688) lr: 0.0001 time: 1729.76
pred_count_train 41644

Test...
loss: tensor(287.1340) lr: 0.0001 time: 1935.49
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.53 	r: 75.03 	f1: 79.05 	 13884 	 16621 	 18504
wo 	p: 93.28 	r: 86.59 	f1: 89.81 	 8801 	 9435 	 10164
ni 	p: 85.55 	r: 75.84 	f1: 80.4 	 3463 	 4048 	 4566

[32m iter_1[0m
ga 	p: 83.53 	r: 75.03 	f1: 79.05 	 13884 	 16621 	 18504
wo 	p: 93.28 	r: 86.59 	f1: 89.81 	 8801 	 9435 	 10164
ni 	p: 85.55 	r: 75.84 	f1: 80.4 	 3463 	 4048 	 4566

[32m iter_2[0m
ga 	p: 83.53 	r: 75.03 	f1: 79.05 	 13884 	 16621 	 18504
wo 	p: 93.28 	r: 86.59 	f1: 89.81 	 8801 	 9435 	 10164
ni 	p: 85.55 	r: 75.84 	f1: 80.4 	 3463 	 4048 	 4566
dev_num_of_high:  0
best_thres [0.5, 0.59, 0.12]
f [0.8257, 0.8257, 0.8257]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 8 	 [0.47, 0.59, 0.17] 	 lr: 0.0001 	 f: 83.29490738562399
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 14 [0m
Train...

[32m iter_0[0m
ga 	p: 78.95 	r: 73.72 	f1: 76.25 	 4547 	 5759 	 6168
wo 	p: 91.87 	r: 84.74 	f1: 88.16 	 2871 	 3125 	 3388
ni 	p: 80.83 	r: 82.85 	f1: 81.83 	 1261 	 1560 	 1522

[32m iter_1[0m
ga 	p: 80.03 	r: 73.43 	f1: 76.59 	 4529 	 5659 	 6168
wo 	p: 91.54 	r: 85.3 	f1: 88.31 	 2890 	 3157 	 3388
ni 	p: 81.42 	r: 82.06 	f1: 81.74 	 1249 	 1534 	 1522

[32m iter_2[0m
ga 	p: 79.93 	r: 73.41 	f1: 76.53 	 4528 	 5665 	 6168
wo 	p: 91.71 	r: 85.18 	f1: 88.32 	 2886 	 3147 	 3388
ni 	p: 84.72 	r: 79.04 	f1: 81.78 	 1203 	 1420 	 1522
best_thres [[0.44, 0.35, 0.2], [0.46, 0.28, 0.2], [0.46, 0.29, 0.3]]
f [0.8065, 0.8078, 0.8081]
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1032_th0.2_it3_rs2016_preFalse 	current best epoch 2 	 [0.46, 0.29, 0.3] 	 lr: 0.0001 	 f: 80.80921257391846
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1032_th0.2_it3_rs2016_preFalse[0m [33m epoch 3 [0m
Train...

[32m iter_0[0m
ga 	p: 80.92 	r: 74.42 	f1: 77.53 	 13770 	 17016 	 18504
wo 	p: 91.42 	r: 87.02 	f1: 89.17 	 8845 	 9675 	 10164
ni 	p: 82.35 	r: 73.96 	f1: 77.93 	 3377 	 4101 	 4566

[32m iter_1[0m
ga 	p: 80.92 	r: 74.42 	f1: 77.53 	 13770 	 17016 	 18504
wo 	p: 91.42 	r: 87.02 	f1: 89.17 	 8845 	 9675 	 10164
ni 	p: 82.35 	r: 73.96 	f1: 77.93 	 3377 	 4101 	 4566

[32m iter_2[0m
ga 	p: 80.92 	r: 74.42 	f1: 77.53 	 13770 	 17016 	 18504
wo 	p: 91.42 	r: 87.02 	f1: 89.17 	 8845 	 9675 	 10164
ni 	p: 82.35 	r: 73.96 	f1: 77.93 	 3377 	 4101 	 4566
dev_num_of_high:  0
best_thres [0.36, 0.75, 0.13]
f [0.8119, 0.8119, 0.8119]
load model: epoch8
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 8 	 [0.47, 0.68, 0.17] 	 lr: 0.0001 	 f: 83.15345490633916
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 13 [0m
Train...
loss: tensor(442.1733) lr: 5e-05 time: 1574.32
pred_count_train 41644

Test...
loss: tensor(1857.1415) lr: 0.0001 time: 1773.45
pred_count_train 41644

Test...
loss: tensor(647.0308) lr: 5e-05 time: 1937.32
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.63 	r: 76.8 	f1: 78.67 	 14211 	 17626 	 18504
wo 	p: 93.39 	r: 86.02 	f1: 89.55 	 8743 	 9362 	 10164
ni 	p: 85.82 	r: 76.46 	f1: 80.87 	 3491 	 4068 	 4566

[32m iter_1[0m
ga 	p: 80.63 	r: 76.8 	f1: 78.67 	 14211 	 17626 	 18504
wo 	p: 93.39 	r: 86.02 	f1: 89.55 	 8743 	 9362 	 10164
ni 	p: 85.82 	r: 76.46 	f1: 80.87 	 3491 	 4068 	 4566

[32m iter_2[0m
ga 	p: 80.63 	r: 76.8 	f1: 78.67 	 14211 	 17626 	 18504
wo 	p: 93.39 	r: 86.02 	f1: 89.55 	 8743 	 9362 	 10164
ni 	p: 85.82 	r: 76.46 	f1: 80.87 	 3491 	 4068 	 4566
dev_num_of_high:  0
best_thres [0.31, 0.61, 0.18]
f [0.8227, 0.8227, 0.8227]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 8 	 [0.47, 0.59, 0.17] 	 lr: 0.0001 	 f: 83.29490738562399
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 15 [0m
Train...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               
[32m iter_0[0m
ga 	p: 83.05 	r: 75.25 	f1: 78.96 	 13924 	 16766 	 18504
wo 	p: 92.4 	r: 86.6 	f1: 89.41 	 8802 	 9526 	 10164
ni 	p: 82.75 	r: 76.7 	f1: 79.61 	 3502 	 4232 	 4566

[32m iter_1[0m
ga 	p: 83.05 	r: 75.25 	f1: 78.96 	 13924 	 16766 	 18504
wo 	p: 92.4 	r: 86.6 	f1: 89.41 	 8802 	 9526 	 10164
ni 	p: 82.75 	r: 76.7 	f1: 79.61 	 3502 	 4232 	 4566

[32m iter_2[0m
ga 	p: 83.05 	r: 75.25 	f1: 78.96 	 13924 	 16766 	 18504
wo 	p: 92.4 	r: 86.6 	f1: 89.41 	 8802 	 9526 	 10164
ni 	p: 82.75 	r: 76.7 	f1: 79.61 	 3502 	 4232 	 4566
dev_num_of_high:  0
best_thres [0.42, 0.59, 0.09]
f [0.8227, 0.8227, 0.8227]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 8 	 [0.47, 0.68, 0.17] 	 lr: 0.0001 	 f: 83.15345490633916
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 14 [0m
Train...
loss: tensor(291.9759) lr: 5e-05 time: 1573.89
pred_count_train 41644

Test...
                                                                                 
[32m iter_0[0m
ga 	p: 81.16 	r: 75.58 	f1: 78.27 	 13985 	 17231 	 18504
wo 	p: 92.86 	r: 86.32 	f1: 89.47 	 8774 	 9449 	 10164
ni 	p: 83.07 	r: 73.3 	f1: 77.88 	 3347 	 4029 	 4566

[32m iter_1[0m
ga 	p: 81.16 	r: 75.58 	f1: 78.27 	 13985 	 17231 	 18504
wo 	p: 92.86 	r: 86.32 	f1: 89.47 	 8774 	 9449 	 10164
ni 	p: 83.07 	r: 73.3 	f1: 77.88 	 3347 	 4029 	 4566

[32m iter_2[0m
ga 	p: 81.16 	r: 75.58 	f1: 78.27 	 13985 	 17231 	 18504
wo 	p: 92.86 	r: 86.32 	f1: 89.47 	 8774 	 9449 	 10164
ni 	p: 83.07 	r: 73.3 	f1: 77.88 	 3347 	 4029 	 4566
dev_num_of_high:  0
best_thres [0.45, 0.65, 0.13]
f [0.8165, 0.8165, 0.8165]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 8 	 [0.47, 0.59, 0.17] 	 lr: 0.0001 	 f: 83.29490738562399
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 16 [0m
Train...
loss: tensor(439.7115) lr: 5e-05 time: 1915.7
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.23 	r: 74.82 	f1: 76.96 	 4615 	 5825 	 6168
wo 	p: 93.35 	r: 84.95 	f1: 88.95 	 2878 	 3083 	 3388
ni 	p: 86.2 	r: 80.88 	f1: 83.46 	 1231 	 1428 	 1522

[32m iter_1[0m
ga 	p: 80.97 	r: 72.37 	f1: 76.43 	 4464 	 5513 	 6168
wo 	p: 93.72 	r: 84.5 	f1: 88.87 	 2863 	 3055 	 3388
ni 	p: 85.5 	r: 81.34 	f1: 83.37 	 1238 	 1448 	 1522

[32m iter_2[0m
ga 	p: 79.21 	r: 73.8 	f1: 76.41 	 4552 	 5747 	 6168
wo 	p: 93.55 	r: 84.8 	f1: 88.96 	 2873 	 3071 	 3388
ni 	p: 85.36 	r: 81.21 	f1: 83.23 	 1236 	 1448 	 1522
best_thres [[0.21, 0.51, 0.26], [0.24, 0.59, 0.19], [0.2, 0.59, 0.19]]
f [0.8148, 0.8134, 0.8128]
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1032_th0.2_it3_rs2016_preFalse 	current best epoch 3 	 [0.48, 0.49, 0.15] 	 lr: 0.0001 	 f: 81.45012823942221
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1032_th0.2_it3_rs2016_preFalse[0m [33m epoch 5 [0m
Train...

[32m iter_0[0m
ga 	p: 80.85 	r: 76.18 	f1: 78.45 	 14097 	 17436 	 18504
wo 	p: 93.27 	r: 86.31 	f1: 89.66 	 8773 	 9406 	 10164
ni 	p: 81.53 	r: 77.73 	f1: 79.58 	 3549 	 4353 	 4566

[32m iter_1[0m
ga 	p: 80.85 	r: 76.18 	f1: 78.45 	 14097 	 17436 	 18504
wo 	p: 93.27 	r: 86.31 	f1: 89.66 	 8773 	 9406 	 10164
ni 	p: 81.53 	r: 77.73 	f1: 79.58 	 3549 	 4353 	 4566

[32m iter_2[0m
ga 	p: 80.85 	r: 76.18 	f1: 78.45 	 14097 	 17436 	 18504
wo 	p: 93.27 	r: 86.31 	f1: 89.66 	 8773 	 9406 	 10164
ni 	p: 81.53 	r: 77.73 	f1: 79.58 	 3549 	 4353 	 4566
dev_num_of_high:  0
best_thres [0.32, 0.62, 0.09]
f [0.8201, 0.8201, 0.8201]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 8 	 [0.47, 0.68, 0.17] 	 lr: 0.0001 	 f: 83.15345490633916
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 15 [0m
Train...
loss: tensor(197.5092) lr: 5e-05 time: 1605.44
pred_count_train 41644

Test...
loss: tensor(1281.9331) lr: 0.0001 time: 1467.31
pred_count_train 41644

Test...
vocab size:  29396
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...

[32m iter_0[0m
ga 	p: 81.69 	r: 74.81 	f1: 78.1 	 13843 	 16945 	 18504
wo 	p: 93.6 	r: 85.24 	f1: 89.23 	 8664 	 9256 	 10164
ni 	p: 85.94 	r: 74.0 	f1: 79.52 	 3379 	 3932 	 4566

[32m iter_1[0m
ga 	p: 81.69 	r: 74.81 	f1: 78.1 	 13843 	 16945 	 18504
wo 	p: 93.6 	r: 85.24 	f1: 89.23 	 8664 	 9256 	 10164
ni 	p: 85.94 	r: 74.0 	f1: 79.52 	 3379 	 3932 	 4566

[32m iter_2[0m
ga 	p: 81.69 	r: 74.81 	f1: 78.1 	 13843 	 16945 	 18504
wo 	p: 93.6 	r: 85.24 	f1: 89.23 	 8664 	 9256 	 10164
ni 	p: 85.94 	r: 74.0 	f1: 79.52 	 3379 	 3932 	 4566
dev_num_of_high:  0
best_thres [0.33, 0.81, 0.14]
f [0.817, 0.817, 0.817]
load model: epoch8
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 8 	 [0.47, 0.59, 0.17] 	 lr: 0.0001 	 f: 83.29490738562399
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 17 [0m
Train...

[32m iter_0[0m
ga 	p: 82.05 	r: 74.58 	f1: 78.14 	 4600 	 5606 	 6168
wo 	p: 92.04 	r: 86.98 	f1: 89.44 	 2947 	 3202 	 3388
ni 	p: 82.76 	r: 75.69 	f1: 79.07 	 1152 	 1392 	 1522

[32m iter_1[0m
ga 	p: 80.73 	r: 76.43 	f1: 78.52 	 4714 	 5839 	 6168
wo 	p: 93.88 	r: 85.63 	f1: 89.56 	 2901 	 3090 	 3388
ni 	p: 82.49 	r: 76.15 	f1: 79.19 	 1159 	 1405 	 1522

[32m iter_2[0m
ga 	p: 81.34 	r: 75.81 	f1: 78.48 	 4676 	 5749 	 6168
wo 	p: 94.32 	r: 85.3 	f1: 89.58 	 2890 	 3064 	 3388
ni 	p: 83.53 	r: 75.3 	f1: 79.2 	 1146 	 1372 	 1522
best_thres [[0.42, 0.39, 0.15], [0.34, 0.57, 0.12], [0.37, 0.68, 0.13]]
f [0.8177, 0.8186, 0.8189]
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1032_th0.2_it3_rs2016_preFalse 	current best epoch 5 	 [0.37, 0.68, 0.13] 	 lr: 0.0001 	 f: 81.88826169218018
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1032_th0.2_it3_rs2016_preFalse 	best in epoch 5 	 [0.37, 0.68, 0.13] 	 lr: 0.0001 	 f: 81.88826169218018
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse
gpu: 0
6 3 2
vocab size:  29396
vocab size:  29396
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1032_th0.5_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(297.8275) lr: 5e-05 time: 1951.79
pred_count_train 41644

Test...
                                                                                 loss: tensor(620.4509) lr: 2.5e-05 time: 1563.03
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.71 	r: 74.94 	f1: 78.18 	 13866 	 16969 	 18504
wo 	p: 92.71 	r: 86.32 	f1: 89.4 	 8774 	 9464 	 10164
ni 	p: 81.55 	r: 70.48 	f1: 75.61 	 3218 	 3946 	 4566

[32m iter_1[0m
ga 	p: 81.71 	r: 74.94 	f1: 78.18 	 13866 	 16969 	 18504
wo 	p: 92.71 	r: 86.32 	f1: 89.4 	 8774 	 9464 	 10164
ni 	p: 81.55 	r: 70.48 	f1: 75.61 	 3218 	 3946 	 4566

[32m iter_2[0m
ga 	p: 81.71 	r: 74.94 	f1: 78.18 	 13866 	 16969 	 18504
wo 	p: 92.71 	r: 86.32 	f1: 89.4 	 8774 	 9464 	 10164
ni 	p: 81.55 	r: 70.48 	f1: 75.61 	 3218 	 3946 	 4566
dev_num_of_high:  0
best_thres [0.47, 0.68, 0.1]
f [0.813, 0.813, 0.813]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 8 	 [0.47, 0.68, 0.17] 	 lr: 0.0001 	 f: 83.15345490633916
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 16 [0m
Train...
                                                                                                                                                                                                                                                                                                                           
[32m iter_0[0m
ga 	p: 81.99 	r: 76.64 	f1: 79.23 	 14181 	 17295 	 18504
wo 	p: 92.75 	r: 87.37 	f1: 89.98 	 8880 	 9574 	 10164
ni 	p: 86.54 	r: 76.85 	f1: 81.41 	 3509 	 4055 	 4566

[32m iter_1[0m
ga 	p: 81.99 	r: 76.64 	f1: 79.23 	 14181 	 17295 	 18504
wo 	p: 92.75 	r: 87.37 	f1: 89.98 	 8880 	 9574 	 10164
ni 	p: 86.54 	r: 76.85 	f1: 81.41 	 3509 	 4055 	 4566

[32m iter_2[0m
ga 	p: 81.99 	r: 76.64 	f1: 79.23 	 14181 	 17295 	 18504
wo 	p: 92.75 	r: 87.37 	f1: 89.98 	 8880 	 9574 	 10164
ni 	p: 86.54 	r: 76.85 	f1: 81.41 	 3509 	 4055 	 4566
dev_num_of_high:  0
best_thres [0.44, 0.45, 0.17]
f [0.8283, 0.8283, 0.8283]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 8 	 [0.47, 0.59, 0.17] 	 lr: 0.0001 	 f: 83.29490738562399
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 18 [0m
Train...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            loss: tensor(209.7113) lr: 5e-05 time: 2013.22
pred_count_train 41644

Test...
loss: tensor(462.3521) lr: 2.5e-05 time: 2312.17
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.07 	r: 75.75 	f1: 78.32 	 14017 	 17289 	 18504
wo 	p: 92.17 	r: 86.6 	f1: 89.3 	 8802 	 9550 	 10164
ni 	p: 77.7 	r: 76.78 	f1: 77.24 	 3506 	 4512 	 4566

[32m iter_1[0m
ga 	p: 81.07 	r: 75.75 	f1: 78.32 	 14017 	 17289 	 18504
wo 	p: 92.17 	r: 86.6 	f1: 89.3 	 8802 	 9550 	 10164
ni 	p: 77.7 	r: 76.78 	f1: 77.24 	 3506 	 4512 	 4566

[32m iter_2[0m
ga 	p: 81.07 	r: 75.75 	f1: 78.32 	 14017 	 17289 	 18504
wo 	p: 92.17 	r: 86.6 	f1: 89.3 	 8802 	 9550 	 10164
ni 	p: 77.7 	r: 76.78 	f1: 77.24 	 3506 	 4512 	 4566
dev_num_of_high:  0
best_thres [0.3, 0.39, 0.02]
f [0.8152, 0.8152, 0.8152]
load model: epoch8
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 8 	 [0.47, 0.68, 0.17] 	 lr: 0.0001 	 f: 83.15345490633916
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 17 [0m
Train...
                                                                                                                                                                  
[32m iter_0[0m
ga 	p: 83.21 	r: 74.91 	f1: 78.84 	 13861 	 16658 	 18504
wo 	p: 93.22 	r: 86.55 	f1: 89.76 	 8797 	 9437 	 10164
ni 	p: 85.18 	r: 76.78 	f1: 80.76 	 3506 	 4116 	 4566

[32m iter_1[0m
ga 	p: 83.21 	r: 74.91 	f1: 78.84 	 13861 	 16658 	 18504
wo 	p: 93.22 	r: 86.55 	f1: 89.76 	 8797 	 9437 	 10164
ni 	p: 85.18 	r: 76.78 	f1: 80.76 	 3506 	 4116 	 4566

[32m iter_2[0m
ga 	p: 83.21 	r: 74.91 	f1: 78.84 	 13861 	 16658 	 18504
wo 	p: 93.22 	r: 86.55 	f1: 89.76 	 8797 	 9437 	 10164
ni 	p: 85.18 	r: 76.78 	f1: 80.76 	 3506 	 4116 	 4566
dev_num_of_high:  0
best_thres [0.52, 0.5, 0.12]
f [0.8248, 0.8248, 0.8248]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 8 	 [0.47, 0.59, 0.17] 	 lr: 0.0001 	 f: 83.29490738562399
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 19 [0m
Train...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          loss: tensor(613.9365) lr: 2.5e-05 time: 2016.04
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.76 	r: 76.84 	f1: 79.22 	 14218 	 17390 	 18504
wo 	p: 92.89 	r: 87.32 	f1: 90.02 	 8875 	 9554 	 10164
ni 	p: 84.88 	r: 79.54 	f1: 82.13 	 3632 	 4279 	 4566

[32m iter_1[0m
ga 	p: 81.76 	r: 76.84 	f1: 79.22 	 14218 	 17390 	 18504
wo 	p: 92.89 	r: 87.32 	f1: 90.02 	 8875 	 9554 	 10164
ni 	p: 84.88 	r: 79.54 	f1: 82.13 	 3632 	 4279 	 4566

[32m iter_2[0m
ga 	p: 81.76 	r: 76.84 	f1: 79.22 	 14218 	 17390 	 18504
wo 	p: 92.89 	r: 87.32 	f1: 90.02 	 8875 	 9554 	 10164
ni 	p: 84.88 	r: 79.54 	f1: 82.13 	 3632 	 4279 	 4566
dev_num_of_high:  0
best_thres [0.38, 0.47, 0.12]
f [0.8292, 0.8292, 0.8292]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 8 	 [0.47, 0.68, 0.17] 	 lr: 0.0001 	 f: 83.15345490633916
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 18 [0m
Train...
loss: tensor(345.8653) lr: 2.5e-05 time: 2317.45
pred_count_train 41644

Test...
                                                                                                                                                                  
[32m iter_0[0m
ga 	p: 81.33 	r: 76.07 	f1: 78.61 	 14076 	 17307 	 18504
wo 	p: 92.06 	r: 87.1 	f1: 89.51 	 8853 	 9617 	 10164
ni 	p: 85.37 	r: 73.63 	f1: 79.07 	 3362 	 3938 	 4566

[32m iter_1[0m
ga 	p: 81.33 	r: 76.07 	f1: 78.61 	 14076 	 17307 	 18504
wo 	p: 92.06 	r: 87.1 	f1: 89.51 	 8853 	 9617 	 10164
ni 	p: 85.37 	r: 73.63 	f1: 79.07 	 3362 	 3938 	 4566

[32m iter_2[0m
ga 	p: 81.33 	r: 76.07 	f1: 78.61 	 14076 	 17307 	 18504
wo 	p: 92.06 	r: 87.1 	f1: 89.51 	 8853 	 9617 	 10164
ni 	p: 85.37 	r: 73.63 	f1: 79.07 	 3362 	 3938 	 4566
dev_num_of_high:  0
best_thres [0.41, 0.39, 0.14]
f [0.8204, 0.8204, 0.8204]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 8 	 [0.47, 0.59, 0.17] 	 lr: 0.0001 	 f: 83.29490738562399
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 20 [0m
Train...
loss: tensor(456.0925) lr: 2.5e-05 time: 2044.63
pred_count_train 41644

Test...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     
[32m iter_0[0m
ga 	p: 82.25 	r: 75.38 	f1: 78.66 	 13948 	 16959 	 18504
wo 	p: 92.65 	r: 86.76 	f1: 89.6 	 8818 	 9518 	 10164
ni 	p: 83.86 	r: 76.04 	f1: 79.76 	 3472 	 4140 	 4566

[32m iter_1[0m
ga 	p: 82.25 	r: 75.38 	f1: 78.66 	 13948 	 16959 	 18504
wo 	p: 92.65 	r: 86.76 	f1: 89.6 	 8818 	 9518 	 10164
ni 	p: 83.86 	r: 76.04 	f1: 79.76 	 3472 	 4140 	 4566

[32m iter_2[0m
ga 	p: 82.25 	r: 75.38 	f1: 78.66 	 13948 	 16959 	 18504
wo 	p: 92.65 	r: 86.76 	f1: 89.6 	 8818 	 9518 	 10164
ni 	p: 83.86 	r: 76.04 	f1: 79.76 	 3472 	 4140 	 4566
dev_num_of_high:  0
best_thres [0.44, 0.37, 0.11]
f [0.8219, 0.8219, 0.8219]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 8 	 [0.47, 0.68, 0.17] 	 lr: 0.0001 	 f: 83.15345490633916
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 19 [0m
Train...
loss: tensor(249.9985) lr: 2.5e-05 time: 2293.5
pred_count_train 41644

Test...
                                                                                 
[32m iter_0[0m
ga 	p: 82.68 	r: 74.28 	f1: 78.25 	 13744 	 16624 	 18504
wo 	p: 93.93 	r: 85.45 	f1: 89.49 	 8685 	 9246 	 10164
ni 	p: 84.41 	r: 72.1 	f1: 77.77 	 3292 	 3900 	 4566

[32m iter_1[0m
ga 	p: 82.68 	r: 74.28 	f1: 78.25 	 13744 	 16624 	 18504
wo 	p: 93.93 	r: 85.45 	f1: 89.49 	 8685 	 9246 	 10164
ni 	p: 84.41 	r: 72.1 	f1: 77.77 	 3292 	 3900 	 4566

[32m iter_2[0m
ga 	p: 82.68 	r: 74.28 	f1: 78.25 	 13744 	 16624 	 18504
wo 	p: 93.93 	r: 85.45 	f1: 89.49 	 8685 	 9246 	 10164
ni 	p: 84.41 	r: 72.1 	f1: 77.77 	 3292 	 3900 	 4566
dev_num_of_high:  0
best_thres [0.54, 0.64, 0.14]
f [0.8165, 0.8165, 0.8165]
load model: epoch8
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 8 	 [0.47, 0.59, 0.17] 	 lr: 0.0001 	 f: 83.29490738562399
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 21 [0m
Train...
loss: tensor(341.8715) lr: 2.5e-05 time: 2037.51
pred_count_train 41644

Test...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     
[32m iter_0[0m
ga 	p: 79.91 	r: 77.06 	f1: 78.46 	 14260 	 17845 	 18504
wo 	p: 92.93 	r: 86.65 	f1: 89.68 	 8807 	 9477 	 10164
ni 	p: 84.96 	r: 74.38 	f1: 79.32 	 3396 	 3997 	 4566

[32m iter_1[0m
ga 	p: 79.91 	r: 77.06 	f1: 78.46 	 14260 	 17845 	 18504
wo 	p: 92.93 	r: 86.65 	f1: 89.68 	 8807 	 9477 	 10164
ni 	p: 84.96 	r: 74.38 	f1: 79.32 	 3396 	 3997 	 4566

[32m iter_2[0m
ga 	p: 79.91 	r: 77.06 	f1: 78.46 	 14260 	 17845 	 18504
wo 	p: 92.93 	r: 86.65 	f1: 89.68 	 8807 	 9477 	 10164
ni 	p: 84.96 	r: 74.38 	f1: 79.32 	 3396 	 3997 	 4566
dev_num_of_high:  0
best_thres [0.22, 0.45, 0.13]
f [0.8199, 0.8199, 0.8199]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 8 	 [0.47, 0.68, 0.17] 	 lr: 0.0001 	 f: 83.15345490633916
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 20 [0m
Train...

[32m iter_0[0m
ga 	p: 80.19 	r: 75.13 	f1: 77.58 	 4634 	 5779 	 6168
wo 	p: 91.87 	r: 86.1 	f1: 88.89 	 2917 	 3175 	 3388
ni 	p: 84.28 	r: 81.01 	f1: 82.61 	 1233 	 1463 	 1522

[32m iter_1[0m
ga 	p: 79.66 	r: 75.83 	f1: 77.7 	 4677 	 5871 	 6168
wo 	p: 91.51 	r: 86.48 	f1: 88.92 	 2930 	 3202 	 3388
ni 	p: 85.99 	r: 79.83 	f1: 82.79 	 1215 	 1413 	 1522

[32m iter_2[0m
ga 	p: 80.55 	r: 75.0 	f1: 77.68 	 4626 	 5743 	 6168
wo 	p: 91.57 	r: 86.51 	f1: 88.97 	 2931 	 3201 	 3388
ni 	p: 81.91 	r: 83.9 	f1: 82.9 	 1277 	 1559 	 1522
best_thres [[0.27, 0.42, 0.33], [0.24, 0.37, 0.35], [0.27, 0.37, 0.2]]
f [0.8173, 0.8178, 0.8181]
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 4 	 [0.27, 0.37, 0.2] 	 lr: 0.0001 	 f: 81.8069306930693
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 5 [0m
Train...

[32m iter_0[0m
ga 	p: 78.08 	r: 73.18 	f1: 75.55 	 4514 	 5781 	 6168
wo 	p: 93.08 	r: 83.32 	f1: 87.93 	 2823 	 3033 	 3388
ni 	p: 85.28 	r: 78.38 	f1: 81.68 	 1193 	 1399 	 1522

[32m iter_1[0m
ga 	p: 78.83 	r: 72.5 	f1: 75.53 	 4472 	 5673 	 6168
wo 	p: 92.86 	r: 83.26 	f1: 87.8 	 2821 	 3038 	 3388
ni 	p: 84.13 	r: 79.04 	f1: 81.5 	 1203 	 1430 	 1522

[32m iter_2[0m
ga 	p: 78.81 	r: 72.47 	f1: 75.51 	 4470 	 5672 	 6168
wo 	p: 92.86 	r: 83.29 	f1: 87.82 	 2822 	 3039 	 3388
ni 	p: 84.66 	r: 78.71 	f1: 81.58 	 1198 	 1415 	 1522

[32m iter_3[0m
ga 	p: 78.81 	r: 72.47 	f1: 75.51 	 4470 	 5672 	 6168
wo 	p: 92.86 	r: 83.29 	f1: 87.82 	 2822 	 3039 	 3388
ni 	p: 84.66 	r: 78.71 	f1: 81.58 	 1198 	 1415 	 1522

[32m iter_4[0m
ga 	p: 78.56 	r: 72.7 	f1: 75.51 	 4484 	 5708 	 6168
wo 	p: 92.86 	r: 83.29 	f1: 87.82 	 2822 	 3039 	 3388
ni 	p: 84.66 	r: 78.71 	f1: 81.58 	 1198 	 1415 	 1522

[32m iter_5[0m
ga 	p: 78.55 	r: 72.68 	f1: 75.5 	 4483 	 5707 	 6168
wo 	p: 92.86 	r: 83.29 	f1: 87.82 	 2822 	 3039 	 3388
ni 	p: 84.66 	r: 78.71 	f1: 81.58 	 1198 	 1415 	 1522

[32m iter_6[0m
ga 	p: 78.56 	r: 72.7 	f1: 75.51 	 4484 	 5708 	 6168
wo 	p: 92.86 	r: 83.29 	f1: 87.82 	 2822 	 3039 	 3388
ni 	p: 84.66 	r: 78.71 	f1: 81.58 	 1198 	 1415 	 1522

[32m iter_7[0m
ga 	p: 78.55 	r: 72.68 	f1: 75.5 	 4483 	 5707 	 6168
wo 	p: 92.86 	r: 83.29 	f1: 87.82 	 2822 	 3039 	 3388
ni 	p: 84.66 	r: 78.71 	f1: 81.58 	 1198 	 1415 	 1522

[32m iter_8[0m
ga 	p: 78.56 	r: 72.7 	f1: 75.51 	 4484 	 5708 	 6168
wo 	p: 92.86 	r: 83.29 	f1: 87.82 	 2822 	 3039 	 3388
ni 	p: 84.66 	r: 78.71 	f1: 81.58 	 1198 	 1415 	 1522

[32m iter_9[0m
ga 	p: 78.55 	r: 72.68 	f1: 75.5 	 4483 	 5707 	 6168
wo 	p: 92.86 	r: 83.29 	f1: 87.82 	 2822 	 3039 	 3388
ni 	p: 84.66 	r: 78.71 	f1: 81.58 	 1198 	 1415 	 1522
best_thres [[0.5, 0.34, 0.26], [0.53, 0.33, 0.24], [0.53, 0.33, 0.25], [0.53, 0.33, 0.25], [0.52, 0.33, 0.25], [0.52, 0.33, 0.25], [0.52, 0.33, 0.25], [0.52, 0.33, 0.25], [0.52, 0.33, 0.25], [0.52, 0.33, 0.25]]
f [0.8013, 0.801, 0.801, 0.8009, 0.8009, 0.8009, 0.8008, 0.8008, 0.8008, 0.8008]
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse 	current best epoch 1 	 [0.52, 0.33, 0.25] 	 lr: 0.0002 	 f: 80.08005462550918
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse[0m [33m epoch 2 [0m
Train...
loss: tensor(618.9695) lr: 1.25e-05 time: 2374.1
pred_count_train 41644

Test...
loss: tensor(251.5029) lr: 2.5e-05 time: 2027.97
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.08 	r: 76.79 	f1: 79.35 	 14209 	 17311 	 18504
wo 	p: 93.51 	r: 86.79 	f1: 90.02 	 8821 	 9433 	 10164
ni 	p: 85.34 	r: 77.79 	f1: 81.39 	 3552 	 4162 	 4566

[32m iter_1[0m
ga 	p: 82.08 	r: 76.79 	f1: 79.35 	 14209 	 17311 	 18504
wo 	p: 93.51 	r: 86.79 	f1: 90.02 	 8821 	 9433 	 10164
ni 	p: 85.34 	r: 77.79 	f1: 81.39 	 3552 	 4162 	 4566

[32m iter_2[0m
ga 	p: 82.08 	r: 76.79 	f1: 79.35 	 14209 	 17311 	 18504
wo 	p: 93.51 	r: 86.79 	f1: 90.02 	 8821 	 9433 	 10164
ni 	p: 85.34 	r: 77.79 	f1: 81.39 	 3552 	 4162 	 4566
dev_num_of_high:  0
best_thres [0.42, 0.55, 0.11]
f [0.8289, 0.8289, 0.8289]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 8 	 [0.47, 0.59, 0.17] 	 lr: 0.0001 	 f: 83.29490738562399
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 22 [0m
Train...
                                                                                                                                                                  
[32m iter_0[0m
ga 	p: 80.18 	r: 75.78 	f1: 77.92 	 14023 	 17489 	 18504
wo 	p: 92.57 	r: 85.94 	f1: 89.13 	 8735 	 9436 	 10164
ni 	p: 82.99 	r: 72.76 	f1: 77.54 	 3322 	 4003 	 4566

[32m iter_1[0m
ga 	p: 80.18 	r: 75.78 	f1: 77.92 	 14023 	 17489 	 18504
wo 	p: 92.57 	r: 85.94 	f1: 89.13 	 8735 	 9436 	 10164
ni 	p: 82.99 	r: 72.76 	f1: 77.54 	 3322 	 4003 	 4566

[32m iter_2[0m
ga 	p: 80.18 	r: 75.78 	f1: 77.92 	 14023 	 17489 	 18504
wo 	p: 92.57 	r: 85.94 	f1: 89.13 	 8735 	 9436 	 10164
ni 	p: 82.99 	r: 72.76 	f1: 77.54 	 3322 	 4003 	 4566
dev_num_of_high:  0
best_thres [0.3, 0.44, 0.09]
f [0.8129, 0.8129, 0.8129]
load model: epoch8
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 8 	 [0.47, 0.68, 0.17] 	 lr: 0.0001 	 f: 83.15345490633916
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 21 [0m
Train...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  loss: tensor(508.4464) lr: 1.25e-05 time: 2297.59
pred_count_train 41644

Test...
loss: tensor(609.0480) lr: 1.25e-05 time: 2023.58
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.55 	r: 75.86 	f1: 79.07 	 14038 	 17006 	 18504
wo 	p: 93.18 	r: 86.95 	f1: 89.96 	 8838 	 9485 	 10164
ni 	p: 85.99 	r: 77.84 	f1: 81.71 	 3554 	 4133 	 4566

[32m iter_1[0m
ga 	p: 82.55 	r: 75.86 	f1: 79.07 	 14038 	 17006 	 18504
wo 	p: 93.18 	r: 86.95 	f1: 89.96 	 8838 	 9485 	 10164
ni 	p: 85.99 	r: 77.84 	f1: 81.71 	 3554 	 4133 	 4566

[32m iter_2[0m
ga 	p: 82.55 	r: 75.86 	f1: 79.07 	 14038 	 17006 	 18504
wo 	p: 93.18 	r: 86.95 	f1: 89.96 	 8838 	 9485 	 10164
ni 	p: 85.99 	r: 77.84 	f1: 81.71 	 3554 	 4133 	 4566
dev_num_of_high:  0
best_thres [0.43, 0.47, 0.13]
f [0.8278, 0.8278, 0.8278]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 8 	 [0.47, 0.59, 0.17] 	 lr: 0.0001 	 f: 83.29490738562399
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 23 [0m
Train...

[32m iter_0[0m
ga 	p: 83.03 	r: 75.53 	f1: 79.1 	 13976 	 16833 	 18504
wo 	p: 92.98 	r: 87.42 	f1: 90.11 	 8885 	 9556 	 10164
ni 	p: 84.88 	r: 77.44 	f1: 80.99 	 3536 	 4166 	 4566

[32m iter_1[0m
ga 	p: 83.03 	r: 75.53 	f1: 79.1 	 13976 	 16833 	 18504
wo 	p: 92.98 	r: 87.42 	f1: 90.11 	 8885 	 9556 	 10164
ni 	p: 84.88 	r: 77.44 	f1: 80.99 	 3536 	 4166 	 4566

[32m iter_2[0m
ga 	p: 83.03 	r: 75.53 	f1: 79.1 	 13976 	 16833 	 18504
wo 	p: 92.98 	r: 87.42 	f1: 90.11 	 8885 	 9556 	 10164
ni 	p: 84.88 	r: 77.44 	f1: 80.99 	 3536 	 4166 	 4566
dev_num_of_high:  0
best_thres [0.43, 0.57, 0.11]
f [0.8276, 0.8276, 0.8276]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 8 	 [0.47, 0.68, 0.17] 	 lr: 0.0001 	 f: 83.15345490633916
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 22 [0m
Train...
                                                                                                                                                                                                                                                  loss: tensor(431.6147) lr: 1.25e-05 time: 2293.67
pred_count_train 41644

Test...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           loss: tensor(498.5566) lr: 1.25e-05 time: 2038.66
pred_count_train 41644

Test...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            
[32m iter_0[0m
ga 	p: 82.38 	r: 75.64 	f1: 78.87 	 13997 	 16990 	 18504
wo 	p: 92.61 	r: 87.18 	f1: 89.81 	 8861 	 9568 	 10164
ni 	p: 85.88 	r: 76.35 	f1: 80.83 	 3486 	 4059 	 4566

[32m iter_1[0m
ga 	p: 82.38 	r: 75.64 	f1: 78.87 	 13997 	 16990 	 18504
wo 	p: 92.61 	r: 87.18 	f1: 89.81 	 8861 	 9568 	 10164
ni 	p: 85.88 	r: 76.35 	f1: 80.83 	 3486 	 4059 	 4566

[32m iter_2[0m
ga 	p: 82.38 	r: 75.64 	f1: 78.87 	 13997 	 16990 	 18504
wo 	p: 92.61 	r: 87.18 	f1: 89.81 	 8861 	 9568 	 10164
ni 	p: 85.88 	r: 76.35 	f1: 80.83 	 3486 	 4059 	 4566
dev_num_of_high:  0
best_thres [0.45, 0.47, 0.17]
f [0.8252, 0.8252, 0.8252]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 8 	 [0.47, 0.59, 0.17] 	 lr: 0.0001 	 f: 83.29490738562399
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 24 [0m
Train...

[32m iter_0[0m
ga 	p: 81.23 	r: 76.76 	f1: 78.93 	 14204 	 17486 	 18504
wo 	p: 91.67 	r: 88.04 	f1: 89.82 	 8948 	 9761 	 10164
ni 	p: 84.72 	r: 77.57 	f1: 80.99 	 3542 	 4181 	 4566

[32m iter_1[0m
ga 	p: 81.23 	r: 76.76 	f1: 78.93 	 14204 	 17486 	 18504
wo 	p: 91.67 	r: 88.04 	f1: 89.82 	 8948 	 9761 	 10164
ni 	p: 84.72 	r: 77.57 	f1: 80.99 	 3542 	 4181 	 4566

[32m iter_2[0m
ga 	p: 81.23 	r: 76.76 	f1: 78.93 	 14204 	 17486 	 18504
wo 	p: 91.67 	r: 88.04 	f1: 89.82 	 8948 	 9761 	 10164
ni 	p: 84.72 	r: 77.57 	f1: 80.99 	 3542 	 4181 	 4566
dev_num_of_high:  0
best_thres [0.3, 0.31, 0.1]
f [0.8256, 0.8256, 0.8256]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 8 	 [0.47, 0.68, 0.17] 	 lr: 0.0001 	 f: 83.15345490633916
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 23 [0m
Train...
                                                                                                                                                               loss: tensor(418.6846) lr: 1.25e-05 time: 2018.4
pred_count_train 41644

Test...
loss: tensor(359.9518) lr: 1.25e-05 time: 2286.92
pred_count_train 41644

Test...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       
[32m iter_0[0m
ga 	p: 79.58 	r: 77.77 	f1: 78.67 	 14391 	 18083 	 18504
wo 	p: 93.13 	r: 86.22 	f1: 89.54 	 8763 	 9409 	 10164
ni 	p: 85.61 	r: 76.19 	f1: 80.63 	 3479 	 4064 	 4566

[32m iter_1[0m
ga 	p: 79.58 	r: 77.77 	f1: 78.67 	 14391 	 18083 	 18504
wo 	p: 93.13 	r: 86.22 	f1: 89.54 	 8763 	 9409 	 10164
ni 	p: 85.61 	r: 76.19 	f1: 80.63 	 3479 	 4064 	 4566

[32m iter_2[0m
ga 	p: 79.58 	r: 77.77 	f1: 78.67 	 14391 	 18083 	 18504
wo 	p: 93.13 	r: 86.22 	f1: 89.54 	 8763 	 9409 	 10164
ni 	p: 85.61 	r: 76.19 	f1: 80.63 	 3479 	 4064 	 4566
dev_num_of_high:  0
best_thres [0.24, 0.68, 0.15]
f [0.8221, 0.8221, 0.8221]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 8 	 [0.47, 0.68, 0.17] 	 lr: 0.0001 	 f: 83.15345490633916
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 24 [0m
Train...

[32m iter_0[0m
ga 	p: 82.77 	r: 74.59 	f1: 78.47 	 13802 	 16675 	 18504
wo 	p: 93.22 	r: 86.31 	f1: 89.63 	 8773 	 9411 	 10164
ni 	p: 85.39 	r: 75.41 	f1: 80.09 	 3443 	 4032 	 4566

[32m iter_1[0m
ga 	p: 82.77 	r: 74.59 	f1: 78.47 	 13802 	 16675 	 18504
wo 	p: 93.22 	r: 86.31 	f1: 89.63 	 8773 	 9411 	 10164
ni 	p: 85.39 	r: 75.41 	f1: 80.09 	 3443 	 4032 	 4566

[32m iter_2[0m
ga 	p: 82.77 	r: 74.59 	f1: 78.47 	 13802 	 16675 	 18504
wo 	p: 93.22 	r: 86.31 	f1: 89.63 	 8773 	 9411 	 10164
ni 	p: 85.39 	r: 75.41 	f1: 80.09 	 3443 	 4032 	 4566
dev_num_of_high:  0
best_thres [0.52, 0.56, 0.14]
f [0.8214, 0.8214, 0.8214]
load model: epoch8
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 8 	 [0.47, 0.59, 0.17] 	 lr: 0.0001 	 f: 83.29490738562399
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 25 [0m
Train...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    loss: tensor(349.5589) lr: 1.25e-05 time: 2021.13
pred_count_train 41644

Test...
                                                                                loss: tensor(620.7841) lr: 1e-05 time: 2374.32
pred_count_train 41644

Test...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              
[32m iter_0[0m
ga 	p: 80.81 	r: 75.78 	f1: 78.22 	 14023 	 17353 	 18504
wo 	p: 90.61 	r: 88.18 	f1: 89.38 	 8963 	 9892 	 10164
ni 	p: 82.6 	r: 76.3 	f1: 79.33 	 3484 	 4218 	 4566

[32m iter_1[0m
ga 	p: 80.81 	r: 75.78 	f1: 78.22 	 14023 	 17353 	 18504
wo 	p: 90.61 	r: 88.18 	f1: 89.38 	 8963 	 9892 	 10164
ni 	p: 82.6 	r: 76.3 	f1: 79.33 	 3484 	 4218 	 4566

[32m iter_2[0m
ga 	p: 80.81 	r: 75.78 	f1: 78.22 	 14023 	 17353 	 18504
wo 	p: 90.61 	r: 88.18 	f1: 89.38 	 8963 	 9892 	 10164
ni 	p: 82.6 	r: 76.3 	f1: 79.33 	 3484 	 4218 	 4566
dev_num_of_high:  0
best_thres [0.3, 0.24, 0.09]
f [0.8183, 0.8183, 0.8183]
load model: epoch8
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 8 	 [0.47, 0.68, 0.17] 	 lr: 0.0001 	 f: 83.15345490633916
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 25 [0m
Train...

[32m iter_0[0m
ga 	p: 82.68 	r: 76.3 	f1: 79.36 	 14118 	 17075 	 18504
wo 	p: 92.59 	r: 87.42 	f1: 89.93 	 8885 	 9596 	 10164
ni 	p: 85.5 	r: 78.93 	f1: 82.09 	 3604 	 4215 	 4566

[32m iter_1[0m
ga 	p: 82.68 	r: 76.3 	f1: 79.36 	 14118 	 17075 	 18504
wo 	p: 92.59 	r: 87.42 	f1: 89.93 	 8885 	 9596 	 10164
ni 	p: 85.5 	r: 78.93 	f1: 82.09 	 3604 	 4215 	 4566

[32m iter_2[0m
ga 	p: 82.68 	r: 76.3 	f1: 79.36 	 14118 	 17075 	 18504
wo 	p: 92.59 	r: 87.42 	f1: 89.93 	 8885 	 9596 	 10164
ni 	p: 85.5 	r: 78.93 	f1: 82.09 	 3604 	 4215 	 4566
dev_num_of_high:  0
best_thres [0.49, 0.43, 0.11]
f [0.8299, 0.8299, 0.8299]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 8 	 [0.47, 0.59, 0.17] 	 lr: 0.0001 	 f: 83.29490738562399
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 26 [0m
Train...
                             loss: tensor(608.4825) lr: 1e-05 time: 2042.36
pred_count_train 41644

Test...
                                                                                 loss: tensor(525.1731) lr: 1e-05 time: 2327.29
pred_count_train 41644

Test...
loss: tensor(1115.9927) lr: 5e-05 time: 2745.32
pred_count_train 41644

Test...
p: 93.26 	r: 87.32 	f1: 90.19 	 8875 	 9516 	 10164
ni 	p: 85.23 	r: 78.36 	f1: 81.65 	 3578 	 4198 	 4566

[32m iter_1[0m
ga 	p: 82.82 	r: 75.66 	f1: 79.08 	 14000 	 16905 	 18504
wo 	p: 93.26 	r: 87.32 	f1: 90.19 	 8875 	 9516 	 10164
ni 	p: 85.23 	r: 78.36 	f1: 81.65 	 3578 	 4198 	 4566

[32m iter_2[0m
ga 	p: 82.82 	r: 75.66 	f1: 79.08 	 14000 	 16905 	 18504
wo 	p: 93.26 	r: 87.32 	f1: 90.19 	 8875 	 9516 	 10164
ni 	p: 85.23 	r: 78.36 	f1: 81.65 	 3578 	 4198 	 4566
dev_num_of_high:  0
best_thres [0.48, 0.55, 0.11]
f [0.8286, 0.8286, 0.8286]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 8 	 [0.47, 0.68, 0.17] 	 lr: 0.0001 	 f: 83.15345490633916
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 26 [0m
Train...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    
[32m iter_0[0m
ga 	p: 82.52 	r: 76.24 	f1: 79.25 	 14107 	 17096 	 18504
wo 	p: 92.72 	r: 87.35 	f1: 89.95 	 8878 	 9575 	 10164
ni 	p: 85.34 	r: 78.14 	f1: 81.58 	 3568 	 4181 	 4566

[32m iter_1[0m
ga 	p: 82.52 	r: 76.24 	f1: 79.25 	 14107 	 17096 	 18504
wo 	p: 92.72 	r: 87.35 	f1: 89.95 	 8878 	 9575 	 10164
ni 	p: 85.34 	r: 78.14 	f1: 81.58 	 3568 	 4181 	 4566

[32m iter_2[0m
ga 	p: 82.52 	r: 76.24 	f1: 79.25 	 14107 	 17096 	 18504
wo 	p: 92.72 	r: 87.35 	f1: 89.95 	 8878 	 9575 	 10164
ni 	p: 85.34 	r: 78.14 	f1: 81.58 	 3568 	 4181 	 4566
dev_num_of_high:  0
best_thres [0.44, 0.44, 0.13]
f [0.8287, 0.8287, 0.8287]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 8 	 [0.47, 0.59, 0.17] 	 lr: 0.0001 	 f: 83.29490738562399
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 27 [0m
Train...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 loss: tensor(514.5077) lr: 1e-05 time: 2013.85
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.88 	r: 76.49 	f1: 79.09 	 14153 	 17286 	 18504
wo 	p: 92.99 	r: 87.04 	f1: 89.92 	 8847 	 9514 	 10164
ni 	p: 85.53 	r: 76.92 	f1: 81.0 	 3512 	 4106 	 4566

[32m iter_1[0m
ga 	p: 81.88 	r: 76.49 	f1: 79.09 	 14153 	 17286 	 18504
wo 	p: 92.99 	r: 87.04 	f1: 89.92 	 8847 	 9514 	 10164
ni 	p: 85.53 	r: 76.92 	f1: 81.0 	 3512 	 4106 	 4566

[32m iter_2[0m
ga 	p: 81.88 	r: 76.49 	f1: 79.09 	 14153 	 17286 	 18504
wo 	p: 92.99 	r: 87.04 	f1: 89.92 	 8847 	 9514 	 10164
ni 	p: 85.53 	r: 76.92 	f1: 81.0 	 3512 	 4106 	 4566
dev_num_of_high:  0
best_thres [0.34, 0.56, 0.11]
f [0.8267, 0.8267, 0.8267]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 8 	 [0.47, 0.68, 0.17] 	 lr: 0.0001 	 f: 83.15345490633916
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 27 [0m
Train...
loss: tensor(453.9781) lr: 1e-05 time: 2317.73
pred_count_train 41644

Test...
                                                                                                                                                               
[32m iter_0[0m
ga 	p: 81.27 	r: 76.7 	f1: 78.92 	 14193 	 17463 	 18504
wo 	p: 93.29 	r: 86.83 	f1: 89.94 	 8825 	 9460 	 10164
ni 	p: 84.71 	r: 77.4 	f1: 80.89 	 3534 	 4172 	 4566

[32m iter_1[0m
ga 	p: 81.27 	r: 76.7 	f1: 78.92 	 14193 	 17463 	 18504
wo 	p: 93.29 	r: 86.83 	f1: 89.94 	 8825 	 9460 	 10164
ni 	p: 84.71 	r: 77.4 	f1: 80.89 	 3534 	 4172 	 4566

[32m iter_2[0m
ga 	p: 81.27 	r: 76.7 	f1: 78.92 	 14193 	 17463 	 18504
wo 	p: 93.29 	r: 86.83 	f1: 89.94 	 8825 	 9460 	 10164
ni 	p: 84.71 	r: 77.4 	f1: 80.89 	 3534 	 4172 	 4566
dev_num_of_high:  0
best_thres [0.38, 0.52, 0.12]
f [0.8255, 0.8255, 0.8255]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 8 	 [0.47, 0.59, 0.17] 	 lr: 0.0001 	 f: 83.29490738562399
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 28 [0m
Train...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 
[32m iter_0[0m
ga 	p: 82.51 	r: 73.9 	f1: 77.97 	 4558 	 5524 	 6168
wo 	p: 92.77 	r: 85.24 	f1: 88.85 	 2888 	 3113 	 3388
ni 	p: 83.32 	r: 77.79 	f1: 80.46 	 1184 	 1421 	 1522

[32m iter_1[0m
ga 	p: 81.66 	r: 74.84 	f1: 78.1 	 4616 	 5653 	 6168
wo 	p: 90.91 	r: 86.81 	f1: 88.81 	 2941 	 3235 	 3388
ni 	p: 84.24 	r: 78.65 	f1: 81.35 	 1197 	 1421 	 1522

[32m iter_2[0m
ga 	p: 82.03 	r: 74.58 	f1: 78.12 	 4600 	 5608 	 6168
wo 	p: 91.33 	r: 86.42 	f1: 88.81 	 2928 	 3206 	 3388
ni 	p: 84.91 	r: 77.99 	f1: 81.3 	 1187 	 1398 	 1522
best_thres [[0.49, 0.42, 0.21], [0.44, 0.23, 0.17], [0.47, 0.27, 0.18]]
f [0.8166, 0.8176, 0.818]
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 4 	 [0.27, 0.37, 0.2] 	 lr: 0.0001 	 f: 81.8069306930693
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 11 [0m
Train...

[32m iter_0[0m
ga 	p: 80.95 	r: 75.5 	f1: 78.13 	 4657 	 5753 	 6168
wo 	p: 91.21 	r: 86.69 	f1: 88.89 	 2937 	 3220 	 3388
ni 	p: 87.24 	r: 76.35 	f1: 81.43 	 1162 	 1332 	 1522

[32m iter_1[0m
ga 	p: 81.61 	r: 74.97 	f1: 78.15 	 4624 	 5666 	 6168
wo 	p: 91.21 	r: 86.66 	f1: 88.88 	 2936 	 3219 	 3388
ni 	p: 87.68 	r: 75.76 	f1: 81.28 	 1153 	 1315 	 1522

[32m iter_2[0m
ga 	p: 78.87 	r: 77.38 	f1: 78.12 	 4773 	 6052 	 6168
wo 	p: 93.58 	r: 84.71 	f1: 88.92 	 2870 	 3067 	 3388
ni 	p: 88.48 	r: 75.16 	f1: 81.28 	 1144 	 1293 	 1522

[32m iter_3[0m
ga 	p: 79.03 	r: 77.24 	f1: 78.12 	 4764 	 6028 	 6168
wo 	p: 94.07 	r: 84.24 	f1: 88.88 	 2854 	 3034 	 3388
ni 	p: 88.48 	r: 75.16 	f1: 81.28 	 1144 	 1293 	 1522

[32m iter_4[0m
ga 	p: 79.02 	r: 77.24 	f1: 78.12 	 4764 	 6029 	 6168
wo 	p: 93.58 	r: 84.71 	f1: 88.92 	 2870 	 3067 	 3388
ni 	p: 88.48 	r: 75.16 	f1: 81.28 	 1144 	 1293 	 1522

[32m iter_5[0m
ga 	p: 79.03 	r: 77.24 	f1: 78.12 	 4764 	 6028 	 6168
wo 	p: 94.07 	r: 84.24 	f1: 88.88 	 2854 	 3034 	 3388
ni 	p: 88.48 	r: 75.16 	f1: 81.28 	 1144 	 1293 	 1522

[32m iter_6[0m
ga 	p: 79.02 	r: 77.24 	f1: 78.12 	 4764 	 6029 	 6168
wo 	p: 93.58 	r: 84.71 	f1: 88.92 	 2870 	 3067 	 3388
ni 	p: 88.48 	r: 75.16 	f1: 81.28 	 1144 	 1293 	 1522

[32m iter_7[0m
ga 	p: 79.03 	r: 77.24 	f1: 78.12 	 4764 	 6028 	 6168
wo 	p: 94.07 	r: 84.24 	f1: 88.88 	 2854 	 3034 	 3388
ni 	p: 88.48 	r: 75.16 	f1: 81.28 	 1144 	 1293 	 1522

[32m iter_8[0m
ga 	p: 79.03 	r: 77.24 	f1: 78.12 	 4764 	 6028 	 6168
wo 	p: 93.58 	r: 84.71 	f1: 88.92 	 2870 	 3067 	 3388
ni 	p: 88.48 	r: 75.16 	f1: 81.28 	 1144 	 1293 	 1522

[32m iter_9[0m
ga 	p: 79.03 	r: 77.24 	f1: 78.12 	 4764 	 6028 	 6168
wo 	p: 94.07 	r: 84.24 	f1: 88.88 	 2854 	 3034 	 3388
ni 	p: 88.48 	r: 75.16 	f1: 81.28 	 1144 	 1293 	 1522
best_thres [[0.44, 0.33, 0.23], [0.46, 0.32, 0.23], [0.34, 0.52, 0.25], [0.35, 0.57, 0.25], [0.35, 0.52, 0.25], [0.35, 0.57, 0.25], [0.35, 0.52, 0.25], [0.35, 0.57, 0.25], [0.35, 0.52, 0.25], [0.35, 0.57, 0.25]]
f [0.819, 0.819, 0.8186, 0.8183, 0.8182, 0.8181, 0.8181, 0.818, 0.818, 0.818]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse 	current best epoch 2 	 [0.45, 0.28, 0.31] 	 lr: 0.0002 	 f: 81.96025052738916
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse[0m [33m epoch 4 [0m
Train...

[32m iter_0[0m
ga 	p: 79.65 	r: 77.5 	f1: 78.56 	 14341 	 18004 	 18504
wo 	p: 92.8 	r: 86.63 	f1: 89.61 	 8805 	 9488 	 10164
ni 	p: 85.34 	r: 75.76 	f1: 80.26 	 3459 	 4053 	 4566

[32m iter_1[0m
ga 	p: 79.65 	r: 77.5 	f1: 78.56 	 14341 	 18004 	 18504
wo 	p: 92.8 	r: 86.63 	f1: 89.61 	 8805 	 9488 	 10164
ni 	p: 85.34 	r: 75.76 	f1: 80.26 	 3459 	 4053 	 4566

[32m iter_2[0m
ga 	p: 79.65 	r: 77.5 	f1: 78.56 	 14341 	 18004 	 18504
wo 	p: 92.8 	r: 86.63 	f1: 89.61 	 8805 	 9488 	 10164
ni 	p: 85.34 	r: 75.76 	f1: 80.26 	 3459 	 4053 	 4566
dev_num_of_high:  0
best_thres [0.25, 0.5, 0.11]
f [0.8214, 0.8214, 0.8214]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 8 	 [0.47, 0.68, 0.17] 	 lr: 0.0001 	 f: 83.15345490633916
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 28 [0m
Train...
loss: tensor(395.0145) lr: 1e-05 time: 2297.6
pred_count_train 41644

Test...
loss: tensor(612.5461) lr: 0.0001 time: 2459.96
pred_count_train 41644

Test...
loss: tensor(697.8372) lr: 5e-05 time: 2578.04
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.32 	r: 74.45 	f1: 78.64 	 13777 	 16535 	 18504
wo 	p: 93.05 	r: 86.62 	f1: 89.72 	 8804 	 9462 	 10164
ni 	p: 85.65 	r: 75.69 	f1: 80.36 	 3456 	 4035 	 4566

[32m iter_1[0m
ga 	p: 83.32 	r: 74.45 	f1: 78.64 	 13777 	 16535 	 18504
wo 	p: 93.05 	r: 86.62 	f1: 89.72 	 8804 	 9462 	 10164
ni 	p: 85.65 	r: 75.69 	f1: 80.36 	 3456 	 4035 	 4566

[32m iter_2[0m
ga 	p: 83.32 	r: 74.45 	f1: 78.64 	 13777 	 16535 	 18504
wo 	p: 93.05 	r: 86.62 	f1: 89.72 	 8804 	 9462 	 10164
ni 	p: 85.65 	r: 75.69 	f1: 80.36 	 3456 	 4035 	 4566
dev_num_of_high:  0
best_thres [0.54, 0.49, 0.14]
f [0.8231, 0.8231, 0.8231]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	best in epoch 8 	 [0.47, 0.59, 0.17] 	 lr: 0.0001 	 f: 83.29490738562399
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse
gpu: 0
6 3 2
vocab size:  29396
prediction mode: model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse test [0.5, 0.5, 0.5]
vocab size:  29396
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...

[32m iter_0[0m
ga 	p: 78.63 	r: 78.21 	f1: 78.42 	 4824 	 6135 	 6168
wo 	p: 92.06 	r: 85.57 	f1: 88.7 	 2899 	 3149 	 3388
ni 	p: 85.23 	r: 74.31 	f1: 79.4 	 1131 	 1327 	 1522

[32m iter_1[0m
ga 	p: 80.42 	r: 76.77 	f1: 78.55 	 4735 	 5888 	 6168
wo 	p: 91.89 	r: 85.98 	f1: 88.84 	 2913 	 3170 	 3388
ni 	p: 84.72 	r: 75.43 	f1: 79.81 	 1148 	 1355 	 1522

[32m iter_2[0m
ga 	p: 81.83 	r: 75.63 	f1: 78.61 	 4665 	 5701 	 6168
wo 	p: 91.67 	r: 86.1 	f1: 88.8 	 2917 	 3182 	 3388
ni 	p: 85.36 	r: 75.1 	f1: 79.9 	 1143 	 1339 	 1522
best_thres [[0.28, 0.34, 0.39], [0.39, 0.31, 0.29], [0.55, 0.29, 0.29]]
f [0.8165, 0.8175, 0.8181]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.1_it3_rs2016_preFalse 	current best epoch 7 	 [0.46, 0.47, 0.15] 	 lr: 0.0001 	 f: 82.79055216643204
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.1_it3_rs2016_preFalse[0m [33m epoch 11 [0m
Train...
loss: tensor(380.8698) lr: 1e-05 time: 2045.25
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.6 	r: 74.69 	f1: 77.53 	 4607 	 5716 	 6168
wo 	p: 91.9 	r: 85.71 	f1: 88.7 	 2904 	 3160 	 3388
ni 	p: 81.64 	r: 73.06 	f1: 77.12 	 1112 	 1362 	 1522

[32m iter_1[0m
ga 	p: 81.03 	r: 75.05 	f1: 77.92 	 4629 	 5713 	 6168
wo 	p: 91.82 	r: 85.83 	f1: 88.73 	 2908 	 3167 	 3388
ni 	p: 82.31 	r: 74.9 	f1: 78.43 	 1140 	 1385 	 1522

[32m iter_2[0m
ga 	p: 79.16 	r: 76.65 	f1: 77.88 	 4728 	 5973 	 6168
wo 	p: 91.87 	r: 86.01 	f1: 88.84 	 2914 	 3172 	 3388
ni 	p: 82.65 	r: 74.51 	f1: 78.37 	 1134 	 1372 	 1522
best_thres [[0.47, 0.46, 0.23], [0.46, 0.46, 0.16], [0.33, 0.46, 0.16]]
f [0.8091, 0.8111, 0.8117]
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 4 	 [0.27, 0.37, 0.2] 	 lr: 0.0001 	 f: 81.8069306930693
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 12 [0m
Train...

[32m iter_0[0m
ga 	p: 81.48 	r: 75.38 	f1: 78.31 	 13948 	 17119 	 18504
wo 	p: 92.54 	r: 86.74 	f1: 89.54 	 8816 	 9527 	 10164
ni 	p: 84.9 	r: 75.25 	f1: 79.79 	 3436 	 4047 	 4566

[32m iter_1[0m
ga 	p: 81.48 	r: 75.38 	f1: 78.31 	 13948 	 17119 	 18504
wo 	p: 92.54 	r: 86.74 	f1: 89.54 	 8816 	 9527 	 10164
ni 	p: 84.9 	r: 75.25 	f1: 79.79 	 3436 	 4047 	 4566

[32m iter_2[0m
ga 	p: 81.48 	r: 75.38 	f1: 78.31 	 13948 	 17119 	 18504
wo 	p: 92.54 	r: 86.74 	f1: 89.54 	 8816 	 9527 	 10164
ni 	p: 84.9 	r: 75.25 	f1: 79.79 	 3436 	 4047 	 4566
dev_num_of_high:  0
best_thres [0.34, 0.41, 0.12]
f [0.8197, 0.8197, 0.8197]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	best in epoch 8 	 [0.47, 0.68, 0.17] 	 lr: 0.0001 	 f: 83.15345490633916
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse
gpu: 0
6 3 2
vocab size:  29396
vocab size:  29396
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(2974.6658) lr: 0.0005 time: 2357.38
pred_count_train 41644

Test...
loss: tensor(437.6337) lr: 0.0001 time: 2691.25
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 78.97 	r: 73.52 	f1: 76.15 	 4535 	 5743 	 6168
wo 	p: 93.47 	r: 83.18 	f1: 88.02 	 2818 	 3015 	 3388
ni 	p: 85.26 	r: 78.65 	f1: 81.82 	 1197 	 1404 	 1522

[32m iter_1[0m
ga 	p: 79.26 	r: 73.15 	f1: 76.08 	 4512 	 5693 	 6168
wo 	p: 93.41 	r: 83.26 	f1: 88.05 	 2821 	 3020 	 3388
ni 	p: 87.84 	r: 77.33 	f1: 82.25 	 1177 	 1340 	 1522

[32m iter_2[0m
ga 	p: 79.24 	r: 73.2 	f1: 76.1 	 4515 	 5698 	 6168
wo 	p: 93.38 	r: 83.23 	f1: 88.01 	 2820 	 3020 	 3388
ni 	p: 87.84 	r: 77.4 	f1: 82.29 	 1178 	 1341 	 1522
best_thres [[0.46, 0.3, 0.19], [0.47, 0.29, 0.24], [0.47, 0.29, 0.24]]
f [0.8051, 0.8053, 0.8053]
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 1 	 [0.47, 0.29, 0.24] 	 lr: 0.0005 	 f: 80.53473578131889
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 2 [0m
Train...
loss: tensor(2967.6892) lr: 0.0005 time: 2029.61
pred_count_train 41644

Test...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       
[32m iter_0[0m
ga 	p: 78.85 	r: 73.15 	f1: 75.9 	 4512 	 5722 	 6168
wo 	p: 90.85 	r: 84.45 	f1: 87.53 	 2861 	 3149 	 3388
ni 	p: 85.1 	r: 78.78 	f1: 81.82 	 1199 	 1409 	 1522

[32m iter_1[0m
ga 	p: 77.75 	r: 73.41 	f1: 75.52 	 4528 	 5824 	 6168
wo 	p: 93.04 	r: 82.05 	f1: 87.2 	 2780 	 2988 	 3388
ni 	p: 82.45 	r: 79.96 	f1: 81.19 	 1217 	 1476 	 1522

[32m iter_2[0m
ga 	p: 78.18 	r: 73.48 	f1: 75.75 	 4532 	 5797 	 6168
wo 	p: 92.46 	r: 82.88 	f1: 87.41 	 2808 	 3037 	 3388
ni 	p: 81.92 	r: 80.68 	f1: 81.3 	 1228 	 1499 	 1522
best_thres [[0.48, 0.33, 0.22], [0.44, 0.4, 0.16], [0.46, 0.42, 0.15]]
f [0.8027, 0.8003, 0.8003]
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 1 	 [0.46, 0.42, 0.15] 	 lr: 0.0005 	 f: 80.03430264286271
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 2 [0m
Train...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      loss: tensor(2079.0710) lr: 0.0005 time: 2318.29
pred_count_train 41644

Test...
loss: tensor(2083.2156) lr: 0.0005 time: 2061.66
pred_count_train 41644

Test...
                                                                                
[32m iter_0[0m
ga 	p: 80.47 	r: 74.69 	f1: 77.47 	 4607 	 5725 	 6168
wo 	p: 91.92 	r: 85.01 	f1: 88.33 	 2880 	 3133 	 3388
ni 	p: 86.61 	r: 78.19 	f1: 82.18 	 1190 	 1374 	 1522

[32m iter_1[0m
ga 	p: 80.45 	r: 74.58 	f1: 77.4 	 4600 	 5718 	 6168
wo 	p: 91.75 	r: 85.06 	f1: 88.28 	 2882 	 3141 	 3388
ni 	p: 86.03 	r: 79.3 	f1: 82.53 	 1207 	 1403 	 1522

[32m iter_2[0m
ga 	p: 80.42 	r: 74.64 	f1: 77.42 	 4604 	 5725 	 6168
wo 	p: 91.73 	r: 85.15 	f1: 88.32 	 2885 	 3145 	 3388
ni 	p: 86.28 	r: 78.91 	f1: 82.43 	 1201 	 1392 	 1522
best_thres [[0.43, 0.37, 0.27], [0.43, 0.36, 0.26], [0.43, 0.36, 0.27]]
f [0.8144, 0.8143, 0.8144]
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 2 	 [0.43, 0.36, 0.27] 	 lr: 0.0005 	 f: 81.4377246444757
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 3 [0m
Train...
                                                                                  
[32m iter_0[0m
ga 	p: 80.3 	r: 74.98 	f1: 77.55 	 4625 	 5760 	 6168
wo 	p: 91.58 	r: 85.42 	f1: 88.39 	 2894 	 3160 	 3388
ni 	p: 84.1 	r: 78.91 	f1: 81.42 	 1201 	 1428 	 1522

[32m iter_1[0m
ga 	p: 80.4 	r: 74.56 	f1: 77.37 	 4599 	 5720 	 6168
wo 	p: 91.89 	r: 84.95 	f1: 88.28 	 2878 	 3132 	 3388
ni 	p: 80.73 	r: 81.73 	f1: 81.23 	 1244 	 1541 	 1522

[32m iter_2[0m
ga 	p: 80.79 	r: 74.45 	f1: 77.49 	 4592 	 5684 	 6168
wo 	p: 91.83 	r: 85.21 	f1: 88.4 	 2887 	 3144 	 3388
ni 	p: 80.37 	r: 82.33 	f1: 81.34 	 1253 	 1559 	 1522
best_thres [[0.36, 0.36, 0.3], [0.38, 0.36, 0.2], [0.4, 0.36, 0.19]]
f [0.814, 0.8132, 0.8133]
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 2 	 [0.4, 0.36, 0.19] 	 lr: 0.0005 	 f: 81.33059880053449
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 3 [0m
Train...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        loss: tensor(1807.7568) lr: 0.0005 time: 2301.63
pred_count_train 41644

Test...
loss: tensor(1817.0988) lr: 0.0005 time: 2078.44
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.27 	r: 74.3 	f1: 77.63 	 4583 	 5639 	 6168
wo 	p: 92.03 	r: 85.51 	f1: 88.65 	 2897 	 3148 	 3388
ni 	p: 85.57 	r: 77.92 	f1: 81.57 	 1186 	 1386 	 1522

[32m iter_1[0m
ga 	p: 80.08 	r: 75.28 	f1: 77.6 	 4643 	 5798 	 6168
wo 	p: 91.41 	r: 85.77 	f1: 88.5 	 2906 	 3179 	 3388
ni 	p: 85.84 	r: 78.06 	f1: 81.76 	 1188 	 1384 	 1522

[32m iter_2[0m
ga 	p: 80.54 	r: 74.59 	f1: 77.45 	 4601 	 5713 	 6168
wo 	p: 92.19 	r: 85.3 	f1: 88.61 	 2890 	 3135 	 3388
ni 	p: 84.49 	r: 79.11 	f1: 81.71 	 1204 	 1425 	 1522
best_thres [[0.43, 0.33, 0.15], [0.38, 0.29, 0.16], [0.41, 0.35, 0.14]]
f [0.8156, 0.8153, 0.815]
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 3 	 [0.41, 0.35, 0.14] 	 lr: 0.0005 	 f: 81.50403647663217
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 4 [0m
Train...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      
[32m iter_0[0m
ga 	p: 79.2 	r: 75.57 	f1: 77.34 	 4661 	 5885 	 6168
wo 	p: 92.68 	r: 84.42 	f1: 88.35 	 2860 	 3086 	 3388
ni 	p: 87.07 	r: 78.32 	f1: 82.46 	 1192 	 1369 	 1522

[32m iter_1[0m
ga 	p: 79.22 	r: 75.36 	f1: 77.24 	 4648 	 5867 	 6168
wo 	p: 92.54 	r: 84.56 	f1: 88.37 	 2865 	 3096 	 3388
ni 	p: 81.27 	r: 79.24 	f1: 80.24 	 1206 	 1484 	 1522

[32m iter_2[0m
ga 	p: 79.29 	r: 75.73 	f1: 77.47 	 4671 	 5891 	 6168
wo 	p: 93.15 	r: 84.27 	f1: 88.49 	 2855 	 3065 	 3388
ni 	p: 85.32 	r: 79.43 	f1: 82.27 	 1209 	 1417 	 1522
best_thres [[0.38, 0.36, 0.28], [0.37, 0.26, 0.13], [0.38, 0.36, 0.2]]
f [0.8136, 0.8119, 0.8127]
e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 2 	 [0.4, 0.36, 0.19] 	 lr: 0.0005 	 f: 81.33059880053449
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 4 [0m
Train...
loss: tensor(872.9434) lr: 2.5e-05 time: 2476.56
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.39 	r: 76.83 	f1: 78.57 	 4739 	 5895 	 6168
wo 	p: 91.56 	r: 87.75 	f1: 89.62 	 2973 	 3247 	 3388
ni 	p: 83.49 	r: 77.4 	f1: 80.33 	 1178 	 1411 	 1522

[32m iter_1[0m
ga 	p: 81.22 	r: 76.56 	f1: 78.82 	 4722 	 5814 	 6168
wo 	p: 93.64 	r: 86.07 	f1: 89.7 	 2916 	 3114 	 3388
ni 	p: 87.14 	r: 74.77 	f1: 80.48 	 1138 	 1306 	 1522

[32m iter_2[0m
ga 	p: 81.95 	r: 75.81 	f1: 78.76 	 4676 	 5706 	 6168
wo 	p: 93.34 	r: 86.42 	f1: 89.75 	 2928 	 3137 	 3388
ni 	p: 87.21 	r: 74.38 	f1: 80.28 	 1132 	 1298 	 1522
best_thres [[0.31, 0.43, 0.15], [0.32, 0.83, 0.17], [0.36, 0.82, 0.17]]
f [0.822, 0.8228, 0.823]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.1_it3_rs2016_preFalse 	current best epoch 7 	 [0.46, 0.47, 0.15] 	 lr: 0.0001 	 f: 82.79055216643204
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.1_it3_rs2016_preFalse[0m [33m epoch 14 [0m
Train...
loss: tensor(1635.9299) lr: 0.0005 time: 2320.77
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.13 	r: 76.23 	f1: 78.13 	 4702 	 5868 	 6168
wo 	p: 92.3 	r: 85.95 	f1: 89.01 	 2912 	 3155 	 3388
ni 	p: 83.81 	r: 78.91 	f1: 81.29 	 1201 	 1433 	 1522

[32m iter_1[0m
ga 	p: 80.62 	r: 76.28 	f1: 78.39 	 4705 	 5836 	 6168
wo 	p: 92.93 	r: 85.33 	f1: 88.97 	 2891 	 3111 	 3388
ni 	p: 85.63 	r: 78.32 	f1: 81.81 	 1192 	 1392 	 1522

[32m iter_2[0m
ga 	p: 79.73 	r: 77.11 	f1: 78.4 	 4756 	 5965 	 6168
wo 	p: 93.48 	r: 85.06 	f1: 89.07 	 2882 	 3083 	 3388
ni 	p: 85.95 	r: 77.99 	f1: 81.78 	 1187 	 1381 	 1522
best_thres [[0.3, 0.45, 0.18], [0.29, 0.51, 0.17], [0.25, 0.69, 0.17]]
f [0.8187, 0.8197, 0.82]
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 13 	 [0.34, 0.34, 0.1] 	 lr: 2.5e-05 	 f: 82.02331468575179
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 15 [0m
Train...
loss: tensor(1648.8529) lr: 0.0005 time: 2039.83
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.37 	r: 74.27 	f1: 76.73 	 4581 	 5772 	 6168
wo 	p: 91.27 	r: 86.07 	f1: 88.59 	 2916 	 3195 	 3388
ni 	p: 84.51 	r: 77.07 	f1: 80.62 	 1173 	 1388 	 1522

[32m iter_1[0m
ga 	p: 79.86 	r: 73.95 	f1: 76.79 	 4561 	 5711 	 6168
wo 	p: 90.62 	r: 86.42 	f1: 88.47 	 2928 	 3231 	 3388
ni 	p: 84.32 	r: 77.73 	f1: 80.89 	 1183 	 1403 	 1522

[32m iter_2[0m
ga 	p: 79.9 	r: 73.98 	f1: 76.82 	 4563 	 5711 	 6168
wo 	p: 91.5 	r: 85.77 	f1: 88.54 	 2906 	 3176 	 3388
ni 	p: 84.42 	r: 78.32 	f1: 81.25 	 1192 	 1412 	 1522
best_thres [[0.25, 0.47, 0.19], [0.27, 0.41, 0.2], [0.27, 0.49, 0.19]]
f [0.809, 0.8093, 0.8096]
e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 3 	 [0.41, 0.35, 0.14] 	 lr: 0.0005 	 f: 81.50403647663217
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 5 [0m
Train...

[32m iter_0[0m
ga 	p: 77.78 	r: 75.63 	f1: 76.69 	 4665 	 5998 	 6168
wo 	p: 90.69 	r: 86.01 	f1: 88.29 	 2914 	 3213 	 3388
ni 	p: 83.3 	r: 78.98 	f1: 81.08 	 1202 	 1443 	 1522

[32m iter_1[0m
ga 	p: 77.8 	r: 76.13 	f1: 76.96 	 4696 	 6036 	 6168
wo 	p: 91.55 	r: 85.09 	f1: 88.21 	 2883 	 3149 	 3388
ni 	p: 82.61 	r: 76.48 	f1: 79.43 	 1164 	 1409 	 1522

[32m iter_2[0m
ga 	p: 77.87 	r: 75.88 	f1: 76.86 	 4680 	 6010 	 6168
wo 	p: 91.41 	r: 85.45 	f1: 88.33 	 2895 	 3167 	 3388
ni 	p: 87.19 	r: 74.7 	f1: 80.47 	 1137 	 1304 	 1522
best_thres [[0.21, 0.38, 0.3], [0.22, 0.42, 0.24], [0.22, 0.43, 0.39]]
f [0.8081, 0.8075, 0.8077]
e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 2 	 [0.4, 0.36, 0.19] 	 lr: 0.0005 	 f: 81.33059880053449
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 5 [0m
Train...
loss: tensor(582.5453) lr: 5e-05 time: 2681.13
pred_count_train 41644

Test...
loss: tensor(735.7404) lr: 2.5e-05 time: 2692.37
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.12 	r: 76.46 	f1: 78.25 	 4716 	 5886 	 6168
wo 	p: 92.35 	r: 86.63 	f1: 89.4 	 2935 	 3178 	 3388
ni 	p: 84.63 	r: 75.62 	f1: 79.88 	 1151 	 1360 	 1522

[32m iter_1[0m
ga 	p: 81.4 	r: 75.23 	f1: 78.19 	 4640 	 5700 	 6168
wo 	p: 93.49 	r: 85.68 	f1: 89.42 	 2903 	 3105 	 3388
ni 	p: 87.83 	r: 73.52 	f1: 80.04 	 1119 	 1274 	 1522

[32m iter_2[0m
ga 	p: 81.35 	r: 75.26 	f1: 78.19 	 4642 	 5706 	 6168
wo 	p: 93.53 	r: 85.74 	f1: 89.47 	 2905 	 3106 	 3388
ni 	p: 88.69 	r: 73.13 	f1: 80.16 	 1113 	 1255 	 1522
best_thres [[0.25, 0.29, 0.23], [0.3, 0.37, 0.28], [0.29, 0.37, 0.3]]
f [0.8187, 0.8188, 0.8189]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.1_it3_rs2016_preFalse 	current best epoch 7 	 [0.46, 0.47, 0.15] 	 lr: 0.0001 	 f: 82.79055216643204
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.1_it3_rs2016_preFalse[0m [33m epoch 15 [0m
Train...
loss: tensor(1526.3645) lr: 0.0005 time: 2257.03
pred_count_train 41644

Test...
loss: tensor(1537.4417) lr: 0.0005 time: 1921.83
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.59 	r: 75.29 	f1: 77.38 	 4644 	 5835 	 6168
wo 	p: 92.04 	r: 86.04 	f1: 88.94 	 2915 	 3167 	 3388
ni 	p: 82.8 	r: 77.79 	f1: 80.22 	 1184 	 1430 	 1522

[32m iter_1[0m
ga 	p: 80.45 	r: 75.47 	f1: 77.88 	 4655 	 5786 	 6168
wo 	p: 91.85 	r: 86.19 	f1: 88.93 	 2920 	 3179 	 3388
ni 	p: 84.46 	r: 77.14 	f1: 80.63 	 1174 	 1390 	 1522

[32m iter_2[0m
ga 	p: 80.47 	r: 75.42 	f1: 77.86 	 4652 	 5781 	 6168
wo 	p: 91.9 	r: 86.07 	f1: 88.89 	 2916 	 3173 	 3388
ni 	p: 84.29 	r: 77.53 	f1: 80.77 	 1180 	 1400 	 1522
best_thres [[0.4, 0.44, 0.22], [0.39, 0.42, 0.2], [0.39, 0.42, 0.18]]
f [0.8129, 0.8147, 0.8152]
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 13 	 [0.34, 0.34, 0.1] 	 lr: 2.5e-05 	 f: 82.02331468575179
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 16 [0m
Train...

[32m iter_0[0m
ga 	p: 80.32 	r: 74.77 	f1: 77.45 	 4612 	 5742 	 6168
wo 	p: 91.88 	r: 85.12 	f1: 88.37 	 2884 	 3139 	 3388
ni 	p: 86.3 	r: 74.11 	f1: 79.75 	 1128 	 1307 	 1522

[32m iter_1[0m
ga 	p: 78.78 	r: 75.73 	f1: 77.23 	 4671 	 5929 	 6168
wo 	p: 91.09 	r: 85.36 	f1: 88.13 	 2892 	 3175 	 3388
ni 	p: 84.47 	r: 75.43 	f1: 79.69 	 1148 	 1359 	 1522

[32m iter_2[0m
ga 	p: 79.84 	r: 75.06 	f1: 77.38 	 4630 	 5799 	 6168
wo 	p: 92.39 	r: 84.53 	f1: 88.29 	 2864 	 3100 	 3388
ni 	p: 87.11 	r: 73.72 	f1: 79.86 	 1122 	 1288 	 1522
best_thres [[0.39, 0.55, 0.2], [0.33, 0.47, 0.16], [0.38, 0.61, 0.21]]
f [0.8111, 0.8099, 0.8101]
e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 3 	 [0.41, 0.35, 0.14] 	 lr: 0.0005 	 f: 81.50403647663217
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 6 [0m
Train...

[32m iter_0[0m
ga 	p: 82.5 	r: 72.24 	f1: 77.03 	 4456 	 5401 	 6168
wo 	p: 92.49 	r: 85.04 	f1: 88.61 	 2881 	 3115 	 3388
ni 	p: 85.11 	r: 75.1 	f1: 79.79 	 1143 	 1343 	 1522

[32m iter_1[0m
ga 	p: 80.07 	r: 74.4 	f1: 77.13 	 4589 	 5731 	 6168
wo 	p: 92.95 	r: 84.42 	f1: 88.48 	 2860 	 3077 	 3388
ni 	p: 83.49 	r: 75.43 	f1: 79.25 	 1148 	 1375 	 1522

[32m iter_2[0m
ga 	p: 80.2 	r: 74.51 	f1: 77.25 	 4596 	 5731 	 6168
wo 	p: 92.08 	r: 85.42 	f1: 88.62 	 2894 	 3143 	 3388
ni 	p: 83.89 	r: 75.62 	f1: 79.54 	 1151 	 1372 	 1522
best_thres [[0.43, 0.48, 0.27], [0.3, 0.48, 0.19], [0.33, 0.46, 0.21]]
f [0.81, 0.8094, 0.8097]
e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 2 	 [0.4, 0.36, 0.19] 	 lr: 0.0005 	 f: 81.33059880053449
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 6 [0m
Train...
loss: tensor(990.7980) lr: 0.0002 time: 7649.75
pred_count_train 41644

Test...
loss: tensor(424.1406) lr: 5e-05 time: 2597.61
pred_count_train 41644

Test...
loss: tensor(611.0557) lr: 2.5e-05 time: 2518.2
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.62 	r: 75.42 	f1: 77.94 	 4652 	 5770 	 6168
wo 	p: 93.53 	r: 85.77 	f1: 89.48 	 2906 	 3107 	 3388
ni 	p: 83.47 	r: 73.32 	f1: 78.07 	 1116 	 1337 	 1522

[32m iter_1[0m
ga 	p: 81.94 	r: 74.45 	f1: 78.02 	 4592 	 5604 	 6168
wo 	p: 92.73 	r: 86.16 	f1: 89.32 	 2919 	 3148 	 3388
ni 	p: 85.94 	r: 71.09 	f1: 77.81 	 1082 	 1259 	 1522

[32m iter_2[0m
ga 	p: 81.97 	r: 74.5 	f1: 78.05 	 4595 	 5606 	 6168
wo 	p: 92.2 	r: 86.48 	f1: 89.25 	 2930 	 3178 	 3388
ni 	p: 83.13 	r: 73.19 	f1: 77.85 	 1114 	 1340 	 1522
best_thres [[0.37, 0.58, 0.19], [0.49, 0.68, 0.19], [0.51, 0.56, 0.12]]
f [0.8148, 0.8148, 0.8149]
load model: epoch7
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.1_it3_rs2016_preFalse 	current best epoch 7 	 [0.46, 0.47, 0.15] 	 lr: 0.0001 	 f: 82.79055216643204
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.1_it3_rs2016_preFalse[0m [33m epoch 16 [0m
Train...
loss: tensor(1470.2507) lr: 0.0005 time: 2491.53
pred_count_train 41644

Test...
loss: tensor(1486.3264) lr: 0.0005 time: 2907.75
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.9 	r: 74.56 	f1: 77.14 	 4599 	 5756 	 6168
wo 	p: 92.31 	r: 85.42 	f1: 88.73 	 2894 	 3135 	 3388
ni 	p: 81.64 	r: 77.99 	f1: 79.77 	 1187 	 1454 	 1522

[32m iter_1[0m
ga 	p: 79.1 	r: 75.86 	f1: 77.45 	 4679 	 5915 	 6168
wo 	p: 92.33 	r: 85.27 	f1: 88.66 	 2889 	 3129 	 3388
ni 	p: 83.37 	r: 77.73 	f1: 80.45 	 1183 	 1419 	 1522

[32m iter_2[0m
ga 	p: 79.05 	r: 76.02 	f1: 77.5 	 4689 	 5932 	 6168
wo 	p: 92.46 	r: 85.01 	f1: 88.57 	 2880 	 3115 	 3388
ni 	p: 83.16 	r: 77.86 	f1: 80.42 	 1185 	 1425 	 1522
best_thres [[0.27, 0.52, 0.13], [0.19, 0.67, 0.1], [0.18, 0.81, 0.09]]
f [0.8103, 0.8114, 0.8118]
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 13 	 [0.34, 0.34, 0.1] 	 lr: 2.5e-05 	 f: 82.02331468575179
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 17 [0m
Train...

[32m iter_0[0m
ga 	p: 80.88 	r: 72.03 	f1: 76.2 	 4443 	 5493 	 6168
wo 	p: 93.09 	r: 83.06 	f1: 87.79 	 2814 	 3023 	 3388
ni 	p: 84.94 	r: 78.19 	f1: 81.42 	 1190 	 1401 	 1522

[32m iter_1[0m
ga 	p: 77.63 	r: 74.82 	f1: 76.2 	 4615 	 5945 	 6168
wo 	p: 92.75 	r: 83.12 	f1: 87.67 	 2816 	 3036 	 3388
ni 	p: 82.74 	r: 79.04 	f1: 80.85 	 1203 	 1454 	 1522

[32m iter_2[0m
ga 	p: 77.87 	r: 74.68 	f1: 76.24 	 4606 	 5915 	 6168
wo 	p: 93.09 	r: 83.06 	f1: 87.79 	 2814 	 3023 	 3388
ni 	p: 82.27 	r: 79.57 	f1: 80.9 	 1211 	 1472 	 1522
best_thres [[0.52, 0.51, 0.14], [0.36, 0.48, 0.13], [0.37, 0.52, 0.12]]
f [0.8047, 0.8037, 0.8036]
e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 3 	 [0.41, 0.35, 0.14] 	 lr: 0.0005 	 f: 81.50403647663217
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 7 [0m
Train...
loss: tensor(974.2321) lr: 2.5e-05 time: 2280.53
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 76.64 	r: 77.33 	f1: 76.99 	 4770 	 6224 	 6168
wo 	p: 92.84 	r: 83.41 	f1: 87.87 	 2826 	 3044 	 3388
ni 	p: 82.38 	r: 79.57 	f1: 80.95 	 1211 	 1470 	 1522

[32m iter_1[0m
ga 	p: 79.1 	r: 74.68 	f1: 76.82 	 4606 	 5823 	 6168
wo 	p: 92.88 	r: 82.02 	f1: 87.12 	 2779 	 2992 	 3388
ni 	p: 84.19 	r: 79.43 	f1: 81.74 	 1209 	 1436 	 1522

[32m iter_2[0m
ga 	p: 79.11 	r: 75.08 	f1: 77.04 	 4631 	 5854 	 6168
wo 	p: 91.61 	r: 84.12 	f1: 87.71 	 2850 	 3111 	 3388
ni 	p: 84.97 	r: 78.38 	f1: 81.54 	 1193 	 1404 	 1522
best_thres [[0.27, 0.57, 0.08], [0.39, 0.52, 0.09], [0.4, 0.48, 0.1]]
f [0.8074, 0.8066, 0.8074]
load model: epoch2
e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 2 	 [0.4, 0.36, 0.19] 	 lr: 0.0005 	 f: 81.33059880053449
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 7 [0m
Train...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                loss: tensor(1449.3973) lr: 0.0005 time: 2448.64
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.74 	r: 73.59 	f1: 77.0 	 4539 	 5622 	 6168
wo 	p: 92.34 	r: 84.68 	f1: 88.34 	 2869 	 3107 	 3388
ni 	p: 83.98 	r: 74.05 	f1: 78.7 	 1127 	 1342 	 1522

[32m iter_1[0m
ga 	p: 81.67 	r: 73.39 	f1: 77.31 	 4527 	 5543 	 6168
wo 	p: 92.84 	r: 84.53 	f1: 88.49 	 2864 	 3085 	 3388
ni 	p: 83.91 	r: 75.36 	f1: 79.4 	 1147 	 1367 	 1522

[32m iter_2[0m
ga 	p: 81.53 	r: 73.57 	f1: 77.35 	 4538 	 5566 	 6168
wo 	p: 92.3 	r: 84.95 	f1: 88.47 	 2878 	 3118 	 3388
ni 	p: 83.26 	r: 75.82 	f1: 79.37 	 1154 	 1386 	 1522
best_thres [[0.47, 0.56, 0.18], [0.59, 0.8, 0.12], [0.63, 0.74, 0.1]]
f [0.8071, 0.8087, 0.8093]
load model: epoch13
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 13 	 [0.34, 0.34, 0.1] 	 lr: 2.5e-05 	 f: 82.02331468575179
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 18 [0m
Train...
loss: tensor(1528.6338) lr: 0.00025 time: 2790.99
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.81 	r: 74.08 	f1: 77.3 	 4569 	 5654 	 6168
wo 	p: 92.9 	r: 83.83 	f1: 88.13 	 2840 	 3057 	 3388
ni 	p: 83.63 	r: 74.84 	f1: 78.99 	 1139 	 1362 	 1522

[32m iter_1[0m
ga 	p: 81.66 	r: 73.36 	f1: 77.29 	 4525 	 5541 	 6168
wo 	p: 91.67 	r: 84.15 	f1: 87.75 	 2851 	 3110 	 3388
ni 	p: 86.31 	r: 73.72 	f1: 79.52 	 1122 	 1300 	 1522

[32m iter_2[0m
ga 	p: 78.2 	r: 76.23 	f1: 77.2 	 4702 	 6013 	 6168
wo 	p: 92.64 	r: 83.91 	f1: 88.06 	 2843 	 3069 	 3388
ni 	p: 87.78 	r: 73.13 	f1: 79.78 	 1113 	 1268 	 1522
best_thres [[0.49, 0.43, 0.08], [0.52, 0.34, 0.11], [0.36, 0.41, 0.12]]
f [0.8083, 0.8083, 0.8082]
load model: epoch3
e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 3 	 [0.41, 0.35, 0.14] 	 lr: 0.0005 	 f: 81.50403647663217
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 8 [0m
Train...
loss: tensor(804.6561) lr: 2.5e-05 time: 2622.49
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.15 	r: 76.85 	f1: 78.94 	 4740 	 5841 	 6168
wo 	p: 94.35 	r: 85.68 	f1: 89.81 	 2903 	 3077 	 3388
ni 	p: 84.66 	r: 78.71 	f1: 81.58 	 1198 	 1415 	 1522

[32m iter_1[0m
ga 	p: 83.23 	r: 74.43 	f1: 78.59 	 4591 	 5516 	 6168
wo 	p: 91.37 	r: 86.51 	f1: 88.87 	 2931 	 3208 	 3388
ni 	p: 85.73 	r: 78.98 	f1: 82.22 	 1202 	 1402 	 1522

[32m iter_2[0m
ga 	p: 80.95 	r: 76.73 	f1: 78.78 	 4733 	 5847 	 6168
wo 	p: 93.97 	r: 86.01 	f1: 89.81 	 2914 	 3101 	 3388
ni 	p: 86.1 	r: 78.52 	f1: 82.13 	 1195 	 1388 	 1522
best_thres [[0.43, 0.64, 0.13], [0.52, 0.33, 0.13], [0.43, 0.58, 0.14]]
f [0.8258, 0.8244, 0.8248]
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 7 	 [0.43, 0.58, 0.14] 	 lr: 0.00025 	 f: 82.48449921129487
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 8 [0m
Train...
loss: tensor(829.9339) lr: 1.25e-05 time: 2728.36
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 85.78 	r: 73.46 	f1: 79.14 	 4531 	 5282 	 6168
wo 	p: 92.43 	r: 87.22 	f1: 89.75 	 2955 	 3197 	 3388
ni 	p: 84.2 	r: 77.4 	f1: 80.66 	 1178 	 1399 	 1522

[32m iter_1[0m
ga 	p: 85.75 	r: 73.46 	f1: 79.13 	 4531 	 5284 	 6168
wo 	p: 93.83 	r: 86.16 	f1: 89.83 	 2919 	 3111 	 3388
ni 	p: 85.94 	r: 75.89 	f1: 80.6 	 1155 	 1344 	 1522

[32m iter_2[0m
ga 	p: 82.54 	r: 76.04 	f1: 79.16 	 4690 	 5682 	 6168
wo 	p: 93.54 	r: 86.39 	f1: 89.83 	 2927 	 3129 	 3388
ni 	p: 85.66 	r: 76.15 	f1: 80.63 	 1159 	 1353 	 1522
best_thres [[0.55, 0.38, 0.13], [0.7, 0.67, 0.13], [0.4, 0.63, 0.12]]
f [0.8269, 0.8268, 0.8266]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.1_it3_rs2016_preFalse 	current best epoch 16 	 [0.34, 0.3, 0.16] 	 lr: 2.5e-05 	 f: 83.05701577031944
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.1_it3_rs2016_preFalse[0m [33m epoch 18 [0m
Train...
loss: tensor(1332.0710) lr: 0.00025 time: 2466.86
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.75 	r: 76.69 	f1: 78.19 	 4730 	 5931 	 6168
wo 	p: 91.29 	r: 87.28 	f1: 89.24 	 2957 	 3239 	 3388
ni 	p: 86.29 	r: 76.48 	f1: 81.09 	 1164 	 1349 	 1522

[32m iter_1[0m
ga 	p: 80.75 	r: 76.44 	f1: 78.54 	 4715 	 5839 	 6168
wo 	p: 91.49 	r: 87.31 	f1: 89.35 	 2958 	 3233 	 3388
ni 	p: 86.09 	r: 77.66 	f1: 81.66 	 1182 	 1373 	 1522

[32m iter_2[0m
ga 	p: 79.94 	r: 77.27 	f1: 78.58 	 4766 	 5962 	 6168
wo 	p: 91.5 	r: 87.1 	f1: 89.25 	 2951 	 3225 	 3388
ni 	p: 87.12 	r: 76.87 	f1: 81.68 	 1170 	 1343 	 1522
best_thres [[0.31, 0.3, 0.16], [0.32, 0.28, 0.11], [0.28, 0.28, 0.12]]
f [0.8197, 0.8212, 0.8217]
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 18 	 [0.28, 0.28, 0.12] 	 lr: 1.25e-05 	 f: 82.168458781362
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 19 [0m
Train...
loss: tensor(1142.4443) lr: 0.00025 time: 2756.94
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.16 	r: 75.92 	f1: 78.46 	 4683 	 5770 	 6168
wo 	p: 92.45 	r: 86.39 	f1: 89.32 	 2927 	 3166 	 3388
ni 	p: 86.53 	r: 78.52 	f1: 82.33 	 1195 	 1381 	 1522

[32m iter_1[0m
ga 	p: 81.58 	r: 75.49 	f1: 78.42 	 4656 	 5707 	 6168
wo 	p: 93.29 	r: 85.36 	f1: 89.15 	 2892 	 3100 	 3388
ni 	p: 83.24 	r: 80.95 	f1: 82.08 	 1232 	 1480 	 1522

[32m iter_2[0m
ga 	p: 81.91 	r: 75.26 	f1: 78.45 	 4642 	 5667 	 6168
wo 	p: 93.19 	r: 85.66 	f1: 89.26 	 2902 	 3114 	 3388
ni 	p: 84.62 	r: 79.5 	f1: 81.98 	 1210 	 1430 	 1522
best_thres [[0.48, 0.39, 0.15], [0.5, 0.46, 0.1], [0.53, 0.46, 0.13]]
f [0.8231, 0.8225, 0.8225]
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 8 	 [0.53, 0.46, 0.13] 	 lr: 0.00025 	 f: 82.2464050960983
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 9 [0m
Train...
loss: tensor(662.2054) lr: 2.5e-05 time: 2622.19
pred_count_train 41644

Test...
loss: tensor(749.4280) lr: 0.0002 time: 7679.06
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.08 	r: 75.68 	f1: 78.29 	 4668 	 5757 	 6168
wo 	p: 92.32 	r: 87.6 	f1: 89.9 	 2968 	 3215 	 3388
ni 	p: 84.51 	r: 77.07 	f1: 80.62 	 1173 	 1388 	 1522

[32m iter_1[0m
ga 	p: 80.3 	r: 76.26 	f1: 78.23 	 4704 	 5858 	 6168
wo 	p: 92.66 	r: 87.19 	f1: 89.84 	 2954 	 3188 	 3388
ni 	p: 83.59 	r: 78.98 	f1: 81.22 	 1202 	 1438 	 1522

[32m iter_2[0m
ga 	p: 82.99 	r: 74.45 	f1: 78.49 	 4592 	 5533 	 6168
wo 	p: 92.46 	r: 87.57 	f1: 89.95 	 2967 	 3209 	 3388
ni 	p: 85.93 	r: 77.07 	f1: 81.26 	 1173 	 1365 	 1522
best_thres [[0.46, 0.44, 0.12], [0.4, 0.43, 0.09], [0.58, 0.44, 0.12]]
f [0.8218, 0.8218, 0.8227]
e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 7 	 [0.43, 0.58, 0.14] 	 lr: 0.00025 	 f: 82.48449921129487
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 9 [0m
Train...
loss: tensor(745.8234) lr: 1.25e-05 time: 2555.24
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.66 	r: 75.57 	f1: 78.95 	 4661 	 5639 	 6168
wo 	p: 91.43 	r: 87.57 	f1: 89.46 	 2967 	 3245 	 3388
ni 	p: 84.65 	r: 77.53 	f1: 80.93 	 1180 	 1394 	 1522

[32m iter_1[0m
ga 	p: 81.87 	r: 76.23 	f1: 78.95 	 4702 	 5743 	 6168
wo 	p: 93.2 	r: 86.51 	f1: 89.73 	 2931 	 3145 	 3388
ni 	p: 86.52 	r: 75.49 	f1: 80.63 	 1149 	 1328 	 1522

[32m iter_2[0m
ga 	p: 82.61 	r: 75.7 	f1: 79.0 	 4669 	 5652 	 6168
wo 	p: 93.14 	r: 86.57 	f1: 89.74 	 2933 	 3149 	 3388
ni 	p: 87.0 	r: 75.16 	f1: 80.65 	 1144 	 1315 	 1522
best_thres [[0.44, 0.3, 0.13], [0.4, 0.43, 0.12], [0.44, 0.43, 0.12]]
f [0.8249, 0.8249, 0.825]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.1_it3_rs2016_preFalse 	current best epoch 16 	 [0.34, 0.3, 0.16] 	 lr: 2.5e-05 	 f: 83.05701577031944
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.1_it3_rs2016_preFalse[0m [33m epoch 19 [0m
Train...
loss: tensor(974.7835) lr: 0.00025 time: 2635.05
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.12 	r: 75.1 	f1: 77.99 	 4632 	 5710 	 6168
wo 	p: 92.51 	r: 85.98 	f1: 89.12 	 2913 	 3149 	 3388
ni 	p: 82.06 	r: 79.04 	f1: 80.52 	 1203 	 1466 	 1522

[32m iter_1[0m
ga 	p: 82.83 	r: 73.98 	f1: 78.15 	 4563 	 5509 	 6168
wo 	p: 91.77 	r: 86.6 	f1: 89.11 	 2934 	 3197 	 3388
ni 	p: 83.47 	r: 79.3 	f1: 81.33 	 1207 	 1446 	 1522

[32m iter_2[0m
ga 	p: 82.93 	r: 74.14 	f1: 78.29 	 4573 	 5514 	 6168
wo 	p: 91.29 	r: 86.98 	f1: 89.09 	 2947 	 3228 	 3388
ni 	p: 83.41 	r: 79.3 	f1: 81.31 	 1207 	 1447 	 1522
best_thres [[0.37, 0.4, 0.15], [0.43, 0.3, 0.12], [0.43, 0.25, 0.11]]
f [0.8175, 0.8187, 0.8194]
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 18 	 [0.28, 0.28, 0.12] 	 lr: 1.25e-05 	 f: 82.168458781362
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 20 [0m
Train...
loss: tensor(542.9436) lr: 2.5e-05 time: 2245.35
pred_count_train 41644

Test...
loss: tensor(851.6852) lr: 0.00025 time: 2794.53
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.56 	r: 75.89 	f1: 78.63 	 4681 	 5739 	 6168
wo 	p: 93.2 	r: 85.71 	f1: 89.3 	 2904 	 3116 	 3388
ni 	p: 84.88 	r: 74.11 	f1: 79.13 	 1128 	 1329 	 1522

[32m iter_1[0m
ga 	p: 81.86 	r: 75.49 	f1: 78.54 	 4656 	 5688 	 6168
wo 	p: 93.37 	r: 85.57 	f1: 89.3 	 2899 	 3105 	 3388
ni 	p: 79.68 	r: 78.58 	f1: 79.13 	 1196 	 1501 	 1522

[32m iter_2[0m
ga 	p: 81.77 	r: 75.55 	f1: 78.54 	 4660 	 5699 	 6168
wo 	p: 93.64 	r: 85.24 	f1: 89.25 	 2888 	 3084 	 3388
ni 	p: 79.21 	r: 79.11 	f1: 79.16 	 1204 	 1520 	 1522
best_thres [[0.39, 0.53, 0.22], [0.4, 0.53, 0.12], [0.4, 0.59, 0.11]]
f [0.8196, 0.8193, 0.8191]
e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 8 	 [0.53, 0.46, 0.13] 	 lr: 0.00025 	 f: 82.2464050960983
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 10 [0m
Train...
loss: tensor(671.1130) lr: 1.25e-05 time: 2270.15
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.03 	r: 76.3 	f1: 78.59 	 4706 	 5808 	 6168
wo 	p: 92.9 	r: 86.07 	f1: 89.35 	 2916 	 3139 	 3388
ni 	p: 83.73 	r: 74.05 	f1: 78.59 	 1127 	 1346 	 1522

[32m iter_1[0m
ga 	p: 81.54 	r: 75.83 	f1: 78.58 	 4677 	 5736 	 6168
wo 	p: 92.18 	r: 86.69 	f1: 89.35 	 2937 	 3186 	 3388
ni 	p: 84.49 	r: 73.72 	f1: 78.74 	 1122 	 1328 	 1522

[32m iter_2[0m
ga 	p: 81.65 	r: 75.68 	f1: 78.55 	 4668 	 5717 	 6168
wo 	p: 91.74 	r: 86.92 	f1: 89.27 	 2945 	 3210 	 3388
ni 	p: 84.13 	r: 74.18 	f1: 78.84 	 1129 	 1342 	 1522
best_thres [[0.3, 0.41, 0.13], [0.31, 0.33, 0.1], [0.32, 0.28, 0.09]]
f [0.8188, 0.819, 0.819]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.1_it3_rs2016_preFalse 	current best epoch 16 	 [0.34, 0.3, 0.16] 	 lr: 2.5e-05 	 f: 83.05701577031944
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.1_it3_rs2016_preFalse[0m [33m epoch 20 [0m
Train...

[32m iter_0[0m
ga 	p: 81.73 	r: 74.98 	f1: 78.21 	 4625 	 5659 	 6168
wo 	p: 92.99 	r: 85.8 	f1: 89.25 	 2907 	 3126 	 3388
ni 	p: 78.81 	r: 74.77 	f1: 76.74 	 1138 	 1444 	 1522

[32m iter_1[0m
ga 	p: 82.84 	r: 74.45 	f1: 78.42 	 4592 	 5543 	 6168
wo 	p: 92.74 	r: 86.3 	f1: 89.41 	 2924 	 3153 	 3388
ni 	p: 79.28 	r: 75.16 	f1: 77.17 	 1144 	 1443 	 1522

[32m iter_2[0m
ga 	p: 82.68 	r: 74.63 	f1: 78.45 	 4603 	 5567 	 6168
wo 	p: 93.43 	r: 85.66 	f1: 89.37 	 2902 	 3106 	 3388
ni 	p: 79.33 	r: 75.16 	f1: 77.19 	 1144 	 1442 	 1522

[32m iter_3[0m
ga 	p: 82.68 	r: 74.46 	f1: 78.36 	 4593 	 5555 	 6168
wo 	p: 93.53 	r: 85.71 	f1: 89.45 	 2904 	 3105 	 3388
ni 	p: 79.61 	r: 75.16 	f1: 77.32 	 1144 	 1437 	 1522

[32m iter_4[0m
ga 	p: 82.38 	r: 74.87 	f1: 78.44 	 4618 	 5606 	 6168
wo 	p: 93.44 	r: 85.71 	f1: 89.41 	 2904 	 3108 	 3388
ni 	p: 79.53 	r: 75.3 	f1: 77.35 	 1146 	 1441 	 1522

[32m iter_5[0m
ga 	p: 82.86 	r: 74.32 	f1: 78.36 	 4584 	 5532 	 6168
wo 	p: 93.52 	r: 85.68 	f1: 89.43 	 2903 	 3104 	 3388
ni 	p: 79.61 	r: 75.16 	f1: 77.32 	 1144 	 1437 	 1522

[32m iter_6[0m
ga 	p: 82.38 	r: 74.89 	f1: 78.45 	 4619 	 5607 	 6168
wo 	p: 93.47 	r: 85.77 	f1: 89.46 	 2906 	 3109 	 3388
ni 	p: 79.51 	r: 75.23 	f1: 77.31 	 1145 	 1440 	 1522

[32m iter_7[0m
ga 	p: 82.85 	r: 74.34 	f1: 78.36 	 4585 	 5534 	 6168
wo 	p: 93.53 	r: 85.71 	f1: 89.45 	 2904 	 3105 	 3388
ni 	p: 79.62 	r: 75.23 	f1: 77.36 	 1145 	 1438 	 1522

[32m iter_8[0m
ga 	p: 82.36 	r: 74.85 	f1: 78.43 	 4617 	 5606 	 6168
wo 	p: 93.44 	r: 85.71 	f1: 89.41 	 2904 	 3108 	 3388
ni 	p: 79.51 	r: 75.23 	f1: 77.31 	 1145 	 1440 	 1522

[32m iter_9[0m
ga 	p: 82.87 	r: 74.34 	f1: 78.37 	 4585 	 5533 	 6168
wo 	p: 93.56 	r: 85.71 	f1: 89.46 	 2904 	 3104 	 3388
ni 	p: 79.61 	r: 75.16 	f1: 77.32 	 1144 	 1437 	 1522
best_thres [[0.65, 0.79, 0.08], [0.69, 0.74, 0.07], [0.69, 0.85, 0.07], [0.69, 0.85, 0.07], [0.67, 0.85, 0.07], [0.7, 0.85, 0.07], [0.67, 0.85, 0.07], [0.7, 0.85, 0.07], [0.67, 0.85, 0.07], [0.7, 0.85, 0.07]]
f [0.8138, 0.8151, 0.8154, 0.8156, 0.8158, 0.8158, 0.816, 0.816, 0.816, 0.8161]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse 	current best epoch 4 	 [0.27, 0.56, 0.3] 	 lr: 0.0002 	 f: 82.13793103448276
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse[0m [33m epoch 7 [0m
Train...
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth2_adam_lr0.0002_du0.1_dh0.0_True_size1_sub0_th0.8_it10_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
> [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/train_edit.py[0m(128)[0;36mtrain[0;34m()[0m
[0;32m    127 [0;31m           [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 128 [0;31m           [0mpred[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    129 [0;31m           [0;32mfor[0m [0mbatch[0m [0;32min[0m [0mscores[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/train_edit.py[0m(129)[0;36mtrain[0;34m()[0m
[0;32m    128 [0;31m           [0mpred[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 129 [0;31m           [0;32mfor[0m [0mbatch[0m [0;32min[0m [0mscores[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    130 [0;31m               [0mpr[0m [0;34m=[0m [0;34m[[0m[0mint[0m[0;34m([0m[0mtorch[0m[0;34m.[0m[0margmax[0m[0;34m([0m[0mword[0m[0;34m)[0m[0;34m)[0m [0;32mfor[0m [0mword[0m [0;32min[0m [0mbatch[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/train_edit.py[0m(130)[0;36mtrain[0;34m()[0m
[0;32m    129 [0;31m           [0;32mfor[0m [0mbatch[0m [0;32min[0m [0mscores[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m               [0mpr[0m [0;34m=[0m [0;34m[[0m[0mint[0m[0;34m([0m[0mtorch[0m[0;34m.[0m[0margmax[0m[0;34m([0m[0mword[0m[0;34m)[0m[0;34m)[0m [0;32mfor[0m [0mword[0m [0;32min[0m [0mbatch[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m               [0mpred[0m[0;34m.[0m[0mappend[0m[0;34m([0m[0mpr[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/train_edit.py[0m(127)[0;36mtrain[0;34m()[0m
[0;32m    126 [0;31m[0;34m[0m[0m
[0m[0;32m--> 127 [0;31m           [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    128 [0;31m           [0mpred[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/train_edit.py[0m(128)[0;36mtrain[0;34m()[0m
[0;32m    127 [0;31m           [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 128 [0;31m           [0mpred[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    129 [0;31m           [0;32mfor[0m [0mbatch[0m [0;32min[0m [0mscores[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> [[3, 3, 3, 2, 3, 2, 3, 3]]
ipdb> tensor([[3, 0, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3]], device='cuda:0')
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/train_edit.py[0m(129)[0;36mtrain[0;34m()[0m
[0;32m    128 [0;31m           [0mpred[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 129 [0;31m           [0;32mfor[0m [0mbatch[0m [0;32min[0m [0mscores[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    130 [0;31m               [0mpr[0m [0;34m=[0m [0;34m[[0m[0mint[0m[0;34m([0m[0mtorch[0m[0;34m.[0m[0margmax[0m[0;34m([0m[0mword[0m[0;34m)[0m[0;34m)[0m [0;32mfor[0m [0mword[0m [0;32min[0m [0mbatch[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/train_edit.py[0m(130)[0;36mtrain[0;34m()[0m
[0;32m    129 [0;31m           [0;32mfor[0m [0mbatch[0m [0;32min[0m [0mscores[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 130 [0;31m               [0mpr[0m [0;34m=[0m [0;34m[[0m[0mint[0m[0;34m([0m[0mtorch[0m[0;34m.[0m[0margmax[0m[0;34m([0m[0mword[0m[0;34m)[0m[0;34m)[0m [0;32mfor[0m [0mword[0m [0;32min[0m [0mbatch[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    131 [0;31m               [0mpred[0m[0;34m.[0m[0mappend[0m[0;34m([0m[0mpr[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/train_edit.py[0m(131)[0;36mtrain[0;34m()[0m
[0;32m    130 [0;31m               [0mpr[0m [0;34m=[0m [0;34m[[0m[0mint[0m[0;34m([0m[0mtorch[0m[0;34m.[0m[0margmax[0m[0;34m([0m[0mword[0m[0;34m)[0m[0;34m)[0m [0;32mfor[0m [0mword[0m [0;32min[0m [0mbatch[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 131 [0;31m               [0mpred[0m[0;34m.[0m[0mappend[0m[0;34m([0m[0mpr[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    132 [0;31m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/train_edit.py[0m(129)[0;36mtrain[0;34m()[0m
[0;32m    128 [0;31m           [0mpred[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 129 [0;31m           [0;32mfor[0m [0mbatch[0m [0;32min[0m [0mscores[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    130 [0;31m               [0mpr[0m [0;34m=[0m [0;34m[[0m[0mint[0m[0;34m([0m[0mtorch[0m[0;34m.[0m[0margmax[0m[0;34m([0m[0mword[0m[0;34m)[0m[0;34m)[0m [0;32mfor[0m [0mword[0m [0;32min[0m [0mbatch[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/train_edit.py[0m(133)[0;36mtrain[0;34m()[0m
[0;32m    132 [0;31m[0;34m[0m[0m
[0m[0;32m--> 133 [0;31m           [0mcount[0m [0;34m+=[0m [0;36m1[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    134 [0;31m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/train_edit.py[0m(135)[0;36mtrain[0;34m()[0m
[0;32m    134 [0;31m[0;34m[0m[0m
[0m[0;32m--> 135 [0;31m           [0mloss[0m [0;34m=[0m [0;36m0[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    136 [0;31m           [0;32mfor[0m [0mi[0m [0;32min[0m [0mrange[0m[0;34m([0m[0myss[0m[0;34m.[0m[0msize[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/train_edit.py[0m(136)[0;36mtrain[0;34m()[0m
[0;32m    135 [0;31m           [0mloss[0m [0;34m=[0m [0;36m0[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 136 [0;31m           [0;32mfor[0m [0mi[0m [0;32min[0m [0mrange[0m[0;34m([0m[0myss[0m[0;34m.[0m[0msize[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    137 [0;31m               [0mloss[0m [0;34m+=[0m [0mloss_function[0m[0;34m([0m[0mscores[0m[0;34m[[0m[0mi[0m[0;34m][0m[0;34m,[0m [0myss[0m[0;34m[[0m[0mi[0m[0;34m][0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/train_edit.py[0m(137)[0;36mtrain[0;34m()[0m
[0;32m    136 [0;31m           [0;32mfor[0m [0mi[0m [0;32min[0m [0mrange[0m[0;34m([0m[0myss[0m[0;34m.[0m[0msize[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 137 [0;31m               [0mloss[0m [0;34m+=[0m [0mloss_function[0m[0;34m([0m[0mscores[0m[0;34m[[0m[0mi[0m[0;34m][0m[0;34m,[0m [0myss[0m[0;34m[[0m[0mi[0m[0;34m][0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    138 [0;31m           [0mloss[0m[0;34m.[0m[0mbackward[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> [[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]]
ipdb> tensor([[3, 0, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3]], device='cuda:0')
ipdb> 1
ipdb> 12
ipdb> torch.Size([1, 12])
ipdb> Exiting Debugger.
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth2_adam_lr0.0002_du0.1_dh0.0_True_size1_sub0_th0.8_it10_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth2_adam_lr0.0002_du0.1_dh0.0_True_size1_sub0_th0.8_it10_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth2_adam_lr0.0002_du0.1_dh0.0_True_size1_sub0_th0.8_it10_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth2_adam_lr0.0002_du0.1_dh0.0_True_size1_sub0_th0.8_it10_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
> [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/train_edit.py[0m(135)[0;36mtrain[0;34m()[0m
[0;32m    134 [0;31m                [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 135 [0;31m                [0;32mif[0m [0;32mnot[0m [0myss[0m [0;34m==[0m [0mpred[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    136 [0;31m                    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> tensor([[3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 1, 2],
        [3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 1, 2]], device='cuda:0')
ipdb> tensor([[3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3],
        [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3]], device='cuda:0')
ipdb> tensor([[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0',
       dtype=torch.uint8)
ipdb> Exiting Debugger.

[32m iter_0[0m
ga 	p: 80.3 	r: 74.46 	f1: 77.27 	 4593 	 5720 	 6168
wo 	p: 91.19 	r: 86.75 	f1: 88.91 	 2939 	 3223 	 3388
ni 	p: 82.75 	r: 74.7 	f1: 78.52 	 1137 	 1374 	 1522

[32m iter_1[0m
ga 	p: 81.96 	r: 73.59 	f1: 77.55 	 4539 	 5538 	 6168
wo 	p: 91.95 	r: 85.98 	f1: 88.87 	 2913 	 3168 	 3388
ni 	p: 82.83 	r: 75.76 	f1: 79.14 	 1153 	 1392 	 1522

[32m iter_2[0m
ga 	p: 78.98 	r: 76.33 	f1: 77.63 	 4708 	 5961 	 6168
wo 	p: 92.18 	r: 85.98 	f1: 88.97 	 2913 	 3160 	 3388
ni 	p: 83.44 	r: 75.49 	f1: 79.27 	 1149 	 1377 	 1522
best_thres [[0.38, 0.35, 0.17], [0.44, 0.41, 0.12], [0.26, 0.41, 0.12]]
f [0.8104, 0.8115, 0.812]
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 18 	 [0.28, 0.28, 0.12] 	 lr: 1.25e-05 	 f: 82.168458781362
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 21 [0m
Train...
loss: tensor(737.7108) lr: 0.00025 time: 3000.41
pred_count_train 41644

Test...
loss: tensor(427.8889) lr: 2.5e-05 time: 2565.62
pred_count_train 41644

Test...
loss: tensor(671.9662) lr: 0.00025 time: 2782.66
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.55 	r: 75.83 	f1: 78.12 	 4677 	 5806 	 6168
wo 	p: 91.34 	r: 86.51 	f1: 88.86 	 2931 	 3209 	 3388
ni 	p: 83.89 	r: 74.57 	f1: 78.96 	 1135 	 1353 	 1522

[32m iter_1[0m
ga 	p: 81.58 	r: 74.84 	f1: 78.07 	 4616 	 5658 	 6168
wo 	p: 91.84 	r: 86.01 	f1: 88.83 	 2914 	 3173 	 3388
ni 	p: 83.69 	r: 75.16 	f1: 79.2 	 1144 	 1367 	 1522

[32m iter_2[0m
ga 	p: 81.25 	r: 75.19 	f1: 78.11 	 4638 	 5708 	 6168
wo 	p: 91.46 	r: 86.3 	f1: 88.81 	 2924 	 3197 	 3388
ni 	p: 83.47 	r: 75.62 	f1: 79.35 	 1151 	 1379 	 1522
best_thres [[0.4, 0.49, 0.22], [0.47, 0.49, 0.22], [0.45, 0.49, 0.21]]
f [0.8154, 0.8154, 0.8155]
e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 8 	 [0.53, 0.46, 0.13] 	 lr: 0.00025 	 f: 82.2464050960983
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 11 [0m
Train...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           
[32m iter_0[0m
ga 	p: 81.13 	r: 73.96 	f1: 77.38 	 4562 	 5623 	 6168
wo 	p: 92.66 	r: 86.42 	f1: 89.43 	 2928 	 3160 	 3388
ni 	p: 80.93 	r: 75.82 	f1: 78.29 	 1154 	 1426 	 1522

[32m iter_1[0m
ga 	p: 81.0 	r: 74.77 	f1: 77.76 	 4612 	 5694 	 6168
wo 	p: 92.87 	r: 85.68 	f1: 89.13 	 2903 	 3126 	 3388
ni 	p: 84.35 	r: 73.65 	f1: 78.64 	 1121 	 1329 	 1522

[32m iter_2[0m
ga 	p: 80.84 	r: 74.85 	f1: 77.73 	 4617 	 5711 	 6168
wo 	p: 92.94 	r: 85.92 	f1: 89.29 	 2911 	 3132 	 3388
ni 	p: 84.91 	r: 73.59 	f1: 78.85 	 1120 	 1319 	 1522
best_thres [[0.5, 0.31, 0.14], [0.47, 0.32, 0.19], [0.5, 0.32, 0.2]]
f [0.8121, 0.8129, 0.8134]
e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 7 	 [0.43, 0.58, 0.14] 	 lr: 0.00025 	 f: 82.48449921129487
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 11 [0m
Train...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         loss: tensor(591.1511) lr: 0.00025 time: 2954.2
pred_count_train 41644

Test...
loss: tensor(575.0219) lr: 0.00025 time: 2621.32
pred_count_train 41644

Test...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     
[32m iter_0[0m
ga 	p: 81.29 	r: 74.32 	f1: 77.65 	 4584 	 5639 	 6168
wo 	p: 92.7 	r: 85.09 	f1: 88.73 	 2883 	 3110 	 3388
ni 	p: 85.71 	r: 72.14 	f1: 78.34 	 1098 	 1281 	 1522

[32m iter_1[0m
ga 	p: 80.7 	r: 74.63 	f1: 77.54 	 4603 	 5704 	 6168
wo 	p: 92.92 	r: 84.45 	f1: 88.48 	 2861 	 3079 	 3388
ni 	p: 81.69 	r: 75.03 	f1: 78.22 	 1142 	 1398 	 1522

[32m iter_2[0m
ga 	p: 81.85 	r: 74.06 	f1: 77.76 	 4568 	 5581 	 6168
wo 	p: 93.16 	r: 84.36 	f1: 88.54 	 2858 	 3068 	 3388
ni 	p: 81.98 	r: 74.7 	f1: 78.17 	 1137 	 1387 	 1522
best_thres [[0.62, 0.64, 0.32], [0.55, 0.65, 0.18], [0.65, 0.73, 0.18]]
f [0.8115, 0.8106, 0.8108]
e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 8 	 [0.53, 0.46, 0.13] 	 lr: 0.00025 	 f: 82.2464050960983
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 12 [0m
Train...

[32m iter_0[0m
ga 	p: 82.6 	r: 72.52 	f1: 77.23 	 4473 	 5415 	 6168
wo 	p: 92.68 	r: 85.63 	f1: 89.02 	 2901 	 3130 	 3388
ni 	p: 81.95 	r: 73.39 	f1: 77.44 	 1117 	 1363 	 1522

[32m iter_1[0m
ga 	p: 78.91 	r: 75.89 	f1: 77.37 	 4681 	 5932 	 6168
wo 	p: 91.83 	r: 86.3 	f1: 88.98 	 2924 	 3184 	 3388
ni 	p: 79.47 	r: 75.03 	f1: 77.19 	 1142 	 1437 	 1522

[32m iter_2[0m
ga 	p: 80.85 	r: 74.45 	f1: 77.52 	 4592 	 5680 	 6168
wo 	p: 91.48 	r: 86.57 	f1: 88.96 	 2933 	 3206 	 3388
ni 	p: 81.67 	r: 73.46 	f1: 77.34 	 1118 	 1369 	 1522
best_thres [[0.64, 0.78, 0.08], [0.34, 0.75, 0.05], [0.54, 0.77, 0.07]]
f [0.8092, 0.809, 0.8094]
load model: epoch7
model-GATE_e2e-stack_ve256_vu256_depth2_adam_lr0.0002_du0.1_dh0.0_True_size1_sub0_th0.8_it10_rs2016_preFalse.h5
e2e-stack model-GATE_e2e-stack_ve256_vu256_depth2_adam_lr0.0002_du0.1_dh0.0_True_size1_sub0_th0.8_it10_rs2016_preFalse
gpu: 0
6 3 2
vocab size:  29396
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...

[32m iter_0[0m
ga 	p: 81.71 	r: 73.14 	f1: 77.18 	 4511 	 5521 	 6168
wo 	p: 92.24 	r: 84.86 	f1: 88.39 	 2875 	 3117 	 3388
ni 	p: 81.6 	r: 75.16 	f1: 78.25 	 1144 	 1402 	 1522

[32m iter_1[0m
ga 	p: 81.95 	r: 73.31 	f1: 77.39 	 4522 	 5518 	 6168
wo 	p: 90.9 	r: 86.42 	f1: 88.61 	 2928 	 3221 	 3388
ni 	p: 82.61 	r: 75.56 	f1: 78.93 	 1150 	 1392 	 1522

[32m iter_2[0m
ga 	p: 81.99 	r: 73.28 	f1: 77.39 	 4520 	 5513 	 6168
wo 	p: 91.04 	r: 86.36 	f1: 88.64 	 2926 	 3214 	 3388
ni 	p: 82.67 	r: 76.15 	f1: 79.27 	 1159 	 1402 	 1522
best_thres [[0.36, 0.47, 0.13], [0.34, 0.28, 0.09], [0.34, 0.26, 0.08]]
f [0.8078, 0.8094, 0.8101]
load model: epoch18
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 18 	 [0.28, 0.28, 0.12] 	 lr: 1.25e-05 	 f: 82.168458781362
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 23 [0m
Train...
loss: tensor(691.5371) lr: 1.25e-05 time: 2256.1
pred_count_train 41644

Test...
loss: tensor(523.8643) lr: 0.00025 time: 2931.42
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.65 	r: 74.98 	f1: 79.08 	 4625 	 5529 	 6168
wo 	p: 92.1 	r: 87.43 	f1: 89.7 	 2962 	 3216 	 3388
ni 	p: 85.79 	r: 76.15 	f1: 80.68 	 1159 	 1351 	 1522

[32m iter_1[0m
ga 	p: 82.99 	r: 75.52 	f1: 79.08 	 4658 	 5613 	 6168
wo 	p: 92.01 	r: 87.34 	f1: 89.61 	 2959 	 3216 	 3388
ni 	p: 86.81 	r: 74.84 	f1: 80.38 	 1139 	 1312 	 1522

[32m iter_2[0m
ga 	p: 81.63 	r: 76.67 	f1: 79.07 	 4729 	 5793 	 6168
wo 	p: 92.79 	r: 86.66 	f1: 89.62 	 2936 	 3164 	 3388
ni 	p: 84.17 	r: 77.2 	f1: 80.53 	 1175 	 1396 	 1522
best_thres [[0.41, 0.34, 0.16], [0.37, 0.34, 0.15], [0.3, 0.45, 0.1]]
f [0.8261, 0.8257, 0.8255]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.1_it3_rs2016_preFalse 	current best epoch 16 	 [0.34, 0.3, 0.16] 	 lr: 2.5e-05 	 f: 83.05701577031944
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.1_it3_rs2016_preFalse[0m [33m epoch 23 [0m
Train...
loss: tensor(727.4635) lr: 6.25e-06 time: 2282.74
pred_count_train 41644

Test...
loss: tensor(3177.4785) lr: 0.001 time: 2539.83
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.0 	r: 75.11 	f1: 77.94 	 4633 	 5720 	 6168
wo 	p: 93.06 	r: 85.15 	f1: 88.93 	 2885 	 3100 	 3388
ni 	p: 79.46 	r: 71.42 	f1: 75.22 	 1087 	 1368 	 1522

[32m iter_1[0m
ga 	p: 80.48 	r: 75.94 	f1: 78.14 	 4684 	 5820 	 6168
wo 	p: 92.96 	r: 85.01 	f1: 88.81 	 2880 	 3098 	 3388
ni 	p: 78.56 	r: 72.21 	f1: 75.25 	 1099 	 1399 	 1522

[32m iter_2[0m
ga 	p: 80.44 	r: 75.89 	f1: 78.1 	 4681 	 5819 	 6168
wo 	p: 92.32 	r: 85.48 	f1: 88.77 	 2896 	 3137 	 3388
ni 	p: 78.54 	r: 72.14 	f1: 75.21 	 1098 	 1398 	 1522

[32m iter_3[0m
ga 	p: 80.37 	r: 75.96 	f1: 78.1 	 4685 	 5829 	 6168
wo 	p: 92.54 	r: 85.3 	f1: 88.77 	 2890 	 3123 	 3388
ni 	p: 78.56 	r: 72.21 	f1: 75.25 	 1099 	 1399 	 1522

[32m iter_4[0m
ga 	p: 80.34 	r: 75.94 	f1: 78.08 	 4684 	 5830 	 6168
wo 	p: 92.73 	r: 85.09 	f1: 88.75 	 2883 	 3109 	 3388
ni 	p: 78.54 	r: 72.14 	f1: 75.21 	 1098 	 1398 	 1522

[32m iter_5[0m
ga 	p: 80.34 	r: 75.94 	f1: 78.08 	 4684 	 5830 	 6168
wo 	p: 92.54 	r: 85.3 	f1: 88.77 	 2890 	 3123 	 3388
ni 	p: 78.54 	r: 72.14 	f1: 75.21 	 1098 	 1398 	 1522

[32m iter_6[0m
ga 	p: 80.34 	r: 75.94 	f1: 78.08 	 4684 	 5830 	 6168
wo 	p: 92.29 	r: 85.45 	f1: 88.74 	 2895 	 3137 	 3388
ni 	p: 78.54 	r: 72.14 	f1: 75.21 	 1098 	 1398 	 1522

[32m iter_7[0m
ga 	p: 80.34 	r: 75.94 	f1: 78.08 	 4684 	 5830 	 6168
wo 	p: 92.54 	r: 85.3 	f1: 88.77 	 2890 	 3123 	 3388
ni 	p: 78.54 	r: 72.14 	f1: 75.21 	 1098 	 1398 	 1522

[32m iter_8[0m
ga 	p: 80.34 	r: 75.94 	f1: 78.08 	 4684 	 5830 	 6168
wo 	p: 92.29 	r: 85.45 	f1: 88.74 	 2895 	 3137 	 3388
ni 	p: 78.54 	r: 72.14 	f1: 75.21 	 1098 	 1398 	 1522

[32m iter_9[0m
ga 	p: 80.34 	r: 75.94 	f1: 78.08 	 4684 	 5830 	 6168
wo 	p: 92.54 	r: 85.3 	f1: 88.77 	 2890 	 3123 	 3388
ni 	p: 78.54 	r: 72.14 	f1: 75.21 	 1098 	 1398 	 1522
best_thres [[0.53, 0.84, 0.1], [0.46, 0.84, 0.07], [0.46, 0.78, 0.07], [0.45, 0.78, 0.07], [0.45, 0.85, 0.07], [0.45, 0.78, 0.07], [0.45, 0.78, 0.07], [0.45, 0.78, 0.07], [0.45, 0.78, 0.07], [0.45, 0.78, 0.07]]
f [0.8093, 0.8095, 0.8095, 0.8095, 0.8095, 0.8095, 0.8094, 0.8094, 0.8094, 0.8094]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse 	current best epoch 4 	 [0.27, 0.56, 0.3] 	 lr: 0.0002 	 f: 82.13793103448276
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse[0m [33m epoch 8 [0m
Train...

[32m iter_0[0m
ga 	p: 81.21 	r: 74.55 	f1: 77.73 	 4598 	 5662 	 6168
wo 	p: 91.35 	r: 86.98 	f1: 89.11 	 2947 	 3226 	 3388
ni 	p: 83.19 	r: 78.38 	f1: 80.72 	 1193 	 1434 	 1522

[32m iter_1[0m
ga 	p: 79.27 	r: 77.06 	f1: 78.15 	 4753 	 5996 	 6168
wo 	p: 91.57 	r: 86.89 	f1: 89.17 	 2944 	 3215 	 3388
ni 	p: 82.08 	r: 80.35 	f1: 81.21 	 1223 	 1490 	 1522

[32m iter_2[0m
ga 	p: 79.87 	r: 76.62 	f1: 78.21 	 4726 	 5917 	 6168
wo 	p: 92.07 	r: 86.66 	f1: 89.28 	 2936 	 3189 	 3388
ni 	p: 82.9 	r: 80.29 	f1: 81.58 	 1222 	 1474 	 1522
best_thres [[0.39, 0.37, 0.16], [0.26, 0.36, 0.1], [0.28, 0.4, 0.1]]
f [0.8166, 0.8179, 0.8187]
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 18 	 [0.28, 0.28, 0.12] 	 lr: 1.25e-05 	 f: 82.168458781362
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 24 [0m
Train...

[32m iter_0[0m
ga 	p: 80.3 	r: 74.61 	f1: 77.35 	 4602 	 5731 	 6168
wo 	p: 93.36 	r: 84.18 	f1: 88.53 	 2852 	 3055 	 3388
ni 	p: 85.7 	r: 71.29 	f1: 77.83 	 1085 	 1266 	 1522

[32m iter_1[0m
ga 	p: 80.9 	r: 74.37 	f1: 77.5 	 4587 	 5670 	 6168
wo 	p: 92.22 	r: 85.36 	f1: 88.66 	 2892 	 3136 	 3388
ni 	p: 81.61 	r: 73.78 	f1: 77.5 	 1123 	 1376 	 1522

[32m iter_2[0m
ga 	p: 81.24 	r: 74.35 	f1: 77.64 	 4586 	 5645 	 6168
wo 	p: 92.38 	r: 85.21 	f1: 88.65 	 2887 	 3125 	 3388
ni 	p: 83.97 	r: 72.27 	f1: 77.68 	 1100 	 1310 	 1522
best_thres [[0.53, 0.74, 0.28], [0.57, 0.51, 0.15], [0.61, 0.57, 0.2]]
f [0.8082, 0.8087, 0.8093]
load model: epoch8
e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 8 	 [0.53, 0.46, 0.13] 	 lr: 0.00025 	 f: 82.2464050960983
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 13 [0m
Train...

[32m iter_0[0m
ga 	p: 72.56 	r: 71.0 	f1: 71.77 	 4379 	 6035 	 6168
wo 	p: 91.33 	r: 80.84 	f1: 85.77 	 2739 	 2999 	 3388
ni 	p: 81.67 	r: 75.23 	f1: 78.32 	 1145 	 1402 	 1522

[32m iter_1[0m
ga 	p: 72.57 	r: 70.65 	f1: 71.6 	 4358 	 6005 	 6168
wo 	p: 91.71 	r: 80.28 	f1: 85.62 	 2720 	 2966 	 3388
ni 	p: 80.3 	r: 74.44 	f1: 77.26 	 1133 	 1411 	 1522

[32m iter_2[0m
ga 	p: 72.0 	r: 71.6 	f1: 71.8 	 4416 	 6133 	 6168
wo 	p: 91.24 	r: 80.84 	f1: 85.73 	 2739 	 3002 	 3388
ni 	p: 82.66 	r: 73.59 	f1: 77.86 	 1120 	 1355 	 1522
best_thres [[0.36, 0.31, 0.18], [0.35, 0.32, 0.16], [0.34, 0.31, 0.2]]
f [0.7682, 0.7667, 0.7669]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 1 	 [0.34, 0.31, 0.2] 	 lr: 0.001 	 f: 76.69114685011309
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 2 [0m
Train...
loss: tensor(614.1354) lr: 1.25e-05 time: 2427.81
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.8 	r: 76.07 	f1: 78.83 	 4692 	 5736 	 6168
wo 	p: 92.33 	r: 87.04 	f1: 89.61 	 2949 	 3194 	 3388
ni 	p: 86.48 	r: 73.52 	f1: 79.47 	 1119 	 1294 	 1522

[32m iter_1[0m
ga 	p: 81.88 	r: 75.91 	f1: 78.78 	 4682 	 5718 	 6168
wo 	p: 92.46 	r: 87.25 	f1: 89.78 	 2956 	 3197 	 3388
ni 	p: 85.04 	r: 74.31 	f1: 79.31 	 1131 	 1330 	 1522

[32m iter_2[0m
ga 	p: 81.86 	r: 75.92 	f1: 78.78 	 4683 	 5721 	 6168
wo 	p: 92.7 	r: 86.98 	f1: 89.75 	 2947 	 3179 	 3388
ni 	p: 86.56 	r: 73.65 	f1: 79.59 	 1121 	 1295 	 1522
best_thres [[0.39, 0.42, 0.23], [0.39, 0.43, 0.15], [0.39, 0.47, 0.16]]
f [0.8225, 0.8225, 0.8226]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.1_it3_rs2016_preFalse 	current best epoch 16 	 [0.34, 0.3, 0.16] 	 lr: 2.5e-05 	 f: 83.05701577031944
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.1_it3_rs2016_preFalse[0m [33m epoch 24 [0m
Train...
loss: tensor(680.6514) lr: 6.25e-06 time: 2712.88
pred_count_train 41644

Test...
loss: tensor(2567.1255) lr: 0.001 time: 2687.33
pred_count_train 41644

Test...
loss: tensor(802.4143) lr: 0.000125 time: 3038.12
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.88 	r: 75.89 	f1: 77.84 	 4681 	 5860 	 6168
wo 	p: 92.86 	r: 85.24 	f1: 88.89 	 2888 	 3110 	 3388
ni 	p: 83.71 	r: 77.0 	f1: 80.22 	 1172 	 1400 	 1522

[32m iter_1[0m
ga 	p: 79.48 	r: 76.69 	f1: 78.06 	 4730 	 5951 	 6168
wo 	p: 91.72 	r: 86.28 	f1: 88.91 	 2923 	 3187 	 3388
ni 	p: 81.73 	r: 79.96 	f1: 80.84 	 1217 	 1489 	 1522

[32m iter_2[0m
ga 	p: 79.57 	r: 76.57 	f1: 78.04 	 4723 	 5936 	 6168
wo 	p: 90.91 	r: 87.07 	f1: 88.95 	 2950 	 3245 	 3388
ni 	p: 82.58 	r: 79.43 	f1: 80.98 	 1209 	 1464 	 1522
best_thres [[0.35, 0.53, 0.18], [0.29, 0.39, 0.1], [0.29, 0.29, 0.1]]
f [0.8151, 0.8162, 0.8167]
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 18 	 [0.28, 0.28, 0.12] 	 lr: 1.25e-05 	 f: 82.168458781362
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 25 [0m
Train...

[32m iter_0[0m
ga 	p: 73.94 	r: 71.11 	f1: 72.5 	 4386 	 5932 	 6168
wo 	p: 91.25 	r: 80.93 	f1: 85.78 	 2742 	 3005 	 3388
ni 	p: 81.2 	r: 72.67 	f1: 76.7 	 1106 	 1362 	 1522

[32m iter_1[0m
ga 	p: 73.68 	r: 71.13 	f1: 72.38 	 4387 	 5954 	 6168
wo 	p: 89.67 	r: 81.76 	f1: 85.53 	 2770 	 3089 	 3388
ni 	p: 80.36 	r: 72.86 	f1: 76.43 	 1109 	 1380 	 1522

[32m iter_2[0m
ga 	p: 73.38 	r: 71.85 	f1: 72.61 	 4432 	 6040 	 6168
wo 	p: 89.26 	r: 82.41 	f1: 85.7 	 2792 	 3128 	 3388
ni 	p: 81.48 	r: 73.13 	f1: 77.08 	 1113 	 1366 	 1522
best_thres [[0.38, 0.38, 0.17], [0.35, 0.26, 0.16], [0.35, 0.25, 0.18]]
f [0.7704, 0.7696, 0.7703]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 2 	 [0.35, 0.25, 0.18] 	 lr: 0.001 	 f: 77.02589548767251
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 3 [0m
Train...
loss: tensor(542.8159) lr: 1.25e-05 time: 2556.92
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.33 	r: 75.63 	f1: 78.38 	 4665 	 5736 	 6168
wo 	p: 92.82 	r: 87.01 	f1: 89.82 	 2948 	 3176 	 3388
ni 	p: 83.35 	r: 77.6 	f1: 80.37 	 1181 	 1417 	 1522

[32m iter_1[0m
ga 	p: 81.46 	r: 75.52 	f1: 78.38 	 4658 	 5718 	 6168
wo 	p: 92.21 	r: 87.31 	f1: 89.69 	 2958 	 3208 	 3388
ni 	p: 83.11 	r: 77.27 	f1: 80.08 	 1176 	 1415 	 1522

[32m iter_2[0m
ga 	p: 82.23 	r: 74.85 	f1: 78.37 	 4617 	 5615 	 6168
wo 	p: 92.1 	r: 87.46 	f1: 89.72 	 2963 	 3217 	 3388
ni 	p: 80.98 	r: 78.91 	f1: 79.93 	 1201 	 1483 	 1522
best_thres [[0.35, 0.57, 0.08], [0.35, 0.48, 0.08], [0.39, 0.48, 0.06]]
f [0.8216, 0.8213, 0.8212]
e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 8 	 [0.53, 0.46, 0.13] 	 lr: 0.00025 	 f: 82.2464050960983
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 14 [0m
Train...

[32m iter_0[0m
ga 	p: 81.5 	r: 75.94 	f1: 78.62 	 4684 	 5747 	 6168
wo 	p: 92.29 	r: 86.87 	f1: 89.49 	 2943 	 3189 	 3388
ni 	p: 85.17 	r: 73.59 	f1: 78.96 	 1120 	 1315 	 1522

[32m iter_1[0m
ga 	p: 82.2 	r: 75.24 	f1: 78.57 	 4641 	 5646 	 6168
wo 	p: 92.36 	r: 86.75 	f1: 89.47 	 2939 	 3182 	 3388
ni 	p: 84.38 	r: 73.85 	f1: 78.77 	 1124 	 1332 	 1522

[32m iter_2[0m
ga 	p: 81.19 	r: 75.92 	f1: 78.47 	 4683 	 5768 	 6168
wo 	p: 92.5 	r: 86.69 	f1: 89.5 	 2937 	 3175 	 3388
ni 	p: 86.3 	r: 72.86 	f1: 79.02 	 1109 	 1285 	 1522
best_thres [[0.38, 0.4, 0.19], [0.42, 0.43, 0.13], [0.36, 0.43, 0.14]]
f [0.8202, 0.8199, 0.8198]
load model: epoch16
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.1_it3_rs2016_preFalse 	current best epoch 16 	 [0.34, 0.3, 0.16] 	 lr: 2.5e-05 	 f: 83.05701577031944
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.1_it3_rs2016_preFalse[0m [33m epoch 25 [0m
Train...
loss: tensor(638.1956) lr: 6.25e-06 time: 2210.76
pred_count_train 41644

Test...
loss: tensor(490.2558) lr: 0.0002 time: 7131.96
pred_count_train 41644

Test...
loss: tensor(2544.8428) lr: 0.001 time: 2628.57
pred_count_train 41644

Test...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      loss: tensor(466.8542) lr: 0.000125 time: 2807.7
pred_count_train 41644

Test...
                                                                               
[32m iter_0[0m
ga 	p: 74.79 	r: 70.74 	f1: 72.7 	 4363 	 5834 	 6168
wo 	p: 91.43 	r: 80.61 	f1: 85.68 	 2731 	 2987 	 3388
ni 	p: 79.93 	r: 77.73 	f1: 78.81 	 1183 	 1480 	 1522

[32m iter_1[0m
ga 	p: 74.31 	r: 70.02 	f1: 72.1 	 4319 	 5812 	 6168
wo 	p: 91.86 	r: 79.28 	f1: 85.11 	 2686 	 2924 	 3388
ni 	p: 78.62 	r: 76.61 	f1: 77.6 	 1166 	 1483 	 1522

[32m iter_2[0m
ga 	p: 74.71 	r: 70.98 	f1: 72.8 	 4378 	 5860 	 6168
wo 	p: 92.11 	r: 80.58 	f1: 85.96 	 2730 	 2964 	 3388
ni 	p: 80.41 	r: 77.4 	f1: 78.88 	 1178 	 1465 	 1522
best_thres [[0.48, 0.38, 0.16], [0.43, 0.34, 0.13], [0.47, 0.38, 0.17]]
f [0.7743, 0.7708, 0.7724]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 3 	 [0.47, 0.38, 0.17] 	 lr: 0.001 	 f: 77.24185312992834
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 4 [0m
Train...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    
[32m iter_0[0m
ga 	p: 80.75 	r: 76.59 	f1: 78.62 	 4724 	 5850 	 6168
wo 	p: 93.34 	r: 85.24 	f1: 89.11 	 2888 	 3094 	 3388
ni 	p: 82.26 	r: 77.4 	f1: 79.76 	 1178 	 1432 	 1522

[32m iter_1[0m
ga 	p: 80.63 	r: 76.39 	f1: 78.45 	 4712 	 5844 	 6168
wo 	p: 93.84 	r: 84.59 	f1: 88.98 	 2866 	 3054 	 3388
ni 	p: 83.54 	r: 76.02 	f1: 79.6 	 1157 	 1385 	 1522

[32m iter_2[0m
ga 	p: 80.8 	r: 76.43 	f1: 78.55 	 4714 	 5834 	 6168
wo 	p: 93.6 	r: 85.06 	f1: 89.13 	 2882 	 3079 	 3388
ni 	p: 83.12 	r: 76.02 	f1: 79.41 	 1157 	 1392 	 1522
best_thres [[0.45, 0.67, 0.11], [0.44, 0.73, 0.12], [0.44, 0.7, 0.12]]
f [0.8194, 0.8186, 0.8187]
e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 8 	 [0.53, 0.46, 0.13] 	 lr: 0.00025 	 f: 82.2464050960983
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 15 [0m
Train...

[32m iter_0[0m
ga 	p: 81.69 	r: 73.98 	f1: 77.64 	 4563 	 5586 	 6168
wo 	p: 92.41 	r: 85.57 	f1: 88.86 	 2899 	 3137 	 3388
ni 	p: 80.87 	r: 77.79 	f1: 79.3 	 1184 	 1464 	 1522

[32m iter_1[0m
ga 	p: 80.48 	r: 75.42 	f1: 77.87 	 4652 	 5780 	 6168
wo 	p: 91.71 	r: 86.51 	f1: 89.03 	 2931 	 3196 	 3388
ni 	p: 83.42 	r: 76.35 	f1: 79.73 	 1162 	 1393 	 1522

[32m iter_2[0m
ga 	p: 81.13 	r: 75.19 	f1: 78.05 	 4638 	 5717 	 6168
wo 	p: 91.73 	r: 86.42 	f1: 89.0 	 2928 	 3192 	 3388
ni 	p: 84.3 	r: 75.82 	f1: 79.83 	 1154 	 1369 	 1522
best_thres [[0.39, 0.44, 0.11], [0.3, 0.32, 0.1], [0.32, 0.32, 0.1]]
f [0.8132, 0.8143, 0.8151]
load model: epoch18
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 18 	 [0.28, 0.28, 0.12] 	 lr: 1.25e-05 	 f: 82.168458781362
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 27 [0m
Train...

[32m iter_0[0m
ga 	p: 81.25 	r: 74.84 	f1: 77.91 	 4616 	 5681 	 6168
wo 	p: 92.4 	r: 86.48 	f1: 89.34 	 2930 	 3171 	 3388
ni 	p: 81.19 	r: 75.43 	f1: 78.2 	 1148 	 1414 	 1522

[32m iter_1[0m
ga 	p: 81.55 	r: 74.74 	f1: 78.0 	 4610 	 5653 	 6168
wo 	p: 92.41 	r: 86.28 	f1: 89.24 	 2923 	 3163 	 3388
ni 	p: 84.16 	r: 74.38 	f1: 78.97 	 1132 	 1345 	 1522

[32m iter_2[0m
ga 	p: 81.91 	r: 74.66 	f1: 78.12 	 4605 	 5622 	 6168
wo 	p: 92.37 	r: 86.45 	f1: 89.31 	 2929 	 3171 	 3388
ni 	p: 80.84 	r: 76.81 	f1: 78.77 	 1169 	 1446 	 1522

[32m iter_3[0m
ga 	p: 82.5 	r: 74.06 	f1: 78.05 	 4568 	 5537 	 6168
wo 	p: 92.44 	r: 86.28 	f1: 89.25 	 2923 	 3162 	 3388
ni 	p: 83.99 	r: 74.11 	f1: 78.74 	 1128 	 1343 	 1522

[32m iter_4[0m
ga 	p: 81.89 	r: 74.61 	f1: 78.08 	 4602 	 5620 	 6168
wo 	p: 92.36 	r: 86.39 	f1: 89.28 	 2927 	 3169 	 3388
ni 	p: 80.73 	r: 76.81 	f1: 78.72 	 1169 	 1448 	 1522

[32m iter_5[0m
ga 	p: 82.5 	r: 74.06 	f1: 78.05 	 4568 	 5537 	 6168
wo 	p: 92.36 	r: 86.3 	f1: 89.23 	 2924 	 3166 	 3388
ni 	p: 80.68 	r: 76.81 	f1: 78.69 	 1169 	 1449 	 1522

[32m iter_6[0m
ga 	p: 81.9 	r: 74.63 	f1: 78.1 	 4603 	 5620 	 6168
wo 	p: 92.33 	r: 86.33 	f1: 89.23 	 2925 	 3168 	 3388
ni 	p: 83.98 	r: 74.05 	f1: 78.7 	 1127 	 1342 	 1522

[32m iter_7[0m
ga 	p: 82.51 	r: 74.06 	f1: 78.06 	 4568 	 5536 	 6168
wo 	p: 92.41 	r: 86.22 	f1: 89.2 	 2921 	 3161 	 3388
ni 	p: 80.68 	r: 76.81 	f1: 78.69 	 1169 	 1449 	 1522

[32m iter_8[0m
ga 	p: 81.9 	r: 74.63 	f1: 78.1 	 4603 	 5620 	 6168
wo 	p: 92.36 	r: 86.33 	f1: 89.24 	 2925 	 3167 	 3388
ni 	p: 83.98 	r: 74.05 	f1: 78.7 	 1127 	 1342 	 1522

[32m iter_9[0m
ga 	p: 82.51 	r: 74.06 	f1: 78.06 	 4568 	 5536 	 6168
wo 	p: 92.41 	r: 86.22 	f1: 89.2 	 2921 	 3161 	 3388
ni 	p: 80.68 	r: 76.81 	f1: 78.69 	 1169 	 1449 	 1522
best_thres [[0.62, 0.78, 0.22], [0.63, 0.78, 0.27], [0.66, 0.78, 0.16], [0.7, 0.8, 0.27], [0.66, 0.79, 0.16], [0.7, 0.78, 0.16], [0.66, 0.79, 0.27], [0.7, 0.79, 0.16], [0.66, 0.79, 0.27], [0.7, 0.79, 0.16]]
f [0.8147, 0.8153, 0.8157, 0.8158, 0.8159, 0.8159, 0.8159, 0.8159, 0.816, 0.816]
load model: epoch4
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse 	current best epoch 4 	 [0.27, 0.56, 0.3] 	 lr: 0.0002 	 f: 82.13793103448276
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse[0m [33m epoch 9 [0m
Train...
loss: tensor(702.2056) lr: 1e-05 time: 1814.48
pred_count_train 41644

Test...
loss: tensor(2616.9436) lr: 0.001 time: 2119.83
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.48 	r: 75.86 	f1: 79.03 	 4679 	 5673 	 6168
wo 	p: 92.68 	r: 87.4 	f1: 89.96 	 2961 	 3195 	 3388
ni 	p: 86.81 	r: 75.69 	f1: 80.87 	 1152 	 1327 	 1522

[32m iter_1[0m
ga 	p: 82.34 	r: 76.35 	f1: 79.23 	 4709 	 5719 	 6168
wo 	p: 93.14 	r: 86.92 	f1: 89.92 	 2945 	 3162 	 3388
ni 	p: 88.19 	r: 74.57 	f1: 80.81 	 1135 	 1287 	 1522

[32m iter_2[0m
ga 	p: 82.32 	r: 76.26 	f1: 79.18 	 4704 	 5714 	 6168
wo 	p: 94.02 	r: 86.36 	f1: 90.03 	 2926 	 3112 	 3388
ni 	p: 87.6 	r: 75.16 	f1: 80.91 	 1144 	 1306 	 1522
best_thres [[0.38, 0.39, 0.19], [0.35, 0.48, 0.18], [0.35, 0.71, 0.16]]
f [0.8266, 0.827, 0.8271]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.1_it3_rs2016_preFalse 	current best epoch 16 	 [0.34, 0.3, 0.16] 	 lr: 2.5e-05 	 f: 83.05701577031944
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.1_it3_rs2016_preFalse[0m [33m epoch 27 [0m
Train...
loss: tensor(293.8134) lr: 0.000125 time: 2952.84
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 75.04 	r: 67.85 	f1: 71.26 	 4185 	 5577 	 6168
wo 	p: 89.93 	r: 79.63 	f1: 84.47 	 2698 	 3000 	 3388
ni 	p: 80.01 	r: 74.7 	f1: 77.27 	 1137 	 1421 	 1522

[32m iter_1[0m
ga 	p: 75.11 	r: 66.97 	f1: 70.81 	 4131 	 5500 	 6168
wo 	p: 89.09 	r: 79.55 	f1: 84.05 	 2695 	 3025 	 3388
ni 	p: 79.32 	r: 73.59 	f1: 76.35 	 1120 	 1412 	 1522

[32m iter_2[0m
ga 	p: 75.61 	r: 67.15 	f1: 71.13 	 4142 	 5478 	 6168
wo 	p: 89.42 	r: 79.87 	f1: 84.38 	 2706 	 3026 	 3388
ni 	p: 77.75 	r: 76.68 	f1: 77.21 	 1167 	 1501 	 1522
best_thres [[0.28, 0.43, 0.19], [0.28, 0.35, 0.19], [0.3, 0.41, 0.16]]
f [0.7611, 0.7586, 0.7592]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 3 	 [0.47, 0.38, 0.17] 	 lr: 0.001 	 f: 77.24185312992834
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 5 [0m
Train...
loss: tensor(721.2465) lr: 5e-06 time: 2126.95
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.66 	r: 73.74 	f1: 77.94 	 4548 	 5502 	 6168
wo 	p: 91.52 	r: 87.01 	f1: 89.21 	 2948 	 3221 	 3388
ni 	p: 82.85 	r: 78.71 	f1: 80.73 	 1198 	 1446 	 1522

[32m iter_1[0m
ga 	p: 79.54 	r: 77.09 	f1: 78.3 	 4755 	 5978 	 6168
wo 	p: 92.17 	r: 86.48 	f1: 89.23 	 2930 	 3179 	 3388
ni 	p: 81.98 	r: 80.09 	f1: 81.02 	 1219 	 1487 	 1522

[32m iter_2[0m
ga 	p: 79.54 	r: 76.98 	f1: 78.24 	 4748 	 5969 	 6168
wo 	p: 92.31 	r: 86.45 	f1: 89.29 	 2929 	 3173 	 3388
ni 	p: 82.9 	r: 79.63 	f1: 81.23 	 1212 	 1462 	 1522
best_thres [[0.43, 0.33, 0.14], [0.24, 0.35, 0.09], [0.24, 0.35, 0.09]]
f [0.8184, 0.8191, 0.8194]
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 18 	 [0.28, 0.28, 0.12] 	 lr: 1.25e-05 	 f: 82.168458781362
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 28 [0m
Train...

[32m iter_0[0m
ga 	p: 81.72 	r: 74.43 	f1: 77.91 	 4591 	 5618 	 6168
wo 	p: 91.62 	r: 86.48 	f1: 88.98 	 2930 	 3198 	 3388
ni 	p: 85.21 	r: 74.97 	f1: 79.76 	 1141 	 1339 	 1522

[32m iter_1[0m
ga 	p: 80.69 	r: 75.73 	f1: 78.13 	 4671 	 5789 	 6168
wo 	p: 91.57 	r: 86.51 	f1: 88.97 	 2931 	 3201 	 3388
ni 	p: 82.98 	r: 76.54 	f1: 79.63 	 1165 	 1404 	 1522

[32m iter_2[0m
ga 	p: 80.93 	r: 75.42 	f1: 78.08 	 4652 	 5748 	 6168
wo 	p: 91.63 	r: 86.6 	f1: 89.04 	 2934 	 3202 	 3388
ni 	p: 82.86 	r: 76.22 	f1: 79.4 	 1160 	 1400 	 1522
best_thres [[0.55, 0.51, 0.22], [0.41, 0.5, 0.15], [0.44, 0.5, 0.15]]
f [0.8159, 0.8163, 0.8163]
e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 8 	 [0.53, 0.46, 0.13] 	 lr: 0.00025 	 f: 82.2464050960983
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 16 [0m
Train...
loss: tensor(636.6675) lr: 1e-05 time: 2132.69
pred_count_train 41644

Test...
loss: tensor(2698.5835) lr: 0.001 time: 2183.07
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.95 	r: 76.96 	f1: 78.91 	 4747 	 5864 	 6168
wo 	p: 91.92 	r: 87.66 	f1: 89.74 	 2970 	 3231 	 3388
ni 	p: 83.95 	r: 75.62 	f1: 79.57 	 1151 	 1371 	 1522

[32m iter_1[0m
ga 	p: 81.08 	r: 76.98 	f1: 78.98 	 4748 	 5856 	 6168
wo 	p: 92.8 	r: 86.78 	f1: 89.69 	 2940 	 3168 	 3388
ni 	p: 86.1 	r: 73.65 	f1: 79.39 	 1121 	 1302 	 1522

[32m iter_2[0m
ga 	p: 81.16 	r: 76.75 	f1: 78.89 	 4734 	 5833 	 6168
wo 	p: 93.97 	r: 85.6 	f1: 89.59 	 2900 	 3086 	 3388
ni 	p: 86.03 	r: 74.05 	f1: 79.59 	 1127 	 1310 	 1522
best_thres [[0.31, 0.29, 0.15], [0.3, 0.38, 0.15], [0.31, 0.72, 0.14]]
f [0.8232, 0.8232, 0.8229]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.1_it3_rs2016_preFalse 	current best epoch 16 	 [0.34, 0.3, 0.16] 	 lr: 2.5e-05 	 f: 83.05701577031944
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.1_it3_rs2016_preFalse[0m [33m epoch 28 [0m
Train...
loss: tensor(687.3049) lr: 5e-06 time: 2170.61
pred_count_train 41644

Test...
 90.32 	r: 79.9 	f1: 84.79 	 2707 	 2997 	 3388
ni 	p: 78.0 	r: 67.54 	f1: 72.39 	 1028 	 1318 	 1522

[32m iter_1[0m
ga 	p: 73.2 	r: 69.83 	f1: 71.47 	 4307 	 5884 	 6168
wo 	p: 88.98 	r: 79.81 	f1: 84.15 	 2704 	 3039 	 3388
ni 	p: 68.82 	r: 68.59 	f1: 68.71 	 1044 	 1517 	 1522

[32m iter_2[0m
ga 	p: 72.68 	r: 70.49 	f1: 71.57 	 4348 	 5982 	 6168
wo 	p: 91.45 	r: 78.9 	f1: 84.71 	 2673 	 2923 	 3388
ni 	p: 78.73 	r: 66.62 	f1: 72.17 	 1014 	 1288 	 1522
best_thres [[0.14, 0.6, 0.16], [0.16, 0.49, 0.08], [0.15, 0.7, 0.15]]
f [0.7579, 0.7533, 0.754]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 3 	 [0.47, 0.38, 0.17] 	 lr: 0.001 	 f: 77.24185312992834
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 6 [0m
Train...
loss: tensor(202.8948) lr: 0.000125 time: 3022.72
pred_count_train 41644

Test...
loss: tensor(778.2806) lr: 0.0001 time: 6157.51
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.36 	r: 73.75 	f1: 77.82 	 4549 	 5523 	 6168
wo 	p: 92.23 	r: 86.19 	f1: 89.11 	 2920 	 3166 	 3388
ni 	p: 83.69 	r: 76.54 	f1: 79.96 	 1165 	 1392 	 1522

[32m iter_1[0m
ga 	p: 79.51 	r: 76.61 	f1: 78.03 	 4725 	 5943 	 6168
wo 	p: 91.78 	r: 86.63 	f1: 89.13 	 2935 	 3198 	 3388
ni 	p: 81.59 	r: 79.5 	f1: 80.53 	 1210 	 1483 	 1522

[32m iter_2[0m
ga 	p: 79.35 	r: 76.62 	f1: 77.96 	 4726 	 5956 	 6168
wo 	p: 91.88 	r: 86.51 	f1: 89.12 	 2931 	 3190 	 3388
ni 	p: 82.17 	r: 79.04 	f1: 80.58 	 1203 	 1464 	 1522
best_thres [[0.42, 0.4, 0.15], [0.25, 0.31, 0.08], [0.24, 0.32, 0.08]]
f [0.8161, 0.8168, 0.8169]
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 18 	 [0.28, 0.28, 0.12] 	 lr: 1.25e-05 	 f: 82.168458781362
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 29 [0m
Train...
loss: tensor(573.5909) lr: 1e-05 time: 2066.03
pred_count_train 41644

Test...
loss: tensor(2762.2661) lr: 0.001 time: 2067.85
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.51 	r: 76.67 	f1: 78.54 	 4729 	 5874 	 6168
wo 	p: 92.96 	r: 85.8 	f1: 89.24 	 2907 	 3127 	 3388
ni 	p: 85.66 	r: 72.21 	f1: 78.36 	 1099 	 1283 	 1522

[32m iter_1[0m
ga 	p: 80.51 	r: 76.64 	f1: 78.53 	 4727 	 5871 	 6168
wo 	p: 93.02 	r: 85.68 	f1: 89.2 	 2903 	 3121 	 3388
ni 	p: 85.53 	r: 72.6 	f1: 78.54 	 1105 	 1292 	 1522

[32m iter_2[0m
ga 	p: 81.19 	r: 76.07 	f1: 78.55 	 4692 	 5779 	 6168
wo 	p: 93.02 	r: 85.77 	f1: 89.25 	 2906 	 3124 	 3388
ni 	p: 86.89 	r: 71.88 	f1: 78.68 	 1094 	 1259 	 1522
best_thres [[0.17, 0.43, 0.17], [0.16, 0.45, 0.15], [0.21, 0.43, 0.18]]
f [0.8178, 0.8178, 0.818]
load model: epoch8
e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 8 	 [0.53, 0.46, 0.13] 	 lr: 0.00025 	 f: 82.2464050960983
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 17 [0m
Train...
6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse
gpu: 0
6 3 2
vocab size:  29396
prediction mode: model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse test [0.5, 0.5, 0.5]
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(651.5634) lr: 5e-06 time: 1854.84
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 71.28 	r: 69.24 	f1: 70.25 	 4271 	 5992 	 6168
wo 	p: 89.68 	r: 78.72 	f1: 83.84 	 2667 	 2974 	 3388
ni 	p: 77.76 	r: 71.22 	f1: 74.35 	 1084 	 1394 	 1522

[32m iter_1[0m
ga 	p: 72.19 	r: 67.93 	f1: 70.0 	 4190 	 5804 	 6168
wo 	p: 89.09 	r: 78.31 	f1: 83.35 	 2653 	 2978 	 3388
ni 	p: 75.31 	r: 72.73 	f1: 74.0 	 1107 	 1470 	 1522

[32m iter_2[0m
ga 	p: 71.48 	r: 69.03 	f1: 70.24 	 4258 	 5957 	 6168
wo 	p: 92.2 	r: 76.36 	f1: 83.53 	 2587 	 2806 	 3388
ni 	p: 77.5 	r: 71.75 	f1: 74.51 	 1092 	 1409 	 1522
best_thres [[0.35, 0.41, 0.06], [0.35, 0.35, 0.05], [0.35, 0.56, 0.06]]
f [0.7484, 0.7469, 0.7469]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 3 	 [0.47, 0.38, 0.17] 	 lr: 0.001 	 f: 77.24185312992834
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 7 [0m
Train...

[32m iter_0[0m
ga 	p: 82.56 	r: 73.2 	f1: 77.6 	 4515 	 5469 	 6168
wo 	p: 91.35 	r: 86.66 	f1: 88.94 	 2936 	 3214 	 3388
ni 	p: 80.32 	r: 78.06 	f1: 79.17 	 1188 	 1479 	 1522

[32m iter_1[0m
ga 	p: 79.44 	r: 76.44 	f1: 77.91 	 4715 	 5935 	 6168
wo 	p: 90.97 	r: 87.1 	f1: 88.99 	 2951 	 3244 	 3388
ni 	p: 82.19 	r: 77.6 	f1: 79.82 	 1181 	 1437 	 1522

[32m iter_2[0m
ga 	p: 82.51 	r: 73.88 	f1: 77.96 	 4557 	 5523 	 6168
wo 	p: 92.01 	r: 86.33 	f1: 89.08 	 2925 	 3179 	 3388
ni 	p: 84.79 	r: 75.82 	f1: 80.06 	 1154 	 1361 	 1522
best_thres [[0.43, 0.33, 0.11], [0.24, 0.24, 0.09], [0.39, 0.31, 0.11]]
f [0.8135, 0.8146, 0.8154]
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 18 	 [0.28, 0.28, 0.12] 	 lr: 1.25e-05 	 f: 82.168458781362
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 30 [0m
Train...
loss: tensor(3186.3394) lr: 0.0002 time: 1825.16
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.24 	r: 75.15 	f1: 78.99 	 4635 	 5568 	 6168
wo 	p: 91.95 	r: 87.37 	f1: 89.6 	 2960 	 3219 	 3388
ni 	p: 85.4 	r: 75.69 	f1: 80.25 	 1152 	 1349 	 1522

[32m iter_1[0m
ga 	p: 81.62 	r: 76.75 	f1: 79.11 	 4734 	 5800 	 6168
wo 	p: 91.94 	r: 87.54 	f1: 89.69 	 2966 	 3226 	 3388
ni 	p: 83.47 	r: 77.99 	f1: 80.64 	 1187 	 1422 	 1522

[32m iter_2[0m
ga 	p: 81.57 	r: 76.72 	f1: 79.07 	 4732 	 5801 	 6168
wo 	p: 92.04 	r: 87.43 	f1: 89.68 	 2962 	 3218 	 3388
ni 	p: 82.95 	r: 78.65 	f1: 80.74 	 1197 	 1443 	 1522

[32m iter_3[0m
ga 	p: 83.45 	r: 75.11 	f1: 79.06 	 4633 	 5552 	 6168
wo 	p: 92.07 	r: 87.43 	f1: 89.69 	 2962 	 3217 	 3388
ni 	p: 83.69 	r: 77.86 	f1: 80.67 	 1185 	 1416 	 1522

[32m iter_4[0m
ga 	p: 83.46 	r: 75.11 	f1: 79.07 	 4633 	 5551 	 6168
wo 	p: 92.04 	r: 87.43 	f1: 89.68 	 2962 	 3218 	 3388
ni 	p: 82.88 	r: 78.58 	f1: 80.67 	 1196 	 1443 	 1522

[32m iter_5[0m
ga 	p: 83.45 	r: 75.11 	f1: 79.06 	 4633 	 5552 	 6168
wo 	p: 92.07 	r: 87.43 	f1: 89.69 	 2962 	 3217 	 3388
ni 	p: 82.87 	r: 78.52 	f1: 80.63 	 1195 	 1442 	 1522

[32m iter_6[0m
ga 	p: 83.25 	r: 75.28 	f1: 79.06 	 4643 	 5577 	 6168
wo 	p: 92.04 	r: 87.43 	f1: 89.68 	 2962 	 3218 	 3388
ni 	p: 83.59 	r: 77.99 	f1: 80.69 	 1187 	 1420 	 1522

[32m iter_7[0m
ga 	p: 83.24 	r: 75.29 	f1: 79.07 	 4644 	 5579 	 6168
wo 	p: 92.07 	r: 87.43 	f1: 89.69 	 2962 	 3217 	 3388
ni 	p: 82.87 	r: 78.52 	f1: 80.63 	 1195 	 1442 	 1522

[32m iter_8[0m
ga 	p: 83.25 	r: 75.28 	f1: 79.06 	 4643 	 5577 	 6168
wo 	p: 92.04 	r: 87.43 	f1: 89.68 	 2962 	 3218 	 3388
ni 	p: 83.59 	r: 77.99 	f1: 80.69 	 1187 	 1420 	 1522

[32m iter_9[0m
ga 	p: 83.24 	r: 75.29 	f1: 79.07 	 4644 	 5579 	 6168
wo 	p: 92.07 	r: 87.43 	f1: 89.69 	 2962 	 3217 	 3388
ni 	p: 82.87 	r: 78.52 	f1: 80.63 	 1195 	 1442 	 1522
best_thres [[0.51, 0.46, 0.16], [0.39, 0.43, 0.11], [0.39, 0.44, 0.1], [0.5, 0.44, 0.11], [0.5, 0.44, 0.1], [0.5, 0.44, 0.1], [0.49, 0.44, 0.11], [0.49, 0.44, 0.1], [0.49, 0.44, 0.11], [0.49, 0.44, 0.1]]
f [0.8246, 0.8252, 0.8253, 0.8254, 0.8255, 0.8256, 0.8256, 0.8256, 0.8256, 0.8256]
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse 	current best epoch 9 	 [0.49, 0.44, 0.1] 	 lr: 0.0001 	 f: 82.5649198462548
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse[0m [33m epoch 10 [0m
Train...
loss: tensor(762.7650) lr: 6.25e-05 time: 2832.65
pred_count_train 41644

Test...
loss: tensor(2869.0579) lr: 0.001 time: 2168.27
pred_count_train 41644

Test...
1.78 	r: 84.39 	f1: 87.93 	 2859 	 3115 	 3388
ni 	p: 84.19 	r: 78.71 	f1: 81.36 	 1198 	 1423 	 1522

[32m iter_1[0m
ga 	p: 77.9 	r: 73.05 	f1: 75.4 	 4506 	 5784 	 6168
wo 	p: 92.12 	r: 84.21 	f1: 87.99 	 2853 	 3097 	 3388
ni 	p: 83.96 	r: 78.06 	f1: 80.9 	 1188 	 1415 	 1522

[32m iter_2[0m
ga 	p: 77.81 	r: 73.18 	f1: 75.43 	 4514 	 5801 	 6168
wo 	p: 92.06 	r: 84.18 	f1: 87.94 	 2852 	 3098 	 3388
ni 	p: 83.9 	r: 78.06 	f1: 80.87 	 1188 	 1416 	 1522
best_thres [[0.45, 0.27, 0.22], [0.46, 0.27, 0.22], [0.46, 0.27, 0.22]]
f [0.801, 0.8004, 0.8002]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse 	current best epoch 1 	 [0.46, 0.27, 0.22] 	 lr: 0.0002 	 f: 80.01557996416608
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse[0m [33m epoch 2 [0m
Train...
loss: tensor(618.2386) lr: 5e-06 time: 1996.18
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.97 	r: 75.6 	f1: 78.65 	 4663 	 5689 	 6168
wo 	p: 93.18 	r: 86.25 	f1: 89.58 	 2922 	 3136 	 3388
ni 	p: 83.07 	r: 80.95 	f1: 82.0 	 1232 	 1483 	 1522

[32m iter_1[0m
ga 	p: 81.42 	r: 76.44 	f1: 78.85 	 4715 	 5791 	 6168
wo 	p: 92.64 	r: 86.22 	f1: 89.31 	 2921 	 3153 	 3388
ni 	p: 84.79 	r: 80.22 	f1: 82.44 	 1221 	 1440 	 1522

[32m iter_2[0m
ga 	p: 81.87 	r: 76.12 	f1: 78.89 	 4695 	 5735 	 6168
wo 	p: 93.07 	r: 86.04 	f1: 89.42 	 2915 	 3132 	 3388
ni 	p: 84.82 	r: 80.03 	f1: 82.35 	 1218 	 1436 	 1522
best_thres [[0.52, 0.58, 0.1], [0.46, 0.52, 0.12], [0.49, 0.58, 0.12]]
f [0.8246, 0.825, 0.8252]
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 17 	 [0.49, 0.58, 0.12] 	 lr: 6.25e-05 	 f: 82.52347070637875
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 18 [0m
Train...

[32m iter_0[0m
ga 	p: 71.93 	r: 67.25 	f1: 69.51 	 4148 	 5767 	 6168
wo 	p: 89.36 	r: 77.36 	f1: 82.93 	 2621 	 2933 	 3388
ni 	p: 74.67 	r: 69.91 	f1: 72.21 	 1064 	 1425 	 1522

[32m iter_1[0m
ga 	p: 70.18 	r: 68.45 	f1: 69.3 	 4222 	 6016 	 6168
wo 	p: 88.44 	r: 78.34 	f1: 83.08 	 2654 	 3001 	 3388
ni 	p: 73.71 	r: 68.53 	f1: 71.02 	 1043 	 1415 	 1522

[32m iter_2[0m
ga 	p: 69.71 	r: 69.75 	f1: 69.73 	 4302 	 6171 	 6168
wo 	p: 87.54 	r: 79.19 	f1: 83.16 	 2683 	 3065 	 3388
ni 	p: 76.08 	r: 68.33 	f1: 72.0 	 1040 	 1367 	 1522
best_thres [[0.44, 0.47, 0.13], [0.33, 0.38, 0.13], [0.33, 0.36, 0.15]]
f [0.7389, 0.7376, 0.7385]
load model: epoch3
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 3 	 [0.47, 0.38, 0.17] 	 lr: 0.001 	 f: 77.24185312992834
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 8 [0m
Train...

[32m iter_0[0m
ga 	p: 82.41 	r: 73.31 	f1: 77.6 	 4522 	 5487 	 6168
wo 	p: 91.96 	r: 86.1 	f1: 88.93 	 2917 	 3172 	 3388
ni 	p: 81.31 	r: 77.46 	f1: 79.34 	 1179 	 1450 	 1522

[32m iter_1[0m
ga 	p: 79.91 	r: 75.71 	f1: 77.76 	 4670 	 5844 	 6168
wo 	p: 91.63 	r: 86.28 	f1: 88.87 	 2923 	 3190 	 3388
ni 	p: 83.93 	r: 76.87 	f1: 80.25 	 1170 	 1394 	 1522

[32m iter_2[0m
ga 	p: 82.49 	r: 73.62 	f1: 77.8 	 4541 	 5505 	 6168
wo 	p: 91.49 	r: 86.28 	f1: 88.8 	 2923 	 3195 	 3388
ni 	p: 83.74 	r: 77.14 	f1: 80.3 	 1174 	 1402 	 1522
best_thres [[0.42, 0.39, 0.14], [0.27, 0.32, 0.12], [0.4, 0.3, 0.11]]
f [0.8135, 0.8142, 0.8147]
e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	best in epoch 18 	 [0.28, 0.28, 0.12] 	 lr: 1.25e-05 	 f: 82.168458781362
model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse.h5
e2e-stack model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse
gpu: 0
6 3 2
vocab size:  29396
prediction mode: model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse test [0.5, 0.5, 0.5]
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(2035.6141) lr: 0.0002 time: 2136.21
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.11 	r: 74.09 	f1: 77.89 	 4570 	 5566 	 6168
wo 	p: 93.7 	r: 85.21 	f1: 89.26 	 2887 	 3081 	 3388
ni 	p: 83.88 	r: 81.73 	f1: 82.8 	 1244 	 1483 	 1522

[32m iter_1[0m
ga 	p: 80.46 	r: 75.45 	f1: 77.88 	 4654 	 5784 	 6168
wo 	p: 93.52 	r: 85.18 	f1: 89.16 	 2886 	 3086 	 3388
ni 	p: 82.23 	r: 82.98 	f1: 82.6 	 1263 	 1536 	 1522

[32m iter_2[0m
ga 	p: 82.06 	r: 74.17 	f1: 77.92 	 4575 	 5575 	 6168
wo 	p: 93.43 	r: 85.27 	f1: 89.17 	 2889 	 3092 	 3388
ni 	p: 82.21 	r: 83.18 	f1: 82.69 	 1266 	 1540 	 1522
best_thres [[0.51, 0.37, 0.31], [0.44, 0.37, 0.25], [0.51, 0.36, 0.25]]
f [0.8205, 0.82, 0.8201]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse 	current best epoch 2 	 [0.51, 0.36, 0.25] 	 lr: 0.0002 	 f: 82.01072260343561
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse[0m [33m epoch 3 [0m
Train...
loss: tensor(2199.7610) lr: 0.0005 time: 2082.17
pred_count_train 41644

Test...
loss: tensor(3189.8748) lr: 0.0002 time: 2105.49
pred_count_train 41644

Test...
loss: tensor(458.3583) lr: 6.25e-05 time: 2942.76
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 77.04 	r: 72.91 	f1: 74.92 	 4497 	 5837 	 6168
wo 	p: 90.82 	r: 84.36 	f1: 87.47 	 2858 	 3147 	 3388
ni 	p: 82.85 	r: 79.04 	f1: 80.9 	 1203 	 1452 	 1522

[32m iter_1[0m
ga 	p: 77.5 	r: 72.16 	f1: 74.74 	 4451 	 5743 	 6168
wo 	p: 93.63 	r: 82.0 	f1: 87.43 	 2778 	 2967 	 3388
ni 	p: 81.25 	r: 79.7 	f1: 80.46 	 1213 	 1493 	 1522

[32m iter_2[0m
ga 	p: 77.58 	r: 72.7 	f1: 75.06 	 4484 	 5780 	 6168
wo 	p: 93.49 	r: 82.64 	f1: 87.73 	 2800 	 2995 	 3388
ni 	p: 86.29 	r: 76.08 	f1: 80.87 	 1158 	 1342 	 1522
best_thres [[0.44, 0.36, 0.16], [0.42, 0.5, 0.15], [0.45, 0.55, 0.26]]
f [0.7956, 0.7945, 0.7952]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 8 	 [0.45, 0.55, 0.26] 	 lr: 0.0005 	 f: 79.5186747929364
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 9 [0m
Train...

[32m iter_0[0m
ga 	p: 77.62 	r: 73.78 	f1: 75.65 	 4551 	 5863 	 6168
wo 	p: 91.01 	r: 85.12 	f1: 87.97 	 2884 	 3169 	 3388
ni 	p: 87.26 	r: 76.94 	f1: 81.77 	 1171 	 1342 	 1522

[32m iter_1[0m
ga 	p: 77.5 	r: 73.83 	f1: 75.62 	 4554 	 5876 	 6168
wo 	p: 92.03 	r: 84.18 	f1: 87.93 	 2852 	 3099 	 3388
ni 	p: 83.44 	r: 80.09 	f1: 81.73 	 1219 	 1461 	 1522

[32m iter_2[0m
ga 	p: 77.3 	r: 74.21 	f1: 75.72 	 4577 	 5921 	 6168
wo 	p: 91.92 	r: 84.24 	f1: 87.91 	 2854 	 3105 	 3388
ni 	p: 83.21 	r: 80.42 	f1: 81.79 	 1224 	 1471 	 1522
best_thres [[0.47, 0.25, 0.22], [0.47, 0.28, 0.16], [0.46, 0.27, 0.15]]
f [0.8023, 0.8021, 0.8022]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 1 	 [0.46, 0.27, 0.15] 	 lr: 0.0002 	 f: 80.21567685657178
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 2 [0m
Train...
loss: tensor(1642.8929) lr: 0.0002 time: 2148.24
pred_count_train 41644

Test...
loss: tensor(494.7708) lr: 0.0001 time: 6178.84
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.69 	r: 75.52 	f1: 78.48 	 4658 	 5702 	 6168
wo 	p: 91.98 	r: 86.95 	f1: 89.39 	 2946 	 3203 	 3388
ni 	p: 84.35 	r: 77.2 	f1: 80.62 	 1175 	 1393 	 1522

[32m iter_1[0m
ga 	p: 82.01 	r: 75.32 	f1: 78.53 	 4646 	 5665 	 6168
wo 	p: 91.4 	r: 87.49 	f1: 89.4 	 2964 	 3243 	 3388
ni 	p: 84.95 	r: 76.02 	f1: 80.24 	 1157 	 1362 	 1522

[32m iter_2[0m
ga 	p: 81.75 	r: 75.6 	f1: 78.55 	 4663 	 5704 	 6168
wo 	p: 91.53 	r: 87.43 	f1: 89.43 	 2962 	 3236 	 3388
ni 	p: 85.92 	r: 75.36 	f1: 80.29 	 1147 	 1335 	 1522
best_thres [[0.49, 0.39, 0.14], [0.51, 0.3, 0.17], [0.49, 0.32, 0.19]]
f [0.8214, 0.8214, 0.8214]
e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 17 	 [0.49, 0.58, 0.12] 	 lr: 6.25e-05 	 f: 82.52347070637875
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 19 [0m
Train...

[32m iter_0[0m
ga 	p: 80.25 	r: 76.56 	f1: 78.36 	 4722 	 5884 	 6168
wo 	p: 93.45 	r: 85.09 	f1: 89.08 	 2883 	 3085 	 3388
ni 	p: 85.58 	r: 77.6 	f1: 81.39 	 1181 	 1380 	 1522

[32m iter_1[0m
ga 	p: 80.13 	r: 76.62 	f1: 78.34 	 4726 	 5898 	 6168
wo 	p: 93.11 	r: 85.3 	f1: 89.03 	 2890 	 3104 	 3388
ni 	p: 87.75 	r: 76.22 	f1: 81.58 	 1160 	 1322 	 1522

[32m iter_2[0m
ga 	p: 79.81 	r: 76.93 	f1: 78.35 	 4745 	 5945 	 6168
wo 	p: 93.48 	r: 85.04 	f1: 89.06 	 2881 	 3082 	 3388
ni 	p: 86.72 	r: 76.81 	f1: 81.46 	 1169 	 1348 	 1522
best_thres [[0.4, 0.53, 0.16], [0.38, 0.47, 0.19], [0.37, 0.53, 0.18]]
f [0.8201, 0.8201, 0.82]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse 	current best epoch 2 	 [0.51, 0.36, 0.25] 	 lr: 0.0002 	 f: 82.01072260343561
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse[0m [33m epoch 4 [0m
Train...
loss: tensor(2035.5914) lr: 0.0002 time: 1854.03
pred_count_train 41644

Test...
loss: tensor(2052.5950) lr: 0.0005 time: 2130.3
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.32 	r: 74.34 	f1: 77.67 	 4585 	 5638 	 6168
wo 	p: 93.14 	r: 84.62 	f1: 88.68 	 2867 	 3078 	 3388
ni 	p: 83.27 	r: 83.38 	f1: 83.32 	 1269 	 1524 	 1522

[32m iter_1[0m
ga 	p: 82.24 	r: 73.59 	f1: 77.68 	 4539 	 5519 	 6168
wo 	p: 93.66 	r: 84.21 	f1: 88.69 	 2853 	 3046 	 3388
ni 	p: 82.88 	r: 83.31 	f1: 83.09 	 1268 	 1530 	 1522

[32m iter_2[0m
ga 	p: 81.77 	r: 74.04 	f1: 77.72 	 4567 	 5585 	 6168
wo 	p: 93.63 	r: 84.15 	f1: 88.64 	 2851 	 3045 	 3388
ni 	p: 82.91 	r: 83.51 	f1: 83.21 	 1271 	 1533 	 1522
best_thres [[0.49, 0.42, 0.24], [0.52, 0.46, 0.22], [0.51, 0.46, 0.22]]
f [0.8182, 0.8181, 0.8181]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 2 	 [0.51, 0.46, 0.22] 	 lr: 0.0002 	 f: 81.81133496516664
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 3 [0m
Train...

[32m iter_0[0m
ga 	p: 76.87 	r: 74.32 	f1: 75.57 	 4584 	 5963 	 6168
wo 	p: 90.85 	r: 83.53 	f1: 87.04 	 2830 	 3115 	 3388
ni 	p: 84.09 	r: 75.36 	f1: 79.49 	 1147 	 1364 	 1522

[32m iter_1[0m
ga 	p: 78.91 	r: 72.49 	f1: 75.56 	 4471 	 5666 	 6168
wo 	p: 90.15 	r: 83.74 	f1: 86.82 	 2837 	 3147 	 3388
ni 	p: 81.72 	r: 77.27 	f1: 79.43 	 1176 	 1439 	 1522

[32m iter_2[0m
ga 	p: 77.18 	r: 74.21 	f1: 75.67 	 4577 	 5930 	 6168
wo 	p: 90.72 	r: 83.65 	f1: 87.04 	 2834 	 3124 	 3388
ni 	p: 79.05 	r: 79.83 	f1: 79.44 	 1215 	 1537 	 1522
best_thres [[0.24, 0.4, 0.25], [0.28, 0.32, 0.22], [0.24, 0.39, 0.16]]
f [0.7956, 0.7956, 0.7958]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 9 	 [0.24, 0.39, 0.16] 	 lr: 0.0005 	 f: 79.57655884313148
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 10 [0m
Train...
                                                                                 loss: tensor(268.6801) lr: 6.25e-05 time: 2759.47
pred_count_train 41644

Test...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
[32m iter_0[0m
ga 	p: 82.75 	r: 74.14 	f1: 78.21 	 4573 	 5526 	 6168
wo 	p: 92.5 	r: 86.69 	f1: 89.5 	 2937 	 3175 	 3388
ni 	p: 85.77 	r: 72.86 	f1: 78.79 	 1109 	 1293 	 1522

[32m iter_1[0m
ga 	p: 82.74 	r: 74.51 	f1: 78.41 	 4596 	 5555 	 6168
wo 	p: 91.97 	r: 86.84 	f1: 89.33 	 2942 	 3199 	 3388
ni 	p: 83.52 	r: 75.23 	f1: 79.16 	 1145 	 1371 	 1522

[32m iter_2[0m
ga 	p: 82.8 	r: 74.43 	f1: 78.39 	 4591 	 5545 	 6168
wo 	p: 92.48 	r: 86.42 	f1: 89.35 	 2928 	 3166 	 3388
ni 	p: 83.19 	r: 75.43 	f1: 79.12 	 1148 	 1380 	 1522
best_thres [[0.54, 0.44, 0.2], [0.51, 0.37, 0.14], [0.52, 0.45, 0.13]]
f [0.8181, 0.8185, 0.8186]
e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 17 	 [0.49, 0.58, 0.12] 	 lr: 6.25e-05 	 f: 82.52347070637875
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 20 [0m
Train...
loss: tensor(1974.0474) lr: 0.0005 time: 2082.57
pred_count_train 41644

Test...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          
[32m iter_0[0m
ga 	p: 79.27 	r: 71.56 	f1: 75.22 	 4414 	 5568 	 6168
wo 	p: 91.91 	r: 83.83 	f1: 87.68 	 2840 	 3090 	 3388
ni 	p: 83.13 	r: 76.08 	f1: 79.45 	 1158 	 1393 	 1522

[32m iter_1[0m
ga 	p: 80.51 	r: 71.14 	f1: 75.54 	 4388 	 5450 	 6168
wo 	p: 91.08 	r: 83.74 	f1: 87.25 	 2837 	 3115 	 3388
ni 	p: 84.97 	r: 76.15 	f1: 80.32 	 1159 	 1364 	 1522

[32m iter_2[0m
ga 	p: 79.61 	r: 71.95 	f1: 75.59 	 4438 	 5575 	 6168
wo 	p: 92.13 	r: 83.65 	f1: 87.69 	 2834 	 3076 	 3388
ni 	p: 84.55 	r: 76.94 	f1: 80.56 	 1171 	 1385 	 1522
best_thres [[0.33, 0.3, 0.14], [0.37, 0.23, 0.18], [0.34, 0.29, 0.17]]
f [0.7963, 0.7972, 0.7981]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 10 	 [0.34, 0.29, 0.17] 	 lr: 0.0005 	 f: 79.80711462450593
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 11 [0m
Train...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            loss: tensor(156.3546) lr: 6.25e-05 time: 2984.47
pred_count_train 41644

Test...
loss: tensor(1919.0273) lr: 0.0005 time: 2125.05
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.41 	r: 75.05 	f1: 77.17 	 4629 	 5829 	 6168
wo 	p: 90.01 	r: 88.02 	f1: 89.0 	 2982 	 3313 	 3388
ni 	p: 86.4 	r: 77.66 	f1: 81.8 	 1182 	 1368 	 1522

[32m iter_1[0m
ga 	p: 81.27 	r: 73.72 	f1: 77.31 	 4547 	 5595 	 6168
wo 	p: 91.78 	r: 86.39 	f1: 89.01 	 2927 	 3189 	 3388
ni 	p: 86.66 	r: 78.12 	f1: 82.17 	 1189 	 1372 	 1522

[32m iter_2[0m
ga 	p: 79.44 	r: 75.31 	f1: 77.32 	 4645 	 5847 	 6168
wo 	p: 92.26 	r: 86.19 	f1: 89.12 	 2920 	 3165 	 3388
ni 	p: 86.28 	r: 78.12 	f1: 82.0 	 1189 	 1378 	 1522
best_thres [[0.29, 0.26, 0.22], [0.34, 0.4, 0.19], [0.27, 0.44, 0.19]]
f [0.8146, 0.8153, 0.8154]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 2 	 [0.51, 0.46, 0.22] 	 lr: 0.0002 	 f: 81.81133496516664
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 5 [0m
Train...

[32m iter_0[0m
ga 	p: 82.77 	r: 74.14 	f1: 78.22 	 4573 	 5525 	 6168
wo 	p: 92.94 	r: 85.92 	f1: 89.29 	 2911 	 3132 	 3388
ni 	p: 85.88 	r: 71.94 	f1: 78.3 	 1095 	 1275 	 1522

[32m iter_1[0m
ga 	p: 82.61 	r: 74.5 	f1: 78.35 	 4595 	 5562 	 6168
wo 	p: 93.05 	r: 85.8 	f1: 89.28 	 2907 	 3124 	 3388
ni 	p: 85.17 	r: 73.19 	f1: 78.73 	 1114 	 1308 	 1522

[32m iter_2[0m
ga 	p: 82.67 	r: 74.42 	f1: 78.33 	 4590 	 5552 	 6168
wo 	p: 93.04 	r: 85.95 	f1: 89.35 	 2912 	 3130 	 3388
ni 	p: 85.51 	r: 72.86 	f1: 78.68 	 1109 	 1297 	 1522
best_thres [[0.51, 0.58, 0.17], [0.48, 0.6, 0.14], [0.49, 0.6, 0.15]]
f [0.8167, 0.8172, 0.8174]
e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 17 	 [0.49, 0.58, 0.12] 	 lr: 6.25e-05 	 f: 82.52347070637875
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 21 [0m
Train...

[32m iter_0[0m
ga 	p: 79.41 	r: 72.78 	f1: 75.95 	 4489 	 5653 	 6168
wo 	p: 91.29 	r: 83.85 	f1: 87.42 	 2841 	 3112 	 3388
ni 	p: 82.99 	r: 75.95 	f1: 79.31 	 1156 	 1393 	 1522

[32m iter_1[0m
ga 	p: 80.61 	r: 71.66 	f1: 75.87 	 4420 	 5483 	 6168
wo 	p: 90.26 	r: 84.27 	f1: 87.16 	 2855 	 3163 	 3388
ni 	p: 78.39 	r: 77.2 	f1: 77.79 	 1175 	 1499 	 1522

[32m iter_2[0m
ga 	p: 79.55 	r: 72.73 	f1: 75.99 	 4486 	 5639 	 6168
wo 	p: 91.85 	r: 83.44 	f1: 87.44 	 2827 	 3078 	 3388
ni 	p: 82.8 	r: 74.97 	f1: 78.69 	 1141 	 1378 	 1522
best_thres [[0.43, 0.55, 0.22], [0.43, 0.46, 0.17], [0.43, 0.62, 0.25]]
f [0.7992, 0.7978, 0.798]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 10 	 [0.34, 0.29, 0.17] 	 lr: 0.0005 	 f: 79.80711462450593
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 12 [0m
Train...
loss: tensor(326.0254) lr: 0.0001 time: 6198.56
pred_count_train 41644

Test...
loss: tensor(1002.8351) lr: 0.0002 time: 2042.33
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.23 	r: 73.75 	f1: 77.76 	 4549 	 5532 	 6168
wo 	p: 92.43 	r: 85.42 	f1: 88.79 	 2894 	 3131 	 3388
ni 	p: 81.42 	r: 76.02 	f1: 78.63 	 1157 	 1421 	 1522

[32m iter_1[0m
ga 	p: 79.83 	r: 76.12 	f1: 77.93 	 4695 	 5881 	 6168
wo 	p: 92.53 	r: 85.15 	f1: 88.69 	 2885 	 3118 	 3388
ni 	p: 84.7 	r: 73.85 	f1: 78.9 	 1124 	 1327 	 1522

[32m iter_2[0m
ga 	p: 81.18 	r: 74.9 	f1: 77.92 	 4620 	 5691 	 6168
wo 	p: 91.77 	r: 85.89 	f1: 88.73 	 2910 	 3171 	 3388
ni 	p: 84.37 	r: 73.78 	f1: 78.72 	 1123 	 1331 	 1522
best_thres [[0.54, 0.71, 0.13], [0.36, 0.8, 0.14], [0.47, 0.74, 0.14]]
f [0.8128, 0.813, 0.8132]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse 	current best epoch 4 	 [0.28, 0.8, 0.09] 	 lr: 0.0002 	 f: 82.08707627781776
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse[0m [33m epoch 7 [0m
Train...

[32m iter_0[0m
ga 	p: 79.75 	r: 75.92 	f1: 77.79 	 4683 	 5872 	 6168
wo 	p: 92.66 	r: 86.39 	f1: 89.41 	 2927 	 3159 	 3388
ni 	p: 83.92 	r: 73.06 	f1: 78.12 	 1112 	 1325 	 1522

[32m iter_1[0m
ga 	p: 82.42 	r: 73.98 	f1: 77.97 	 4563 	 5536 	 6168
wo 	p: 94.2 	r: 85.36 	f1: 89.56 	 2892 	 3070 	 3388
ni 	p: 82.25 	r: 74.9 	f1: 78.4 	 1140 	 1386 	 1522

[32m iter_2[0m
ga 	p: 81.39 	r: 74.85 	f1: 77.98 	 4617 	 5673 	 6168
wo 	p: 94.08 	r: 85.39 	f1: 89.52 	 2893 	 3075 	 3388
ni 	p: 82.24 	r: 74.84 	f1: 78.36 	 1139 	 1385 	 1522
best_thres [[0.34, 0.53, 0.26], [0.45, 0.71, 0.17], [0.41, 0.73, 0.17]]
f [0.8138, 0.8148, 0.8151]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 2 	 [0.51, 0.46, 0.22] 	 lr: 0.0002 	 f: 81.81133496516664
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 6 [0m
Train...
loss: tensor(97.8592) lr: 6.25e-05 time: 3018.13
pred_count_train 41644

Test...
loss: tensor(592.1472) lr: 0.0002 time: 1866.41
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 78.59 	r: 72.62 	f1: 75.49 	 4479 	 5699 	 6168
wo 	p: 91.53 	r: 84.18 	f1: 87.7 	 2852 	 3116 	 3388
ni 	p: 87.5 	r: 73.13 	f1: 79.67 	 1113 	 1272 	 1522

[32m iter_1[0m
ga 	p: 81.85 	r: 70.14 	f1: 75.54 	 4326 	 5285 	 6168
wo 	p: 92.17 	r: 82.73 	f1: 87.2 	 2803 	 3041 	 3388
ni 	p: 82.33 	r: 77.46 	f1: 79.82 	 1179 	 1432 	 1522

[32m iter_2[0m
ga 	p: 77.28 	r: 73.96 	f1: 75.59 	 4562 	 5903 	 6168
wo 	p: 91.27 	r: 84.21 	f1: 87.6 	 2853 	 3126 	 3388
ni 	p: 84.79 	r: 75.43 	f1: 79.83 	 1148 	 1354 	 1522
best_thres [[0.4, 0.53, 0.31], [0.51, 0.5, 0.18], [0.35, 0.51, 0.24]]
f [0.7979, 0.7977, 0.7978]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 10 	 [0.34, 0.29, 0.17] 	 lr: 0.0005 	 f: 79.80711462450593
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 13 [0m
Train...

[32m iter_0[0m
ga 	p: 79.59 	r: 76.01 	f1: 77.76 	 4688 	 5890 	 6168
wo 	p: 93.18 	r: 85.51 	f1: 89.18 	 2897 	 3109 	 3388
ni 	p: 83.27 	r: 74.9 	f1: 78.87 	 1140 	 1369 	 1522

[32m iter_1[0m
ga 	p: 82.62 	r: 73.69 	f1: 77.9 	 4545 	 5501 	 6168
wo 	p: 92.04 	r: 86.63 	f1: 89.25 	 2935 	 3189 	 3388
ni 	p: 83.76 	r: 74.9 	f1: 79.08 	 1140 	 1361 	 1522

[32m iter_2[0m
ga 	p: 80.15 	r: 75.94 	f1: 77.99 	 4684 	 5844 	 6168
wo 	p: 91.53 	r: 87.1 	f1: 89.26 	 2951 	 3224 	 3388
ni 	p: 84.3 	r: 74.77 	f1: 79.25 	 1138 	 1350 	 1522
best_thres [[0.36, 0.85, 0.09], [0.7, 0.62, 0.09], [0.37, 0.53, 0.1]]
f [0.8137, 0.8148, 0.8153]
load model: epoch17
e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 17 	 [0.49, 0.58, 0.12] 	 lr: 6.25e-05 	 f: 82.52347070637875
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 22 [0m
Train...

[32m iter_0[0m
ga 	p: 80.33 	r: 76.2 	f1: 78.21 	 4700 	 5851 	 6168
wo 	p: 93.01 	r: 84.77 	f1: 88.7 	 2872 	 3088 	 3388
ni 	p: 84.56 	r: 71.62 	f1: 77.55 	 1090 	 1289 	 1522

[32m iter_1[0m
ga 	p: 82.96 	r: 74.09 	f1: 78.27 	 4570 	 5509 	 6168
wo 	p: 93.59 	r: 84.03 	f1: 88.55 	 2847 	 3042 	 3388
ni 	p: 85.89 	r: 71.62 	f1: 78.11 	 1090 	 1269 	 1522

[32m iter_2[0m
ga 	p: 82.14 	r: 74.85 	f1: 78.33 	 4617 	 5621 	 6168
wo 	p: 91.75 	r: 85.66 	f1: 88.6 	 2902 	 3163 	 3388
ni 	p: 86.54 	r: 71.42 	f1: 78.26 	 1087 	 1256 	 1522
best_thres [[0.46, 0.57, 0.33], [0.59, 0.8, 0.29], [0.59, 0.47, 0.31]]
f [0.8131, 0.8136, 0.8141]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse 	current best epoch 4 	 [0.28, 0.8, 0.09] 	 lr: 0.0002 	 f: 82.08707627781776
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse[0m [33m epoch 8 [0m
Train...

[32m iter_0[0m
ga 	p: 81.95 	r: 75.52 	f1: 78.6 	 4658 	 5684 	 6168
wo 	p: 93.46 	r: 84.77 	f1: 88.9 	 2872 	 3073 	 3388
ni 	p: 86.92 	r: 73.32 	f1: 79.54 	 1116 	 1284 	 1522

[32m iter_1[0m
ga 	p: 81.14 	r: 76.51 	f1: 78.76 	 4719 	 5816 	 6168
wo 	p: 92.01 	r: 85.98 	f1: 88.89 	 2913 	 3166 	 3388
ni 	p: 87.02 	r: 73.98 	f1: 79.97 	 1126 	 1294 	 1522

[32m iter_2[0m
ga 	p: 80.76 	r: 76.9 	f1: 78.78 	 4743 	 5873 	 6168
wo 	p: 92.15 	r: 85.89 	f1: 88.91 	 2910 	 3158 	 3388
ni 	p: 85.94 	r: 75.1 	f1: 80.15 	 1143 	 1330 	 1522

[32m iter_3[0m
ga 	p: 80.36 	r: 77.2 	f1: 78.75 	 4762 	 5926 	 6168
wo 	p: 92.13 	r: 86.04 	f1: 88.98 	 2915 	 3164 	 3388
ni 	p: 85.94 	r: 75.1 	f1: 80.15 	 1143 	 1330 	 1522

[32m iter_4[0m
ga 	p: 80.33 	r: 77.22 	f1: 78.75 	 4763 	 5929 	 6168
wo 	p: 92.06 	r: 85.95 	f1: 88.9 	 2912 	 3163 	 3388
ni 	p: 86.06 	r: 75.03 	f1: 80.17 	 1142 	 1327 	 1522

[32m iter_5[0m
ga 	p: 80.3 	r: 77.17 	f1: 78.7 	 4760 	 5928 	 6168
wo 	p: 92.1 	r: 85.98 	f1: 88.93 	 2913 	 3163 	 3388
ni 	p: 86.06 	r: 75.03 	f1: 80.17 	 1142 	 1327 	 1522

[32m iter_6[0m
ga 	p: 80.33 	r: 77.22 	f1: 78.75 	 4763 	 5929 	 6168
wo 	p: 92.06 	r: 85.95 	f1: 88.9 	 2912 	 3163 	 3388
ni 	p: 86.06 	r: 75.03 	f1: 80.17 	 1142 	 1327 	 1522

[32m iter_7[0m
ga 	p: 80.31 	r: 77.17 	f1: 78.71 	 4760 	 5927 	 6168
wo 	p: 92.1 	r: 86.01 	f1: 88.95 	 2914 	 3164 	 3388
ni 	p: 86.06 	r: 75.03 	f1: 80.17 	 1142 	 1327 	 1522

[32m iter_8[0m
ga 	p: 80.32 	r: 77.22 	f1: 78.74 	 4763 	 5930 	 6168
wo 	p: 92.06 	r: 85.92 	f1: 88.89 	 2911 	 3162 	 3388
ni 	p: 86.06 	r: 75.03 	f1: 80.17 	 1142 	 1327 	 1522

[32m iter_9[0m
ga 	p: 80.31 	r: 77.17 	f1: 78.71 	 4760 	 5927 	 6168
wo 	p: 92.1 	r: 86.01 	f1: 88.95 	 2914 	 3164 	 3388
ni 	p: 86.06 	r: 75.03 	f1: 80.17 	 1142 	 1327 	 1522
best_thres [[0.54, 0.85, 0.29], [0.43, 0.66, 0.2], [0.39, 0.67, 0.16], [0.35, 0.65, 0.16], [0.35, 0.65, 0.16], [0.35, 0.65, 0.16], [0.35, 0.65, 0.16], [0.35, 0.65, 0.16], [0.35, 0.65, 0.16], [0.35, 0.65, 0.16]]
f [0.8188, 0.8195, 0.8199, 0.82, 0.8201, 0.8201, 0.8201, 0.8201, 0.8202, 0.8202]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse 	current best epoch 9 	 [0.49, 0.44, 0.1] 	 lr: 0.0001 	 f: 82.5649198462548
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse[0m [33m epoch 12 [0m
Train...

[32m iter_0[0m
ga 	p: 80.7 	r: 74.56 	f1: 77.51 	 4599 	 5699 	 6168
wo 	p: 94.63 	r: 83.74 	f1: 88.85 	 2837 	 2998 	 3388
ni 	p: 84.47 	r: 76.48 	f1: 80.28 	 1164 	 1378 	 1522

[32m iter_1[0m
ga 	p: 79.18 	r: 76.26 	f1: 77.69 	 4704 	 5941 	 6168
wo 	p: 93.7 	r: 84.33 	f1: 88.77 	 2857 	 3049 	 3388
ni 	p: 85.28 	r: 74.97 	f1: 79.79 	 1141 	 1338 	 1522

[32m iter_2[0m
ga 	p: 79.44 	r: 75.97 	f1: 77.67 	 4686 	 5899 	 6168
wo 	p: 93.72 	r: 84.62 	f1: 88.94 	 2867 	 3059 	 3388
ni 	p: 85.18 	r: 75.16 	f1: 79.86 	 1144 	 1343 	 1522
best_thres [[0.47, 0.72, 0.11], [0.35, 0.64, 0.1], [0.37, 0.69, 0.1]]
f [0.8131, 0.8131, 0.8133]
load model: epoch2
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 2 	 [0.51, 0.46, 0.22] 	 lr: 0.0002 	 f: 81.81133496516664
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 7 [0m
Train...
loss: tensor(1875.9423) lr: 0.0005 time: 2103.97
pred_count_train 41644

Test...
loss: tensor(485.4987) lr: 0.0002 time: 2158.97
pred_count_train 41644

Test...
loss: tensor(434.7043) lr: 3.125e-05 time: 2805.39
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 75.66 	r: 72.99 	f1: 74.3 	 4502 	 5950 	 6168
wo 	p: 91.41 	r: 84.15 	f1: 87.63 	 2851 	 3119 	 3388
ni 	p: 81.34 	r: 73.59 	f1: 77.27 	 1120 	 1377 	 1522

[32m iter_1[0m
ga 	p: 76.45 	r: 72.62 	f1: 74.48 	 4479 	 5859 	 6168
wo 	p: 93.2 	r: 82.88 	f1: 87.74 	 2808 	 3013 	 3388
ni 	p: 80.65 	r: 74.51 	f1: 77.46 	 1134 	 1406 	 1522

[32m iter_2[0m
ga 	p: 76.83 	r: 72.75 	f1: 74.73 	 4487 	 5840 	 6168
wo 	p: 93.28 	r: 83.12 	f1: 87.9 	 2816 	 3019 	 3388
ni 	p: 83.58 	r: 73.26 	f1: 78.08 	 1115 	 1334 	 1522
best_thres [[0.2, 0.38, 0.09], [0.21, 0.44, 0.09], [0.22, 0.51, 0.11]]
f [0.7873, 0.788, 0.7891]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 10 	 [0.34, 0.29, 0.17] 	 lr: 0.0005 	 f: 79.80711462450593
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 14 [0m
Train...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 
[32m iter_0[0m
ga 	p: 80.97 	r: 76.23 	f1: 78.53 	 4702 	 5807 	 6168
wo 	p: 93.28 	r: 86.04 	f1: 89.51 	 2915 	 3125 	 3388
ni 	p: 83.85 	r: 78.78 	f1: 81.23 	 1199 	 1430 	 1522

[32m iter_1[0m
ga 	p: 81.91 	r: 75.6 	f1: 78.63 	 4663 	 5693 	 6168
wo 	p: 92.58 	r: 86.6 	f1: 89.49 	 2934 	 3169 	 3388
ni 	p: 83.74 	r: 79.17 	f1: 81.39 	 1205 	 1439 	 1522

[32m iter_2[0m
ga 	p: 81.29 	r: 76.09 	f1: 78.6 	 4693 	 5773 	 6168
wo 	p: 92.52 	r: 86.57 	f1: 89.45 	 2933 	 3170 	 3388
ni 	p: 83.36 	r: 79.3 	f1: 81.28 	 1207 	 1448 	 1522
best_thres [[0.38, 0.58, 0.12], [0.43, 0.47, 0.12], [0.39, 0.47, 0.11]]
f [0.8224, 0.8229, 0.8229]
e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 17 	 [0.49, 0.58, 0.12] 	 lr: 6.25e-05 	 f: 82.52347070637875
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 23 [0m
Train...
                 loss: tensor(1877.7047) lr: 0.0005 time: 2033.23
pred_count_train 41644

Test...
                                                                                
[32m iter_0[0m
ga 	p: 77.59 	r: 69.96 	f1: 73.58 	 4315 	 5561 	 6168
wo 	p: 90.82 	r: 83.85 	f1: 87.2 	 2841 	 3128 	 3388
ni 	p: 82.34 	r: 74.77 	f1: 78.37 	 1138 	 1382 	 1522

[32m iter_1[0m
ga 	p: 77.84 	r: 71.19 	f1: 74.37 	 4391 	 5641 	 6168
wo 	p: 92.65 	r: 82.91 	f1: 87.51 	 2809 	 3032 	 3388
ni 	p: 81.61 	r: 76.08 	f1: 78.75 	 1158 	 1419 	 1522

[32m iter_2[0m
ga 	p: 76.05 	r: 72.5 	f1: 74.24 	 4472 	 5880 	 6168
wo 	p: 92.55 	r: 82.91 	f1: 87.47 	 2809 	 3035 	 3388
ni 	p: 82.25 	r: 75.49 	f1: 78.73 	 1149 	 1397 	 1522
best_thres [[0.38, 0.52, 0.21], [0.35, 0.53, 0.19], [0.29, 0.63, 0.22]]
f [0.7843, 0.787, 0.7874]
load model: epoch10
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 10 	 [0.34, 0.29, 0.17] 	 lr: 0.0005 	 f: 79.80711462450593
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 15 [0m
Train...
loss: tensor(1135.2905) lr: 0.0001 time: 2199.93
pred_count_train 41644

Test...
loss: tensor(244.4435) lr: 0.0001 time: 6212.42
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.34 	r: 74.68 	f1: 78.77 	 4606 	 5527 	 6168
wo 	p: 92.78 	r: 86.81 	f1: 89.69 	 2941 	 3170 	 3388
ni 	p: 85.59 	r: 76.87 	f1: 81.0 	 1170 	 1367 	 1522

[32m iter_1[0m
ga 	p: 83.22 	r: 75.44 	f1: 79.14 	 4653 	 5591 	 6168
wo 	p: 92.92 	r: 86.81 	f1: 89.76 	 2941 	 3165 	 3388
ni 	p: 84.74 	r: 78.45 	f1: 81.47 	 1194 	 1409 	 1522

[32m iter_2[0m
ga 	p: 82.79 	r: 75.75 	f1: 79.11 	 4672 	 5643 	 6168
wo 	p: 93.32 	r: 86.6 	f1: 89.83 	 2934 	 3144 	 3388
ni 	p: 85.5 	r: 77.86 	f1: 81.5 	 1185 	 1386 	 1522
best_thres [[0.59, 0.45, 0.14], [0.59, 0.47, 0.1], [0.59, 0.54, 0.11]]
f [0.8246, 0.826, 0.8265]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse 	current best epoch 9 	 [0.59, 0.54, 0.11] 	 lr: 0.0001 	 f: 82.64504368596393
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse[0m [33m epoch 10 [0m
Train...
loss: tensor(292.6238) lr: 3.125e-05 time: 2940.31
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.7 	r: 76.69 	f1: 78.64 	 4730 	 5861 	 6168
wo 	p: 92.23 	r: 87.19 	f1: 89.64 	 2954 	 3203 	 3388
ni 	p: 87.24 	r: 76.81 	f1: 81.69 	 1169 	 1340 	 1522

[32m iter_1[0m
ga 	p: 84.08 	r: 73.99 	f1: 78.72 	 4564 	 5428 	 6168
wo 	p: 93.18 	r: 86.3 	f1: 89.61 	 2924 	 3138 	 3388
ni 	p: 85.01 	r: 78.98 	f1: 81.88 	 1202 	 1414 	 1522

[32m iter_2[0m
ga 	p: 83.53 	r: 74.35 	f1: 78.68 	 4586 	 5490 	 6168
wo 	p: 92.44 	r: 87.01 	f1: 89.65 	 2948 	 3189 	 3388
ni 	p: 84.99 	r: 78.84 	f1: 81.8 	 1200 	 1412 	 1522
best_thres [[0.43, 0.46, 0.2], [0.62, 0.63, 0.12], [0.62, 0.46, 0.12]]
f [0.8242, 0.8248, 0.8249]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 7 	 [0.43, 0.47, 0.08] 	 lr: 0.0001 	 f: 82.85957183708169
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 9 [0m
Train...
loss: tensor(1709.8468) lr: 0.00025 time: 2084.23
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.64 	r: 74.34 	f1: 78.27 	 4585 	 5548 	 6168
wo 	p: 92.12 	r: 86.95 	f1: 89.46 	 2946 	 3198 	 3388
ni 	p: 83.04 	r: 77.2 	f1: 80.01 	 1175 	 1415 	 1522

[32m iter_1[0m
ga 	p: 82.53 	r: 74.51 	f1: 78.32 	 4596 	 5569 	 6168
wo 	p: 92.88 	r: 86.3 	f1: 89.47 	 2924 	 3148 	 3388
ni 	p: 85.69 	r: 74.77 	f1: 79.86 	 1138 	 1328 	 1522

[32m iter_2[0m
ga 	p: 81.21 	r: 75.7 	f1: 78.36 	 4669 	 5749 	 6168
wo 	p: 92.94 	r: 86.22 	f1: 89.45 	 2921 	 3143 	 3388
ni 	p: 86.18 	r: 74.57 	f1: 79.96 	 1135 	 1317 	 1522
best_thres [[0.53, 0.48, 0.12], [0.51, 0.59, 0.18], [0.41, 0.61, 0.19]]
f [0.8198, 0.8198, 0.8198]
e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 17 	 [0.49, 0.58, 0.12] 	 lr: 6.25e-05 	 f: 82.52347070637875
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 24 [0m
Train...
loss: tensor(495.9810) lr: 0.0001 time: 1921.9
pred_count_train 41644

Test...
loss: tensor(843.3781) lr: 0.0001 time: 1908.09
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.55 	r: 71.29 	f1: 76.07 	 4397 	 5392 	 6168
wo 	p: 91.07 	r: 86.04 	f1: 88.48 	 2915 	 3201 	 3388
ni 	p: 82.65 	r: 79.17 	f1: 80.87 	 1205 	 1458 	 1522

[32m iter_1[0m
ga 	p: 81.19 	r: 71.3 	f1: 75.93 	 4398 	 5417 	 6168
wo 	p: 91.57 	r: 84.98 	f1: 88.15 	 2879 	 3144 	 3388
ni 	p: 83.47 	r: 77.99 	f1: 80.64 	 1187 	 1422 	 1522

[32m iter_2[0m
ga 	p: 80.01 	r: 72.75 	f1: 76.21 	 4487 	 5608 	 6168
wo 	p: 92.54 	r: 84.92 	f1: 88.56 	 2877 	 3109 	 3388
ni 	p: 85.16 	r: 76.94 	f1: 80.84 	 1171 	 1375 	 1522
best_thres [[0.42, 0.34, 0.17], [0.4, 0.34, 0.23], [0.37, 0.43, 0.27]]
f [0.8062, 0.805, 0.8054]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 15 	 [0.37, 0.43, 0.27] 	 lr: 0.00025 	 f: 80.5429292929293
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 16 [0m
Train...

[32m iter_0[0m
ga 	p: 81.67 	r: 76.26 	f1: 78.87 	 4704 	 5760 	 6168
wo 	p: 92.5 	r: 85.95 	f1: 89.11 	 2912 	 3148 	 3388
ni 	p: 82.19 	r: 71.88 	f1: 76.69 	 1094 	 1331 	 1522

[32m iter_1[0m
ga 	p: 81.6 	r: 76.62 	f1: 79.03 	 4726 	 5792 	 6168
wo 	p: 92.7 	r: 85.83 	f1: 89.13 	 2908 	 3137 	 3388
ni 	p: 80.99 	r: 74.18 	f1: 77.43 	 1129 	 1394 	 1522

[32m iter_2[0m
ga 	p: 81.41 	r: 76.61 	f1: 78.93 	 4725 	 5804 	 6168
wo 	p: 93.01 	r: 85.66 	f1: 89.18 	 2902 	 3120 	 3388
ni 	p: 87.11 	r: 69.71 	f1: 77.45 	 1061 	 1218 	 1522
best_thres [[0.48, 0.61, 0.21], [0.47, 0.77, 0.14], [0.48, 0.82, 0.3]]
f [0.8172, 0.8181, 0.8183]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse 	current best epoch 9 	 [0.59, 0.54, 0.11] 	 lr: 0.0001 	 f: 82.64504368596393
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse[0m [33m epoch 11 [0m
Train...

[32m iter_0[0m
ga 	p: 82.53 	r: 74.61 	f1: 78.37 	 4602 	 5576 	 6168
wo 	p: 93.0 	r: 85.83 	f1: 89.27 	 2908 	 3127 	 3388
ni 	p: 83.43 	r: 72.47 	f1: 77.57 	 1103 	 1322 	 1522

[32m iter_1[0m
ga 	p: 81.68 	r: 75.7 	f1: 78.58 	 4669 	 5716 	 6168
wo 	p: 92.87 	r: 86.13 	f1: 89.37 	 2918 	 3142 	 3388
ni 	p: 80.95 	r: 75.36 	f1: 78.05 	 1147 	 1417 	 1522

[32m iter_2[0m
ga 	p: 81.97 	r: 75.47 	f1: 78.59 	 4655 	 5679 	 6168
wo 	p: 92.93 	r: 86.16 	f1: 89.42 	 2919 	 3141 	 3388
ni 	p: 81.99 	r: 74.77 	f1: 78.21 	 1138 	 1388 	 1522

[32m iter_3[0m
ga 	p: 81.8 	r: 75.65 	f1: 78.61 	 4666 	 5704 	 6168
wo 	p: 92.39 	r: 86.66 	f1: 89.43 	 2936 	 3178 	 3388
ni 	p: 82.7 	r: 74.11 	f1: 78.17 	 1128 	 1364 	 1522

[32m iter_4[0m
ga 	p: 81.8 	r: 75.65 	f1: 78.61 	 4666 	 5704 	 6168
wo 	p: 92.94 	r: 86.22 	f1: 89.45 	 2921 	 3143 	 3388
ni 	p: 82.03 	r: 74.7 	f1: 78.2 	 1137 	 1386 	 1522

[32m iter_5[0m
ga 	p: 81.8 	r: 75.62 	f1: 78.58 	 4664 	 5702 	 6168
wo 	p: 92.41 	r: 86.66 	f1: 89.44 	 2936 	 3177 	 3388
ni 	p: 82.03 	r: 74.7 	f1: 78.2 	 1137 	 1386 	 1522

[32m iter_6[0m
ga 	p: 81.8 	r: 75.65 	f1: 78.61 	 4666 	 5704 	 6168
wo 	p: 92.94 	r: 86.22 	f1: 89.45 	 2921 	 3143 	 3388
ni 	p: 82.09 	r: 74.7 	f1: 78.22 	 1137 	 1385 	 1522

[32m iter_7[0m
ga 	p: 81.8 	r: 75.62 	f1: 78.58 	 4664 	 5702 	 6168
wo 	p: 92.41 	r: 86.66 	f1: 89.44 	 2936 	 3177 	 3388
ni 	p: 82.03 	r: 74.7 	f1: 78.2 	 1137 	 1386 	 1522

[32m iter_8[0m
ga 	p: 81.8 	r: 75.65 	f1: 78.61 	 4666 	 5704 	 6168
wo 	p: 92.93 	r: 86.19 	f1: 89.43 	 2920 	 3142 	 3388
ni 	p: 82.09 	r: 74.7 	f1: 78.22 	 1137 	 1385 	 1522

[32m iter_9[0m
ga 	p: 81.8 	r: 75.62 	f1: 78.58 	 4664 	 5702 	 6168
wo 	p: 92.41 	r: 86.66 	f1: 89.44 	 2936 	 3177 	 3388
ni 	p: 82.03 	r: 74.7 	f1: 78.2 	 1137 	 1386 	 1522
best_thres [[0.52, 0.79, 0.18], [0.37, 0.77, 0.08], [0.4, 0.77, 0.09], [0.38, 0.68, 0.1], [0.38, 0.77, 0.09], [0.38, 0.68, 0.09], [0.38, 0.77, 0.09], [0.38, 0.68, 0.09], [0.38, 0.77, 0.09], [0.38, 0.68, 0.09]]
f [0.8163, 0.8172, 0.8176, 0.8179, 0.8181, 0.8182, 0.8183, 0.8183, 0.8184, 0.8184]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse 	current best epoch 9 	 [0.49, 0.44, 0.1] 	 lr: 0.0001 	 f: 82.5649198462548
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse[0m [33m epoch 13 [0m
Train...

[32m iter_0[0m
ga 	p: 79.98 	r: 76.09 	f1: 77.98 	 4693 	 5868 	 6168
wo 	p: 92.39 	r: 86.3 	f1: 89.24 	 2924 	 3165 	 3388
ni 	p: 84.76 	r: 76.02 	f1: 80.15 	 1157 	 1365 	 1522

[32m iter_1[0m
ga 	p: 81.33 	r: 75.49 	f1: 78.3 	 4656 	 5725 	 6168
wo 	p: 93.08 	r: 86.1 	f1: 89.45 	 2917 	 3134 	 3388
ni 	p: 86.65 	r: 75.49 	f1: 80.69 	 1149 	 1326 	 1522

[32m iter_2[0m
ga 	p: 80.33 	r: 76.28 	f1: 78.25 	 4705 	 5857 	 6168
wo 	p: 93.81 	r: 85.51 	f1: 89.47 	 2897 	 3088 	 3388
ni 	p: 85.68 	r: 76.28 	f1: 80.71 	 1161 	 1355 	 1522
best_thres [[0.36, 0.52, 0.15], [0.41, 0.62, 0.15], [0.36, 0.75, 0.13]]
f [0.8171, 0.8187, 0.8191]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 7 	 [0.43, 0.47, 0.08] 	 lr: 0.0001 	 f: 82.85957183708169
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 10 [0m
Train...
loss: tensor(191.6170) lr: 3.125e-05 time: 2758.2
pred_count_train 41644

Test...
loss: tensor(1511.1466) lr: 0.00025 time: 2011.47
pred_count_train 41644

Test...
loss: tensor(337.4766) lr: 0.0001 time: 2153.26
pred_count_train 41644

Test...
loss: tensor(619.6763) lr: 0.0001 time: 2217.63
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.05 	r: 72.41 	f1: 76.49 	 4466 	 5510 	 6168
wo 	p: 92.36 	r: 84.15 	f1: 88.06 	 2851 	 3087 	 3388
ni 	p: 85.76 	r: 75.16 	f1: 80.11 	 1144 	 1334 	 1522

[32m iter_1[0m
ga 	p: 78.2 	r: 74.85 	f1: 76.49 	 4617 	 5904 	 6168
wo 	p: 93.9 	r: 82.7 	f1: 87.95 	 2802 	 2984 	 3388
ni 	p: 83.62 	r: 77.46 	f1: 80.42 	 1179 	 1410 	 1522

[32m iter_2[0m
ga 	p: 81.15 	r: 72.44 	f1: 76.55 	 4468 	 5506 	 6168
wo 	p: 92.75 	r: 83.83 	f1: 88.06 	 2840 	 3062 	 3388
ni 	p: 84.92 	r: 76.94 	f1: 80.73 	 1171 	 1379 	 1522
best_thres [[0.37, 0.47, 0.13], [0.25, 0.59, 0.12], [0.38, 0.54, 0.14]]
f [0.8055, 0.805, 0.8055]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 16 	 [0.38, 0.54, 0.14] 	 lr: 0.00025 	 f: 80.54880933606687
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 17 [0m
Train...

[32m iter_0[0m
ga 	p: 82.06 	r: 74.43 	f1: 78.06 	 4591 	 5595 	 6168
wo 	p: 93.48 	r: 85.45 	f1: 89.28 	 2895 	 3097 	 3388
ni 	p: 86.67 	r: 72.21 	f1: 78.78 	 1099 	 1268 	 1522

[32m iter_1[0m
ga 	p: 79.96 	r: 76.28 	f1: 78.08 	 4705 	 5884 	 6168
wo 	p: 92.71 	r: 86.28 	f1: 89.37 	 2923 	 3153 	 3388
ni 	p: 84.56 	r: 73.78 	f1: 78.81 	 1123 	 1328 	 1522

[32m iter_2[0m
ga 	p: 82.67 	r: 74.09 	f1: 78.15 	 4570 	 5528 	 6168
wo 	p: 92.43 	r: 86.45 	f1: 89.34 	 2929 	 3169 	 3388
ni 	p: 85.44 	r: 73.26 	f1: 78.88 	 1115 	 1305 	 1522
best_thres [[0.52, 0.69, 0.22], [0.32, 0.52, 0.16], [0.55, 0.48, 0.18]]
f [0.8161, 0.8162, 0.8165]
e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 17 	 [0.49, 0.58, 0.12] 	 lr: 6.25e-05 	 f: 82.52347070637875
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 25 [0m
Train...

[32m iter_0[0m
ga 	p: 82.96 	r: 74.5 	f1: 78.5 	 4595 	 5539 	 6168
wo 	p: 92.93 	r: 85.77 	f1: 89.21 	 2906 	 3127 	 3388
ni 	p: 81.89 	r: 73.98 	f1: 77.74 	 1126 	 1375 	 1522

[32m iter_1[0m
ga 	p: 83.0 	r: 75.02 	f1: 78.8 	 4627 	 5575 	 6168
wo 	p: 92.51 	r: 86.04 	f1: 89.16 	 2915 	 3151 	 3388
ni 	p: 84.64 	r: 73.85 	f1: 78.88 	 1124 	 1328 	 1522

[32m iter_2[0m
ga 	p: 82.7 	r: 75.36 	f1: 78.86 	 4648 	 5620 	 6168
wo 	p: 92.32 	r: 86.25 	f1: 89.18 	 2922 	 3165 	 3388
ni 	p: 84.57 	r: 73.85 	f1: 78.85 	 1124 	 1329 	 1522
best_thres [[0.49, 0.57, 0.1], [0.5, 0.67, 0.1], [0.48, 0.64, 0.09]]
f [0.817, 0.8186, 0.8192]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse 	current best epoch 9 	 [0.59, 0.54, 0.11] 	 lr: 0.0001 	 f: 82.64504368596393
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse[0m [33m epoch 12 [0m
Train...

[32m iter_0[0m
ga 	p: 79.71 	r: 76.75 	f1: 78.2 	 4734 	 5939 	 6168
wo 	p: 92.55 	r: 85.74 	f1: 89.01 	 2905 	 3139 	 3388
ni 	p: 85.56 	r: 73.59 	f1: 79.12 	 1120 	 1309 	 1522

[32m iter_1[0m
ga 	p: 81.45 	r: 75.79 	f1: 78.52 	 4675 	 5740 	 6168
wo 	p: 93.34 	r: 85.68 	f1: 89.35 	 2903 	 3110 	 3388
ni 	p: 85.26 	r: 73.72 	f1: 79.07 	 1122 	 1316 	 1522

[32m iter_2[0m
ga 	p: 82.98 	r: 74.48 	f1: 78.5 	 4594 	 5536 	 6168
wo 	p: 93.06 	r: 85.83 	f1: 89.3 	 2908 	 3125 	 3388
ni 	p: 85.74 	r: 73.46 	f1: 79.12 	 1118 	 1304 	 1522
best_thres [[0.36, 0.54, 0.25], [0.46, 0.71, 0.17], [0.7, 0.71, 0.17]]
f [0.8161, 0.8176, 0.8181]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 7 	 [0.43, 0.47, 0.08] 	 lr: 0.0001 	 f: 82.85957183708169
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 11 [0m
Train...
loss: tensor(1390.9269) lr: 0.00025 time: 2082.99
pred_count_train 41644

Test...
loss: tensor(255.5448) lr: 0.0001 time: 2248.45
pred_count_train 41644

Test...
loss: tensor(448.7588) lr: 0.0001 time: 2236.99
pred_count_train 41644

Test...
loss: tensor(118.0845) lr: 3.125e-05 time: 3024.56
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 78.73 	r: 74.09 	f1: 76.34 	 4570 	 5805 	 6168
wo 	p: 93.55 	r: 83.5 	f1: 88.24 	 2829 	 3024 	 3388
ni 	p: 85.28 	r: 74.97 	f1: 79.79 	 1141 	 1338 	 1522

[32m iter_1[0m
ga 	p: 82.02 	r: 71.68 	f1: 76.5 	 4421 	 5390 	 6168
wo 	p: 92.95 	r: 84.06 	f1: 88.28 	 2848 	 3064 	 3388
ni 	p: 82.22 	r: 78.71 	f1: 80.43 	 1198 	 1457 	 1522

[32m iter_2[0m
ga 	p: 81.54 	r: 72.7 	f1: 76.87 	 4484 	 5499 	 6168
wo 	p: 92.65 	r: 84.09 	f1: 88.16 	 2849 	 3075 	 3388
ni 	p: 82.42 	r: 78.84 	f1: 80.59 	 1200 	 1456 	 1522
best_thres [[0.45, 0.65, 0.18], [0.58, 0.54, 0.14], [0.59, 0.61, 0.14]]
f [0.804, 0.8054, 0.8064]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 17 	 [0.59, 0.61, 0.14] 	 lr: 0.00025 	 f: 80.64159641312241
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 18 [0m
Train...
loss: tensor(192.8880) lr: 0.0001 time: 6145.37
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 84.13 	r: 74.34 	f1: 78.93 	 4585 	 5450 	 6168
wo 	p: 92.24 	r: 86.3 	f1: 89.17 	 2924 	 3170 	 3388
ni 	p: 80.79 	r: 75.43 	f1: 78.02 	 1148 	 1421 	 1522

[32m iter_1[0m
ga 	p: 82.49 	r: 75.47 	f1: 78.82 	 4655 	 5643 	 6168
wo 	p: 92.47 	r: 86.28 	f1: 89.27 	 2923 	 3161 	 3388
ni 	p: 83.22 	r: 73.98 	f1: 78.33 	 1126 	 1353 	 1522

[32m iter_2[0m
ga 	p: 82.84 	r: 75.31 	f1: 78.9 	 4645 	 5607 	 6168
wo 	p: 93.63 	r: 85.45 	f1: 89.35 	 2895 	 3092 	 3388
ni 	p: 81.77 	r: 75.16 	f1: 78.33 	 1144 	 1399 	 1522
best_thres [[0.41, 0.38, 0.09], [0.28, 0.4, 0.1], [0.31, 0.83, 0.07]]
f [0.8198, 0.8198, 0.8199]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse 	current best epoch 9 	 [0.59, 0.54, 0.11] 	 lr: 0.0001 	 f: 82.64504368596393
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse[0m [33m epoch 13 [0m
Train...

[32m iter_0[0m
ga 	p: 80.68 	r: 75.42 	f1: 77.96 	 4652 	 5766 	 6168
wo 	p: 91.62 	r: 86.48 	f1: 88.98 	 2930 	 3198 	 3388
ni 	p: 82.01 	r: 72.21 	f1: 76.8 	 1099 	 1340 	 1522

[32m iter_1[0m
ga 	p: 80.8 	r: 75.73 	f1: 78.18 	 4671 	 5781 	 6168
wo 	p: 91.88 	r: 86.45 	f1: 89.08 	 2929 	 3188 	 3388
ni 	p: 83.26 	r: 71.55 	f1: 76.96 	 1089 	 1308 	 1522

[32m iter_2[0m
ga 	p: 80.75 	r: 75.7 	f1: 78.14 	 4669 	 5782 	 6168
wo 	p: 92.51 	r: 85.98 	f1: 89.12 	 2913 	 3149 	 3388
ni 	p: 83.49 	r: 70.76 	f1: 76.6 	 1077 	 1290 	 1522
best_thres [[0.41, 0.51, 0.22], [0.4, 0.65, 0.2], [0.4, 0.83, 0.21]]
f [0.812, 0.8129, 0.8129]
load model: epoch7
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 7 	 [0.43, 0.47, 0.08] 	 lr: 0.0001 	 f: 82.85957183708169
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 12 [0m
Train...

[32m iter_0[0m
ga 	p: 81.3 	r: 74.56 	f1: 77.78 	 4599 	 5657 	 6168
wo 	p: 92.78 	r: 86.04 	f1: 89.28 	 2915 	 3142 	 3388
ni 	p: 83.68 	r: 74.44 	f1: 78.79 	 1133 	 1354 	 1522

[32m iter_1[0m
ga 	p: 81.19 	r: 75.16 	f1: 78.06 	 4636 	 5710 	 6168
wo 	p: 92.87 	r: 86.1 	f1: 89.36 	 2917 	 3141 	 3388
ni 	p: 86.5 	r: 72.01 	f1: 78.59 	 1096 	 1267 	 1522

[32m iter_2[0m
ga 	p: 81.14 	r: 75.19 	f1: 78.05 	 4638 	 5716 	 6168
wo 	p: 93.43 	r: 85.63 	f1: 89.36 	 2901 	 3105 	 3388
ni 	p: 86.47 	r: 71.81 	f1: 78.46 	 1093 	 1264 	 1522
best_thres [[0.46, 0.6, 0.12], [0.41, 0.61, 0.19], [0.41, 0.7, 0.19]]
f [0.8146, 0.8153, 0.8155]
load model: epoch17
e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 17 	 [0.49, 0.58, 0.12] 	 lr: 6.25e-05 	 f: 82.52347070637875
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 26 [0m
Train...
loss: tensor(1294.0050) lr: 0.00025 time: 2050.07
pred_count_train 41644

Test...
                                                                                                                                                                
[32m iter_0[0m
ga 	p: 78.29 	r: 74.3 	f1: 76.24 	 4583 	 5854 	 6168
wo 	p: 92.08 	r: 84.03 	f1: 87.87 	 2847 	 3092 	 3388
ni 	p: 82.38 	r: 74.97 	f1: 78.5 	 1141 	 1385 	 1522

[32m iter_1[0m
ga 	p: 81.26 	r: 72.15 	f1: 76.43 	 4450 	 5476 	 6168
wo 	p: 92.56 	r: 83.74 	f1: 87.93 	 2837 	 3065 	 3388
ni 	p: 81.22 	r: 76.41 	f1: 78.74 	 1163 	 1432 	 1522

[32m iter_2[0m
ga 	p: 80.23 	r: 73.54 	f1: 76.74 	 4536 	 5654 	 6168
wo 	p: 92.07 	r: 83.94 	f1: 87.82 	 2844 	 3089 	 3388
ni 	p: 83.21 	r: 75.56 	f1: 79.2 	 1150 	 1382 	 1522
best_thres [[0.46, 0.44, 0.16], [0.59, 0.44, 0.15], [0.57, 0.44, 0.19]]
f [0.8007, 0.8017, 0.8027]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 17 	 [0.59, 0.61, 0.14] 	 lr: 0.00025 	 f: 80.64159641312241
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 19 [0m
Train...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           loss: tensor(431.4235) lr: 2.5e-05 time: 2944.59
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.83 	r: 76.31 	f1: 78.03 	 4707 	 5896 	 6168
wo 	p: 93.47 	r: 85.74 	f1: 89.44 	 2905 	 3108 	 3388
ni 	p: 82.68 	r: 76.22 	f1: 79.32 	 1160 	 1403 	 1522

[32m iter_1[0m
ga 	p: 81.28 	r: 75.24 	f1: 78.14 	 4641 	 5710 	 6168
wo 	p: 92.38 	r: 86.6 	f1: 89.4 	 2934 	 3176 	 3388
ni 	p: 85.67 	r: 75.43 	f1: 80.22 	 1148 	 1340 	 1522

[32m iter_2[0m
ga 	p: 82.11 	r: 74.48 	f1: 78.11 	 4594 	 5595 	 6168
wo 	p: 92.34 	r: 86.81 	f1: 89.49 	 2941 	 3185 	 3388
ni 	p: 85.5 	r: 75.56 	f1: 80.22 	 1150 	 1345 	 1522

[32m iter_3[0m
ga 	p: 82.22 	r: 74.45 	f1: 78.14 	 4592 	 5585 	 6168
wo 	p: 92.34 	r: 86.84 	f1: 89.5 	 2942 	 3186 	 3388
ni 	p: 85.71 	r: 75.69 	f1: 80.39 	 1152 	 1344 	 1522

[32m iter_4[0m
ga 	p: 80.64 	r: 75.76 	f1: 78.12 	 4673 	 5795 	 6168
wo 	p: 92.43 	r: 86.84 	f1: 89.54 	 2942 	 3183 	 3388
ni 	p: 85.7 	r: 75.62 	f1: 80.35 	 1151 	 1343 	 1522

[32m iter_5[0m
ga 	p: 81.43 	r: 75.08 	f1: 78.13 	 4631 	 5687 	 6168
wo 	p: 92.34 	r: 86.81 	f1: 89.49 	 2941 	 3185 	 3388
ni 	p: 85.7 	r: 75.62 	f1: 80.35 	 1151 	 1343 	 1522

[32m iter_6[0m
ga 	p: 81.07 	r: 75.41 	f1: 78.14 	 4651 	 5737 	 6168
wo 	p: 92.43 	r: 86.84 	f1: 89.54 	 2942 	 3183 	 3388
ni 	p: 85.7 	r: 75.62 	f1: 80.35 	 1151 	 1343 	 1522

[32m iter_7[0m
ga 	p: 81.42 	r: 75.08 	f1: 78.12 	 4631 	 5688 	 6168
wo 	p: 92.34 	r: 86.81 	f1: 89.49 	 2941 	 3185 	 3388
ni 	p: 85.7 	r: 75.62 	f1: 80.35 	 1151 	 1343 	 1522

[32m iter_8[0m
ga 	p: 81.06 	r: 75.41 	f1: 78.13 	 4651 	 5738 	 6168
wo 	p: 92.43 	r: 86.84 	f1: 89.54 	 2942 	 3183 	 3388
ni 	p: 85.7 	r: 75.62 	f1: 80.35 	 1151 	 1343 	 1522

[32m iter_9[0m
ga 	p: 81.42 	r: 75.1 	f1: 78.13 	 4632 	 5689 	 6168
wo 	p: 92.34 	r: 86.81 	f1: 89.49 	 2941 	 3185 	 3388
ni 	p: 85.7 	r: 75.62 	f1: 80.35 	 1151 	 1343 	 1522
best_thres [[0.37, 0.79, 0.08], [0.47, 0.55, 0.09], [0.58, 0.49, 0.09], [0.58, 0.49, 0.09], [0.4, 0.5, 0.09], [0.49, 0.5, 0.09], [0.45, 0.5, 0.09], [0.49, 0.5, 0.09], [0.45, 0.5, 0.09], [0.49, 0.5, 0.09]]
f [0.8166, 0.8177, 0.8182, 0.8186, 0.8187, 0.8188, 0.8189, 0.819, 0.819, 0.8191]
load model: epoch9
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse 	current best epoch 9 	 [0.49, 0.44, 0.1] 	 lr: 0.0001 	 f: 82.5649198462548
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse[0m [33m epoch 14 [0m
Train...
loss: tensor(1197.9523) lr: 0.00025 time: 2066.62
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.26 	r: 75.31 	f1: 78.63 	 4645 	 5647 	 6168
wo 	p: 93.91 	r: 85.6 	f1: 89.56 	 2900 	 3088 	 3388
ni 	p: 86.34 	r: 76.41 	f1: 81.07 	 1163 	 1347 	 1522

[32m iter_1[0m
ga 	p: 81.47 	r: 76.07 	f1: 78.68 	 4692 	 5759 	 6168
wo 	p: 93.44 	r: 85.74 	f1: 89.43 	 2905 	 3109 	 3388
ni 	p: 83.47 	r: 78.98 	f1: 81.16 	 1202 	 1440 	 1522

[32m iter_2[0m
ga 	p: 81.81 	r: 75.91 	f1: 78.75 	 4682 	 5723 	 6168
wo 	p: 93.41 	r: 85.8 	f1: 89.45 	 2907 	 3112 	 3388
ni 	p: 83.48 	r: 78.71 	f1: 81.03 	 1198 	 1435 	 1522
best_thres [[0.49, 0.67, 0.15], [0.42, 0.61, 0.11], [0.44, 0.61, 0.11]]
f [0.8231, 0.823, 0.8231]
e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 17 	 [0.49, 0.58, 0.12] 	 lr: 6.25e-05 	 f: 82.52347070637875
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 27 [0m
Train...
loss: tensor(779.2606) lr: 5e-05 time: 2197.91
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 78.57 	r: 74.42 	f1: 76.44 	 4590 	 5842 	 6168
wo 	p: 92.11 	r: 84.39 	f1: 88.08 	 2859 	 3104 	 3388
ni 	p: 82.0 	r: 73.92 	f1: 77.75 	 1125 	 1372 	 1522

[32m iter_1[0m
ga 	p: 80.24 	r: 72.92 	f1: 76.41 	 4498 	 5606 	 6168
wo 	p: 93.93 	r: 83.12 	f1: 88.19 	 2816 	 2998 	 3388
ni 	p: 80.14 	r: 76.35 	f1: 78.2 	 1162 	 1450 	 1522

[32m iter_2[0m
ga 	p: 80.2 	r: 73.22 	f1: 76.55 	 4516 	 5631 	 6168
wo 	p: 93.87 	r: 83.18 	f1: 88.2 	 2818 	 3002 	 3388
ni 	p: 79.38 	r: 77.14 	f1: 78.24 	 1174 	 1479 	 1522
best_thres [[0.39, 0.51, 0.14], [0.48, 0.67, 0.13], [0.51, 0.72, 0.11]]
f [0.8015, 0.8018, 0.8022]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 17 	 [0.59, 0.61, 0.14] 	 lr: 0.00025 	 f: 80.64159641312241
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 20 [0m
Train...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           loss: tensor(306.4258) lr: 2.5e-05 time: 2787.3
pred_count_train 41644

Test...
loss: tensor(1127.7634) lr: 0.00025 time: 2053.05
pred_count_train 41644

Test...
                                                                                                                                                             
[32m iter_0[0m
ga 	p: 83.49 	r: 73.87 	f1: 78.38 	 4556 	 5457 	 6168
wo 	p: 93.4 	r: 85.66 	f1: 89.36 	 2902 	 3107 	 3388
ni 	p: 86.53 	r: 73.85 	f1: 79.69 	 1124 	 1299 	 1522

[32m iter_1[0m
ga 	p: 81.41 	r: 75.78 	f1: 78.5 	 4674 	 5741 	 6168
wo 	p: 93.23 	r: 85.74 	f1: 89.33 	 2905 	 3116 	 3388
ni 	p: 86.33 	r: 73.85 	f1: 79.6 	 1124 	 1302 	 1522

[32m iter_2[0m
ga 	p: 81.21 	r: 76.02 	f1: 78.53 	 4689 	 5774 	 6168
wo 	p: 93.35 	r: 85.74 	f1: 89.38 	 2905 	 3112 	 3388
ni 	p: 86.43 	r: 73.65 	f1: 79.53 	 1121 	 1297 	 1522
best_thres [[0.58, 0.65, 0.17], [0.41, 0.61, 0.18], [0.39, 0.62, 0.18]]
f [0.8196, 0.8196, 0.8197]
e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 17 	 [0.49, 0.58, 0.12] 	 lr: 6.25e-05 	 f: 82.52347070637875
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 28 [0m
Train...

[32m iter_0[0m
ga 	p: 78.03 	r: 73.87 	f1: 75.89 	 4556 	 5839 	 6168
wo 	p: 92.92 	r: 83.65 	f1: 88.04 	 2834 	 3050 	 3388
ni 	p: 80.8 	r: 75.76 	f1: 78.2 	 1153 	 1427 	 1522

[32m iter_1[0m
ga 	p: 79.53 	r: 73.87 	f1: 76.59 	 4556 	 5729 	 6168
wo 	p: 90.34 	r: 85.54 	f1: 87.87 	 2898 	 3208 	 3388
ni 	p: 85.03 	r: 74.24 	f1: 79.27 	 1130 	 1329 	 1522

[32m iter_2[0m
ga 	p: 78.56 	r: 74.24 	f1: 76.34 	 4579 	 5829 	 6168
wo 	p: 92.72 	r: 83.88 	f1: 88.08 	 2842 	 3065 	 3388
ni 	p: 84.85 	r: 73.98 	f1: 79.05 	 1126 	 1327 	 1522
best_thres [[0.4, 0.68, 0.13], [0.41, 0.45, 0.23], [0.4, 0.78, 0.24]]
f [0.7986, 0.8015, 0.8018]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 17 	 [0.59, 0.61, 0.14] 	 lr: 0.00025 	 f: 80.64159641312241
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 21 [0m
Train...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      loss: tensor(1090.0067) lr: 0.00025 time: 1997.59
pred_count_train 41644

Test...
                                                                                                                                                              loss: tensor(212.4411) lr: 2.5e-05 time: 2942.76
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.74 	r: 74.97 	f1: 78.21 	 4624 	 5657 	 6168
wo 	p: 91.8 	r: 87.25 	f1: 89.47 	 2956 	 3220 	 3388
ni 	p: 83.37 	r: 73.46 	f1: 78.1 	 1118 	 1341 	 1522

[32m iter_1[0m
ga 	p: 81.67 	r: 75.29 	f1: 78.35 	 4644 	 5686 	 6168
wo 	p: 92.29 	r: 86.51 	f1: 89.31 	 2931 	 3176 	 3388
ni 	p: 79.53 	r: 77.86 	f1: 78.69 	 1185 	 1490 	 1522

[32m iter_2[0m
ga 	p: 80.41 	r: 76.35 	f1: 78.33 	 4709 	 5856 	 6168
wo 	p: 92.69 	r: 86.07 	f1: 89.26 	 2916 	 3146 	 3388
ni 	p: 81.41 	r: 76.81 	f1: 79.04 	 1169 	 1436 	 1522
best_thres [[0.52, 0.44, 0.21], [0.6, 0.55, 0.09], [0.42, 0.76, 0.11]]
f [0.8169, 0.8172, 0.8173]
load model: epoch7
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 7 	 [0.43, 0.47, 0.08] 	 lr: 0.0001 	 f: 82.85957183708169
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 16 [0m
Train...

[32m iter_0[0m
ga 	p: 84.47 	r: 73.12 	f1: 78.39 	 4510 	 5339 	 6168
wo 	p: 91.45 	r: 86.81 	f1: 89.07 	 2941 	 3216 	 3388
ni 	p: 81.82 	r: 73.92 	f1: 77.67 	 1125 	 1375 	 1522

[32m iter_1[0m
ga 	p: 83.06 	r: 74.82 	f1: 78.73 	 4615 	 5556 	 6168
wo 	p: 93.01 	r: 85.66 	f1: 89.18 	 2902 	 3120 	 3388
ni 	p: 80.92 	r: 75.49 	f1: 78.11 	 1149 	 1420 	 1522

[32m iter_2[0m
ga 	p: 83.1 	r: 75.18 	f1: 78.94 	 4637 	 5580 	 6168
wo 	p: 92.73 	r: 85.89 	f1: 89.18 	 2910 	 3138 	 3388
ni 	p: 81.0 	r: 74.77 	f1: 77.76 	 1138 	 1405 	 1522
best_thres [[0.42, 0.38, 0.13], [0.28, 0.85, 0.07], [0.27, 0.78, 0.07]]
f [0.8165, 0.8175, 0.8181]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse 	current best epoch 9 	 [0.59, 0.54, 0.11] 	 lr: 0.0001 	 f: 82.64504368596393
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse[0m [33m epoch 17 [0m
Train...

[32m iter_0[0m
ga 	p: 77.89 	r: 72.97 	f1: 75.35 	 4501 	 5779 	 6168
wo 	p: 91.18 	r: 85.45 	f1: 88.22 	 2895 	 3175 	 3388
ni 	p: 82.89 	r: 70.37 	f1: 76.12 	 1071 	 1292 	 1522

[32m iter_1[0m
ga 	p: 78.69 	r: 73.57 	f1: 76.05 	 4538 	 5767 	 6168
wo 	p: 91.91 	r: 85.21 	f1: 88.44 	 2887 	 3141 	 3388
ni 	p: 83.36 	r: 72.73 	f1: 77.68 	 1107 	 1328 	 1522

[32m iter_2[0m
ga 	p: 80.01 	r: 72.99 	f1: 76.34 	 4502 	 5627 	 6168
wo 	p: 93.74 	r: 83.53 	f1: 88.34 	 2830 	 3019 	 3388
ni 	p: 82.3 	r: 73.32 	f1: 77.55 	 1116 	 1356 	 1522
best_thres [[0.31, 0.44, 0.16], [0.31, 0.44, 0.16], [0.37, 0.76, 0.15]]
f [0.7941, 0.7974, 0.7987]
load model: epoch17
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 17 	 [0.59, 0.61, 0.14] 	 lr: 0.00025 	 f: 80.64159641312241
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 22 [0m
Train...

[32m iter_0[0m
ga 	p: 81.67 	r: 76.43 	f1: 78.96 	 4714 	 5772 	 6168
wo 	p: 92.95 	r: 86.39 	f1: 89.55 	 2927 	 3149 	 3388
ni 	p: 81.63 	r: 79.7 	f1: 80.65 	 1213 	 1486 	 1522

[32m iter_1[0m
ga 	p: 79.81 	r: 78.18 	f1: 78.98 	 4822 	 6042 	 6168
wo 	p: 93.33 	r: 86.3 	f1: 89.68 	 2924 	 3133 	 3388
ni 	p: 82.67 	r: 79.63 	f1: 81.12 	 1212 	 1466 	 1522

[32m iter_2[0m
ga 	p: 79.8 	r: 78.16 	f1: 78.97 	 4821 	 6041 	 6168
wo 	p: 93.19 	r: 86.45 	f1: 89.7 	 2929 	 3143 	 3388
ni 	p: 82.62 	r: 79.63 	f1: 81.1 	 1212 	 1467 	 1522

[32m iter_3[0m
ga 	p: 79.86 	r: 78.18 	f1: 79.01 	 4822 	 6038 	 6168
wo 	p: 93.34 	r: 86.42 	f1: 89.75 	 2928 	 3137 	 3388
ni 	p: 82.83 	r: 79.57 	f1: 81.17 	 1211 	 1462 	 1522

[32m iter_4[0m
ga 	p: 79.83 	r: 78.18 	f1: 79.0 	 4822 	 6040 	 6168
wo 	p: 93.22 	r: 86.45 	f1: 89.71 	 2929 	 3142 	 3388
ni 	p: 82.79 	r: 79.63 	f1: 81.18 	 1212 	 1464 	 1522

[32m iter_5[0m
ga 	p: 79.86 	r: 78.19 	f1: 79.02 	 4823 	 6039 	 6168
wo 	p: 93.34 	r: 86.42 	f1: 89.75 	 2928 	 3137 	 3388
ni 	p: 82.83 	r: 79.57 	f1: 81.17 	 1211 	 1462 	 1522

[32m iter_6[0m
ga 	p: 79.83 	r: 78.18 	f1: 79.0 	 4822 	 6040 	 6168
wo 	p: 93.25 	r: 86.45 	f1: 89.72 	 2929 	 3141 	 3388
ni 	p: 82.79 	r: 79.63 	f1: 81.18 	 1212 	 1464 	 1522

[32m iter_7[0m
ga 	p: 79.86 	r: 78.18 	f1: 79.01 	 4822 	 6038 	 6168
wo 	p: 93.34 	r: 86.42 	f1: 89.75 	 2928 	 3137 	 3388
ni 	p: 82.78 	r: 79.57 	f1: 81.14 	 1211 	 1463 	 1522

[32m iter_8[0m
ga 	p: 79.84 	r: 78.19 	f1: 79.01 	 4823 	 6041 	 6168
wo 	p: 93.25 	r: 86.45 	f1: 89.72 	 2929 	 3141 	 3388
ni 	p: 82.79 	r: 79.63 	f1: 81.18 	 1212 	 1464 	 1522

[32m iter_9[0m
ga 	p: 79.86 	r: 78.18 	f1: 79.01 	 4822 	 6038 	 6168
wo 	p: 93.34 	r: 86.42 	f1: 89.75 	 2928 	 3137 	 3388
ni 	p: 82.78 	r: 79.57 	f1: 81.14 	 1211 	 1463 	 1522
best_thres [[0.35, 0.69, 0.11], [0.21, 0.71, 0.1], [0.21, 0.69, 0.1], [0.21, 0.69, 0.1], [0.21, 0.69, 0.1], [0.21, 0.69, 0.1], [0.21, 0.69, 0.1], [0.21, 0.69, 0.1], [0.21, 0.69, 0.1], [0.21, 0.69, 0.1]]
f [0.8242, 0.8246, 0.8247, 0.8248, 0.8249, 0.825, 0.825, 0.825, 0.8251, 0.8251]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse 	current best epoch 9 	 [0.49, 0.44, 0.1] 	 lr: 0.0001 	 f: 82.5649198462548
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse[0m [33m epoch 15 [0m
Train...

[32m iter_0[0m
ga 	p: 81.52 	r: 74.76 	f1: 77.99 	 4611 	 5656 	 6168
wo 	p: 92.11 	r: 86.81 	f1: 89.38 	 2941 	 3193 	 3388
ni 	p: 85.53 	r: 73.39 	f1: 79.0 	 1117 	 1306 	 1522

[32m iter_1[0m
ga 	p: 81.9 	r: 74.63 	f1: 78.1 	 4603 	 5620 	 6168
wo 	p: 93.16 	r: 85.63 	f1: 89.23 	 2901 	 3114 	 3388
ni 	p: 84.77 	r: 74.24 	f1: 79.16 	 1130 	 1333 	 1522

[32m iter_2[0m
ga 	p: 82.02 	r: 74.56 	f1: 78.11 	 4599 	 5607 	 6168
wo 	p: 93.36 	r: 85.54 	f1: 89.28 	 2898 	 3104 	 3388
ni 	p: 84.84 	r: 73.92 	f1: 79.0 	 1125 	 1326 	 1522
best_thres [[0.46, 0.41, 0.18], [0.48, 0.57, 0.16], [0.49, 0.59, 0.16]]
f [0.8166, 0.8166, 0.8166]
e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 17 	 [0.49, 0.58, 0.12] 	 lr: 6.25e-05 	 f: 82.52347070637875
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 29 [0m
Train...
loss: tensor(1121.9359) lr: 0.000125 time: 2051.26
pred_count_train 41644

Test...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        
[32m iter_0[0m
ga 	p: 79.38 	r: 73.75 	f1: 76.46 	 4549 	 5731 	 6168
wo 	p: 91.62 	r: 85.18 	f1: 88.28 	 2886 	 3150 	 3388
ni 	p: 85.0 	r: 73.32 	f1: 78.73 	 1116 	 1313 	 1522

[32m iter_1[0m
ga 	p: 79.44 	r: 74.11 	f1: 76.68 	 4571 	 5754 	 6168
wo 	p: 92.34 	r: 85.06 	f1: 88.55 	 2882 	 3121 	 3388
ni 	p: 84.11 	r: 75.82 	f1: 79.75 	 1154 	 1372 	 1522

[32m iter_2[0m
ga 	p: 79.6 	r: 74.35 	f1: 76.89 	 4586 	 5761 	 6168
wo 	p: 91.97 	r: 85.54 	f1: 88.64 	 2898 	 3151 	 3388
ni 	p: 84.76 	r: 75.62 	f1: 79.93 	 1151 	 1358 	 1522
best_thres [[0.38, 0.34, 0.13], [0.37, 0.34, 0.12], [0.38, 0.33, 0.12]]
f [0.804, 0.8056, 0.8067]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 22 	 [0.38, 0.33, 0.12] 	 lr: 0.000125 	 f: 80.67245288920165
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 23 [0m
Train...
loss: tensor(141.0959) lr: 2.5e-05 time: 2807.18
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.83 	r: 75.19 	f1: 77.91 	 4638 	 5738 	 6168
wo 	p: 93.33 	r: 85.89 	f1: 89.46 	 2910 	 3118 	 3388
ni 	p: 83.3 	r: 74.05 	f1: 78.4 	 1127 	 1353 	 1522

[32m iter_1[0m
ga 	p: 80.91 	r: 75.19 	f1: 77.95 	 4638 	 5732 	 6168
wo 	p: 93.87 	r: 85.39 	f1: 89.43 	 2893 	 3082 	 3388
ni 	p: 84.94 	r: 72.27 	f1: 78.1 	 1100 	 1295 	 1522

[32m iter_2[0m
ga 	p: 79.27 	r: 76.67 	f1: 77.95 	 4729 	 5966 	 6168
wo 	p: 93.35 	r: 85.77 	f1: 89.4 	 2906 	 3113 	 3388
ni 	p: 86.78 	r: 70.76 	f1: 77.96 	 1077 	 1241 	 1522
best_thres [[0.41, 0.63, 0.14], [0.4, 0.7, 0.18], [0.24, 0.63, 0.23]]
f [0.8151, 0.8149, 0.8147]
e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	best in epoch 17 	 [0.49, 0.58, 0.12] 	 lr: 6.25e-05 	 f: 82.52347070637875
model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse.h5
e2e-stack model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse
gpu: 0
6 3 2
vocab size:  29396
prediction mode: model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse test [0.5, 0.5, 0.5]
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(912.7549) lr: 0.000125 time: 2028.46
pred_count_train 41644

Test...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
[32m iter_0[0m
ga 	p: 80.2 	r: 72.57 	f1: 76.19 	 4476 	 5581 	 6168
wo 	p: 93.76 	r: 83.44 	f1: 88.3 	 2827 	 3015 	 3388
ni 	p: 81.61 	r: 72.6 	f1: 76.84 	 1105 	 1354 	 1522

[32m iter_1[0m
ga 	p: 81.45 	r: 71.74 	f1: 76.29 	 4425 	 5433 	 6168
wo 	p: 92.2 	r: 84.77 	f1: 88.33 	 2872 	 3115 	 3388
ni 	p: 82.06 	r: 74.84 	f1: 78.28 	 1139 	 1388 	 1522

[32m iter_2[0m
ga 	p: 80.98 	r: 72.97 	f1: 76.77 	 4501 	 5558 	 6168
wo 	p: 92.71 	r: 84.42 	f1: 88.37 	 2860 	 3085 	 3388
ni 	p: 80.52 	r: 75.76 	f1: 78.06 	 1153 	 1432 	 1522
best_thres [[0.47, 0.68, 0.16], [0.57, 0.56, 0.17], [0.53, 0.66, 0.14]]
f [0.7997, 0.8013, 0.8025]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 22 	 [0.38, 0.33, 0.12] 	 lr: 0.000125 	 f: 80.67245288920165
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 24 [0m
Train...

[32m iter_0[0m
ga 	p: 81.88 	r: 76.56 	f1: 79.13 	 4722 	 5767 	 6168
wo 	p: 92.93 	r: 86.51 	f1: 89.61 	 2931 	 3154 	 3388
ni 	p: 83.8 	r: 76.48 	f1: 79.97 	 1164 	 1389 	 1522

[32m iter_1[0m
ga 	p: 81.69 	r: 77.3 	f1: 79.43 	 4768 	 5837 	 6168
wo 	p: 93.5 	r: 86.19 	f1: 89.69 	 2920 	 3123 	 3388
ni 	p: 85.63 	r: 75.95 	f1: 80.5 	 1156 	 1350 	 1522

[32m iter_2[0m
ga 	p: 82.25 	r: 76.95 	f1: 79.51 	 4746 	 5770 	 6168
wo 	p: 94.25 	r: 85.57 	f1: 89.7 	 2899 	 3076 	 3388
ni 	p: 86.9 	r: 74.97 	f1: 80.49 	 1141 	 1313 	 1522
best_thres [[0.3, 0.38, 0.11], [0.26, 0.43, 0.11], [0.29, 0.68, 0.13]]
f [0.8245, 0.8257, 0.8263]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse 	current best epoch 9 	 [0.59, 0.54, 0.11] 	 lr: 0.0001 	 f: 82.64504368596393
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse[0m [33m epoch 19 [0m
Train...
loss: tensor(3226.1221) lr: 0.001 time: 2977.95
pred_count_train 41644

Test...
loss: tensor(671.6790) lr: 2.5e-05 time: 1963.56
pred_count_train 41644

Test...
loss: tensor(245.7187) lr: 2.5e-05 time: 1965.35
pred_count_train 41644

Test...
loss: tensor(772.1203) lr: 0.000125 time: 2039.03
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.67 	r: 76.3 	f1: 78.89 	 4706 	 5762 	 6168
wo 	p: 93.85 	r: 85.57 	f1: 89.52 	 2899 	 3089 	 3388
ni 	p: 84.82 	r: 76.02 	f1: 80.18 	 1157 	 1364 	 1522

[32m iter_1[0m
ga 	p: 81.76 	r: 76.46 	f1: 79.02 	 4716 	 5768 	 6168
wo 	p: 93.25 	r: 86.51 	f1: 89.76 	 2931 	 3143 	 3388
ni 	p: 83.84 	r: 77.4 	f1: 80.49 	 1178 	 1405 	 1522

[32m iter_2[0m
ga 	p: 81.55 	r: 76.65 	f1: 79.02 	 4728 	 5798 	 6168
wo 	p: 93.62 	r: 86.25 	f1: 89.78 	 2922 	 3121 	 3388
ni 	p: 84.34 	r: 77.14 	f1: 80.58 	 1174 	 1392 	 1522
best_thres [[0.4, 0.49, 0.14], [0.38, 0.38, 0.09], [0.36, 0.43, 0.09]]
f [0.823, 0.824, 0.8244]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 16 	 [0.31, 0.42, 0.15] 	 lr: 2.5e-05 	 f: 83.03189329334957
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 19 [0m
Train...

[32m iter_0[0m
ga 	p: 74.33 	r: 68.97 	f1: 71.55 	 4254 	 5723 	 6168
wo 	p: 92.17 	r: 79.55 	f1: 85.39 	 2695 	 2924 	 3388
ni 	p: 81.32 	r: 72.93 	f1: 76.9 	 1110 	 1365 	 1522

[32m iter_1[0m
ga 	p: 73.14 	r: 69.7 	f1: 71.38 	 4299 	 5878 	 6168
wo 	p: 91.94 	r: 79.49 	f1: 85.26 	 2693 	 2929 	 3388
ni 	p: 78.63 	r: 75.69 	f1: 77.13 	 1152 	 1465 	 1522

[32m iter_2[0m
ga 	p: 74.44 	r: 68.92 	f1: 71.57 	 4251 	 5711 	 6168
wo 	p: 92.09 	r: 79.37 	f1: 85.26 	 2689 	 2920 	 3388
ni 	p: 78.49 	r: 75.76 	f1: 77.1 	 1153 	 1469 	 1522
best_thres [[0.52, 0.24, 0.15], [0.47, 0.24, 0.11], [0.52, 0.24, 0.11]]
f [0.7642, 0.7636, 0.7638]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 1 	 [0.52, 0.24, 0.11] 	 lr: 0.001 	 f: 76.38089848784935
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 2 [0m
Train...

[32m iter_0[0m
ga 	p: 83.98 	r: 74.27 	f1: 78.83 	 4581 	 5455 	 6168
wo 	p: 91.66 	r: 86.95 	f1: 89.25 	 2946 	 3214 	 3388
ni 	p: 85.06 	r: 73.32 	f1: 78.76 	 1116 	 1312 	 1522

[32m iter_1[0m
ga 	p: 83.01 	r: 75.34 	f1: 78.99 	 4647 	 5598 	 6168
wo 	p: 92.1 	r: 86.72 	f1: 89.33 	 2938 	 3190 	 3388
ni 	p: 82.17 	r: 76.28 	f1: 79.11 	 1161 	 1413 	 1522

[32m iter_2[0m
ga 	p: 82.56 	r: 75.92 	f1: 79.1 	 4683 	 5672 	 6168
wo 	p: 93.52 	r: 85.57 	f1: 89.36 	 2899 	 3100 	 3388
ni 	p: 83.92 	r: 74.77 	f1: 79.08 	 1138 	 1356 	 1522
best_thres [[0.44, 0.32, 0.15], [0.36, 0.32, 0.07], [0.32, 0.72, 0.09]]
f [0.8208, 0.8214, 0.8218]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse 	current best epoch 9 	 [0.59, 0.54, 0.11] 	 lr: 0.0001 	 f: 82.64504368596393
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse[0m [33m epoch 20 [0m
Train...

[32m iter_0[0m
ga 	p: 79.55 	r: 72.02 	f1: 75.6 	 4442 	 5584 	 6168
wo 	p: 94.52 	r: 82.5 	f1: 88.1 	 2795 	 2957 	 3388
ni 	p: 80.78 	r: 72.34 	f1: 76.33 	 1101 	 1363 	 1522

[32m iter_1[0m
ga 	p: 80.13 	r: 72.65 	f1: 76.21 	 4481 	 5592 	 6168
wo 	p: 93.63 	r: 83.35 	f1: 88.19 	 2824 	 3016 	 3388
ni 	p: 83.93 	r: 72.4 	f1: 77.74 	 1102 	 1313 	 1522

[32m iter_2[0m
ga 	p: 78.93 	r: 73.65 	f1: 76.2 	 4543 	 5756 	 6168
wo 	p: 92.92 	r: 84.0 	f1: 88.23 	 2846 	 3063 	 3388
ni 	p: 78.73 	r: 75.89 	f1: 77.28 	 1155 	 1467 	 1522
best_thres [[0.48, 0.71, 0.11], [0.47, 0.71, 0.15], [0.43, 0.59, 0.08]]
f [0.7948, 0.7977, 0.7985]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 22 	 [0.38, 0.33, 0.12] 	 lr: 0.000125 	 f: 80.67245288920165
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 25 [0m
Train...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               loss: tensor(657.1654) lr: 0.000125 time: 2036.19
pred_count_train 41644

Test...
loss: tensor(2535.4485) lr: 0.001 time: 2871.46
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.8 	r: 75.78 	f1: 78.21 	 4674 	 5785 	 6168
wo 	p: 92.26 	r: 86.16 	f1: 89.1 	 2919 	 3164 	 3388
ni 	p: 86.0 	r: 73.85 	f1: 79.46 	 1124 	 1307 	 1522

[32m iter_1[0m
ga 	p: 82.58 	r: 74.84 	f1: 78.52 	 4616 	 5590 	 6168
wo 	p: 92.53 	r: 86.25 	f1: 89.28 	 2922 	 3158 	 3388
ni 	p: 86.12 	r: 74.18 	f1: 79.7 	 1129 	 1311 	 1522

[32m iter_2[0m
ga 	p: 82.59 	r: 74.92 	f1: 78.57 	 4621 	 5595 	 6168
wo 	p: 92.56 	r: 86.25 	f1: 89.29 	 2922 	 3157 	 3388
ni 	p: 83.83 	r: 75.95 	f1: 79.7 	 1156 	 1379 	 1522
best_thres [[0.31, 0.38, 0.17], [0.38, 0.38, 0.12], [0.38, 0.37, 0.09]]
f [0.8172, 0.8186, 0.8192]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 16 	 [0.31, 0.42, 0.15] 	 lr: 2.5e-05 	 f: 83.03189329334957
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 20 [0m
Train...

[32m iter_0[0m
ga 	p: 82.95 	r: 75.02 	f1: 78.78 	 4627 	 5578 	 6168
wo 	p: 92.0 	r: 86.51 	f1: 89.17 	 2931 	 3186 	 3388
ni 	p: 80.44 	r: 73.78 	f1: 76.97 	 1123 	 1396 	 1522

[32m iter_1[0m
ga 	p: 81.9 	r: 76.23 	f1: 78.97 	 4702 	 5741 	 6168
wo 	p: 91.86 	r: 86.66 	f1: 89.19 	 2936 	 3196 	 3388
ni 	p: 79.83 	r: 75.16 	f1: 77.43 	 1144 	 1433 	 1522

[32m iter_2[0m
ga 	p: 82.69 	r: 75.65 	f1: 79.01 	 4666 	 5643 	 6168
wo 	p: 92.47 	r: 86.3 	f1: 89.28 	 2924 	 3162 	 3388
ni 	p: 79.39 	r: 75.43 	f1: 77.36 	 1148 	 1446 	 1522
best_thres [[0.31, 0.31, 0.1], [0.22, 0.25, 0.06], [0.26, 0.32, 0.05]]
f [0.8175, 0.8182, 0.8186]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse 	current best epoch 9 	 [0.59, 0.54, 0.11] 	 lr: 0.0001 	 f: 82.64504368596393
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse[0m [33m epoch 21 [0m
Train...

[32m iter_0[0m
ga 	p: 78.48 	r: 72.03 	f1: 75.12 	 4443 	 5661 	 6168
wo 	p: 91.64 	r: 84.74 	f1: 88.05 	 2871 	 3133 	 3388
ni 	p: 80.75 	r: 67.81 	f1: 73.71 	 1032 	 1278 	 1522

[32m iter_1[0m
ga 	p: 81.49 	r: 70.87 	f1: 75.81 	 4371 	 5364 	 6168
wo 	p: 92.31 	r: 84.62 	f1: 88.3 	 2867 	 3106 	 3388
ni 	p: 84.53 	r: 67.87 	f1: 75.29 	 1033 	 1222 	 1522

[32m iter_2[0m
ga 	p: 81.68 	r: 70.98 	f1: 75.95 	 4378 	 5360 	 6168
wo 	p: 92.77 	r: 84.8 	f1: 88.6 	 2873 	 3097 	 3388
ni 	p: 84.56 	r: 67.67 	f1: 75.18 	 1030 	 1218 	 1522
best_thres [[0.43, 0.51, 0.14], [0.64, 0.6, 0.21], [0.7, 0.74, 0.22]]
f [0.7892, 0.7928, 0.7945]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 22 	 [0.38, 0.33, 0.12] 	 lr: 0.000125 	 f: 80.67245288920165
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 26 [0m
Train...

[32m iter_0[0m
ga 	p: 74.2 	r: 70.14 	f1: 72.11 	 4326 	 5830 	 6168
wo 	p: 90.76 	r: 81.2 	f1: 85.71 	 2751 	 3031 	 3388
ni 	p: 82.28 	r: 75.03 	f1: 78.49 	 1142 	 1388 	 1522

[32m iter_1[0m
ga 	p: 74.47 	r: 69.96 	f1: 72.15 	 4315 	 5794 	 6168
wo 	p: 90.96 	r: 81.08 	f1: 85.74 	 2747 	 3020 	 3388
ni 	p: 79.67 	r: 77.0 	f1: 78.32 	 1172 	 1471 	 1522

[32m iter_2[0m
ga 	p: 74.28 	r: 70.22 	f1: 72.19 	 4331 	 5831 	 6168
wo 	p: 90.8 	r: 81.29 	f1: 85.78 	 2754 	 3033 	 3388
ni 	p: 82.15 	r: 74.7 	f1: 78.25 	 1137 	 1384 	 1522
best_thres [[0.33, 0.29, 0.16], [0.33, 0.29, 0.12], [0.33, 0.29, 0.15]]
f [0.7708, 0.7708, 0.7709]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 2 	 [0.33, 0.29, 0.15] 	 lr: 0.001 	 f: 77.09010247438141
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 3 [0m
Train...
loss: tensor(438.1453) lr: 2.5e-05 time: 2221.1
pred_count_train 41644

Test...
loss: tensor(111.7447) lr: 2.5e-05 time: 2233.46
pred_count_train 41644

Test...


[32m iter_0[0m
ga 	p: 80.54 	r: 75.24 	f1: 77.8 	 4641 	 5762 	 6168
wo 	p: 92.38 	r: 86.19 	f1: 89.17 	 2920 	 3161 	 3388
ni 	p: 83.12 	r: 72.14 	f1: 77.24 	 1098 	 1321 	 1522

[32m iter_1[0m
ga 	p: 81.63 	r: 75.36 	f1: 78.37 	 4648 	 5694 	 6168
wo 	p: 92.57 	r: 86.04 	f1: 89.18 	 2915 	 3149 	 3388
ni 	p: 81.95 	r: 73.98 	f1: 77.76 	 1126 	 1374 	 1522

[32m iter_2[0m
ga 	p: 81.25 	r: 75.65 	f1: 78.35 	 4666 	 5743 	 6168
wo 	p: 92.47 	r: 86.3 	f1: 89.28 	 2924 	 3162 	 3388
ni 	p: 83.27 	r: 73.26 	f1: 77.94 	 1115 	 1339 	 1522
best_thres [[0.35, 0.47, 0.18], [0.36, 0.52, 0.12], [0.34, 0.47, 0.13]]
f [0.8122, 0.8141, 0.8149]
load model: epoch16
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 16 	 [0.31, 0.42, 0.15] 	 lr: 2.5e-05 	 f: 83.03189329334957
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 21 [0m
Train...
loss: tensor(155.4695) lr: 5e-05 time: 6037.21
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.1 	r: 74.59 	f1: 78.62 	 4601 	 5537 	 6168
wo 	p: 92.61 	r: 85.48 	f1: 88.9 	 2896 	 3127 	 3388
ni 	p: 79.99 	r: 74.05 	f1: 76.9 	 1127 	 1409 	 1522

[32m iter_1[0m
ga 	p: 83.36 	r: 74.56 	f1: 78.72 	 4599 	 5517 	 6168
wo 	p: 92.95 	r: 85.63 	f1: 89.14 	 2901 	 3121 	 3388
ni 	p: 83.43 	r: 72.8 	f1: 77.75 	 1108 	 1328 	 1522

[32m iter_2[0m
ga 	p: 81.78 	r: 76.04 	f1: 78.8 	 4690 	 5735 	 6168
wo 	p: 93.27 	r: 85.48 	f1: 89.2 	 2896 	 3105 	 3388
ni 	p: 82.67 	r: 73.32 	f1: 77.72 	 1116 	 1350 	 1522
best_thres [[0.36, 0.46, 0.08], [0.39, 0.56, 0.08], [0.24, 0.76, 0.06]]
f [0.8155, 0.8168, 0.8173]
load model: epoch9
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse 	current best epoch 9 	 [0.59, 0.54, 0.11] 	 lr: 0.0001 	 f: 82.64504368596393
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse[0m [33m epoch 22 [0m
Train...

[32m iter_0[0m
ga 	p: 80.03 	r: 71.21 	f1: 75.36 	 4392 	 5488 	 6168
wo 	p: 93.1 	r: 82.85 	f1: 87.68 	 2807 	 3015 	 3388
ni 	p: 80.23 	r: 68.27 	f1: 73.77 	 1039 	 1295 	 1522

[32m iter_1[0m
ga 	p: 80.48 	r: 71.64 	f1: 75.8 	 4419 	 5491 	 6168
wo 	p: 92.34 	r: 83.65 	f1: 87.78 	 2834 	 3069 	 3388
ni 	p: 81.22 	r: 70.17 	f1: 75.29 	 1068 	 1315 	 1522

[32m iter_2[0m
ga 	p: 80.43 	r: 71.76 	f1: 75.85 	 4426 	 5503 	 6168
wo 	p: 92.71 	r: 83.38 	f1: 87.8 	 2825 	 3047 	 3388
ni 	p: 82.65 	r: 68.86 	f1: 75.13 	 1048 	 1268 	 1522
best_thres [[0.5, 0.7, 0.13], [0.54, 0.65, 0.15], [0.59, 0.75, 0.18]]
f [0.7892, 0.7917, 0.7926]
load model: epoch22
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 22 	 [0.38, 0.33, 0.12] 	 lr: 0.000125 	 f: 80.67245288920165
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 27 [0m
Train...
loss: tensor(2517.6973) lr: 0.001 time: 2790.56
pred_count_train 41644

Test...
loss: tensor(786.3718) lr: 1.25e-05 time: 1986.25
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 76.42 	r: 69.76 	f1: 72.94 	 4303 	 5631 	 6168
wo 	p: 90.26 	r: 81.26 	f1: 85.52 	 2753 	 3050 	 3388
ni 	p: 82.92 	r: 70.5 	f1: 76.21 	 1073 	 1294 	 1522

[32m iter_1[0m
ga 	p: 75.41 	r: 70.88 	f1: 73.07 	 4372 	 5798 	 6168
wo 	p: 90.5 	r: 80.99 	f1: 85.48 	 2744 	 3032 	 3388
ni 	p: 82.81 	r: 70.57 	f1: 76.2 	 1074 	 1297 	 1522

[32m iter_2[0m
ga 	p: 75.96 	r: 70.48 	f1: 73.11 	 4347 	 5723 	 6168
wo 	p: 90.44 	r: 81.23 	f1: 85.59 	 2752 	 3043 	 3388
ni 	p: 79.96 	r: 73.13 	f1: 76.39 	 1113 	 1392 	 1522
best_thres [[0.37, 0.38, 0.21], [0.34, 0.38, 0.21], [0.36, 0.38, 0.17]]
f [0.7722, 0.7724, 0.7727]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 3 	 [0.36, 0.38, 0.17] 	 lr: 0.001 	 f: 77.2702932560557
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 4 [0m
Train...
loss: tensor(379.4545) lr: 1.25e-05 time: 1920.6
pred_count_train 41644

Test...
loss: tensor(828.5862) lr: 6.25e-05 time: 2032.91
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.9 	r: 76.96 	f1: 78.88 	 4747 	 5868 	 6168
wo 	p: 94.03 	r: 85.95 	f1: 89.81 	 2912 	 3097 	 3388
ni 	p: 85.94 	r: 74.7 	f1: 79.93 	 1137 	 1323 	 1522

[32m iter_1[0m
ga 	p: 82.99 	r: 75.84 	f1: 79.25 	 4678 	 5637 	 6168
wo 	p: 93.19 	r: 86.87 	f1: 89.92 	 2943 	 3158 	 3388
ni 	p: 85.31 	r: 75.56 	f1: 80.14 	 1150 	 1348 	 1522

[32m iter_2[0m
ga 	p: 83.03 	r: 75.89 	f1: 79.3 	 4681 	 5638 	 6168
wo 	p: 93.14 	r: 87.01 	f1: 89.97 	 2948 	 3165 	 3388
ni 	p: 84.8 	r: 76.22 	f1: 80.28 	 1160 	 1368 	 1522
best_thres [[0.32, 0.55, 0.17], [0.39, 0.44, 0.14], [0.39, 0.42, 0.12]]
f [0.8234, 0.825, 0.8257]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 16 	 [0.31, 0.42, 0.15] 	 lr: 2.5e-05 	 f: 83.03189329334957
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 22 [0m
Train...

[32m iter_0[0m
ga 	p: 81.57 	r: 76.93 	f1: 79.18 	 4745 	 5817 	 6168
wo 	p: 93.87 	r: 85.42 	f1: 89.45 	 2894 	 3083 	 3388
ni 	p: 85.38 	r: 74.84 	f1: 79.76 	 1139 	 1334 	 1522

[32m iter_1[0m
ga 	p: 82.43 	r: 76.73 	f1: 79.48 	 4733 	 5742 	 6168
wo 	p: 93.72 	r: 85.92 	f1: 89.65 	 2911 	 3106 	 3388
ni 	p: 84.25 	r: 77.33 	f1: 80.64 	 1177 	 1397 	 1522

[32m iter_2[0m
ga 	p: 82.05 	r: 77.2 	f1: 79.55 	 4762 	 5804 	 6168
wo 	p: 93.64 	r: 85.98 	f1: 89.64 	 2913 	 3111 	 3388
ni 	p: 84.52 	r: 77.14 	f1: 80.66 	 1174 	 1389 	 1522
best_thres [[0.3, 0.58, 0.17], [0.32, 0.65, 0.11], [0.29, 0.69, 0.11]]
f [0.8238, 0.8256, 0.8263]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse 	current best epoch 9 	 [0.59, 0.54, 0.11] 	 lr: 0.0001 	 f: 82.64504368596393
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse[0m [33m epoch 23 [0m
Train...

[32m iter_0[0m
ga 	p: 80.05 	r: 72.49 	f1: 76.08 	 4471 	 5585 	 6168
wo 	p: 92.62 	r: 84.42 	f1: 88.33 	 2860 	 3088 	 3388
ni 	p: 80.95 	r: 73.72 	f1: 77.17 	 1122 	 1386 	 1522

[32m iter_1[0m
ga 	p: 78.33 	r: 74.71 	f1: 76.47 	 4608 	 5883 	 6168
wo 	p: 92.86 	r: 84.86 	f1: 88.68 	 2875 	 3096 	 3388
ni 	p: 81.17 	r: 75.36 	f1: 78.16 	 1147 	 1413 	 1522

[32m iter_2[0m
ga 	p: 80.91 	r: 72.65 	f1: 76.56 	 4481 	 5538 	 6168
wo 	p: 93.47 	r: 84.56 	f1: 88.8 	 2865 	 3065 	 3388
ni 	p: 81.43 	r: 74.9 	f1: 78.03 	 1140 	 1400 	 1522
best_thres [[0.43, 0.51, 0.1], [0.33, 0.44, 0.1], [0.51, 0.59, 0.1]]
f [0.7998, 0.8019, 0.8029]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 22 	 [0.38, 0.33, 0.12] 	 lr: 0.000125 	 f: 80.67245288920165
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 28 [0m
Train...

[32m iter_0[0m
ga 	p: 80.36 	r: 76.13 	f1: 78.19 	 4696 	 5844 	 6168
wo 	p: 93.5 	r: 85.71 	f1: 89.44 	 2904 	 3106 	 3388
ni 	p: 81.88 	r: 74.84 	f1: 78.2 	 1139 	 1391 	 1522

[32m iter_1[0m
ga 	p: 82.39 	r: 74.63 	f1: 78.32 	 4603 	 5587 	 6168
wo 	p: 93.12 	r: 86.28 	f1: 89.57 	 2923 	 3139 	 3388
ni 	p: 84.71 	r: 73.52 	f1: 78.72 	 1119 	 1321 	 1522

[32m iter_2[0m
ga 	p: 82.26 	r: 74.58 	f1: 78.23 	 4600 	 5592 	 6168
wo 	p: 93.24 	r: 86.25 	f1: 89.6 	 2922 	 3134 	 3388
ni 	p: 83.84 	r: 73.98 	f1: 78.6 	 1126 	 1343 	 1522

[32m iter_3[0m
ga 	p: 82.31 	r: 74.59 	f1: 78.26 	 4601 	 5590 	 6168
wo 	p: 93.24 	r: 86.28 	f1: 89.62 	 2923 	 3135 	 3388
ni 	p: 84.45 	r: 73.52 	f1: 78.61 	 1119 	 1325 	 1522

[32m iter_4[0m
ga 	p: 80.93 	r: 75.76 	f1: 78.26 	 4673 	 5774 	 6168
wo 	p: 93.23 	r: 86.22 	f1: 89.59 	 2921 	 3133 	 3388
ni 	p: 84.29 	r: 73.65 	f1: 78.61 	 1121 	 1330 	 1522

[32m iter_5[0m
ga 	p: 80.94 	r: 75.78 	f1: 78.27 	 4674 	 5775 	 6168
wo 	p: 93.24 	r: 86.28 	f1: 89.62 	 2923 	 3135 	 3388
ni 	p: 84.84 	r: 73.19 	f1: 78.59 	 1114 	 1313 	 1522

[32m iter_6[0m
ga 	p: 80.93 	r: 75.76 	f1: 78.26 	 4673 	 5774 	 6168
wo 	p: 93.23 	r: 86.19 	f1: 89.57 	 2920 	 3132 	 3388
ni 	p: 84.22 	r: 73.65 	f1: 78.58 	 1121 	 1331 	 1522

[32m iter_7[0m
ga 	p: 80.95 	r: 75.78 	f1: 78.28 	 4674 	 5774 	 6168
wo 	p: 93.24 	r: 86.25 	f1: 89.6 	 2922 	 3134 	 3388
ni 	p: 84.34 	r: 73.59 	f1: 78.6 	 1120 	 1328 	 1522

[32m iter_8[0m
ga 	p: 80.92 	r: 75.76 	f1: 78.26 	 4673 	 5775 	 6168
wo 	p: 93.23 	r: 86.22 	f1: 89.59 	 2921 	 3133 	 3388
ni 	p: 84.35 	r: 73.65 	f1: 78.64 	 1121 	 1329 	 1522

[32m iter_9[0m
ga 	p: 80.95 	r: 75.78 	f1: 78.28 	 4674 	 5774 	 6168
wo 	p: 93.24 	r: 86.25 	f1: 89.6 	 2922 	 3134 	 3388
ni 	p: 84.34 	r: 73.59 	f1: 78.6 	 1120 	 1328 	 1522
best_thres [[0.25, 0.68, 0.15], [0.4, 0.59, 0.16], [0.39, 0.59, 0.14], [0.39, 0.59, 0.15], [0.27, 0.59, 0.15], [0.27, 0.59, 0.16], [0.27, 0.59, 0.15], [0.27, 0.59, 0.15], [0.27, 0.59, 0.15], [0.27, 0.59, 0.15]]
f [0.816, 0.8172, 0.8174, 0.8176, 0.8176, 0.8177, 0.8177, 0.8177, 0.8177, 0.8177]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse 	current best epoch 9 	 [0.49, 0.44, 0.1] 	 lr: 0.0001 	 f: 82.5649198462548
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse[0m [33m epoch 17 [0m
Train...
loss: tensor(701.4202) lr: 1.25e-05 time: 1952.34
pred_count_train 41644

Test...
loss: tensor(2576.6206) lr: 0.001 time: 2953.79
pred_count_train 41644

Test...
loss: tensor(279.2698) lr: 1.25e-05 time: 2085.24
pred_count_train 41644

Test...
loss: tensor(654.2322) lr: 6.25e-05 time: 2026.57
pred_count_train 41644

Test...
.77 	r: 85.77 	f1: 89.59 	 2906 	 3099 	 3388
ni 	p: 85.85 	r: 76.15 	f1: 80.71 	 1159 	 1350 	 1522

[32m iter_1[0m
ga 	p: 82.0 	r: 76.51 	f1: 79.16 	 4719 	 5755 	 6168
wo 	p: 92.1 	r: 87.4 	f1: 89.69 	 2961 	 3215 	 3388
ni 	p: 85.43 	r: 76.68 	f1: 80.82 	 1167 	 1366 	 1522

[32m iter_2[0m
ga 	p: 82.16 	r: 76.18 	f1: 79.06 	 4699 	 5719 	 6168
wo 	p: 92.16 	r: 87.75 	f1: 89.9 	 2973 	 3226 	 3388
ni 	p: 85.83 	r: 76.41 	f1: 80.85 	 1163 	 1355 	 1522
best_thres [[0.35, 0.5, 0.15], [0.3, 0.31, 0.12], [0.31, 0.29, 0.12]]
f [0.8242, 0.8253, 0.8257]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 16 	 [0.31, 0.42, 0.15] 	 lr: 2.5e-05 	 f: 83.03189329334957
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 23 [0m
Train...

[32m iter_0[0m
ga 	p: 75.11 	r: 69.07 	f1: 71.96 	 4260 	 5672 	 6168
wo 	p: 93.05 	r: 78.28 	f1: 85.03 	 2652 	 2850 	 3388
ni 	p: 81.65 	r: 74.24 	f1: 77.77 	 1130 	 1384 	 1522

[32m iter_1[0m
ga 	p: 74.68 	r: 69.29 	f1: 71.89 	 4274 	 5723 	 6168
wo 	p: 91.88 	r: 78.78 	f1: 84.82 	 2669 	 2905 	 3388
ni 	p: 79.86 	r: 75.56 	f1: 77.65 	 1150 	 1440 	 1522

[32m iter_2[0m
ga 	p: 74.02 	r: 69.84 	f1: 71.87 	 4308 	 5820 	 6168
wo 	p: 93.15 	r: 78.25 	f1: 85.05 	 2651 	 2846 	 3388
ni 	p: 81.36 	r: 74.84 	f1: 77.96 	 1139 	 1400 	 1522
best_thres [[0.25, 0.73, 0.35], [0.24, 0.63, 0.31], [0.22, 0.73, 0.37]]
f [0.7665, 0.766, 0.766]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 3 	 [0.36, 0.38, 0.17] 	 lr: 0.001 	 f: 77.2702932560557
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 5 [0m
Train...

[32m iter_0[0m
ga 	p: 83.92 	r: 74.81 	f1: 79.1 	 4614 	 5498 	 6168
wo 	p: 92.88 	r: 85.92 	f1: 89.27 	 2911 	 3134 	 3388
ni 	p: 84.93 	r: 74.05 	f1: 79.12 	 1127 	 1327 	 1522

[32m iter_1[0m
ga 	p: 83.8 	r: 75.08 	f1: 79.2 	 4631 	 5526 	 6168
wo 	p: 93.61 	r: 85.66 	f1: 89.46 	 2902 	 3100 	 3388
ni 	p: 81.49 	r: 78.12 	f1: 79.77 	 1189 	 1459 	 1522

[32m iter_2[0m
ga 	p: 83.7 	r: 75.28 	f1: 79.27 	 4643 	 5547 	 6168
wo 	p: 93.71 	r: 85.71 	f1: 89.53 	 2904 	 3099 	 3388
ni 	p: 82.7 	r: 76.94 	f1: 79.71 	 1171 	 1416 	 1522
best_thres [[0.43, 0.5, 0.17], [0.43, 0.8, 0.08], [0.42, 0.84, 0.09]]
f [0.8226, 0.8234, 0.8239]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse 	current best epoch 9 	 [0.59, 0.54, 0.11] 	 lr: 0.0001 	 f: 82.64504368596393
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse[0m [33m epoch 24 [0m
Train...

[32m iter_0[0m
ga 	p: 78.15 	r: 74.09 	f1: 76.07 	 4570 	 5848 	 6168
wo 	p: 93.05 	r: 83.32 	f1: 87.92 	 2823 	 3034 	 3388
ni 	p: 81.15 	r: 70.43 	f1: 75.41 	 1072 	 1321 	 1522

[32m iter_1[0m
ga 	p: 80.58 	r: 72.41 	f1: 76.28 	 4466 	 5542 	 6168
wo 	p: 92.08 	r: 84.8 	f1: 88.29 	 2873 	 3120 	 3388
ni 	p: 81.5 	r: 72.93 	f1: 76.98 	 1110 	 1362 	 1522

[32m iter_2[0m
ga 	p: 79.22 	r: 73.75 	f1: 76.39 	 4549 	 5742 	 6168
wo 	p: 93.61 	r: 83.47 	f1: 88.25 	 2828 	 3021 	 3388
ni 	p: 81.09 	r: 73.0 	f1: 76.83 	 1111 	 1370 	 1522
best_thres [[0.33, 0.53, 0.11], [0.47, 0.41, 0.11], [0.39, 0.68, 0.11]]
f [0.7955, 0.7982, 0.7989]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 22 	 [0.38, 0.33, 0.12] 	 lr: 0.000125 	 f: 80.67245288920165
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 29 [0m
Train...
                                                                                                                                                                    loss: tensor(531.9467) lr: 6.25e-05 time: 1980.31
pred_count_train 41644

Test...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              loss: tensor(2654.2849) lr: 0.001 time: 2788.26
pred_count_train 41644

Test...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              
[32m iter_0[0m
ga 	p: 79.19 	r: 72.62 	f1: 75.76 	 4479 	 5656 	 6168
wo 	p: 93.98 	r: 82.5 	f1: 87.87 	 2795 	 2974 	 3388
ni 	p: 77.38 	r: 71.68 	f1: 74.42 	 1091 	 1410 	 1522

[32m iter_1[0m
ga 	p: 79.95 	r: 72.28 	f1: 75.92 	 4458 	 5576 	 6168
wo 	p: 93.56 	r: 83.65 	f1: 88.33 	 2834 	 3029 	 3388
ni 	p: 79.11 	r: 71.42 	f1: 75.07 	 1087 	 1374 	 1522

[32m iter_2[0m
ga 	p: 79.69 	r: 72.65 	f1: 76.01 	 4481 	 5623 	 6168
wo 	p: 93.38 	r: 84.15 	f1: 88.53 	 2851 	 3053 	 3388
ni 	p: 81.36 	r: 69.97 	f1: 75.24 	 1065 	 1309 	 1522
best_thres [[0.44, 0.7, 0.08], [0.5, 0.71, 0.1], [0.51, 0.66, 0.12]]
f [0.7922, 0.794, 0.7951]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 22 	 [0.38, 0.33, 0.12] 	 lr: 0.000125 	 f: 80.67245288920165
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 30 [0m
Train...

[32m iter_0[0m
ga 	p: 73.63 	r: 69.08 	f1: 71.28 	 4261 	 5787 	 6168
wo 	p: 89.15 	r: 78.81 	f1: 83.66 	 2670 	 2995 	 3388
ni 	p: 76.51 	r: 67.21 	f1: 71.56 	 1023 	 1337 	 1522

[32m iter_1[0m
ga 	p: 73.2 	r: 69.36 	f1: 71.23 	 4278 	 5844 	 6168
wo 	p: 89.33 	r: 77.86 	f1: 83.2 	 2638 	 2953 	 3388
ni 	p: 69.82 	r: 72.67 	f1: 71.22 	 1106 	 1584 	 1522

[32m iter_2[0m
ga 	p: 74.53 	r: 68.17 	f1: 71.21 	 4205 	 5642 	 6168
wo 	p: 89.13 	r: 78.66 	f1: 83.57 	 2665 	 2990 	 3388
ni 	p: 78.42 	r: 65.18 	f1: 71.19 	 992 	 1265 	 1522
best_thres [[0.18, 0.53, 0.14], [0.18, 0.53, 0.08], [0.21, 0.53, 0.16]]
f [0.7505, 0.7491, 0.7493]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 3 	 [0.36, 0.38, 0.17] 	 lr: 0.001 	 f: 77.2702932560557
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 6 [0m
Train...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      loss: tensor(442.4405) lr: 6.25e-05 time: 2019.9
pred_count_train 41644

Test...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            loss: tensor(2752.7607) lr: 0.001 time: 2967.2
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 78.92 	r: 72.23 	f1: 75.43 	 4455 	 5645 	 6168
wo 	p: 92.82 	r: 83.62 	f1: 87.98 	 2833 	 3052 	 3388
ni 	p: 82.4 	r: 68.92 	f1: 75.06 	 1049 	 1273 	 1522

[32m iter_1[0m
ga 	p: 77.94 	r: 73.72 	f1: 75.77 	 4547 	 5834 	 6168
wo 	p: 93.36 	r: 83.88 	f1: 88.37 	 2842 	 3044 	 3388
ni 	p: 83.15 	r: 70.7 	f1: 76.42 	 1076 	 1294 	 1522

[32m iter_2[0m
ga 	p: 78.58 	r: 73.46 	f1: 75.93 	 4531 	 5766 	 6168
wo 	p: 93.87 	r: 83.62 	f1: 88.45 	 2833 	 3018 	 3388
ni 	p: 82.42 	r: 71.75 	f1: 76.71 	 1092 	 1325 	 1522
best_thres [[0.4, 0.58, 0.16], [0.33, 0.73, 0.17], [0.37, 0.84, 0.14]]
f [0.7922, 0.7945, 0.7957]
load model: epoch22
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 22 	 [0.38, 0.33, 0.12] 	 lr: 0.000125 	 f: 80.67245288920165
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 31 [0m
Train...
loss: tensor(780.5126) lr: 1e-05 time: 1840.0
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.38 	r: 73.43 	f1: 78.09 	 4529 	 5432 	 6168
wo 	p: 92.69 	r: 86.42 	f1: 89.45 	 2928 	 3159 	 3388
ni 	p: 81.56 	r: 73.52 	f1: 77.33 	 1119 	 1372 	 1522

[32m iter_1[0m
ga 	p: 82.97 	r: 74.01 	f1: 78.23 	 4565 	 5502 	 6168
wo 	p: 92.36 	r: 87.1 	f1: 89.66 	 2951 	 3195 	 3388
ni 	p: 83.68 	r: 72.4 	f1: 77.63 	 1102 	 1317 	 1522

[32m iter_2[0m
ga 	p: 83.0 	r: 73.99 	f1: 78.24 	 4564 	 5499 	 6168
wo 	p: 92.48 	r: 87.16 	f1: 89.74 	 2953 	 3193 	 3388
ni 	p: 83.79 	r: 72.34 	f1: 77.64 	 1101 	 1314 	 1522

[32m iter_3[0m
ga 	p: 83.01 	r: 73.98 	f1: 78.23 	 4563 	 5497 	 6168
wo 	p: 92.7 	r: 86.95 	f1: 89.73 	 2946 	 3178 	 3388
ni 	p: 83.37 	r: 72.8 	f1: 77.73 	 1108 	 1329 	 1522

[32m iter_4[0m
ga 	p: 83.05 	r: 73.95 	f1: 78.23 	 4561 	 5492 	 6168
wo 	p: 92.67 	r: 86.95 	f1: 89.72 	 2946 	 3179 	 3388
ni 	p: 83.51 	r: 72.86 	f1: 77.82 	 1109 	 1328 	 1522

[32m iter_5[0m
ga 	p: 82.98 	r: 73.98 	f1: 78.22 	 4563 	 5499 	 6168
wo 	p: 92.7 	r: 86.95 	f1: 89.73 	 2946 	 3178 	 3388
ni 	p: 83.56 	r: 72.8 	f1: 77.81 	 1108 	 1326 	 1522

[32m iter_6[0m
ga 	p: 83.04 	r: 73.93 	f1: 78.22 	 4560 	 5491 	 6168
wo 	p: 92.67 	r: 86.95 	f1: 89.72 	 2946 	 3179 	 3388
ni 	p: 83.51 	r: 72.86 	f1: 77.82 	 1109 	 1328 	 1522

[32m iter_7[0m
ga 	p: 83.03 	r: 73.93 	f1: 78.22 	 4560 	 5492 	 6168
wo 	p: 92.7 	r: 86.95 	f1: 89.73 	 2946 	 3178 	 3388
ni 	p: 83.56 	r: 72.8 	f1: 77.81 	 1108 	 1326 	 1522

[32m iter_8[0m
ga 	p: 83.04 	r: 73.93 	f1: 78.22 	 4560 	 5491 	 6168
wo 	p: 92.67 	r: 86.95 	f1: 89.72 	 2946 	 3179 	 3388
ni 	p: 83.51 	r: 72.86 	f1: 77.82 	 1109 	 1328 	 1522

[32m iter_9[0m
ga 	p: 83.03 	r: 73.93 	f1: 78.22 	 4560 	 5492 	 6168
wo 	p: 92.7 	r: 86.95 	f1: 89.73 	 2946 	 3178 	 3388
ni 	p: 83.56 	r: 72.8 	f1: 77.81 	 1108 	 1326 	 1522
best_thres [[0.61, 0.69, 0.18], [0.54, 0.57, 0.17], [0.53, 0.56, 0.17], [0.53, 0.61, 0.15], [0.54, 0.61, 0.15], [0.53, 0.61, 0.15], [0.54, 0.61, 0.15], [0.54, 0.61, 0.15], [0.54, 0.61, 0.15], [0.54, 0.61, 0.15]]
f [0.8152, 0.8162, 0.8166, 0.8168, 0.817, 0.8171, 0.8171, 0.8172, 0.8172, 0.8172]
load model: epoch9
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse 	current best epoch 9 	 [0.49, 0.44, 0.1] 	 lr: 0.0001 	 f: 82.5649198462548
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse[0m [33m epoch 18 [0m
Train...
loss: tensor(378.7300) lr: 1e-05 time: 1996.91
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 71.67 	r: 68.9 	f1: 70.26 	 4250 	 5930 	 6168
wo 	p: 87.9 	r: 79.13 	f1: 83.29 	 2681 	 3050 	 3388
ni 	p: 80.26 	r: 72.14 	f1: 75.99 	 1098 	 1368 	 1522

[32m iter_1[0m
ga 	p: 72.03 	r: 68.68 	f1: 70.31 	 4236 	 5881 	 6168
wo 	p: 86.91 	r: 79.37 	f1: 82.97 	 2689 	 3094 	 3388
ni 	p: 80.33 	r: 71.09 	f1: 75.43 	 1082 	 1347 	 1522

[32m iter_2[0m
ga 	p: 71.79 	r: 69.1 	f1: 70.42 	 4262 	 5937 	 6168
wo 	p: 87.29 	r: 79.46 	f1: 83.19 	 2692 	 3084 	 3388
ni 	p: 80.4 	r: 71.68 	f1: 75.79 	 1091 	 1357 	 1522
best_thres [[0.33, 0.3, 0.1], [0.34, 0.27, 0.11], [0.33, 0.27, 0.11]]
f [0.7495, 0.7489, 0.7492]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 3 	 [0.36, 0.38, 0.17] 	 lr: 0.001 	 f: 77.2702932560557
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 7 [0m
Train...

[32m iter_0[0m
ga 	p: 82.93 	r: 75.15 	f1: 78.85 	 4635 	 5589 	 6168
wo 	p: 93.29 	r: 86.63 	f1: 89.84 	 2935 	 3146 	 3388
ni 	p: 84.55 	r: 77.66 	f1: 80.96 	 1182 	 1398 	 1522

[32m iter_1[0m
ga 	p: 82.91 	r: 75.73 	f1: 79.16 	 4671 	 5634 	 6168
wo 	p: 93.51 	r: 86.75 	f1: 90.0 	 2939 	 3143 	 3388
ni 	p: 86.61 	r: 76.08 	f1: 81.01 	 1158 	 1337 	 1522

[32m iter_2[0m
ga 	p: 82.91 	r: 75.73 	f1: 79.16 	 4671 	 5634 	 6168
wo 	p: 93.67 	r: 86.89 	f1: 90.15 	 2944 	 3143 	 3388
ni 	p: 86.77 	r: 76.68 	f1: 81.41 	 1167 	 1345 	 1522
best_thres [[0.46, 0.47, 0.15], [0.44, 0.47, 0.16], [0.44, 0.47, 0.15]]
f [0.8252, 0.8264, 0.8271]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 16 	 [0.31, 0.42, 0.15] 	 lr: 2.5e-05 	 f: 83.03189329334957
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 26 [0m
Train...
loss: tensor(818.5299) lr: 5e-05 time: 2017.67
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.08 	r: 76.85 	f1: 79.38 	 4740 	 5775 	 6168
wo 	p: 93.35 	r: 86.13 	f1: 89.59 	 2918 	 3126 	 3388
ni 	p: 87.89 	r: 72.47 	f1: 79.44 	 1103 	 1255 	 1522

[32m iter_1[0m
ga 	p: 82.43 	r: 76.73 	f1: 79.48 	 4733 	 5742 	 6168
wo 	p: 93.68 	r: 86.13 	f1: 89.74 	 2918 	 3115 	 3388
ni 	p: 83.8 	r: 77.14 	f1: 80.33 	 1174 	 1401 	 1522

[32m iter_2[0m
ga 	p: 81.47 	r: 77.63 	f1: 79.5 	 4788 	 5877 	 6168
wo 	p: 93.63 	r: 86.33 	f1: 89.83 	 2925 	 3124 	 3388
ni 	p: 84.01 	r: 76.61 	f1: 80.14 	 1166 	 1388 	 1522
best_thres [[0.34, 0.5, 0.2], [0.35, 0.63, 0.09], [0.28, 0.64, 0.09]]
f [0.8252, 0.8262, 0.8266]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse 	current best epoch 26 	 [0.28, 0.64, 0.09] 	 lr: 1e-05 	 f: 82.65533988163092
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse[0m [33m epoch 27 [0m
Train...

[32m iter_0[0m
ga 	p: 78.48 	r: 74.03 	f1: 76.19 	 4566 	 5818 	 6168
wo 	p: 92.32 	r: 84.42 	f1: 88.19 	 2860 	 3098 	 3388
ni 	p: 85.01 	r: 71.55 	f1: 77.7 	 1089 	 1281 	 1522

[32m iter_1[0m
ga 	p: 80.78 	r: 72.21 	f1: 76.25 	 4454 	 5514 	 6168
wo 	p: 94.03 	r: 83.71 	f1: 88.57 	 2836 	 3016 	 3388
ni 	p: 85.55 	r: 73.13 	f1: 78.85 	 1113 	 1301 	 1522

[32m iter_2[0m
ga 	p: 80.59 	r: 72.99 	f1: 76.6 	 4502 	 5586 	 6168
wo 	p: 93.55 	r: 83.97 	f1: 88.51 	 2845 	 3041 	 3388
ni 	p: 83.48 	r: 74.38 	f1: 78.67 	 1132 	 1356 	 1522
best_thres [[0.32, 0.44, 0.18], [0.45, 0.6, 0.19], [0.43, 0.55, 0.15]]
f [0.8005, 0.8021, 0.8031]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 22 	 [0.38, 0.33, 0.12] 	 lr: 0.000125 	 f: 80.67245288920165
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 32 [0m
Train...
loss: tensor(709.1665) lr: 1e-05 time: 2125.06
pred_count_train 41644

Test...
loss: tensor(2867.4973) lr: 0.001 time: 2819.4
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.23 	r: 75.99 	f1: 78.99 	 4687 	 5700 	 6168
wo 	p: 93.35 	r: 86.63 	f1: 89.87 	 2935 	 3144 	 3388
ni 	p: 84.33 	r: 76.74 	f1: 80.36 	 1168 	 1385 	 1522

[32m iter_1[0m
ga 	p: 83.23 	r: 75.42 	f1: 79.14 	 4652 	 5589 	 6168
wo 	p: 93.22 	r: 86.81 	f1: 89.9 	 2941 	 3155 	 3388
ni 	p: 86.06 	r: 75.43 	f1: 80.39 	 1148 	 1334 	 1522

[32m iter_2[0m
ga 	p: 83.45 	r: 75.29 	f1: 79.16 	 4644 	 5565 	 6168
wo 	p: 93.37 	r: 86.84 	f1: 89.98 	 2942 	 3151 	 3388
ni 	p: 83.04 	r: 78.19 	f1: 80.54 	 1190 	 1433 	 1522
best_thres [[0.38, 0.44, 0.15], [0.42, 0.42, 0.15], [0.43, 0.42, 0.1]]
f [0.8251, 0.8257, 0.8261]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 16 	 [0.31, 0.42, 0.15] 	 lr: 2.5e-05 	 f: 83.03189329334957
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 27 [0m
Train...
loss: tensor(291.3715) lr: 1e-05 time: 2145.33
pred_count_train 41644

Test...
loss: tensor(645.7435) lr: 5e-05 time: 1983.09
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 68.77 	r: 70.18 	f1: 69.47 	 4329 	 6295 	 6168
wo 	p: 89.1 	r: 78.19 	f1: 83.29 	 2649 	 2973 	 3388
ni 	p: 78.74 	r: 68.86 	f1: 73.47 	 1048 	 1331 	 1522

[32m iter_1[0m
ga 	p: 68.74 	r: 69.91 	f1: 69.32 	 4312 	 6273 	 6168
wo 	p: 90.22 	r: 76.53 	f1: 82.82 	 2593 	 2874 	 3388
ni 	p: 78.76 	r: 67.74 	f1: 72.84 	 1031 	 1309 	 1522

[32m iter_2[0m
ga 	p: 68.4 	r: 70.61 	f1: 69.49 	 4355 	 6367 	 6168
wo 	p: 89.95 	r: 77.15 	f1: 83.06 	 2614 	 2906 	 3388
ni 	p: 79.54 	r: 67.94 	f1: 73.28 	 1034 	 1300 	 1522
best_thres [[0.28, 0.39, 0.16], [0.28, 0.45, 0.15], [0.26, 0.45, 0.16]]
f [0.7405, 0.7388, 0.739]
load model: epoch3
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 3 	 [0.36, 0.38, 0.17] 	 lr: 0.001 	 f: 77.2702932560557
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 8 [0m
Train...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 
[32m iter_0[0m
ga 	p: 79.19 	r: 72.11 	f1: 75.49 	 4448 	 5617 	 6168
wo 	p: 93.23 	r: 83.26 	f1: 87.96 	 2821 	 3026 	 3388
ni 	p: 82.16 	r: 71.42 	f1: 76.41 	 1087 	 1323 	 1522

[32m iter_1[0m
ga 	p: 81.08 	r: 71.27 	f1: 75.86 	 4396 	 5422 	 6168
wo 	p: 93.62 	r: 83.62 	f1: 88.34 	 2833 	 3026 	 3388
ni 	p: 81.02 	r: 74.05 	f1: 77.38 	 1127 	 1391 	 1522

[32m iter_2[0m
ga 	p: 81.87 	r: 70.88 	f1: 75.98 	 4372 	 5340 	 6168
wo 	p: 93.7 	r: 83.47 	f1: 88.29 	 2828 	 3018 	 3388
ni 	p: 81.86 	r: 73.85 	f1: 77.65 	 1124 	 1373 	 1522
best_thres [[0.4, 0.6, 0.13], [0.52, 0.65, 0.13], [0.66, 0.72, 0.13]]
f [0.7941, 0.7965, 0.7977]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 22 	 [0.38, 0.33, 0.12] 	 lr: 0.000125 	 f: 80.67245288920165
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 33 [0m
Train...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            loss: tensor(2195.0264) lr: 0.0005 time: 2833.78
pred_count_train 41644

Test...
loss: tensor(523.7514) lr: 5e-05 time: 2013.91
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 85.42 	r: 73.23 	f1: 78.86 	 4517 	 5288 	 6168
wo 	p: 93.34 	r: 85.63 	f1: 89.32 	 2901 	 3108 	 3388
ni 	p: 84.93 	r: 72.6 	f1: 78.29 	 1105 	 1301 	 1522

[32m iter_1[0m
ga 	p: 82.89 	r: 75.63 	f1: 79.09 	 4665 	 5628 	 6168
wo 	p: 93.22 	r: 85.98 	f1: 89.45 	 2913 	 3125 	 3388
ni 	p: 82.96 	r: 75.82 	f1: 79.23 	 1154 	 1391 	 1522

[32m iter_2[0m
ga 	p: 82.67 	r: 75.97 	f1: 79.18 	 4686 	 5668 	 6168
wo 	p: 92.94 	r: 86.3 	f1: 89.5 	 2924 	 3146 	 3388
ni 	p: 83.52 	r: 75.23 	f1: 79.16 	 1145 	 1371 	 1522
best_thres [[0.51, 0.49, 0.16], [0.33, 0.5, 0.09], [0.31, 0.41, 0.09]]
f [0.8205, 0.8217, 0.8223]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse 	current best epoch 26 	 [0.28, 0.64, 0.09] 	 lr: 1e-05 	 f: 82.65533988163092
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse[0m [33m epoch 29 [0m
Train...
loss: tensor(582.7773) lr: 1e-05 time: 1790.5
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 78.03 	r: 73.2 	f1: 75.54 	 4515 	 5786 	 6168
wo 	p: 93.81 	r: 82.29 	f1: 87.67 	 2788 	 2972 	 3388
ni 	p: 77.92 	r: 73.72 	f1: 75.76 	 1122 	 1440 	 1522

[32m iter_1[0m
ga 	p: 80.01 	r: 72.15 	f1: 75.87 	 4450 	 5562 	 6168
wo 	p: 92.32 	r: 84.09 	f1: 88.01 	 2849 	 3086 	 3388
ni 	p: 77.76 	r: 75.82 	f1: 76.78 	 1154 	 1484 	 1522

[32m iter_2[0m
ga 	p: 79.83 	r: 72.75 	f1: 76.12 	 4487 	 5621 	 6168
wo 	p: 93.61 	r: 83.44 	f1: 88.23 	 2827 	 3020 	 3388
ni 	p: 79.2 	r: 74.31 	f1: 76.68 	 1131 	 1428 	 1522
best_thres [[0.35, 0.74, 0.11], [0.47, 0.58, 0.11], [0.47, 0.82, 0.13]]
f [0.792, 0.7945, 0.7959]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	current best epoch 22 	 [0.38, 0.33, 0.12] 	 lr: 0.000125 	 f: 80.67245288920165
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse[0m [33m epoch 34 [0m
Train...

[32m iter_0[0m
ga 	p: 77.34 	r: 73.43 	f1: 75.33 	 4529 	 5856 	 6168
wo 	p: 90.53 	r: 84.62 	f1: 87.48 	 2867 	 3167 	 3388
ni 	p: 80.67 	r: 79.5 	f1: 80.08 	 1210 	 1500 	 1522

[32m iter_1[0m
ga 	p: 77.62 	r: 73.49 	f1: 75.5 	 4533 	 5840 	 6168
wo 	p: 90.54 	r: 84.5 	f1: 87.42 	 2863 	 3162 	 3388
ni 	p: 79.34 	r: 79.96 	f1: 79.65 	 1217 	 1534 	 1522

[32m iter_2[0m
ga 	p: 77.29 	r: 73.44 	f1: 75.32 	 4530 	 5861 	 6168
wo 	p: 90.53 	r: 84.68 	f1: 87.51 	 2869 	 3169 	 3388
ni 	p: 78.4 	r: 80.62 	f1: 79.49 	 1227 	 1565 	 1522
best_thres [[0.41, 0.28, 0.14], [0.39, 0.27, 0.13], [0.41, 0.27, 0.12]]
f [0.7968, 0.7969, 0.7966]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 8 	 [0.41, 0.27, 0.12] 	 lr: 0.0005 	 f: 79.66033781284675
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 9 [0m
Train...

[32m iter_0[0m
ga 	p: 81.87 	r: 76.75 	f1: 79.23 	 4734 	 5782 	 6168
wo 	p: 93.63 	r: 85.92 	f1: 89.61 	 2911 	 3109 	 3388
ni 	p: 86.77 	r: 74.57 	f1: 80.21 	 1135 	 1308 	 1522

[32m iter_1[0m
ga 	p: 81.31 	r: 77.61 	f1: 79.42 	 4787 	 5887 	 6168
wo 	p: 94.13 	r: 85.63 	f1: 89.68 	 2901 	 3082 	 3388
ni 	p: 84.61 	r: 76.94 	f1: 80.59 	 1171 	 1384 	 1522

[32m iter_2[0m
ga 	p: 81.28 	r: 77.56 	f1: 79.38 	 4784 	 5886 	 6168
wo 	p: 93.83 	r: 85.74 	f1: 89.61 	 2905 	 3096 	 3388
ni 	p: 85.28 	r: 76.15 	f1: 80.46 	 1159 	 1359 	 1522

[32m iter_3[0m
ga 	p: 81.28 	r: 77.58 	f1: 79.39 	 4785 	 5887 	 6168
wo 	p: 93.89 	r: 85.71 	f1: 89.62 	 2904 	 3093 	 3388
ni 	p: 85.55 	r: 76.22 	f1: 80.61 	 1160 	 1356 	 1522

[32m iter_4[0m
ga 	p: 81.28 	r: 77.56 	f1: 79.38 	 4784 	 5886 	 6168
wo 	p: 93.8 	r: 85.74 	f1: 89.59 	 2905 	 3097 	 3388
ni 	p: 85.61 	r: 76.22 	f1: 80.64 	 1160 	 1355 	 1522

[32m iter_5[0m
ga 	p: 81.28 	r: 77.58 	f1: 79.39 	 4785 	 5887 	 6168
wo 	p: 93.89 	r: 85.71 	f1: 89.62 	 2904 	 3093 	 3388
ni 	p: 85.61 	r: 76.22 	f1: 80.64 	 1160 	 1355 	 1522

[32m iter_6[0m
ga 	p: 81.28 	r: 77.56 	f1: 79.38 	 4784 	 5886 	 6168
wo 	p: 93.8 	r: 85.74 	f1: 89.59 	 2905 	 3097 	 3388
ni 	p: 85.61 	r: 76.22 	f1: 80.64 	 1160 	 1355 	 1522

[32m iter_7[0m
ga 	p: 81.28 	r: 77.58 	f1: 79.39 	 4785 	 5887 	 6168
wo 	p: 93.89 	r: 85.71 	f1: 89.62 	 2904 	 3093 	 3388
ni 	p: 85.61 	r: 76.22 	f1: 80.64 	 1160 	 1355 	 1522

[32m iter_8[0m
ga 	p: 81.28 	r: 77.56 	f1: 79.38 	 4784 	 5886 	 6168
wo 	p: 93.8 	r: 85.74 	f1: 89.59 	 2905 	 3097 	 3388
ni 	p: 85.61 	r: 76.22 	f1: 80.64 	 1160 	 1355 	 1522

[32m iter_9[0m
ga 	p: 81.28 	r: 77.58 	f1: 79.39 	 4785 	 5887 	 6168
wo 	p: 93.89 	r: 85.71 	f1: 89.62 	 2904 	 3093 	 3388
ni 	p: 85.61 	r: 76.22 	f1: 80.64 	 1160 	 1355 	 1522
best_thres [[0.37, 0.69, 0.18], [0.31, 0.72, 0.1], [0.31, 0.7, 0.11], [0.31, 0.7, 0.11], [0.31, 0.7, 0.11], [0.31, 0.7, 0.11], [0.31, 0.7, 0.11], [0.31, 0.7, 0.11], [0.31, 0.7, 0.11], [0.31, 0.7, 0.11]]
f [0.8253, 0.826, 0.8261, 0.8262, 0.8262, 0.8263, 0.8263, 0.8263, 0.8263, 0.8263]
save model
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse 	current best epoch 18 	 [0.31, 0.7, 0.11] 	 lr: 2.5e-05 	 f: 82.63404788158559
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse[0m [33m epoch 19 [0m
Train...

[32m iter_0[0m
ga 	p: 82.47 	r: 75.19 	f1: 78.66 	 4638 	 5624 	 6168
wo 	p: 92.2 	r: 86.84 	f1: 89.44 	 2942 	 3191 	 3388
ni 	p: 83.18 	r: 75.69 	f1: 79.26 	 1152 	 1385 	 1522

[32m iter_1[0m
ga 	p: 82.59 	r: 75.37 	f1: 78.82 	 4649 	 5629 	 6168
wo 	p: 92.32 	r: 86.92 	f1: 89.54 	 2945 	 3190 	 3388
ni 	p: 83.72 	r: 76.02 	f1: 79.68 	 1157 	 1382 	 1522

[32m iter_2[0m
ga 	p: 82.77 	r: 75.24 	f1: 78.83 	 4641 	 5607 	 6168
wo 	p: 92.26 	r: 87.28 	f1: 89.7 	 2957 	 3205 	 3388
ni 	p: 85.09 	r: 74.97 	f1: 79.71 	 1141 	 1341 	 1522
best_thres [[0.4, 0.36, 0.13], [0.39, 0.33, 0.1], [0.4, 0.3, 0.11]]
f [0.8208, 0.8216, 0.8222]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	best in epoch 16 	 [0.31, 0.42, 0.15] 	 lr: 2.5e-05 	 f: 83.03189329334957
model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse.h5
e2e-stack model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse
gpu: 0
6 3 2
vocab size:  29396
prediction mode: model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse test [0.5, 0.5, 0.5]
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(185.1061) lr: 1e-05 time: 1882.96
pred_count_train 41644

Test...
loss: tensor(433.7677) lr: 5e-05 time: 2007.14
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.55 	r: 74.98 	f1: 78.58 	 4625 	 5603 	 6168
wo 	p: 92.77 	r: 85.92 	f1: 89.21 	 2911 	 3138 	 3388
ni 	p: 81.64 	r: 73.92 	f1: 77.59 	 1125 	 1378 	 1522

[32m iter_1[0m
ga 	p: 82.33 	r: 75.7 	f1: 78.87 	 4669 	 5671 	 6168
wo 	p: 93.18 	r: 85.89 	f1: 89.39 	 2910 	 3123 	 3388
ni 	p: 82.19 	r: 75.82 	f1: 78.88 	 1154 	 1404 	 1522

[32m iter_2[0m
ga 	p: 82.92 	r: 75.5 	f1: 79.04 	 4657 	 5616 	 6168
wo 	p: 93.29 	r: 85.77 	f1: 89.37 	 2906 	 3115 	 3388
ni 	p: 83.8 	r: 74.44 	f1: 78.84 	 1133 	 1352 	 1522
best_thres [[0.32, 0.37, 0.11], [0.27, 0.4, 0.07], [0.3, 0.4, 0.08]]
f [0.8172, 0.8191, 0.82]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse 	current best epoch 26 	 [0.28, 0.64, 0.09] 	 lr: 1e-05 	 f: 82.65533988163092
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse[0m [33m epoch 30 [0m
Train...
loss: tensor(2982.0349) lr: 0.0005 time: 2142.29
pred_count_train 41644

Test...
loss: tensor(2022.9324) lr: 0.0005 time: 2985.35
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 78.37 	r: 72.62 	f1: 75.39 	 4479 	 5715 	 6168
wo 	p: 93.0 	r: 83.15 	f1: 87.8 	 2817 	 3029 	 3388
ni 	p: 82.54 	r: 69.58 	f1: 75.51 	 1059 	 1283 	 1522

[32m iter_1[0m
ga 	p: 77.65 	r: 73.51 	f1: 75.52 	 4534 	 5839 	 6168
wo 	p: 93.47 	r: 83.62 	f1: 88.27 	 2833 	 3031 	 3388
ni 	p: 78.67 	r: 73.92 	f1: 76.22 	 1125 	 1430 	 1522

[32m iter_2[0m
ga 	p: 81.05 	r: 71.13 	f1: 75.76 	 4387 	 5413 	 6168
wo 	p: 93.6 	r: 83.71 	f1: 88.38 	 2836 	 3030 	 3388
ni 	p: 81.98 	r: 71.75 	f1: 76.52 	 1092 	 1332 	 1522
best_thres [[0.38, 0.66, 0.15], [0.33, 0.82, 0.1], [0.68, 0.84, 0.14]]
f [0.7918, 0.7931, 0.7946]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse 	best in epoch 22 	 [0.38, 0.33, 0.12] 	 lr: 0.000125 	 f: 80.67245288920165
model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse.h5
e2e-stack model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse
gpu: 0
6 3 2
vocab size:  29396
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      
[32m iter_0[0m
ga 	p: 75.86 	r: 74.84 	f1: 75.34 	 4616 	 6085 	 6168
wo 	p: 91.42 	r: 83.03 	f1: 87.02 	 2813 	 3077 	 3388
ni 	p: 80.92 	r: 75.49 	f1: 78.11 	 1149 	 1420 	 1522

[32m iter_1[0m
ga 	p: 76.33 	r: 74.3 	f1: 75.3 	 4583 	 6004 	 6168
wo 	p: 89.79 	r: 84.12 	f1: 86.86 	 2850 	 3174 	 3388
ni 	p: 82.36 	r: 74.24 	f1: 78.09 	 1130 	 1372 	 1522

[32m iter_2[0m
ga 	p: 75.71 	r: 74.68 	f1: 75.19 	 4606 	 6084 	 6168
wo 	p: 90.7 	r: 83.44 	f1: 86.92 	 2827 	 3117 	 3388
ni 	p: 82.31 	r: 73.98 	f1: 77.92 	 1126 	 1368 	 1522
best_thres [[0.17, 0.3, 0.11], [0.19, 0.23, 0.13], [0.17, 0.27, 0.13]]
f [0.7921, 0.792, 0.7916]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 8 	 [0.41, 0.27, 0.12] 	 lr: 0.0005 	 f: 79.66033781284675
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 10 [0m
Train...

[32m iter_0[0m
ga 	p: 84.06 	r: 73.38 	f1: 78.36 	 4526 	 5384 	 6168
wo 	p: 92.77 	r: 85.54 	f1: 89.0 	 2898 	 3124 	 3388
ni 	p: 82.67 	r: 72.08 	f1: 77.01 	 1097 	 1327 	 1522

[32m iter_1[0m
ga 	p: 82.71 	r: 75.15 	f1: 78.75 	 4635 	 5604 	 6168
wo 	p: 92.8 	r: 85.98 	f1: 89.26 	 2913 	 3139 	 3388
ni 	p: 81.42 	r: 75.16 	f1: 78.17 	 1144 	 1405 	 1522

[32m iter_2[0m
ga 	p: 82.75 	r: 75.21 	f1: 78.8 	 4639 	 5606 	 6168
wo 	p: 92.47 	r: 86.25 	f1: 89.25 	 2922 	 3160 	 3388
ni 	p: 82.08 	r: 74.31 	f1: 78.0 	 1131 	 1378 	 1522
best_thres [[0.45, 0.43, 0.11], [0.33, 0.42, 0.06], [0.34, 0.33, 0.06]]
f [0.8149, 0.817, 0.8177]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.3_it3_rs2016_preFalse 	best in epoch 26 	 [0.28, 0.64, 0.09] 	 lr: 1e-05 	 f: 82.65533988163092
model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse.h5
e2e-stack model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse
gpu: 0
6 3 2
vocab size:  29396
prediction mode: model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse test [0.5, 0.5, 0.5]
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(2081.0498) lr: 0.0005 time: 2140.73
pred_count_train 41644

Test...
loss: tensor(250.4756) lr: 2.5e-05 time: 6175.06
pred_count_train 41644

Test...
loss: tensor(1954.0862) lr: 0.0005 time: 2745.32
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 78.47 	r: 75.29 	f1: 76.85 	 4644 	 5918 	 6168
wo 	p: 93.73 	r: 83.85 	f1: 88.52 	 2841 	 3031 	 3388
ni 	p: 85.23 	r: 80.35 	f1: 82.72 	 1223 	 1435 	 1522

[32m iter_1[0m
ga 	p: 78.57 	r: 75.0 	f1: 76.74 	 4626 	 5888 	 6168
wo 	p: 92.5 	r: 84.86 	f1: 88.52 	 2875 	 3108 	 3388
ni 	p: 82.77 	r: 81.41 	f1: 82.08 	 1239 	 1497 	 1522

[32m iter_2[0m
ga 	p: 81.63 	r: 72.83 	f1: 76.98 	 4492 	 5503 	 6168
wo 	p: 92.62 	r: 84.86 	f1: 88.57 	 2875 	 3104 	 3388
ni 	p: 85.2 	r: 80.16 	f1: 82.6 	 1220 	 1432 	 1522
best_thres [[0.38, 0.54, 0.21], [0.39, 0.39, 0.14], [0.52, 0.42, 0.2]]
f [0.8115, 0.8109, 0.8117]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 2 	 [0.52, 0.42, 0.2] 	 lr: 0.0005 	 f: 81.16913484021825
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 3 [0m
Train...
loss: tensor(3191.5781) lr: 0.0002 time: 1988.12
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 78.5 	r: 72.29 	f1: 75.27 	 4459 	 5680 	 6168
wo 	p: 92.6 	r: 82.05 	f1: 87.01 	 2780 	 3002 	 3388
ni 	p: 81.36 	r: 76.02 	f1: 78.6 	 1157 	 1422 	 1522

[32m iter_1[0m
ga 	p: 78.39 	r: 72.58 	f1: 75.38 	 4477 	 5711 	 6168
wo 	p: 91.63 	r: 82.76 	f1: 86.97 	 2804 	 3060 	 3388
ni 	p: 81.84 	r: 76.08 	f1: 78.86 	 1158 	 1415 	 1522

[32m iter_2[0m
ga 	p: 78.12 	r: 72.92 	f1: 75.43 	 4498 	 5758 	 6168
wo 	p: 92.32 	r: 82.26 	f1: 87.0 	 2787 	 3019 	 3388
ni 	p: 82.68 	r: 75.89 	f1: 79.14 	 1155 	 1397 	 1522
best_thres [[0.32, 0.34, 0.16], [0.31, 0.26, 0.17], [0.3, 0.31, 0.18]]
f [0.7927, 0.7932, 0.7936]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 8 	 [0.41, 0.27, 0.12] 	 lr: 0.0005 	 f: 79.66033781284675
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 11 [0m
Train...

[32m iter_0[0m
ga 	p: 78.55 	r: 72.75 	f1: 75.54 	 4487 	 5712 	 6168
wo 	p: 92.89 	r: 82.85 	f1: 87.58 	 2807 	 3022 	 3388
ni 	p: 86.32 	r: 76.68 	f1: 81.21 	 1167 	 1352 	 1522

[32m iter_1[0m
ga 	p: 78.19 	r: 72.67 	f1: 75.33 	 4482 	 5732 	 6168
wo 	p: 93.23 	r: 82.47 	f1: 87.52 	 2794 	 2997 	 3388
ni 	p: 85.2 	r: 77.53 	f1: 81.18 	 1180 	 1385 	 1522

[32m iter_2[0m
ga 	p: 78.06 	r: 72.91 	f1: 75.4 	 4497 	 5761 	 6168
wo 	p: 92.86 	r: 82.59 	f1: 87.42 	 2798 	 3013 	 3388
ni 	p: 85.13 	r: 77.46 	f1: 81.11 	 1179 	 1385 	 1522
best_thres [[0.47, 0.37, 0.24], [0.47, 0.37, 0.22], [0.47, 0.36, 0.22]]
f [0.7996, 0.7988, 0.7985]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse 	current best epoch 1 	 [0.47, 0.36, 0.22] 	 lr: 0.0002 	 f: 79.85470098910258
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse[0m [33m epoch 2 [0m
Train...
loss: tensor(1802.7330) lr: 0.0005 time: 1838.41
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.22 	r: 76.1 	f1: 77.63 	 4694 	 5925 	 6168
wo 	p: 91.02 	r: 84.98 	f1: 87.89 	 2879 	 3163 	 3388
ni 	p: 85.74 	r: 78.65 	f1: 82.04 	 1197 	 1396 	 1522

[32m iter_1[0m
ga 	p: 81.29 	r: 74.04 	f1: 77.5 	 4567 	 5618 	 6168
wo 	p: 91.73 	r: 83.83 	f1: 87.6 	 2840 	 3096 	 3388
ni 	p: 86.13 	r: 77.53 	f1: 81.6 	 1180 	 1370 	 1522

[32m iter_2[0m
ga 	p: 80.26 	r: 75.06 	f1: 77.57 	 4630 	 5769 	 6168
wo 	p: 91.31 	r: 84.95 	f1: 88.01 	 2878 	 3152 	 3388
ni 	p: 85.93 	r: 78.65 	f1: 82.13 	 1197 	 1393 	 1522
best_thres [[0.38, 0.44, 0.19], [0.45, 0.44, 0.19], [0.43, 0.45, 0.19]]
f [0.8135, 0.8125, 0.813]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 3 	 [0.43, 0.45, 0.19] 	 lr: 0.0005 	 f: 81.29640027450247
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 4 [0m
Train...
loss: tensor(2034.9323) lr: 0.0002 time: 1838.84
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.04 	r: 77.27 	f1: 79.11 	 4766 	 5881 	 6168
wo 	p: 94.54 	r: 84.86 	f1: 89.44 	 2875 	 3041 	 3388
ni 	p: 84.89 	r: 73.85 	f1: 78.99 	 1124 	 1324 	 1522

[32m iter_1[0m
ga 	p: 81.32 	r: 77.16 	f1: 79.18 	 4759 	 5852 	 6168
wo 	p: 94.92 	r: 84.89 	f1: 89.62 	 2876 	 3030 	 3388
ni 	p: 83.8 	r: 75.1 	f1: 79.21 	 1143 	 1364 	 1522

[32m iter_2[0m
ga 	p: 81.3 	r: 77.09 	f1: 79.14 	 4755 	 5849 	 6168
wo 	p: 94.92 	r: 84.92 	f1: 89.64 	 2877 	 3031 	 3388
ni 	p: 83.38 	r: 75.49 	f1: 79.24 	 1149 	 1378 	 1522

[32m iter_3[0m
ga 	p: 84.77 	r: 74.16 	f1: 79.11 	 4574 	 5396 	 6168
wo 	p: 94.95 	r: 84.86 	f1: 89.62 	 2875 	 3028 	 3388
ni 	p: 83.5 	r: 75.49 	f1: 79.3 	 1149 	 1376 	 1522

[32m iter_4[0m
ga 	p: 81.29 	r: 77.04 	f1: 79.11 	 4752 	 5846 	 6168
wo 	p: 94.89 	r: 84.92 	f1: 89.63 	 2877 	 3032 	 3388
ni 	p: 83.5 	r: 75.49 	f1: 79.3 	 1149 	 1376 	 1522

[32m iter_5[0m
ga 	p: 81.4 	r: 76.93 	f1: 79.1 	 4745 	 5829 	 6168
wo 	p: 94.85 	r: 84.89 	f1: 89.6 	 2876 	 3032 	 3388
ni 	p: 83.5 	r: 75.49 	f1: 79.3 	 1149 	 1376 	 1522

[32m iter_6[0m
ga 	p: 81.42 	r: 76.93 	f1: 79.11 	 4745 	 5828 	 6168
wo 	p: 94.89 	r: 84.92 	f1: 89.63 	 2877 	 3032 	 3388
ni 	p: 83.49 	r: 75.43 	f1: 79.25 	 1148 	 1375 	 1522

[32m iter_7[0m
ga 	p: 81.4 	r: 76.93 	f1: 79.1 	 4745 	 5829 	 6168
wo 	p: 94.86 	r: 84.92 	f1: 89.61 	 2877 	 3033 	 3388
ni 	p: 83.49 	r: 75.43 	f1: 79.25 	 1148 	 1375 	 1522

[32m iter_8[0m
ga 	p: 81.42 	r: 76.93 	f1: 79.11 	 4745 	 5828 	 6168
wo 	p: 94.89 	r: 84.89 	f1: 89.61 	 2876 	 3031 	 3388
ni 	p: 83.49 	r: 75.43 	f1: 79.25 	 1148 	 1375 	 1522

[32m iter_9[0m
ga 	p: 81.4 	r: 76.93 	f1: 79.1 	 4745 	 5829 	 6168
wo 	p: 94.86 	r: 84.92 	f1: 89.61 	 2877 	 3033 	 3388
ni 	p: 83.49 	r: 75.43 	f1: 79.25 	 1148 	 1375 	 1522
best_thres [[0.3, 0.77, 0.15], [0.3, 0.8, 0.1], [0.3, 0.81, 0.09], [0.58, 0.81, 0.09], [0.3, 0.81, 0.09], [0.31, 0.79, 0.09], [0.31, 0.81, 0.09], [0.31, 0.79, 0.09], [0.31, 0.81, 0.09], [0.31, 0.79, 0.09]]
f [0.8221, 0.8227, 0.8228, 0.823, 0.823, 0.823, 0.823, 0.823, 0.823, 0.823]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse 	current best epoch 18 	 [0.31, 0.7, 0.11] 	 lr: 2.5e-05 	 f: 82.63404788158559
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse[0m [33m epoch 20 [0m
Train...
loss: tensor(1902.0590) lr: 0.0005 time: 2927.39
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.09 	r: 74.69 	f1: 77.76 	 4607 	 5681 	 6168
wo 	p: 92.53 	r: 85.15 	f1: 88.69 	 2885 	 3118 	 3388
ni 	p: 81.51 	r: 84.3 	f1: 82.88 	 1283 	 1574 	 1522

[32m iter_1[0m
ga 	p: 81.2 	r: 74.71 	f1: 77.82 	 4608 	 5675 	 6168
wo 	p: 92.09 	r: 85.6 	f1: 88.73 	 2900 	 3149 	 3388
ni 	p: 82.75 	r: 82.59 	f1: 82.67 	 1257 	 1519 	 1522

[32m iter_2[0m
ga 	p: 81.12 	r: 74.9 	f1: 77.89 	 4620 	 5695 	 6168
wo 	p: 92.1 	r: 85.66 	f1: 88.76 	 2902 	 3151 	 3388
ni 	p: 82.81 	r: 82.59 	f1: 82.7 	 1257 	 1518 	 1522
best_thres [[0.49, 0.44, 0.21], [0.49, 0.38, 0.24], [0.49, 0.38, 0.24]]
f [0.8181, 0.8182, 0.8185]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse 	current best epoch 2 	 [0.49, 0.38, 0.24] 	 lr: 0.0002 	 f: 81.84532139192089
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse[0m [33m epoch 3 [0m
Train...
loss: tensor(1636.2089) lr: 0.0005 time: 2101.33
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.87 	r: 72.49 	f1: 76.0 	 4471 	 5598 	 6168
wo 	p: 91.75 	r: 82.73 	f1: 87.01 	 2803 	 3055 	 3388
ni 	p: 79.16 	r: 78.38 	f1: 78.77 	 1193 	 1507 	 1522

[32m iter_1[0m
ga 	p: 80.07 	r: 72.03 	f1: 75.84 	 4443 	 5549 	 6168
wo 	p: 92.63 	r: 82.0 	f1: 86.99 	 2778 	 2999 	 3388
ni 	p: 79.58 	r: 76.54 	f1: 78.03 	 1165 	 1464 	 1522

[32m iter_2[0m
ga 	p: 78.95 	r: 72.97 	f1: 75.84 	 4501 	 5701 	 6168
wo 	p: 91.97 	r: 82.82 	f1: 87.16 	 2806 	 3051 	 3388
ni 	p: 80.18 	r: 76.81 	f1: 78.46 	 1169 	 1458 	 1522
best_thres [[0.5, 0.69, 0.24], [0.5, 0.69, 0.29], [0.48, 0.69, 0.3]]
f [0.7973, 0.7963, 0.7963]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 8 	 [0.41, 0.27, 0.12] 	 lr: 0.0005 	 f: 79.66033781284675
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 12 [0m
Train...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                loss: tensor(1869.1071) lr: 0.0005 time: 2697.39
pred_count_train 41644

Test...
loss: tensor(1529.6554) lr: 0.0005 time: 2151.41
pred_count_train 41644

Test...
loss: tensor(166.3548) lr: 2.5e-05 time: 6181.86
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 77.74 	r: 75.44 	f1: 76.57 	 4653 	 5985 	 6168
wo 	p: 93.32 	r: 84.47 	f1: 88.68 	 2862 	 3067 	 3388
ni 	p: 84.67 	r: 74.05 	f1: 79.0 	 1127 	 1331 	 1522

[32m iter_1[0m
ga 	p: 78.47 	r: 75.1 	f1: 76.75 	 4632 	 5903 	 6168
wo 	p: 92.78 	r: 84.21 	f1: 88.29 	 2853 	 3075 	 3388
ni 	p: 79.85 	r: 75.49 	f1: 77.61 	 1149 	 1439 	 1522

[32m iter_2[0m
ga 	p: 78.36 	r: 75.37 	f1: 76.84 	 4649 	 5933 	 6168
wo 	p: 93.56 	r: 84.03 	f1: 88.54 	 2847 	 3043 	 3388
ni 	p: 81.61 	r: 75.82 	f1: 78.61 	 1154 	 1414 	 1522
best_thres [[0.3, 0.63, 0.13], [0.32, 0.52, 0.08], [0.32, 0.68, 0.09]]
f [0.8054, 0.8044, 0.8049]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 3 	 [0.43, 0.45, 0.19] 	 lr: 0.0005 	 f: 81.29640027450247
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 6 [0m
Train...

[32m iter_0[0m
ga 	p: 78.32 	r: 73.05 	f1: 75.6 	 4506 	 5753 	 6168
wo 	p: 92.71 	r: 82.91 	f1: 87.54 	 2809 	 3030 	 3388
ni 	p: 83.22 	r: 74.64 	f1: 78.7 	 1136 	 1365 	 1522

[32m iter_1[0m
ga 	p: 78.6 	r: 72.67 	f1: 75.52 	 4482 	 5702 	 6168
wo 	p: 92.16 	r: 83.23 	f1: 87.47 	 2820 	 3060 	 3388
ni 	p: 82.1 	r: 75.36 	f1: 78.59 	 1147 	 1397 	 1522

[32m iter_2[0m
ga 	p: 78.42 	r: 72.88 	f1: 75.55 	 4495 	 5732 	 6168
wo 	p: 92.15 	r: 83.56 	f1: 87.65 	 2831 	 3072 	 3388
ni 	p: 84.04 	r: 74.05 	f1: 78.73 	 1127 	 1341 	 1522
best_thres [[0.43, 0.57, 0.17], [0.43, 0.5, 0.15], [0.43, 0.51, 0.19]]
f [0.7963, 0.796, 0.7962]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 8 	 [0.41, 0.27, 0.12] 	 lr: 0.0005 	 f: 79.66033781284675
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 13 [0m
Train...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    loss: tensor(1817.2180) lr: 0.00025 time: 2873.65
pred_count_train 41644

Test...
loss: tensor(993.2192) lr: 0.0002 time: 1837.91
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.28 	r: 76.59 	f1: 78.86 	 4724 	 5812 	 6168
wo 	p: 93.82 	r: 85.6 	f1: 89.52 	 2900 	 3091 	 3388
ni 	p: 80.28 	r: 75.16 	f1: 77.64 	 1144 	 1425 	 1522

[32m iter_1[0m
ga 	p: 83.38 	r: 74.76 	f1: 78.83 	 4611 	 5530 	 6168
wo 	p: 93.45 	r: 85.89 	f1: 89.51 	 2910 	 3114 	 3388
ni 	p: 82.92 	r: 73.98 	f1: 78.19 	 1126 	 1358 	 1522

[32m iter_2[0m
ga 	p: 85.3 	r: 73.3 	f1: 78.85 	 4521 	 5300 	 6168
wo 	p: 93.42 	r: 85.86 	f1: 89.48 	 2909 	 3114 	 3388
ni 	p: 82.31 	r: 74.31 	f1: 78.11 	 1131 	 1374 	 1522

[32m iter_3[0m
ga 	p: 81.77 	r: 76.15 	f1: 78.86 	 4697 	 5744 	 6168
wo 	p: 93.96 	r: 85.42 	f1: 89.49 	 2894 	 3080 	 3388
ni 	p: 82.4 	r: 74.11 	f1: 78.04 	 1128 	 1369 	 1522

[32m iter_4[0m
ga 	p: 81.79 	r: 76.18 	f1: 78.89 	 4699 	 5745 	 6168
wo 	p: 93.45 	r: 85.89 	f1: 89.51 	 2910 	 3114 	 3388
ni 	p: 81.82 	r: 74.84 	f1: 78.17 	 1139 	 1392 	 1522

[32m iter_5[0m
ga 	p: 83.02 	r: 75.13 	f1: 78.88 	 4634 	 5582 	 6168
wo 	p: 93.99 	r: 85.42 	f1: 89.5 	 2894 	 3079 	 3388
ni 	p: 81.82 	r: 74.84 	f1: 78.17 	 1139 	 1392 	 1522

[32m iter_6[0m
ga 	p: 81.63 	r: 76.3 	f1: 78.87 	 4706 	 5765 	 6168
wo 	p: 93.45 	r: 85.89 	f1: 89.51 	 2910 	 3114 	 3388
ni 	p: 81.82 	r: 74.84 	f1: 78.17 	 1139 	 1392 	 1522

[32m iter_7[0m
ga 	p: 83.02 	r: 75.13 	f1: 78.88 	 4634 	 5582 	 6168
wo 	p: 93.99 	r: 85.42 	f1: 89.5 	 2894 	 3079 	 3388
ni 	p: 81.82 	r: 74.84 	f1: 78.17 	 1139 	 1392 	 1522

[32m iter_8[0m
ga 	p: 81.63 	r: 76.3 	f1: 78.87 	 4706 	 5765 	 6168
wo 	p: 93.45 	r: 85.89 	f1: 89.51 	 2910 	 3114 	 3388
ni 	p: 81.82 	r: 74.84 	f1: 78.17 	 1139 	 1392 	 1522

[32m iter_9[0m
ga 	p: 83.02 	r: 75.13 	f1: 78.88 	 4634 	 5582 	 6168
wo 	p: 93.99 	r: 85.42 	f1: 89.5 	 2894 	 3079 	 3388
ni 	p: 81.82 	r: 74.84 	f1: 78.17 	 1139 	 1392 	 1522
best_thres [[0.28, 0.73, 0.1], [0.44, 0.68, 0.1], [0.61, 0.68, 0.09], [0.29, 0.74, 0.09], [0.29, 0.68, 0.08], [0.4, 0.74, 0.08], [0.28, 0.68, 0.08], [0.4, 0.74, 0.08], [0.28, 0.68, 0.08], [0.4, 0.74, 0.08]]
f [0.8192, 0.8198, 0.8201, 0.82, 0.8201, 0.8201, 0.8201, 0.8201, 0.8201, 0.8202]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse 	current best epoch 18 	 [0.31, 0.7, 0.11] 	 lr: 2.5e-05 	 f: 82.63404788158559
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse[0m [33m epoch 21 [0m
Train...

[32m iter_0[0m
ga 	p: 77.77 	r: 78.29 	f1: 78.03 	 4829 	 6209 	 6168
wo 	p: 92.66 	r: 86.13 	f1: 89.28 	 2918 	 3149 	 3388
ni 	p: 81.42 	r: 74.57 	f1: 77.85 	 1135 	 1394 	 1522

[32m iter_1[0m
ga 	p: 82.29 	r: 74.81 	f1: 78.37 	 4614 	 5607 	 6168
wo 	p: 93.11 	r: 86.16 	f1: 89.5 	 2919 	 3135 	 3388
ni 	p: 82.86 	r: 74.31 	f1: 78.35 	 1131 	 1365 	 1522

[32m iter_2[0m
ga 	p: 82.19 	r: 74.68 	f1: 78.25 	 4606 	 5604 	 6168
wo 	p: 93.15 	r: 85.89 	f1: 89.37 	 2910 	 3124 	 3388
ni 	p: 84.23 	r: 73.0 	f1: 78.21 	 1111 	 1319 	 1522
best_thres [[0.21, 0.52, 0.17], [0.41, 0.59, 0.16], [0.41, 0.63, 0.19]]
f [0.8137, 0.8158, 0.8161]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse 	current best epoch 3 	 [0.39, 0.38, 0.13] 	 lr: 0.0002 	 f: 81.93662738269224
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse[0m [33m epoch 6 [0m
Train...

[32m iter_0[0m
ga 	p: 79.44 	r: 73.49 	f1: 76.35 	 4533 	 5706 	 6168
wo 	p: 93.9 	r: 82.7 	f1: 87.95 	 2802 	 2984 	 3388
ni 	p: 82.93 	r: 75.95 	f1: 79.29 	 1156 	 1394 	 1522

[32m iter_1[0m
ga 	p: 79.4 	r: 73.61 	f1: 76.39 	 4540 	 5718 	 6168
wo 	p: 92.02 	r: 84.06 	f1: 87.86 	 2848 	 3095 	 3388
ni 	p: 81.31 	r: 78.06 	f1: 79.65 	 1188 	 1461 	 1522

[32m iter_2[0m
ga 	p: 78.76 	r: 74.03 	f1: 76.32 	 4566 	 5797 	 6168
wo 	p: 93.41 	r: 83.32 	f1: 88.08 	 2823 	 3022 	 3388
ni 	p: 81.67 	r: 78.19 	f1: 79.89 	 1190 	 1457 	 1522
best_thres [[0.33, 0.58, 0.09], [0.32, 0.42, 0.08], [0.3, 0.51, 0.08]]
f [0.8025, 0.8029, 0.8031]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 13 	 [0.3, 0.51, 0.08] 	 lr: 0.00025 	 f: 80.30938811298303
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 14 [0m
Train...
loss: tensor(1445.2347) lr: 0.0005 time: 1971.84
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 77.93 	r: 75.11 	f1: 76.5 	 4633 	 5945 	 6168
wo 	p: 91.22 	r: 85.24 	f1: 88.13 	 2888 	 3166 	 3388
ni 	p: 83.0 	r: 73.78 	f1: 78.12 	 1123 	 1353 	 1522

[32m iter_1[0m
ga 	p: 79.29 	r: 74.3 	f1: 76.72 	 4583 	 5780 	 6168
wo 	p: 91.86 	r: 85.3 	f1: 88.46 	 2890 	 3146 	 3388
ni 	p: 82.94 	r: 73.46 	f1: 77.91 	 1118 	 1348 	 1522

[32m iter_2[0m
ga 	p: 81.16 	r: 72.84 	f1: 76.78 	 4493 	 5536 	 6168
wo 	p: 91.69 	r: 85.33 	f1: 88.4 	 2891 	 3153 	 3388
ni 	p: 82.02 	r: 74.31 	f1: 77.97 	 1131 	 1379 	 1522
best_thres [[0.36, 0.51, 0.15], [0.39, 0.48, 0.16], [0.51, 0.55, 0.15]]
f [0.8025, 0.8036, 0.8042]
load model: epoch3
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 3 	 [0.43, 0.45, 0.19] 	 lr: 0.0005 	 f: 81.29640027450247
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 8 [0m
Train...
loss: tensor(756.1671) lr: 0.0002 time: 2148.26
pred_count_train 41644

Test...
loss: tensor(1588.5732) lr: 0.00025 time: 2908.42
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.56 	r: 74.22 	f1: 78.17 	 4578 	 5545 	 6168
wo 	p: 92.75 	r: 85.71 	f1: 89.09 	 2904 	 3131 	 3388
ni 	p: 85.0 	r: 73.32 	f1: 78.73 	 1116 	 1313 	 1522

[32m iter_1[0m
ga 	p: 80.29 	r: 75.88 	f1: 78.02 	 4680 	 5829 	 6168
wo 	p: 91.17 	r: 87.19 	f1: 89.14 	 2954 	 3240 	 3388
ni 	p: 81.66 	r: 76.35 	f1: 78.91 	 1162 	 1423 	 1522

[32m iter_2[0m
ga 	p: 81.1 	r: 75.34 	f1: 78.11 	 4647 	 5730 	 6168
wo 	p: 93.42 	r: 85.12 	f1: 89.08 	 2884 	 3087 	 3388
ni 	p: 82.98 	r: 75.3 	f1: 78.95 	 1146 	 1381 	 1522
best_thres [[0.56, 0.59, 0.13], [0.41, 0.33, 0.06], [0.48, 0.74, 0.07]]
f [0.8163, 0.8159, 0.8158]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse 	current best epoch 3 	 [0.39, 0.38, 0.13] 	 lr: 0.0002 	 f: 81.93662738269224
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse[0m [33m epoch 7 [0m
Train...
loss: tensor(1323.2075) lr: 0.00025 time: 2163.79
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.65 	r: 73.65 	f1: 76.53 	 4543 	 5704 	 6168
wo 	p: 89.46 	r: 86.42 	f1: 87.91 	 2928 	 3273 	 3388
ni 	p: 82.51 	r: 79.04 	f1: 80.74 	 1203 	 1458 	 1522

[32m iter_1[0m
ga 	p: 77.85 	r: 75.05 	f1: 76.42 	 4629 	 5946 	 6168
wo 	p: 91.58 	r: 83.83 	f1: 87.53 	 2840 	 3101 	 3388
ni 	p: 81.91 	r: 79.43 	f1: 80.65 	 1209 	 1476 	 1522

[32m iter_2[0m
ga 	p: 77.39 	r: 75.52 	f1: 76.44 	 4658 	 6019 	 6168
wo 	p: 90.52 	r: 85.42 	f1: 87.9 	 2894 	 3197 	 3388
ni 	p: 82.9 	r: 78.38 	f1: 80.58 	 1193 	 1439 	 1522
best_thres [[0.37, 0.28, 0.16], [0.3, 0.46, 0.15], [0.28, 0.36, 0.17]]
f [0.8064, 0.8049, 0.8049]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 14 	 [0.28, 0.36, 0.17] 	 lr: 0.00025 	 f: 80.48791771400373
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 15 [0m
Train...

[32m iter_0[0m
ga 	p: 81.98 	r: 75.32 	f1: 78.51 	 4646 	 5667 	 6168
wo 	p: 93.58 	r: 85.63 	f1: 89.43 	 2901 	 3100 	 3388
ni 	p: 89.78 	r: 75.62 	f1: 82.1 	 1151 	 1282 	 1522

[32m iter_1[0m
ga 	p: 80.07 	r: 77.01 	f1: 78.51 	 4750 	 5932 	 6168
wo 	p: 93.67 	r: 85.15 	f1: 89.21 	 2885 	 3080 	 3388
ni 	p: 89.47 	r: 75.95 	f1: 82.16 	 1156 	 1292 	 1522

[32m iter_2[0m
ga 	p: 83.33 	r: 74.3 	f1: 78.56 	 4583 	 5500 	 6168
wo 	p: 93.56 	r: 85.33 	f1: 89.26 	 2891 	 3090 	 3388
ni 	p: 85.22 	r: 79.17 	f1: 82.08 	 1205 	 1414 	 1522
best_thres [[0.56, 0.59, 0.22], [0.44, 0.59, 0.22], [0.65, 0.66, 0.14]]
f [0.8234, 0.8228, 0.823]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 8 	 [0.65, 0.66, 0.14] 	 lr: 0.00025 	 f: 82.30095453759179
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 9 [0m
Train...
loss: tensor(594.5925) lr: 0.0002 time: 2154.22
pred_count_train 41644

Test...
loss: tensor(110.7215) lr: 2.5e-05 time: 6213.51
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.03 	r: 75.42 	f1: 78.13 	 4652 	 5741 	 6168
wo 	p: 92.53 	r: 85.21 	f1: 88.72 	 2887 	 3120 	 3388
ni 	p: 86.49 	r: 71.48 	f1: 78.27 	 1088 	 1258 	 1522

[32m iter_1[0m
ga 	p: 81.77 	r: 74.82 	f1: 78.14 	 4615 	 5644 	 6168
wo 	p: 93.21 	r: 84.65 	f1: 88.72 	 2868 	 3077 	 3388
ni 	p: 84.72 	r: 71.75 	f1: 77.69 	 1092 	 1289 	 1522

[32m iter_2[0m
ga 	p: 80.94 	r: 75.54 	f1: 78.14 	 4659 	 5756 	 6168
wo 	p: 93.08 	r: 84.92 	f1: 88.81 	 2877 	 3091 	 3388
ni 	p: 86.83 	r: 70.63 	f1: 77.9 	 1075 	 1238 	 1522
best_thres [[0.47, 0.59, 0.14], [0.5, 0.78, 0.09], [0.47, 0.78, 0.13]]
f [0.814, 0.8136, 0.8137]
load model: epoch3
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse 	current best epoch 3 	 [0.39, 0.38, 0.13] 	 lr: 0.0002 	 f: 81.93662738269224
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse[0m [33m epoch 8 [0m
Train...
loss: tensor(948.9073) lr: 0.00025 time: 1954.5
pred_count_train 41644

Test...
loss: tensor(1432.9801) lr: 0.00025 time: 2836.75
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.23 	r: 75.37 	f1: 78.19 	 4649 	 5723 	 6168
wo 	p: 93.3 	r: 85.54 	f1: 89.25 	 2898 	 3106 	 3388
ni 	p: 83.99 	r: 75.49 	f1: 79.52 	 1149 	 1368 	 1522

[32m iter_1[0m
ga 	p: 82.97 	r: 74.32 	f1: 78.41 	 4584 	 5525 	 6168
wo 	p: 93.04 	r: 85.66 	f1: 89.2 	 2902 	 3119 	 3388
ni 	p: 86.47 	r: 74.31 	f1: 79.93 	 1131 	 1308 	 1522

[32m iter_2[0m
ga 	p: 82.43 	r: 74.84 	f1: 78.45 	 4616 	 5600 	 6168
wo 	p: 93.92 	r: 84.86 	f1: 89.16 	 2875 	 3061 	 3388
ni 	p: 84.68 	r: 75.89 	f1: 80.04 	 1155 	 1364 	 1522
best_thres [[0.41, 0.58, 0.12], [0.47, 0.51, 0.15], [0.47, 0.71, 0.12]]
f [0.8175, 0.8185, 0.8188]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 8 	 [0.65, 0.66, 0.14] 	 lr: 0.00025 	 f: 82.30095453759179
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 10 [0m
Train...
loss: tensor(1102.1809) lr: 0.0001 time: 1828.22
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.46 	r: 73.18 	f1: 76.19 	 4514 	 5681 	 6168
wo 	p: 92.27 	r: 84.86 	f1: 88.41 	 2875 	 3116 	 3388
ni 	p: 82.82 	r: 78.25 	f1: 80.47 	 1191 	 1438 	 1522

[32m iter_1[0m
ga 	p: 77.94 	r: 74.69 	f1: 76.28 	 4607 	 5911 	 6168
wo 	p: 92.55 	r: 83.94 	f1: 88.04 	 2844 	 3073 	 3388
ni 	p: 81.79 	r: 79.7 	f1: 80.73 	 1213 	 1483 	 1522

[32m iter_2[0m
ga 	p: 78.2 	r: 74.45 	f1: 76.28 	 4592 	 5872 	 6168
wo 	p: 92.18 	r: 84.59 	f1: 88.23 	 2866 	 3109 	 3388
ni 	p: 81.89 	r: 79.63 	f1: 80.75 	 1212 	 1480 	 1522
best_thres [[0.4, 0.51, 0.21], [0.33, 0.51, 0.2], [0.34, 0.5, 0.2]]
f [0.8051, 0.8047, 0.8048]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 14 	 [0.28, 0.36, 0.17] 	 lr: 0.00025 	 f: 80.48791771400373
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 16 [0m
Train...

[32m iter_0[0m
ga 	p: 81.42 	r: 76.61 	f1: 78.94 	 4725 	 5803 	 6168
wo 	p: 93.08 	r: 86.51 	f1: 89.67 	 2931 	 3149 	 3388
ni 	p: 88.8 	r: 78.65 	f1: 83.41 	 1197 	 1348 	 1522

[32m iter_1[0m
ga 	p: 81.89 	r: 76.77 	f1: 79.25 	 4735 	 5782 	 6168
wo 	p: 92.63 	r: 87.13 	f1: 89.79 	 2952 	 3187 	 3388
ni 	p: 86.8 	r: 79.89 	f1: 83.2 	 1216 	 1401 	 1522

[32m iter_2[0m
ga 	p: 81.97 	r: 76.61 	f1: 79.2 	 4725 	 5764 	 6168
wo 	p: 94.21 	r: 86.01 	f1: 89.92 	 2914 	 3093 	 3388
ni 	p: 87.48 	r: 79.43 	f1: 83.26 	 1209 	 1382 	 1522
best_thres [[0.49, 0.58, 0.21], [0.48, 0.55, 0.15], [0.52, 0.76, 0.16]]
f [0.8282, 0.8292, 0.8295]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse 	current best epoch 8 	 [0.52, 0.76, 0.16] 	 lr: 0.0001 	 f: 82.9521537813947
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse[0m [33m epoch 9 [0m
Train...

[32m iter_0[0m
ga 	p: 81.37 	r: 75.91 	f1: 78.54 	 4682 	 5754 	 6168
wo 	p: 93.48 	r: 85.48 	f1: 89.3 	 2896 	 3098 	 3388
ni 	p: 81.66 	r: 73.72 	f1: 77.49 	 1122 	 1374 	 1522

[32m iter_1[0m
ga 	p: 83.17 	r: 74.59 	f1: 78.65 	 4601 	 5532 	 6168
wo 	p: 93.25 	r: 86.07 	f1: 89.52 	 2916 	 3127 	 3388
ni 	p: 81.9 	r: 74.64 	f1: 78.1 	 1136 	 1387 	 1522

[32m iter_2[0m
ga 	p: 83.1 	r: 74.48 	f1: 78.56 	 4594 	 5528 	 6168
wo 	p: 93.25 	r: 86.07 	f1: 89.52 	 2916 	 3127 	 3388
ni 	p: 80.43 	r: 75.36 	f1: 77.82 	 1147 	 1426 	 1522

[32m iter_3[0m
ga 	p: 83.11 	r: 74.53 	f1: 78.59 	 4597 	 5531 	 6168
wo 	p: 93.31 	r: 86.07 	f1: 89.54 	 2916 	 3125 	 3388
ni 	p: 85.1 	r: 71.68 	f1: 77.82 	 1091 	 1282 	 1522

[32m iter_4[0m
ga 	p: 83.12 	r: 74.55 	f1: 78.6 	 4598 	 5532 	 6168
wo 	p: 93.28 	r: 86.07 	f1: 89.53 	 2916 	 3126 	 3388
ni 	p: 81.87 	r: 74.18 	f1: 77.84 	 1129 	 1379 	 1522

[32m iter_5[0m
ga 	p: 83.12 	r: 74.55 	f1: 78.6 	 4598 	 5532 	 6168
wo 	p: 93.28 	r: 86.07 	f1: 89.53 	 2916 	 3126 	 3388
ni 	p: 81.86 	r: 74.11 	f1: 77.79 	 1128 	 1378 	 1522

[32m iter_6[0m
ga 	p: 83.12 	r: 74.55 	f1: 78.6 	 4598 	 5532 	 6168
wo 	p: 93.28 	r: 86.07 	f1: 89.53 	 2916 	 3126 	 3388
ni 	p: 84.34 	r: 72.21 	f1: 77.81 	 1099 	 1303 	 1522

[32m iter_7[0m
ga 	p: 83.12 	r: 74.55 	f1: 78.6 	 4598 	 5532 	 6168
wo 	p: 93.28 	r: 86.07 	f1: 89.53 	 2916 	 3126 	 3388
ni 	p: 81.86 	r: 74.11 	f1: 77.79 	 1128 	 1378 	 1522

[32m iter_8[0m
ga 	p: 83.12 	r: 74.55 	f1: 78.6 	 4598 	 5532 	 6168
wo 	p: 93.28 	r: 86.07 	f1: 89.53 	 2916 	 3126 	 3388
ni 	p: 81.86 	r: 74.11 	f1: 77.79 	 1128 	 1378 	 1522

[32m iter_9[0m
ga 	p: 83.12 	r: 74.55 	f1: 78.6 	 4598 	 5532 	 6168
wo 	p: 93.28 	r: 86.07 	f1: 89.53 	 2916 	 3126 	 3388
ni 	p: 84.34 	r: 72.21 	f1: 77.81 	 1099 	 1303 	 1522
best_thres [[0.4, 0.75, 0.1], [0.57, 0.7, 0.06], [0.57, 0.7, 0.05], [0.57, 0.7, 0.1], [0.57, 0.7, 0.06], [0.57, 0.7, 0.06], [0.57, 0.7, 0.09], [0.57, 0.7, 0.06], [0.57, 0.7, 0.06], [0.57, 0.7, 0.09]]
f [0.8167, 0.818, 0.8181, 0.8183, 0.8183, 0.8184, 0.8184, 0.8185, 0.8185, 0.8185]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse 	current best epoch 18 	 [0.31, 0.7, 0.11] 	 lr: 2.5e-05 	 f: 82.63404788158559
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse[0m [33m epoch 22 [0m
Train...
loss: tensor(728.8218) lr: 0.00025 time: 1859.44
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.03 	r: 74.32 	f1: 77.53 	 4584 	 5657 	 6168
wo 	p: 92.67 	r: 84.71 	f1: 88.51 	 2870 	 3097 	 3388
ni 	p: 83.5 	r: 75.49 	f1: 79.3 	 1149 	 1376 	 1522

[32m iter_1[0m
ga 	p: 80.96 	r: 74.74 	f1: 77.73 	 4610 	 5694 	 6168
wo 	p: 92.67 	r: 84.68 	f1: 88.49 	 2869 	 3096 	 3388
ni 	p: 84.03 	r: 73.65 	f1: 78.5 	 1121 	 1334 	 1522

[32m iter_2[0m
ga 	p: 79.58 	r: 76.09 	f1: 77.8 	 4693 	 5897 	 6168
wo 	p: 93.54 	r: 84.18 	f1: 88.61 	 2852 	 3049 	 3388
ni 	p: 83.19 	r: 74.77 	f1: 78.75 	 1138 	 1368 	 1522
best_thres [[0.51, 0.57, 0.17], [0.47, 0.48, 0.2], [0.4, 0.74, 0.17]]
f [0.8113, 0.8113, 0.8114]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 8 	 [0.65, 0.66, 0.14] 	 lr: 0.00025 	 f: 82.30095453759179
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 11 [0m
Train...
loss: tensor(1307.5050) lr: 0.00025 time: 2931.5
pred_count_train 41644

Test...
loss: tensor(768.0674) lr: 0.0001 time: 2099.93
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.36 	r: 74.89 	f1: 78.45 	 4619 	 5608 	 6168
wo 	p: 92.21 	r: 87.31 	f1: 89.69 	 2958 	 3208 	 3388
ni 	p: 86.71 	r: 75.89 	f1: 80.94 	 1155 	 1332 	 1522

[32m iter_1[0m
ga 	p: 80.89 	r: 76.61 	f1: 78.69 	 4725 	 5841 	 6168
wo 	p: 93.26 	r: 86.54 	f1: 89.77 	 2932 	 3144 	 3388
ni 	p: 85.89 	r: 76.81 	f1: 81.1 	 1169 	 1361 	 1522

[32m iter_2[0m
ga 	p: 83.87 	r: 74.27 	f1: 78.78 	 4581 	 5462 	 6168
wo 	p: 93.2 	r: 86.57 	f1: 89.76 	 2933 	 3147 	 3388
ni 	p: 84.45 	r: 77.79 	f1: 80.98 	 1184 	 1402 	 1522
best_thres [[0.51, 0.39, 0.2], [0.4, 0.5, 0.15], [0.69, 0.51, 0.13]]
f [0.8228, 0.8234, 0.8239]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse 	current best epoch 8 	 [0.52, 0.76, 0.16] 	 lr: 0.0001 	 f: 82.9521537813947
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse[0m [33m epoch 10 [0m
Train...

[32m iter_0[0m
ga 	p: 79.71 	r: 74.16 	f1: 76.84 	 4574 	 5738 	 6168
wo 	p: 92.08 	r: 84.39 	f1: 88.06 	 2859 	 3105 	 3388
ni 	p: 84.48 	r: 74.77 	f1: 79.33 	 1138 	 1347 	 1522

[32m iter_1[0m
ga 	p: 79.76 	r: 73.93 	f1: 76.74 	 4560 	 5717 	 6168
wo 	p: 92.33 	r: 84.21 	f1: 88.08 	 2853 	 3090 	 3388
ni 	p: 83.83 	r: 76.28 	f1: 79.88 	 1161 	 1385 	 1522

[32m iter_2[0m
ga 	p: 79.58 	r: 74.04 	f1: 76.71 	 4567 	 5739 	 6168
wo 	p: 92.67 	r: 83.97 	f1: 88.11 	 2845 	 3070 	 3388
ni 	p: 82.61 	r: 77.73 	f1: 80.09 	 1183 	 1432 	 1522
best_thres [[0.29, 0.49, 0.14], [0.29, 0.46, 0.14], [0.29, 0.51, 0.11]]
f [0.806, 0.8061, 0.8062]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 16 	 [0.29, 0.51, 0.11] 	 lr: 0.00025 	 f: 80.61763001706939
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 17 [0m
Train...
loss: tensor(598.8308) lr: 0.00025 time: 2153.4
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.32 	r: 75.13 	f1: 77.17 	 4634 	 5842 	 6168
wo 	p: 91.9 	r: 85.77 	f1: 88.73 	 2906 	 3162 	 3388
ni 	p: 82.34 	r: 73.19 	f1: 77.5 	 1114 	 1353 	 1522

[32m iter_1[0m
ga 	p: 81.25 	r: 74.03 	f1: 77.47 	 4566 	 5620 	 6168
wo 	p: 92.05 	r: 84.42 	f1: 88.07 	 2860 	 3107 	 3388
ni 	p: 87.02 	r: 68.73 	f1: 76.8 	 1046 	 1202 	 1522

[32m iter_2[0m
ga 	p: 81.04 	r: 74.14 	f1: 77.44 	 4573 	 5643 	 6168
wo 	p: 91.57 	r: 85.63 	f1: 88.5 	 2901 	 3168 	 3388
ni 	p: 80.1 	r: 74.31 	f1: 77.1 	 1131 	 1412 	 1522
best_thres [[0.5, 0.57, 0.15], [0.58, 0.52, 0.28], [0.7, 0.53, 0.1]]
f [0.8075, 0.807, 0.8073]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 8 	 [0.65, 0.66, 0.14] 	 lr: 0.00025 	 f: 82.30095453759179
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 12 [0m
Train...
loss: tensor(537.3709) lr: 0.0001 time: 2148.23
pred_count_train 41644

Test...
loss: tensor(1178.0211) lr: 0.00025 time: 2681.6
pred_count_train 41644

Test...
loss: tensor(74.9434) lr: 2.5e-05 time: 6147.37
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.02 	r: 75.91 	f1: 78.38 	 4682 	 5779 	 6168
wo 	p: 92.44 	r: 86.22 	f1: 89.22 	 2921 	 3160 	 3388
ni 	p: 84.85 	r: 75.43 	f1: 79.86 	 1148 	 1353 	 1522

[32m iter_1[0m
ga 	p: 80.11 	r: 77.25 	f1: 78.66 	 4765 	 5948 	 6168
wo 	p: 91.68 	r: 87.19 	f1: 89.38 	 2954 	 3222 	 3388
ni 	p: 85.57 	r: 75.95 	f1: 80.47 	 1156 	 1351 	 1522

[32m iter_2[0m
ga 	p: 83.84 	r: 73.87 	f1: 78.54 	 4556 	 5434 	 6168
wo 	p: 91.63 	r: 87.25 	f1: 89.39 	 2956 	 3226 	 3388
ni 	p: 85.45 	r: 76.02 	f1: 80.46 	 1157 	 1354 	 1522
best_thres [[0.35, 0.47, 0.24], [0.26, 0.34, 0.18], [0.68, 0.32, 0.17]]
f [0.819, 0.8204, 0.8209]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse 	current best epoch 8 	 [0.52, 0.76, 0.16] 	 lr: 0.0001 	 f: 82.9521537813947
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse[0m [33m epoch 11 [0m
Train...
loss: tensor(546.8001) lr: 0.00025 time: 2075.37
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.09 	r: 74.24 	f1: 77.06 	 4579 	 5717 	 6168
wo 	p: 92.11 	r: 84.45 	f1: 88.11 	 2861 	 3106 	 3388
ni 	p: 85.31 	r: 73.65 	f1: 79.06 	 1121 	 1314 	 1522

[32m iter_1[0m
ga 	p: 79.0 	r: 74.84 	f1: 76.86 	 4616 	 5843 	 6168
wo 	p: 92.18 	r: 83.8 	f1: 87.79 	 2839 	 3080 	 3388
ni 	p: 83.98 	r: 75.1 	f1: 79.29 	 1143 	 1361 	 1522

[32m iter_2[0m
ga 	p: 81.55 	r: 72.71 	f1: 76.88 	 4485 	 5500 	 6168
wo 	p: 92.03 	r: 84.47 	f1: 88.09 	 2862 	 3110 	 3388
ni 	p: 85.32 	r: 74.44 	f1: 79.51 	 1133 	 1328 	 1522
best_thres [[0.48, 0.57, 0.2], [0.42, 0.55, 0.17], [0.57, 0.56, 0.2]]
f [0.8071, 0.806, 0.8063]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 17 	 [0.57, 0.56, 0.2] 	 lr: 0.00025 	 f: 80.6346610475996
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 18 [0m
Train...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            loss: tensor(1096.8700) lr: 0.00025 time: 2878.24
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.83 	r: 74.59 	f1: 78.5 	 4601 	 5555 	 6168
wo 	p: 92.67 	r: 86.22 	f1: 89.33 	 2921 	 3152 	 3388
ni 	p: 81.13 	r: 74.84 	f1: 77.85 	 1139 	 1404 	 1522

[32m iter_1[0m
ga 	p: 81.95 	r: 75.37 	f1: 78.52 	 4649 	 5673 	 6168
wo 	p: 93.48 	r: 85.89 	f1: 89.52 	 2910 	 3113 	 3388
ni 	p: 84.16 	r: 73.32 	f1: 78.37 	 1116 	 1326 	 1522

[32m iter_2[0m
ga 	p: 82.86 	r: 74.55 	f1: 78.48 	 4598 	 5549 	 6168
wo 	p: 93.09 	r: 86.28 	f1: 89.55 	 2923 	 3140 	 3388
ni 	p: 85.07 	r: 72.6 	f1: 78.34 	 1105 	 1299 	 1522

[32m iter_3[0m
ga 	p: 82.88 	r: 74.58 	f1: 78.51 	 4600 	 5550 	 6168
wo 	p: 93.13 	r: 86.36 	f1: 89.62 	 2926 	 3142 	 3388
ni 	p: 85.13 	r: 72.6 	f1: 78.37 	 1105 	 1298 	 1522

[32m iter_4[0m
ga 	p: 82.87 	r: 74.58 	f1: 78.5 	 4600 	 5551 	 6168
wo 	p: 93.1 	r: 86.36 	f1: 89.6 	 2926 	 3143 	 3388
ni 	p: 83.56 	r: 73.78 	f1: 78.37 	 1123 	 1344 	 1522

[32m iter_5[0m
ga 	p: 82.88 	r: 74.58 	f1: 78.51 	 4600 	 5550 	 6168
wo 	p: 93.13 	r: 86.36 	f1: 89.62 	 2926 	 3142 	 3388
ni 	p: 82.97 	r: 74.24 	f1: 78.36 	 1130 	 1362 	 1522

[32m iter_6[0m
ga 	p: 82.88 	r: 74.56 	f1: 78.5 	 4599 	 5549 	 6168
wo 	p: 93.1 	r: 86.36 	f1: 89.6 	 2926 	 3143 	 3388
ni 	p: 82.97 	r: 74.24 	f1: 78.36 	 1130 	 1362 	 1522

[32m iter_7[0m
ga 	p: 82.88 	r: 74.58 	f1: 78.51 	 4600 	 5550 	 6168
wo 	p: 93.13 	r: 86.36 	f1: 89.62 	 2926 	 3142 	 3388
ni 	p: 82.97 	r: 74.24 	f1: 78.36 	 1130 	 1362 	 1522

[32m iter_8[0m
ga 	p: 82.88 	r: 74.58 	f1: 78.51 	 4600 	 5550 	 6168
wo 	p: 93.1 	r: 86.36 	f1: 89.6 	 2926 	 3143 	 3388
ni 	p: 82.97 	r: 74.24 	f1: 78.36 	 1130 	 1362 	 1522

[32m iter_9[0m
ga 	p: 82.88 	r: 74.58 	f1: 78.51 	 4600 	 5550 	 6168
wo 	p: 93.13 	r: 86.36 	f1: 89.62 	 2926 	 3142 	 3388
ni 	p: 82.97 	r: 74.24 	f1: 78.36 	 1130 	 1362 	 1522
best_thres [[0.47, 0.52, 0.09], [0.34, 0.61, 0.09], [0.45, 0.52, 0.11], [0.45, 0.5, 0.11], [0.45, 0.5, 0.08], [0.45, 0.5, 0.07], [0.45, 0.5, 0.07], [0.45, 0.5, 0.07], [0.45, 0.5, 0.07], [0.45, 0.5, 0.07]]
f [0.8175, 0.8181, 0.8184, 0.8186, 0.8188, 0.8188, 0.8189, 0.8189, 0.819, 0.819]
load model: epoch18
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse 	current best epoch 18 	 [0.31, 0.7, 0.11] 	 lr: 2.5e-05 	 f: 82.63404788158559
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse[0m [33m epoch 23 [0m
Train...

[32m iter_0[0m
ga 	p: 81.51 	r: 75.66 	f1: 78.48 	 4667 	 5726 	 6168
wo 	p: 92.9 	r: 85.77 	f1: 89.2 	 2906 	 3128 	 3388
ni 	p: 84.15 	r: 75.69 	f1: 79.7 	 1152 	 1369 	 1522

[32m iter_1[0m
ga 	p: 82.09 	r: 75.29 	f1: 78.55 	 4644 	 5657 	 6168
wo 	p: 92.88 	r: 85.89 	f1: 89.25 	 2910 	 3133 	 3388
ni 	p: 85.03 	r: 74.64 	f1: 79.5 	 1136 	 1336 	 1522

[32m iter_2[0m
ga 	p: 82.72 	r: 74.98 	f1: 78.66 	 4625 	 5591 	 6168
wo 	p: 92.86 	r: 85.95 	f1: 89.27 	 2912 	 3136 	 3388
ni 	p: 81.79 	r: 77.6 	f1: 79.64 	 1181 	 1444 	 1522
best_thres [[0.48, 0.64, 0.1], [0.5, 0.67, 0.1], [0.6, 0.71, 0.07]]
f [0.8192, 0.8194, 0.8198]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 8 	 [0.65, 0.66, 0.14] 	 lr: 0.00025 	 f: 82.30095453759179
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 14 [0m
Train...
loss: tensor(289.8020) lr: 0.0001 time: 1975.92
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.23 	r: 73.74 	f1: 76.39 	 4548 	 5740 	 6168
wo 	p: 92.69 	r: 83.5 	f1: 87.86 	 2829 	 3052 	 3388
ni 	p: 84.47 	r: 74.31 	f1: 79.06 	 1131 	 1339 	 1522

[32m iter_1[0m
ga 	p: 79.11 	r: 73.67 	f1: 76.29 	 4544 	 5744 	 6168
wo 	p: 93.06 	r: 82.67 	f1: 87.56 	 2801 	 3010 	 3388
ni 	p: 79.45 	r: 78.25 	f1: 78.85 	 1191 	 1499 	 1522

[32m iter_2[0m
ga 	p: 79.38 	r: 73.82 	f1: 76.5 	 4553 	 5736 	 6168
wo 	p: 93.13 	r: 83.18 	f1: 87.87 	 2818 	 3026 	 3388
ni 	p: 86.02 	r: 73.59 	f1: 79.32 	 1120 	 1302 	 1522
best_thres [[0.48, 0.59, 0.22], [0.45, 0.58, 0.12], [0.48, 0.6, 0.26]]
f [0.8023, 0.8013, 0.802]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 17 	 [0.57, 0.56, 0.2] 	 lr: 0.00025 	 f: 80.6346610475996
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 19 [0m
Train...

[32m iter_0[0m
ga 	p: 80.72 	r: 75.84 	f1: 78.21 	 4678 	 5795 	 6168
wo 	p: 91.69 	r: 85.95 	f1: 88.73 	 2912 	 3176 	 3388
ni 	p: 83.73 	r: 71.68 	f1: 77.24 	 1091 	 1303 	 1522

[32m iter_1[0m
ga 	p: 81.27 	r: 75.49 	f1: 78.27 	 4656 	 5729 	 6168
wo 	p: 91.89 	r: 86.25 	f1: 88.98 	 2922 	 3180 	 3388
ni 	p: 86.73 	r: 69.58 	f1: 77.21 	 1059 	 1221 	 1522

[32m iter_2[0m
ga 	p: 81.52 	r: 75.23 	f1: 78.25 	 4640 	 5692 	 6168
wo 	p: 92.0 	r: 86.25 	f1: 89.03 	 2922 	 3176 	 3388
ni 	p: 86.07 	r: 70.24 	f1: 77.35 	 1069 	 1242 	 1522
best_thres [[0.31, 0.47, 0.18], [0.34, 0.47, 0.19], [0.36, 0.47, 0.15]]
f [0.8131, 0.8138, 0.8141]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse 	current best epoch 8 	 [0.52, 0.76, 0.16] 	 lr: 0.0001 	 f: 82.9521537813947
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse[0m [33m epoch 13 [0m
Train...
loss: tensor(472.0712) lr: 0.000125 time: 2154.48
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.09 	r: 74.81 	f1: 77.82 	 4614 	 5690 	 6168
wo 	p: 92.21 	r: 86.3 	f1: 89.16 	 2924 	 3171 	 3388
ni 	p: 82.45 	r: 77.46 	f1: 79.88 	 1179 	 1430 	 1522

[32m iter_1[0m
ga 	p: 82.03 	r: 74.38 	f1: 78.02 	 4588 	 5593 	 6168
wo 	p: 92.28 	r: 86.42 	f1: 89.25 	 2928 	 3173 	 3388
ni 	p: 84.99 	r: 76.28 	f1: 80.4 	 1161 	 1366 	 1522

[32m iter_2[0m
ga 	p: 82.22 	r: 74.35 	f1: 78.09 	 4586 	 5578 	 6168
wo 	p: 92.47 	r: 86.6 	f1: 89.44 	 2934 	 3173 	 3388
ni 	p: 85.0 	r: 75.95 	f1: 80.22 	 1156 	 1360 	 1522
best_thres [[0.37, 0.57, 0.13], [0.43, 0.61, 0.15], [0.44, 0.69, 0.15]]
f [0.8159, 0.817, 0.8177]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 8 	 [0.65, 0.66, 0.14] 	 lr: 0.00025 	 f: 82.30095453759179
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 15 [0m
Train...
loss: tensor(1017.2129) lr: 0.00025 time: 2803.29
pred_count_train 41644

Test...
loss: tensor(656.1872) lr: 5e-05 time: 2148.21
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 84.85 	r: 73.74 	f1: 78.9 	 4548 	 5360 	 6168
wo 	p: 92.47 	r: 86.63 	f1: 89.45 	 2935 	 3174 	 3388
ni 	p: 87.29 	r: 76.74 	f1: 81.68 	 1168 	 1338 	 1522

[32m iter_1[0m
ga 	p: 83.26 	r: 75.5 	f1: 79.19 	 4657 	 5593 	 6168
wo 	p: 92.72 	r: 86.51 	f1: 89.51 	 2931 	 3161 	 3388
ni 	p: 84.77 	r: 78.98 	f1: 81.77 	 1202 	 1418 	 1522

[32m iter_2[0m
ga 	p: 83.6 	r: 75.23 	f1: 79.19 	 4640 	 5550 	 6168
wo 	p: 92.64 	r: 86.6 	f1: 89.52 	 2934 	 3167 	 3388
ni 	p: 84.88 	r: 78.91 	f1: 81.78 	 1201 	 1415 	 1522
best_thres [[0.57, 0.41, 0.17], [0.48, 0.45, 0.1], [0.54, 0.44, 0.1]]
f [0.8259, 0.8266, 0.8269]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse 	current best epoch 8 	 [0.52, 0.76, 0.16] 	 lr: 0.0001 	 f: 82.9521537813947
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse[0m [33m epoch 14 [0m
Train...

[32m iter_0[0m
ga 	p: 81.52 	r: 71.64 	f1: 76.26 	 4419 	 5421 	 6168
wo 	p: 91.25 	r: 84.03 	f1: 87.49 	 2847 	 3120 	 3388
ni 	p: 87.06 	r: 68.53 	f1: 76.69 	 1043 	 1198 	 1522

[32m iter_1[0m
ga 	p: 81.85 	r: 71.13 	f1: 76.11 	 4387 	 5360 	 6168
wo 	p: 92.0 	r: 82.85 	f1: 87.19 	 2807 	 3051 	 3388
ni 	p: 83.07 	r: 72.21 	f1: 77.26 	 1099 	 1323 	 1522

[32m iter_2[0m
ga 	p: 81.49 	r: 71.58 	f1: 76.21 	 4415 	 5418 	 6168
wo 	p: 91.57 	r: 83.65 	f1: 87.43 	 2834 	 3095 	 3388
ni 	p: 81.67 	r: 73.19 	f1: 77.2 	 1114 	 1364 	 1522
best_thres [[0.63, 0.61, 0.29], [0.63, 0.61, 0.19], [0.63, 0.61, 0.16]]
f [0.7983, 0.7976, 0.7978]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 17 	 [0.57, 0.56, 0.2] 	 lr: 0.00025 	 f: 80.6346610475996
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 20 [0m
Train...
loss: tensor(298.2599) lr: 0.000125 time: 2154.36
pred_count_train 41644

Test...
loss: tensor(238.3326) lr: 1.25e-05 time: 6207.08
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 78.54 	r: 75.7 	f1: 77.09 	 4669 	 5945 	 6168
wo 	p: 93.68 	r: 85.68 	f1: 89.5 	 2903 	 3099 	 3388
ni 	p: 81.78 	r: 72.27 	f1: 76.74 	 1100 	 1345 	 1522

[32m iter_1[0m
ga 	p: 81.31 	r: 73.64 	f1: 77.28 	 4542 	 5586 	 6168
wo 	p: 91.76 	r: 86.1 	f1: 88.84 	 2917 	 3179 	 3388
ni 	p: 80.56 	r: 73.52 	f1: 76.88 	 1119 	 1389 	 1522

[32m iter_2[0m
ga 	p: 79.54 	r: 75.26 	f1: 77.34 	 4642 	 5836 	 6168
wo 	p: 92.62 	r: 85.54 	f1: 88.94 	 2898 	 3129 	 3388
ni 	p: 82.9 	r: 71.68 	f1: 76.89 	 1091 	 1316 	 1522
best_thres [[0.31, 0.74, 0.16], [0.7, 0.62, 0.1], [0.39, 0.85, 0.14]]
f [0.8079, 0.808, 0.808]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 8 	 [0.65, 0.66, 0.14] 	 lr: 0.00025 	 f: 82.30095453759179
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 16 [0m
Train...
loss: tensor(452.7566) lr: 5e-05 time: 1930.5
pred_count_train 41644

Test...
loss: tensor(942.2053) lr: 0.00025 time: 2978.79
pred_count_train 41644

Test...
2.8 	r: 85.63 	f1: 89.07 	 2901 	 3126 	 3388
ni 	p: 85.47 	r: 75.36 	f1: 80.1 	 1147 	 1342 	 1522

[32m iter_1[0m
ga 	p: 81.16 	r: 76.13 	f1: 78.57 	 4696 	 5786 	 6168
wo 	p: 93.35 	r: 85.83 	f1: 89.44 	 2908 	 3115 	 3388
ni 	p: 81.75 	r: 77.99 	f1: 79.83 	 1187 	 1452 	 1522

[32m iter_2[0m
ga 	p: 80.56 	r: 76.65 	f1: 78.56 	 4728 	 5869 	 6168
wo 	p: 92.53 	r: 86.63 	f1: 89.48 	 2935 	 3172 	 3388
ni 	p: 83.24 	r: 76.68 	f1: 79.82 	 1167 	 1402 	 1522
best_thres [[0.25, 0.46, 0.22], [0.23, 0.52, 0.11], [0.2, 0.37, 0.13]]
f [0.8181, 0.8193, 0.8197]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse 	current best epoch 8 	 [0.52, 0.76, 0.16] 	 lr: 0.0001 	 f: 82.9521537813947
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse[0m [33m epoch 15 [0m
Train...
loss: tensor(209.7839) lr: 0.000125 time: 1844.31
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.4 	r: 73.07 	f1: 76.11 	 4507 	 5676 	 6168
wo 	p: 92.47 	r: 83.41 	f1: 87.71 	 2826 	 3056 	 3388
ni 	p: 83.76 	r: 73.52 	f1: 78.31 	 1119 	 1336 	 1522

[32m iter_1[0m
ga 	p: 79.66 	r: 72.83 	f1: 76.09 	 4492 	 5639 	 6168
wo 	p: 93.15 	r: 82.32 	f1: 87.4 	 2789 	 2994 	 3388
ni 	p: 82.21 	r: 74.7 	f1: 78.28 	 1137 	 1383 	 1522

[32m iter_2[0m
ga 	p: 79.12 	r: 73.77 	f1: 76.35 	 4550 	 5751 	 6168
wo 	p: 92.55 	r: 82.91 	f1: 87.47 	 2809 	 3035 	 3388
ni 	p: 82.18 	r: 75.16 	f1: 78.52 	 1144 	 1392 	 1522
best_thres [[0.52, 0.82, 0.23], [0.52, 0.78, 0.2], [0.49, 0.82, 0.19]]
f [0.7994, 0.7988, 0.7992]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 17 	 [0.57, 0.56, 0.2] 	 lr: 0.00025 	 f: 80.6346610475996
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 21 [0m
Train...

[32m iter_0[0m
ga 	p: 81.93 	r: 76.23 	f1: 78.98 	 4702 	 5739 	 6168
wo 	p: 94.18 	r: 85.06 	f1: 89.39 	 2882 	 3060 	 3388
ni 	p: 81.61 	r: 77.86 	f1: 79.69 	 1185 	 1452 	 1522

[32m iter_1[0m
ga 	p: 81.44 	r: 76.85 	f1: 79.08 	 4740 	 5820 	 6168
wo 	p: 94.42 	r: 84.98 	f1: 89.45 	 2879 	 3049 	 3388
ni 	p: 83.79 	r: 77.07 	f1: 80.29 	 1173 	 1400 	 1522

[32m iter_2[0m
ga 	p: 81.41 	r: 76.75 	f1: 79.01 	 4734 	 5815 	 6168
wo 	p: 94.17 	r: 85.36 	f1: 89.55 	 2892 	 3071 	 3388
ni 	p: 84.07 	r: 77.0 	f1: 80.38 	 1172 	 1394 	 1522

[32m iter_3[0m
ga 	p: 81.43 	r: 76.78 	f1: 79.04 	 4736 	 5816 	 6168
wo 	p: 94.08 	r: 85.42 	f1: 89.54 	 2894 	 3076 	 3388
ni 	p: 84.13 	r: 77.0 	f1: 80.41 	 1172 	 1393 	 1522

[32m iter_4[0m
ga 	p: 81.43 	r: 76.77 	f1: 79.03 	 4735 	 5815 	 6168
wo 	p: 94.43 	r: 85.12 	f1: 89.54 	 2884 	 3054 	 3388
ni 	p: 84.13 	r: 77.0 	f1: 80.41 	 1172 	 1393 	 1522

[32m iter_5[0m
ga 	p: 81.43 	r: 76.77 	f1: 79.03 	 4735 	 5815 	 6168
wo 	p: 94.08 	r: 85.42 	f1: 89.54 	 2894 	 3076 	 3388
ni 	p: 84.13 	r: 77.0 	f1: 80.41 	 1172 	 1393 	 1522

[32m iter_6[0m
ga 	p: 81.44 	r: 76.77 	f1: 79.04 	 4735 	 5814 	 6168
wo 	p: 94.46 	r: 85.09 	f1: 89.53 	 2883 	 3052 	 3388
ni 	p: 84.13 	r: 77.0 	f1: 80.41 	 1172 	 1393 	 1522

[32m iter_7[0m
ga 	p: 81.44 	r: 76.77 	f1: 79.04 	 4735 	 5814 	 6168
wo 	p: 94.08 	r: 85.42 	f1: 89.54 	 2894 	 3076 	 3388
ni 	p: 84.13 	r: 77.0 	f1: 80.41 	 1172 	 1393 	 1522

[32m iter_8[0m
ga 	p: 81.44 	r: 76.77 	f1: 79.04 	 4735 	 5814 	 6168
wo 	p: 94.46 	r: 85.09 	f1: 89.53 	 2883 	 3052 	 3388
ni 	p: 84.13 	r: 77.0 	f1: 80.41 	 1172 	 1393 	 1522

[32m iter_9[0m
ga 	p: 81.44 	r: 76.77 	f1: 79.04 	 4735 	 5814 	 6168
wo 	p: 94.08 	r: 85.42 	f1: 89.54 	 2894 	 3076 	 3388
ni 	p: 84.13 	r: 77.0 	f1: 80.41 	 1172 	 1393 	 1522
best_thres [[0.4, 0.78, 0.15], [0.34, 0.85, 0.15], [0.34, 0.77, 0.15], [0.34, 0.75, 0.15], [0.34, 0.84, 0.15], [0.34, 0.75, 0.15], [0.34, 0.85, 0.15], [0.34, 0.75, 0.15], [0.34, 0.85, 0.15], [0.34, 0.75, 0.15]]
f [0.8223, 0.823, 0.8233, 0.8235, 0.8236, 0.8236, 0.8237, 0.8237, 0.8237, 0.8238]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse 	current best epoch 18 	 [0.31, 0.7, 0.11] 	 lr: 2.5e-05 	 f: 82.63404788158559
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse[0m [33m epoch 24 [0m
Train...

[32m iter_0[0m
ga 	p: 82.14 	r: 73.17 	f1: 77.4 	 4513 	 5494 	 6168
wo 	p: 94.48 	r: 84.33 	f1: 89.11 	 2857 	 3024 	 3388
ni 	p: 84.92 	r: 71.42 	f1: 77.59 	 1087 	 1280 	 1522

[32m iter_1[0m
ga 	p: 82.19 	r: 73.46 	f1: 77.58 	 4531 	 5513 	 6168
wo 	p: 93.3 	r: 85.51 	f1: 89.23 	 2897 	 3105 	 3388
ni 	p: 86.41 	r: 70.17 	f1: 77.45 	 1068 	 1236 	 1522

[32m iter_2[0m
ga 	p: 82.02 	r: 73.83 	f1: 77.71 	 4554 	 5552 	 6168
wo 	p: 93.08 	r: 85.71 	f1: 89.24 	 2904 	 3120 	 3388
ni 	p: 85.82 	r: 70.37 	f1: 77.33 	 1071 	 1248 	 1522
best_thres [[0.48, 0.82, 0.18], [0.51, 0.84, 0.19], [0.48, 0.83, 0.17]]
f [0.8102, 0.811, 0.8115]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 8 	 [0.65, 0.66, 0.14] 	 lr: 0.00025 	 f: 82.30095453759179
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 17 [0m
Train...
loss: tensor(303.6104) lr: 5e-05 time: 1853.38
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.88 	r: 75.7 	f1: 78.2 	 4669 	 5773 	 6168
wo 	p: 91.85 	r: 86.54 	f1: 89.12 	 2932 	 3192 	 3388
ni 	p: 82.82 	r: 74.44 	f1: 78.41 	 1133 	 1368 	 1522

[32m iter_1[0m
ga 	p: 80.38 	r: 76.51 	f1: 78.4 	 4719 	 5871 	 6168
wo 	p: 91.74 	r: 87.25 	f1: 89.44 	 2956 	 3222 	 3388
ni 	p: 81.44 	r: 75.23 	f1: 78.21 	 1145 	 1406 	 1522

[32m iter_2[0m
ga 	p: 80.84 	r: 75.84 	f1: 78.26 	 4678 	 5787 	 6168
wo 	p: 91.86 	r: 87.25 	f1: 89.49 	 2956 	 3218 	 3388
ni 	p: 84.5 	r: 72.73 	f1: 78.18 	 1107 	 1310 	 1522
best_thres [[0.4, 0.4, 0.15], [0.34, 0.31, 0.09], [0.39, 0.32, 0.13]]
f [0.8158, 0.8167, 0.8169]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse 	current best epoch 8 	 [0.52, 0.76, 0.16] 	 lr: 0.0001 	 f: 82.9521537813947
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse[0m [33m epoch 16 [0m
Train...
loss: tensor(749.8879) lr: 6.25e-05 time: 2172.35
pred_count_train 41644

Test...
loss: tensor(873.4998) lr: 0.00025 time: 2965.6
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.1 	r: 75.92 	f1: 78.89 	 4683 	 5704 	 6168
wo 	p: 93.8 	r: 85.77 	f1: 89.61 	 2906 	 3098 	 3388
ni 	p: 85.54 	r: 78.52 	f1: 81.88 	 1195 	 1397 	 1522

[32m iter_1[0m
ga 	p: 81.98 	r: 75.96 	f1: 78.85 	 4685 	 5715 	 6168
wo 	p: 93.39 	r: 85.95 	f1: 89.52 	 2912 	 3118 	 3388
ni 	p: 85.25 	r: 78.98 	f1: 81.99 	 1202 	 1410 	 1522

[32m iter_2[0m
ga 	p: 81.82 	r: 76.12 	f1: 78.87 	 4695 	 5738 	 6168
wo 	p: 92.78 	r: 86.42 	f1: 89.49 	 2928 	 3156 	 3388
ni 	p: 85.63 	r: 78.32 	f1: 81.81 	 1192 	 1392 	 1522
best_thres [[0.45, 0.61, 0.16], [0.45, 0.6, 0.14], [0.45, 0.55, 0.15]]
f [0.8257, 0.8255, 0.8254]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 17 	 [0.45, 0.55, 0.15] 	 lr: 6.25e-05 	 f: 82.54275976360965
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 18 [0m
Train...
loss: tensor(213.5726) lr: 5e-05 time: 2186.38
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.47 	r: 70.62 	f1: 75.23 	 4356 	 5413 	 6168
wo 	p: 92.37 	r: 83.56 	f1: 87.74 	 2831 	 3065 	 3388
ni 	p: 79.47 	r: 70.96 	f1: 74.97 	 1080 	 1359 	 1522

[32m iter_1[0m
ga 	p: 80.63 	r: 70.72 	f1: 75.35 	 4362 	 5410 	 6168
wo 	p: 91.15 	r: 83.94 	f1: 87.4 	 2844 	 3120 	 3388
ni 	p: 77.65 	r: 72.14 	f1: 74.8 	 1098 	 1414 	 1522

[32m iter_2[0m
ga 	p: 80.48 	r: 71.17 	f1: 75.54 	 4390 	 5455 	 6168
wo 	p: 92.17 	r: 83.44 	f1: 87.59 	 2827 	 3067 	 3388
ni 	p: 78.31 	r: 71.16 	f1: 74.56 	 1083 	 1383 	 1522
best_thres [[0.46, 0.64, 0.12], [0.43, 0.48, 0.1], [0.43, 0.59, 0.11]]
f [0.7905, 0.7903, 0.7906]
load model: epoch17
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 17 	 [0.57, 0.56, 0.2] 	 lr: 0.00025 	 f: 80.6346610475996
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 22 [0m
Train...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  loss: tensor(907.0856) lr: 0.000125 time: 2726.78
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.94 	r: 77.24 	f1: 79.04 	 4764 	 5886 	 6168
wo 	p: 92.48 	r: 86.81 	f1: 89.56 	 2941 	 3180 	 3388
ni 	p: 86.9 	r: 76.74 	f1: 81.51 	 1168 	 1344 	 1522

[32m iter_1[0m
ga 	p: 82.32 	r: 76.33 	f1: 79.21 	 4708 	 5719 	 6168
wo 	p: 93.07 	r: 86.81 	f1: 89.83 	 2941 	 3160 	 3388
ni 	p: 85.8 	r: 77.79 	f1: 81.6 	 1184 	 1380 	 1522

[32m iter_2[0m
ga 	p: 82.3 	r: 76.31 	f1: 79.2 	 4707 	 5719 	 6168
wo 	p: 93.2 	r: 86.92 	f1: 89.95 	 2945 	 3160 	 3388
ni 	p: 84.45 	r: 79.24 	f1: 81.76 	 1206 	 1428 	 1522
best_thres [[0.34, 0.4, 0.19], [0.39, 0.44, 0.14], [0.39, 0.44, 0.11]]
f [0.8259, 0.8269, 0.8274]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse 	current best epoch 8 	 [0.52, 0.76, 0.16] 	 lr: 0.0001 	 f: 82.9521537813947
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse[0m [33m epoch 18 [0m
Train...

[32m iter_0[0m
ga 	p: 79.79 	r: 73.75 	f1: 76.65 	 4549 	 5701 	 6168
wo 	p: 91.99 	r: 85.06 	f1: 88.39 	 2882 	 3133 	 3388
ni 	p: 83.18 	r: 73.46 	f1: 78.02 	 1118 	 1344 	 1522

[32m iter_1[0m
ga 	p: 78.48 	r: 74.58 	f1: 76.48 	 4600 	 5861 	 6168
wo 	p: 92.13 	r: 84.33 	f1: 88.06 	 2857 	 3101 	 3388
ni 	p: 84.08 	r: 73.59 	f1: 78.49 	 1120 	 1332 	 1522

[32m iter_2[0m
ga 	p: 78.71 	r: 74.32 	f1: 76.45 	 4584 	 5824 	 6168
wo 	p: 92.19 	r: 84.71 	f1: 88.29 	 2870 	 3113 	 3388
ni 	p: 83.59 	r: 74.97 	f1: 79.04 	 1141 	 1365 	 1522
best_thres [[0.39, 0.53, 0.15], [0.31, 0.53, 0.17], [0.32, 0.53, 0.14]]
f [0.8044, 0.8035, 0.8037]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 17 	 [0.57, 0.56, 0.2] 	 lr: 0.00025 	 f: 80.6346610475996
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 23 [0m
Train...
loss: tensor(272.0387) lr: 6.25e-05 time: 1875.88
pred_count_train 41644

Test...
loss: tensor(470.7490) lr: 2.5e-05 time: 1870.46
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.32 	r: 74.24 	f1: 77.62 	 4579 	 5631 	 6168
wo 	p: 92.62 	r: 85.98 	f1: 89.18 	 2913 	 3145 	 3388
ni 	p: 84.7 	r: 74.9 	f1: 79.5 	 1140 	 1346 	 1522

[32m iter_1[0m
ga 	p: 81.0 	r: 74.69 	f1: 77.72 	 4607 	 5688 	 6168
wo 	p: 93.74 	r: 85.27 	f1: 89.3 	 2889 	 3082 	 3388
ni 	p: 85.66 	r: 75.36 	f1: 80.18 	 1147 	 1339 	 1522

[32m iter_2[0m
ga 	p: 81.11 	r: 74.69 	f1: 77.77 	 4607 	 5680 	 6168
wo 	p: 93.58 	r: 85.12 	f1: 89.15 	 2884 	 3082 	 3388
ni 	p: 84.86 	r: 75.49 	f1: 79.9 	 1149 	 1354 	 1522
best_thres [[0.39, 0.45, 0.1], [0.38, 0.85, 0.08], [0.38, 0.84, 0.07]]
f [0.8143, 0.8151, 0.8152]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 17 	 [0.45, 0.55, 0.15] 	 lr: 6.25e-05 	 f: 82.54275976360965
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 20 [0m
Train...

[32m iter_0[0m
ga 	p: 81.97 	r: 76.05 	f1: 78.9 	 4691 	 5723 	 6168
wo 	p: 92.45 	r: 86.42 	f1: 89.34 	 2928 	 3167 	 3388
ni 	p: 82.1 	r: 74.44 	f1: 78.08 	 1133 	 1380 	 1522

[32m iter_1[0m
ga 	p: 83.12 	r: 75.15 	f1: 78.93 	 4635 	 5576 	 6168
wo 	p: 94.87 	r: 84.65 	f1: 89.47 	 2868 	 3023 	 3388
ni 	p: 84.77 	r: 73.52 	f1: 78.75 	 1119 	 1320 	 1522

[32m iter_2[0m
ga 	p: 83.1 	r: 75.11 	f1: 78.91 	 4633 	 5575 	 6168
wo 	p: 94.54 	r: 84.86 	f1: 89.44 	 2875 	 3041 	 3388
ni 	p: 83.02 	r: 75.16 	f1: 78.9 	 1144 	 1378 	 1522

[32m iter_3[0m
ga 	p: 83.18 	r: 75.15 	f1: 78.96 	 4635 	 5572 	 6168
wo 	p: 94.51 	r: 84.86 	f1: 89.42 	 2875 	 3042 	 3388
ni 	p: 84.63 	r: 73.78 	f1: 78.83 	 1123 	 1327 	 1522

[32m iter_4[0m
ga 	p: 83.12 	r: 75.11 	f1: 78.91 	 4633 	 5574 	 6168
wo 	p: 94.48 	r: 84.86 	f1: 89.41 	 2875 	 3043 	 3388
ni 	p: 84.63 	r: 73.78 	f1: 78.83 	 1123 	 1327 	 1522

[32m iter_5[0m
ga 	p: 83.1 	r: 75.11 	f1: 78.91 	 4633 	 5575 	 6168
wo 	p: 94.51 	r: 84.86 	f1: 89.42 	 2875 	 3042 	 3388
ni 	p: 83.16 	r: 74.97 	f1: 78.85 	 1141 	 1372 	 1522

[32m iter_6[0m
ga 	p: 83.13 	r: 75.11 	f1: 78.92 	 4633 	 5573 	 6168
wo 	p: 94.48 	r: 84.83 	f1: 89.39 	 2874 	 3042 	 3388
ni 	p: 83.11 	r: 75.03 	f1: 78.87 	 1142 	 1374 	 1522

[32m iter_7[0m
ga 	p: 83.1 	r: 75.11 	f1: 78.91 	 4633 	 5575 	 6168
wo 	p: 94.51 	r: 84.83 	f1: 89.41 	 2874 	 3041 	 3388
ni 	p: 83.16 	r: 74.97 	f1: 78.85 	 1141 	 1372 	 1522

[32m iter_8[0m
ga 	p: 83.12 	r: 75.11 	f1: 78.91 	 4633 	 5574 	 6168
wo 	p: 94.48 	r: 84.83 	f1: 89.39 	 2874 	 3042 	 3388
ni 	p: 83.11 	r: 75.03 	f1: 78.87 	 1142 	 1374 	 1522

[32m iter_9[0m
ga 	p: 83.12 	r: 75.11 	f1: 78.91 	 4633 	 5574 	 6168
wo 	p: 94.51 	r: 84.83 	f1: 89.41 	 2874 	 3041 	 3388
ni 	p: 83.16 	r: 74.97 	f1: 78.85 	 1141 	 1372 	 1522
best_thres [[0.34, 0.51, 0.12], [0.4, 0.82, 0.12], [0.4, 0.77, 0.09], [0.4, 0.76, 0.11], [0.4, 0.76, 0.11], [0.4, 0.76, 0.09], [0.4, 0.76, 0.09], [0.4, 0.76, 0.09], [0.4, 0.76, 0.09], [0.4, 0.76, 0.09]]
f [0.8199, 0.8206, 0.8208, 0.821, 0.821, 0.821, 0.821, 0.821, 0.821, 0.821]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse 	current best epoch 18 	 [0.31, 0.7, 0.11] 	 lr: 2.5e-05 	 f: 82.63404788158559
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse[0m [33m epoch 25 [0m
Train...

[32m iter_0[0m
ga 	p: 80.37 	r: 77.32 	f1: 78.81 	 4769 	 5934 	 6168
wo 	p: 93.25 	r: 85.98 	f1: 89.47 	 2913 	 3124 	 3388
ni 	p: 85.42 	r: 77.73 	f1: 81.39 	 1183 	 1385 	 1522

[32m iter_1[0m
ga 	p: 81.21 	r: 76.93 	f1: 79.01 	 4745 	 5843 	 6168
wo 	p: 93.67 	r: 86.01 	f1: 89.68 	 2914 	 3111 	 3388
ni 	p: 86.08 	r: 76.81 	f1: 81.18 	 1169 	 1358 	 1522

[32m iter_2[0m
ga 	p: 81.28 	r: 76.99 	f1: 79.08 	 4749 	 5843 	 6168
wo 	p: 93.3 	r: 86.36 	f1: 89.7 	 2926 	 3136 	 3388
ni 	p: 86.03 	r: 76.87 	f1: 81.19 	 1170 	 1360 	 1522
best_thres [[0.3, 0.49, 0.15], [0.32, 0.66, 0.13], [0.32, 0.47, 0.12]]
f [0.8238, 0.8246, 0.8251]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse 	current best epoch 8 	 [0.52, 0.76, 0.16] 	 lr: 0.0001 	 f: 82.9521537813947
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse[0m [33m epoch 19 [0m
Train...
loss: tensor(687.0964) lr: 0.000125 time: 2917.39
pred_count_train 41644

Test...
loss: tensor(161.5079) lr: 6.25e-05 time: 2061.73
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 77.42 	r: 74.66 	f1: 76.02 	 4605 	 5948 	 6168
wo 	p: 92.49 	r: 84.39 	f1: 88.25 	 2859 	 3091 	 3388
ni 	p: 84.33 	r: 72.14 	f1: 77.76 	 1098 	 1302 	 1522

[32m iter_1[0m
ga 	p: 77.35 	r: 75.06 	f1: 76.19 	 4630 	 5986 	 6168
wo 	p: 92.98 	r: 83.26 	f1: 87.85 	 2821 	 3034 	 3388
ni 	p: 83.38 	r: 72.21 	f1: 77.39 	 1099 	 1318 	 1522

[32m iter_2[0m
ga 	p: 78.8 	r: 73.9 	f1: 76.27 	 4558 	 5784 	 6168
wo 	p: 92.58 	r: 83.94 	f1: 88.05 	 2844 	 3072 	 3388
ni 	p: 84.43 	r: 71.62 	f1: 77.5 	 1090 	 1291 	 1522
best_thres [[0.35, 0.6, 0.25], [0.31, 0.64, 0.22], [0.41, 0.6, 0.24]]
f [0.7995, 0.799, 0.7994]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 17 	 [0.57, 0.56, 0.2] 	 lr: 0.00025 	 f: 80.6346610475996
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 24 [0m
Train...

[32m iter_0[0m
ga 	p: 80.88 	r: 74.53 	f1: 77.57 	 4597 	 5684 	 6168
wo 	p: 91.96 	r: 86.45 	f1: 89.12 	 2929 	 3185 	 3388
ni 	p: 82.31 	r: 75.49 	f1: 78.75 	 1149 	 1396 	 1522

[32m iter_1[0m
ga 	p: 81.29 	r: 74.53 	f1: 77.76 	 4597 	 5655 	 6168
wo 	p: 93.39 	r: 85.12 	f1: 89.07 	 2884 	 3088 	 3388
ni 	p: 83.53 	r: 74.31 	f1: 78.65 	 1131 	 1354 	 1522

[32m iter_2[0m
ga 	p: 81.18 	r: 74.59 	f1: 77.75 	 4601 	 5668 	 6168
wo 	p: 93.28 	r: 85.24 	f1: 89.08 	 2888 	 3096 	 3388
ni 	p: 85.79 	r: 73.0 	f1: 78.88 	 1111 	 1295 	 1522
best_thres [[0.41, 0.36, 0.1], [0.49, 0.81, 0.1], [0.49, 0.79, 0.13]]
f [0.8129, 0.8132, 0.8134]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 17 	 [0.45, 0.55, 0.15] 	 lr: 6.25e-05 	 f: 82.54275976360965
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 21 [0m
Train...
loss: tensor(349.4497) lr: 2.5e-05 time: 2180.71
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.56 	r: 76.52 	f1: 78.01 	 4720 	 5933 	 6168
wo 	p: 92.61 	r: 86.19 	f1: 89.28 	 2920 	 3153 	 3388
ni 	p: 83.79 	r: 73.72 	f1: 78.43 	 1122 	 1339 	 1522

[32m iter_1[0m
ga 	p: 80.5 	r: 76.17 	f1: 78.27 	 4698 	 5836 	 6168
wo 	p: 92.93 	r: 86.16 	f1: 89.42 	 2919 	 3141 	 3388
ni 	p: 83.01 	r: 73.85 	f1: 78.16 	 1124 	 1354 	 1522

[32m iter_2[0m
ga 	p: 80.21 	r: 76.38 	f1: 78.25 	 4711 	 5873 	 6168
wo 	p: 92.6 	r: 86.42 	f1: 89.4 	 2928 	 3162 	 3388
ni 	p: 82.95 	r: 74.18 	f1: 78.32 	 1129 	 1361 	 1522
best_thres [[0.26, 0.4, 0.13], [0.28, 0.43, 0.09], [0.26, 0.38, 0.08]]
f [0.815, 0.8158, 0.816]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse 	current best epoch 8 	 [0.52, 0.76, 0.16] 	 lr: 0.0001 	 f: 82.9521537813947
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse[0m [33m epoch 20 [0m
Train...
loss: tensor(111.5527) lr: 6.25e-05 time: 2182.6
pred_count_train 41644

Test...
loss: tensor(545.2948) lr: 0.000125 time: 2771.21
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.83 	r: 74.63 	f1: 77.14 	 4603 	 5766 	 6168
wo 	p: 92.48 	r: 85.98 	f1: 89.11 	 2913 	 3150 	 3388
ni 	p: 85.16 	r: 72.01 	f1: 78.03 	 1096 	 1287 	 1522

[32m iter_1[0m
ga 	p: 78.84 	r: 75.19 	f1: 76.97 	 4638 	 5883 	 6168
wo 	p: 92.3 	r: 85.63 	f1: 88.84 	 2901 	 3143 	 3388
ni 	p: 82.37 	r: 74.57 	f1: 78.28 	 1135 	 1378 	 1522

[32m iter_2[0m
ga 	p: 79.69 	r: 74.56 	f1: 77.04 	 4599 	 5771 	 6168
wo 	p: 91.99 	r: 85.8 	f1: 88.79 	 2907 	 3160 	 3388
ni 	p: 81.64 	r: 75.95 	f1: 78.69 	 1156 	 1416 	 1522
best_thres [[0.29, 0.61, 0.12], [0.23, 0.85, 0.05], [0.3, 0.81, 0.04]]
f [0.8094, 0.8085, 0.8085]
load model: epoch17
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 17 	 [0.45, 0.55, 0.15] 	 lr: 6.25e-05 	 f: 82.54275976360965
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 22 [0m
Train...
loss: tensor(257.9654) lr: 2.5e-05 time: 2177.48
pred_count_train 41644

Test...
loss: tensor(137.9473) lr: 1.25e-05 time: 6307.29
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.34 	r: 73.7 	f1: 76.42 	 4546 	 5730 	 6168
wo 	p: 93.05 	r: 83.44 	f1: 87.99 	 2827 	 3038 	 3388
ni 	p: 81.48 	r: 71.09 	f1: 75.93 	 1082 	 1328 	 1522

[32m iter_1[0m
ga 	p: 78.58 	r: 74.56 	f1: 76.52 	 4599 	 5853 	 6168
wo 	p: 92.73 	r: 83.56 	f1: 87.91 	 2831 	 3053 	 3388
ni 	p: 80.19 	r: 73.92 	f1: 76.92 	 1125 	 1403 	 1522

[32m iter_2[0m
ga 	p: 79.3 	r: 74.04 	f1: 76.58 	 4567 	 5759 	 6168
wo 	p: 92.06 	r: 84.18 	f1: 87.94 	 2852 	 3098 	 3388
ni 	p: 79.44 	r: 74.64 	f1: 76.96 	 1136 	 1430 	 1522
best_thres [[0.56, 0.7, 0.15], [0.47, 0.61, 0.11], [0.55, 0.54, 0.1]]
f [0.7986, 0.7993, 0.7998]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 17 	 [0.57, 0.56, 0.2] 	 lr: 0.00025 	 f: 80.6346610475996
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 25 [0m
Train...

[32m iter_0[0m
ga 	p: 81.94 	r: 74.21 	f1: 77.88 	 4577 	 5586 	 6168
wo 	p: 92.75 	r: 85.36 	f1: 88.9 	 2892 	 3118 	 3388
ni 	p: 84.6 	r: 71.48 	f1: 77.49 	 1088 	 1286 	 1522

[32m iter_1[0m
ga 	p: 81.81 	r: 74.89 	f1: 78.2 	 4619 	 5646 	 6168
wo 	p: 92.3 	r: 86.3 	f1: 89.2 	 2924 	 3168 	 3388
ni 	p: 81.87 	r: 73.0 	f1: 77.18 	 1111 	 1357 	 1522

[32m iter_2[0m
ga 	p: 81.75 	r: 74.97 	f1: 78.21 	 4624 	 5656 	 6168
wo 	p: 91.76 	r: 86.75 	f1: 89.18 	 2939 	 3203 	 3388
ni 	p: 81.36 	r: 73.13 	f1: 77.02 	 1113 	 1368 	 1522
best_thres [[0.31, 0.53, 0.15], [0.28, 0.42, 0.08], [0.28, 0.35, 0.07]]
f [0.8123, 0.8134, 0.8138]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse 	current best epoch 8 	 [0.52, 0.76, 0.16] 	 lr: 0.0001 	 f: 82.9521537813947
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse[0m [33m epoch 21 [0m
Train...
loss: tensor(422.8922) lr: 3.125e-05 time: 1905.44
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.99 	r: 75.65 	f1: 78.23 	 4666 	 5761 	 6168
wo 	p: 92.43 	r: 86.87 	f1: 89.56 	 2943 	 3184 	 3388
ni 	p: 86.94 	r: 74.77 	f1: 80.4 	 1138 	 1309 	 1522

[32m iter_1[0m
ga 	p: 81.17 	r: 75.6 	f1: 78.28 	 4663 	 5745 	 6168
wo 	p: 93.52 	r: 85.98 	f1: 89.59 	 2913 	 3115 	 3388
ni 	p: 86.28 	r: 75.62 	f1: 80.6 	 1151 	 1334 	 1522

[32m iter_2[0m
ga 	p: 81.24 	r: 75.62 	f1: 78.33 	 4664 	 5741 	 6168
wo 	p: 93.12 	r: 86.28 	f1: 89.57 	 2923 	 3139 	 3388
ni 	p: 85.17 	r: 76.22 	f1: 80.44 	 1160 	 1362 	 1522
best_thres [[0.35, 0.45, 0.17], [0.37, 0.74, 0.14], [0.37, 0.64, 0.12]]
f [0.8201, 0.8203, 0.8204]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 17 	 [0.45, 0.55, 0.15] 	 lr: 6.25e-05 	 f: 82.54275976360965
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 23 [0m
Train...
loss: tensor(621.5040) lr: 1.25e-05 time: 1864.13
pred_count_train 41644

Test...
loss: tensor(458.7640) lr: 0.000125 time: 3026.46
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.05 	r: 76.33 	f1: 79.09 	 4708 	 5738 	 6168
wo 	p: 93.16 	r: 86.36 	f1: 89.63 	 2926 	 3141 	 3388
ni 	p: 85.46 	r: 76.87 	f1: 80.94 	 1170 	 1369 	 1522

[32m iter_1[0m
ga 	p: 83.59 	r: 75.18 	f1: 79.16 	 4637 	 5547 	 6168
wo 	p: 93.59 	r: 86.69 	f1: 90.01 	 2937 	 3138 	 3388
ni 	p: 84.41 	r: 77.92 	f1: 81.04 	 1186 	 1405 	 1522

[32m iter_2[0m
ga 	p: 83.86 	r: 74.95 	f1: 79.15 	 4623 	 5513 	 6168
wo 	p: 93.53 	r: 86.66 	f1: 89.96 	 2936 	 3139 	 3388
ni 	p: 83.96 	r: 78.78 	f1: 81.29 	 1199 	 1428 	 1522
best_thres [[0.36, 0.47, 0.15], [0.45, 0.53, 0.11], [0.47, 0.54, 0.1]]
f [0.8257, 0.8267, 0.8271]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse 	current best epoch 8 	 [0.52, 0.76, 0.16] 	 lr: 0.0001 	 f: 82.9521537813947
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse[0m [33m epoch 22 [0m
Train...

[32m iter_0[0m
ga 	p: 81.15 	r: 76.26 	f1: 78.63 	 4704 	 5797 	 6168
wo 	p: 94.63 	r: 84.8 	f1: 89.45 	 2873 	 3036 	 3388
ni 	p: 81.3 	r: 76.54 	f1: 78.85 	 1165 	 1433 	 1522

[32m iter_1[0m
ga 	p: 82.5 	r: 75.23 	f1: 78.7 	 4640 	 5624 	 6168
wo 	p: 94.28 	r: 85.21 	f1: 89.52 	 2887 	 3062 	 3388
ni 	p: 82.71 	r: 75.76 	f1: 79.08 	 1153 	 1394 	 1522

[32m iter_2[0m
ga 	p: 84.1 	r: 73.99 	f1: 78.72 	 4564 	 5427 	 6168
wo 	p: 93.49 	r: 85.98 	f1: 89.58 	 2913 	 3116 	 3388
ni 	p: 83.89 	r: 75.3 	f1: 79.36 	 1146 	 1366 	 1522

[32m iter_3[0m
ga 	p: 84.08 	r: 73.99 	f1: 78.72 	 4564 	 5428 	 6168
wo 	p: 93.52 	r: 86.01 	f1: 89.61 	 2914 	 3116 	 3388
ni 	p: 83.81 	r: 75.16 	f1: 79.25 	 1144 	 1365 	 1522

[32m iter_4[0m
ga 	p: 84.09 	r: 74.03 	f1: 78.74 	 4566 	 5430 	 6168
wo 	p: 93.52 	r: 86.01 	f1: 89.61 	 2914 	 3116 	 3388
ni 	p: 83.82 	r: 75.23 	f1: 79.29 	 1145 	 1366 	 1522

[32m iter_5[0m
ga 	p: 84.09 	r: 74.01 	f1: 78.73 	 4565 	 5429 	 6168
wo 	p: 93.52 	r: 86.01 	f1: 89.61 	 2914 	 3116 	 3388
ni 	p: 83.81 	r: 75.16 	f1: 79.25 	 1144 	 1365 	 1522

[32m iter_6[0m
ga 	p: 84.09 	r: 74.03 	f1: 78.74 	 4566 	 5430 	 6168
wo 	p: 93.52 	r: 86.01 	f1: 89.61 	 2914 	 3116 	 3388
ni 	p: 83.82 	r: 75.23 	f1: 79.29 	 1145 	 1366 	 1522

[32m iter_7[0m
ga 	p: 84.09 	r: 74.01 	f1: 78.73 	 4565 	 5429 	 6168
wo 	p: 93.52 	r: 86.01 	f1: 89.61 	 2914 	 3116 	 3388
ni 	p: 83.81 	r: 75.16 	f1: 79.25 	 1144 	 1365 	 1522

[32m iter_8[0m
ga 	p: 84.09 	r: 74.03 	f1: 78.74 	 4566 	 5430 	 6168
wo 	p: 93.52 	r: 86.01 	f1: 89.61 	 2914 	 3116 	 3388
ni 	p: 83.82 	r: 75.23 	f1: 79.29 	 1145 	 1366 	 1522

[32m iter_9[0m
ga 	p: 84.09 	r: 74.01 	f1: 78.73 	 4565 	 5429 	 6168
wo 	p: 93.52 	r: 86.01 	f1: 89.61 	 2914 	 3116 	 3388
ni 	p: 83.81 	r: 75.16 	f1: 79.25 	 1144 	 1365 	 1522
best_thres [[0.32, 0.83, 0.11], [0.4, 0.81, 0.1], [0.55, 0.66, 0.11], [0.55, 0.66, 0.11], [0.55, 0.66, 0.11], [0.55, 0.66, 0.11], [0.55, 0.66, 0.11], [0.55, 0.66, 0.11], [0.55, 0.66, 0.11], [0.55, 0.66, 0.11]]
f [0.8192, 0.8198, 0.8205, 0.8208, 0.821, 0.8211, 0.8212, 0.8213, 0.8213, 0.8214]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse 	current best epoch 18 	 [0.31, 0.7, 0.11] 	 lr: 2.5e-05 	 f: 82.63404788158559
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse[0m [33m epoch 26 [0m
Train...

[32m iter_0[0m
ga 	p: 77.87 	r: 73.43 	f1: 75.58 	 4529 	 5816 	 6168
wo 	p: 92.25 	r: 83.32 	f1: 87.56 	 2823 	 3060 	 3388
ni 	p: 84.37 	r: 69.84 	f1: 76.42 	 1063 	 1260 	 1522

[32m iter_1[0m
ga 	p: 78.56 	r: 72.88 	f1: 75.61 	 4495 	 5722 	 6168
wo 	p: 90.78 	r: 84.33 	f1: 87.44 	 2857 	 3147 	 3388
ni 	p: 80.62 	r: 74.05 	f1: 77.19 	 1127 	 1398 	 1522

[32m iter_2[0m
ga 	p: 78.15 	r: 73.83 	f1: 75.93 	 4554 	 5827 	 6168
wo 	p: 92.49 	r: 83.26 	f1: 87.64 	 2821 	 3050 	 3388
ni 	p: 85.83 	r: 70.83 	f1: 77.61 	 1078 	 1256 	 1522
best_thres [[0.49, 0.69, 0.24], [0.55, 0.49, 0.14], [0.5, 0.69, 0.26]]
f [0.7933, 0.7939, 0.795]
load model: epoch17
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 17 	 [0.57, 0.56, 0.2] 	 lr: 0.00025 	 f: 80.6346610475996
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 26 [0m
Train...

[32m iter_0[0m
ga 	p: 80.0 	r: 76.13 	f1: 78.02 	 4696 	 5870 	 6168
wo 	p: 92.75 	r: 86.42 	f1: 89.47 	 2928 	 3157 	 3388
ni 	p: 84.34 	r: 74.64 	f1: 79.19 	 1136 	 1347 	 1522

[32m iter_1[0m
ga 	p: 81.53 	r: 74.94 	f1: 78.09 	 4622 	 5669 	 6168
wo 	p: 93.89 	r: 85.71 	f1: 89.62 	 2904 	 3093 	 3388
ni 	p: 86.37 	r: 73.26 	f1: 79.27 	 1115 	 1291 	 1522

[32m iter_2[0m
ga 	p: 81.39 	r: 75.03 	f1: 78.08 	 4628 	 5686 	 6168
wo 	p: 93.82 	r: 85.6 	f1: 89.52 	 2900 	 3091 	 3388
ni 	p: 85.05 	r: 74.38 	f1: 79.36 	 1132 	 1331 	 1522
best_thres [[0.3, 0.49, 0.13], [0.43, 0.85, 0.14], [0.42, 0.85, 0.11]]
f [0.8167, 0.8173, 0.8174]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 17 	 [0.45, 0.55, 0.15] 	 lr: 6.25e-05 	 f: 82.54275976360965
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 24 [0m
Train...
loss: tensor(510.5245) lr: 1.25e-05 time: 2148.16
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.6 	r: 75.41 	f1: 78.84 	 4651 	 5631 	 6168
wo 	p: 93.13 	r: 86.04 	f1: 89.44 	 2915 	 3130 	 3388
ni 	p: 87.14 	r: 76.54 	f1: 81.5 	 1165 	 1337 	 1522

[32m iter_1[0m
ga 	p: 80.65 	r: 77.59 	f1: 79.09 	 4786 	 5934 	 6168
wo 	p: 93.17 	r: 86.51 	f1: 89.72 	 2931 	 3146 	 3388
ni 	p: 87.13 	r: 76.08 	f1: 81.23 	 1158 	 1329 	 1522

[32m iter_2[0m
ga 	p: 81.73 	r: 76.59 	f1: 79.08 	 4724 	 5780 	 6168
wo 	p: 93.81 	r: 85.92 	f1: 89.69 	 2911 	 3103 	 3388
ni 	p: 87.15 	r: 76.22 	f1: 81.32 	 1160 	 1331 	 1522
best_thres [[0.35, 0.43, 0.18], [0.23, 0.39, 0.15], [0.28, 0.56, 0.14]]
f [0.8246, 0.8254, 0.8256]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse 	current best epoch 8 	 [0.52, 0.76, 0.16] 	 lr: 0.0001 	 f: 82.9521537813947
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse[0m [33m epoch 23 [0m
Train...
loss: tensor(867.2980) lr: 6.25e-05 time: 2929.72
pred_count_train 41644

Test...
loss: tensor(189.2832) lr: 3.125e-05 time: 2179.35
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 78.37 	r: 75.5 	f1: 76.91 	 4657 	 5942 	 6168
wo 	p: 91.84 	r: 85.04 	f1: 88.31 	 2881 	 3137 	 3388
ni 	p: 84.84 	r: 74.64 	f1: 79.41 	 1136 	 1339 	 1522

[32m iter_1[0m
ga 	p: 79.37 	r: 73.99 	f1: 76.59 	 4564 	 5750 	 6168
wo 	p: 92.18 	r: 84.15 	f1: 87.98 	 2851 	 3093 	 3388
ni 	p: 84.3 	r: 75.16 	f1: 79.47 	 1144 	 1357 	 1522

[32m iter_2[0m
ga 	p: 80.08 	r: 73.54 	f1: 76.67 	 4536 	 5664 	 6168
wo 	p: 92.42 	r: 84.27 	f1: 88.16 	 2855 	 3089 	 3388
ni 	p: 84.88 	r: 75.23 	f1: 79.76 	 1145 	 1349 	 1522
best_thres [[0.3, 0.52, 0.17], [0.36, 0.52, 0.17], [0.4, 0.56, 0.17]]
f [0.807, 0.8058, 0.8059]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 17 	 [0.57, 0.56, 0.2] 	 lr: 0.00025 	 f: 80.6346610475996
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 27 [0m
Train...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      loss: tensor(632.6130) lr: 6.25e-05 time: 2779.66
pred_count_train 41644

Test...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     
[32m iter_0[0m
ga 	p: 77.93 	r: 75.45 	f1: 76.67 	 4654 	 5972 	 6168
wo 	p: 91.84 	r: 84.03 	f1: 87.76 	 2847 	 3100 	 3388
ni 	p: 81.03 	r: 74.38 	f1: 77.56 	 1132 	 1397 	 1522

[32m iter_1[0m
ga 	p: 79.86 	r: 73.36 	f1: 76.47 	 4525 	 5666 	 6168
wo 	p: 92.85 	r: 82.79 	f1: 87.53 	 2805 	 3021 	 3388
ni 	p: 85.39 	r: 71.42 	f1: 77.78 	 1087 	 1273 	 1522

[32m iter_2[0m
ga 	p: 81.69 	r: 71.98 	f1: 76.53 	 4440 	 5435 	 6168
wo 	p: 93.4 	r: 82.7 	f1: 87.73 	 2802 	 3000 	 3388
ni 	p: 80.34 	r: 74.9 	f1: 77.52 	 1140 	 1419 	 1522
best_thres [[0.28, 0.54, 0.12], [0.4, 0.64, 0.19], [0.53, 0.7, 0.1]]
f [0.8013, 0.8008, 0.8008]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 17 	 [0.57, 0.56, 0.2] 	 lr: 0.00025 	 f: 80.6346610475996
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 28 [0m
Train...

[32m iter_0[0m
ga 	p: 80.25 	r: 75.84 	f1: 77.99 	 4678 	 5829 	 6168
wo 	p: 93.3 	r: 85.48 	f1: 89.22 	 2896 	 3104 	 3388
ni 	p: 86.6 	r: 73.46 	f1: 79.49 	 1118 	 1291 	 1522

[32m iter_1[0m
ga 	p: 80.64 	r: 76.26 	f1: 78.39 	 4704 	 5833 	 6168
wo 	p: 93.36 	r: 85.86 	f1: 89.45 	 2909 	 3116 	 3388
ni 	p: 80.32 	r: 78.32 	f1: 79.31 	 1192 	 1484 	 1522

[32m iter_2[0m
ga 	p: 80.69 	r: 76.17 	f1: 78.37 	 4698 	 5822 	 6168
wo 	p: 93.52 	r: 85.66 	f1: 89.42 	 2902 	 3103 	 3388
ni 	p: 81.11 	r: 77.86 	f1: 79.45 	 1185 	 1461 	 1522
best_thres [[0.29, 0.48, 0.2], [0.27, 0.48, 0.08], [0.27, 0.61, 0.08]]
f [0.8161, 0.8174, 0.8178]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse 	current best epoch 8 	 [0.52, 0.76, 0.16] 	 lr: 0.0001 	 f: 82.9521537813947
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse[0m [33m epoch 25 [0m
Train...
loss: tensor(419.4290) lr: 2.5e-05 time: 1823.05
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.94 	r: 76.54 	f1: 78.68 	 4721 	 5833 	 6168
wo 	p: 93.09 	r: 85.89 	f1: 89.35 	 2910 	 3126 	 3388
ni 	p: 82.79 	r: 71.75 	f1: 76.87 	 1092 	 1319 	 1522

[32m iter_1[0m
ga 	p: 82.94 	r: 74.81 	f1: 78.66 	 4614 	 5563 	 6168
wo 	p: 93.26 	r: 86.19 	f1: 89.58 	 2920 	 3131 	 3388
ni 	p: 83.6 	r: 72.34 	f1: 77.56 	 1101 	 1317 	 1522

[32m iter_2[0m
ga 	p: 82.74 	r: 74.98 	f1: 78.67 	 4625 	 5590 	 6168
wo 	p: 93.31 	r: 86.1 	f1: 89.56 	 2917 	 3126 	 3388
ni 	p: 83.32 	r: 72.86 	f1: 77.74 	 1109 	 1331 	 1522

[32m iter_3[0m
ga 	p: 83.3 	r: 74.5 	f1: 78.65 	 4595 	 5516 	 6168
wo 	p: 93.28 	r: 86.1 	f1: 89.55 	 2917 	 3127 	 3388
ni 	p: 83.25 	r: 72.8 	f1: 77.67 	 1108 	 1331 	 1522

[32m iter_4[0m
ga 	p: 83.25 	r: 74.59 	f1: 78.68 	 4601 	 5527 	 6168
wo 	p: 93.34 	r: 86.1 	f1: 89.57 	 2917 	 3125 	 3388
ni 	p: 82.1 	r: 73.85 	f1: 77.76 	 1124 	 1369 	 1522

[32m iter_5[0m
ga 	p: 83.44 	r: 74.4 	f1: 78.66 	 4589 	 5500 	 6168
wo 	p: 93.31 	r: 86.1 	f1: 89.56 	 2917 	 3126 	 3388
ni 	p: 82.09 	r: 73.78 	f1: 77.72 	 1123 	 1368 	 1522

[32m iter_6[0m
ga 	p: 83.45 	r: 74.42 	f1: 78.68 	 4590 	 5500 	 6168
wo 	p: 93.34 	r: 86.1 	f1: 89.57 	 2917 	 3125 	 3388
ni 	p: 82.16 	r: 73.85 	f1: 77.79 	 1124 	 1368 	 1522

[32m iter_7[0m
ga 	p: 83.44 	r: 74.4 	f1: 78.66 	 4589 	 5500 	 6168
wo 	p: 93.31 	r: 86.1 	f1: 89.56 	 2917 	 3126 	 3388
ni 	p: 82.15 	r: 73.78 	f1: 77.74 	 1123 	 1367 	 1522

[32m iter_8[0m
ga 	p: 83.45 	r: 74.42 	f1: 78.68 	 4590 	 5500 	 6168
wo 	p: 93.34 	r: 86.1 	f1: 89.57 	 2917 	 3125 	 3388
ni 	p: 82.16 	r: 73.85 	f1: 77.79 	 1124 	 1368 	 1522

[32m iter_9[0m
ga 	p: 83.44 	r: 74.4 	f1: 78.66 	 4589 	 5500 	 6168
wo 	p: 93.31 	r: 86.1 	f1: 89.56 	 2917 	 3126 	 3388
ni 	p: 82.15 	r: 73.78 	f1: 77.74 	 1123 	 1367 	 1522
best_thres [[0.24, 0.6, 0.16], [0.39, 0.58, 0.12], [0.37, 0.58, 0.11], [0.42, 0.58, 0.11], [0.41, 0.58, 0.09], [0.44, 0.58, 0.09], [0.44, 0.58, 0.09], [0.44, 0.58, 0.09], [0.44, 0.58, 0.09], [0.44, 0.58, 0.09]]
f [0.8169, 0.8179, 0.8183, 0.8184, 0.8186, 0.8187, 0.8187, 0.8188, 0.8188, 0.8188]
load model: epoch18
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse 	current best epoch 18 	 [0.31, 0.7, 0.11] 	 lr: 2.5e-05 	 f: 82.63404788158559
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse[0m [33m epoch 27 [0m
Train...

[32m iter_0[0m
ga 	p: 81.7 	r: 75.92 	f1: 78.71 	 4683 	 5732 	 6168
wo 	p: 93.04 	r: 86.42 	f1: 89.61 	 2928 	 3147 	 3388
ni 	p: 83.35 	r: 76.94 	f1: 80.01 	 1171 	 1405 	 1522

[32m iter_1[0m
ga 	p: 81.31 	r: 75.83 	f1: 78.47 	 4677 	 5752 	 6168
wo 	p: 93.57 	r: 85.89 	f1: 89.57 	 2910 	 3110 	 3388
ni 	p: 84.1 	r: 76.81 	f1: 80.29 	 1169 	 1390 	 1522

[32m iter_2[0m
ga 	p: 81.4 	r: 75.94 	f1: 78.58 	 4684 	 5754 	 6168
wo 	p: 93.17 	r: 86.22 	f1: 89.56 	 2921 	 3135 	 3388
ni 	p: 84.29 	r: 76.48 	f1: 80.19 	 1164 	 1381 	 1522
best_thres [[0.41, 0.51, 0.1], [0.42, 0.72, 0.09], [0.42, 0.67, 0.09]]
f [0.8222, 0.8216, 0.8216]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 17 	 [0.45, 0.55, 0.15] 	 lr: 6.25e-05 	 f: 82.54275976360965
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 27 [0m
Train...
loss: tensor(623.7462) lr: 1e-05 time: 2033.22
pred_count_train 41644

Test...
loss: tensor(471.7730) lr: 6.25e-05 time: 2927.86
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.86 	r: 75.92 	f1: 79.24 	 4683 	 5652 	 6168
wo 	p: 92.93 	r: 86.45 	f1: 89.57 	 2929 	 3152 	 3388
ni 	p: 88.29 	r: 75.82 	f1: 81.58 	 1154 	 1307 	 1522

[32m iter_1[0m
ga 	p: 82.28 	r: 76.28 	f1: 79.17 	 4705 	 5718 	 6168
wo 	p: 93.19 	r: 86.87 	f1: 89.92 	 2943 	 3158 	 3388
ni 	p: 86.34 	r: 77.66 	f1: 81.77 	 1182 	 1369 	 1522

[32m iter_2[0m
ga 	p: 82.69 	r: 75.96 	f1: 79.18 	 4685 	 5666 	 6168
wo 	p: 93.21 	r: 86.75 	f1: 89.86 	 2939 	 3153 	 3388
ni 	p: 84.02 	r: 79.43 	f1: 81.66 	 1209 	 1439 	 1522
best_thres [[0.45, 0.46, 0.22], [0.43, 0.47, 0.15], [0.45, 0.48, 0.11]]
f [0.8274, 0.8278, 0.8279]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse 	current best epoch 8 	 [0.52, 0.76, 0.16] 	 lr: 0.0001 	 f: 82.9521537813947
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse[0m [33m epoch 26 [0m
Train...
loss: tensor(299.1123) lr: 2.5e-05 time: 2132.36
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.89 	r: 72.91 	f1: 76.24 	 4497 	 5629 	 6168
wo 	p: 92.82 	r: 83.56 	f1: 87.95 	 2831 	 3050 	 3388
ni 	p: 84.77 	r: 71.68 	f1: 77.68 	 1091 	 1287 	 1522

[32m iter_1[0m
ga 	p: 79.74 	r: 72.55 	f1: 75.98 	 4475 	 5612 	 6168
wo 	p: 93.43 	r: 83.09 	f1: 87.96 	 2815 	 3013 	 3388
ni 	p: 83.33 	r: 73.26 	f1: 77.97 	 1115 	 1338 	 1522

[32m iter_2[0m
ga 	p: 79.62 	r: 72.89 	f1: 76.11 	 4496 	 5647 	 6168
wo 	p: 92.7 	r: 83.62 	f1: 87.93 	 2833 	 3056 	 3388
ni 	p: 86.19 	r: 70.96 	f1: 77.84 	 1080 	 1253 	 1522
best_thres [[0.52, 0.72, 0.23], [0.52, 0.72, 0.17], [0.51, 0.68, 0.25]]
f [0.8001, 0.7995, 0.7995]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 17 	 [0.57, 0.56, 0.2] 	 lr: 0.00025 	 f: 80.6346610475996
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 29 [0m
Train...

[32m iter_0[0m
ga 	p: 80.45 	r: 75.5 	f1: 77.9 	 4657 	 5789 	 6168
wo 	p: 94.18 	r: 85.04 	f1: 89.37 	 2881 	 3059 	 3388
ni 	p: 85.81 	r: 72.73 	f1: 78.73 	 1107 	 1290 	 1522

[32m iter_1[0m
ga 	p: 81.01 	r: 74.9 	f1: 77.84 	 4620 	 5703 	 6168
wo 	p: 93.72 	r: 85.83 	f1: 89.6 	 2908 	 3103 	 3388
ni 	p: 85.71 	r: 72.54 	f1: 78.58 	 1104 	 1288 	 1522

[32m iter_2[0m
ga 	p: 80.86 	r: 75.08 	f1: 77.86 	 4631 	 5727 	 6168
wo 	p: 93.57 	r: 85.86 	f1: 89.55 	 2909 	 3109 	 3388
ni 	p: 84.81 	r: 73.0 	f1: 78.46 	 1111 	 1310 	 1522
best_thres [[0.33, 0.65, 0.14], [0.38, 0.73, 0.12], [0.36, 0.72, 0.11]]
f [0.815, 0.8152, 0.8152]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 17 	 [0.45, 0.55, 0.15] 	 lr: 6.25e-05 	 f: 82.54275976360965
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 28 [0m
Train...
loss: tensor(524.5752) lr: 1e-05 time: 2109.3
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.75 	r: 76.04 	f1: 78.79 	 4690 	 5737 	 6168
wo 	p: 93.87 	r: 85.48 	f1: 89.48 	 2896 	 3085 	 3388
ni 	p: 85.35 	r: 77.73 	f1: 81.36 	 1183 	 1386 	 1522

[32m iter_1[0m
ga 	p: 83.8 	r: 74.81 	f1: 79.05 	 4614 	 5506 	 6168
wo 	p: 93.07 	r: 86.78 	f1: 89.81 	 2940 	 3159 	 3388
ni 	p: 84.41 	r: 78.25 	f1: 81.21 	 1191 	 1411 	 1522

[32m iter_2[0m
ga 	p: 83.41 	r: 74.98 	f1: 78.97 	 4625 	 5545 	 6168
wo 	p: 92.7 	r: 87.01 	f1: 89.77 	 2948 	 3180 	 3388
ni 	p: 86.02 	r: 76.81 	f1: 81.15 	 1169 	 1359 	 1522
best_thres [[0.35, 0.56, 0.15], [0.45, 0.42, 0.11], [0.43, 0.38, 0.13]]
f [0.8239, 0.8254, 0.8256]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse 	current best epoch 8 	 [0.52, 0.76, 0.16] 	 lr: 0.0001 	 f: 82.9521537813947
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse[0m [33m epoch 27 [0m
Train...
loss: tensor(236.7301) lr: 1e-05 time: 6049.17
pred_count_train 41644

Test...
loss: tensor(206.7900) lr: 2.5e-05 time: 2068.85
pred_count_train 41644

Test...


[32m iter_0[0m
ga 	p: 81.09 	r: 74.4 	f1: 77.6 	 4589 	 5659 	 6168
wo 	p: 94.5 	r: 84.77 	f1: 89.37 	 2872 	 3039 	 3388
ni 	p: 84.95 	r: 73.78 	f1: 78.97 	 1123 	 1322 	 1522

[32m iter_1[0m
ga 	p: 82.16 	r: 73.72 	f1: 77.71 	 4547 	 5534 	 6168
wo 	p: 93.67 	r: 85.63 	f1: 89.47 	 2901 	 3097 	 3388
ni 	p: 85.2 	r: 74.11 	f1: 79.27 	 1128 	 1324 	 1522

[32m iter_2[0m
ga 	p: 82.04 	r: 73.9 	f1: 77.76 	 4558 	 5556 	 6168
wo 	p: 93.11 	r: 85.8 	f1: 89.31 	 2907 	 3122 	 3388
ni 	p: 85.43 	r: 73.98 	f1: 79.3 	 1126 	 1318 	 1522
best_thres [[0.39, 0.68, 0.15], [0.54, 0.77, 0.12], [0.54, 0.65, 0.12]]
f [0.8137, 0.8146, 0.8148]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 17 	 [0.45, 0.55, 0.15] 	 lr: 6.25e-05 	 f: 82.54275976360965
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 29 [0m
Train...
loss: tensor(456.2878) lr: 1e-05 time: 1831.65
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 78.25 	r: 73.38 	f1: 75.74 	 4526 	 5784 	 6168
wo 	p: 91.54 	r: 84.27 	f1: 87.75 	 2855 	 3119 	 3388
ni 	p: 80.99 	r: 71.94 	f1: 76.2 	 1095 	 1352 	 1522

[32m iter_1[0m
ga 	p: 77.42 	r: 73.75 	f1: 75.54 	 4549 	 5876 	 6168
wo 	p: 92.07 	r: 83.65 	f1: 87.66 	 2834 	 3078 	 3388
ni 	p: 82.09 	r: 71.68 	f1: 76.53 	 1091 	 1329 	 1522

[32m iter_2[0m
ga 	p: 79.7 	r: 72.03 	f1: 75.67 	 4443 	 5575 	 6168
wo 	p: 91.8 	r: 83.88 	f1: 87.66 	 2842 	 3096 	 3388
ni 	p: 80.19 	r: 73.13 	f1: 76.49 	 1113 	 1388 	 1522
best_thres [[0.42, 0.57, 0.16], [0.37, 0.58, 0.16], [0.55, 0.56, 0.12]]
f [0.7946, 0.794, 0.7942]
load model: epoch17
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 17 	 [0.57, 0.56, 0.2] 	 lr: 0.00025 	 f: 80.6346610475996
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 30 [0m
Train...

[32m iter_0[0m
ga 	p: 82.2 	r: 75.32 	f1: 78.61 	 4646 	 5652 	 6168
wo 	p: 93.25 	r: 86.01 	f1: 89.48 	 2914 	 3125 	 3388
ni 	p: 85.48 	r: 75.43 	f1: 80.14 	 1148 	 1343 	 1522

[32m iter_1[0m
ga 	p: 81.68 	r: 76.33 	f1: 78.91 	 4708 	 5764 	 6168
wo 	p: 92.43 	r: 87.25 	f1: 89.77 	 2956 	 3198 	 3388
ni 	p: 85.89 	r: 74.77 	f1: 79.94 	 1138 	 1325 	 1522

[32m iter_2[0m
ga 	p: 81.4 	r: 76.57 	f1: 78.91 	 4723 	 5802 	 6168
wo 	p: 93.04 	r: 86.81 	f1: 89.82 	 2941 	 3161 	 3388
ni 	p: 85.31 	r: 75.16 	f1: 79.92 	 1144 	 1341 	 1522
best_thres [[0.38, 0.49, 0.16], [0.32, 0.35, 0.13], [0.31, 0.44, 0.12]]
f [0.8216, 0.8228, 0.8231]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse 	current best epoch 8 	 [0.52, 0.76, 0.16] 	 lr: 0.0001 	 f: 82.9521537813947
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse[0m [33m epoch 28 [0m
Train...
loss: tensor(142.2888) lr: 2.5e-05 time: 1786.17
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.73 	r: 75.57 	f1: 78.99 	 4661 	 5634 	 6168
wo 	p: 93.0 	r: 86.25 	f1: 89.49 	 2922 	 3142 	 3388
ni 	p: 83.31 	r: 75.43 	f1: 79.17 	 1148 	 1378 	 1522

[32m iter_1[0m
ga 	p: 81.63 	r: 76.67 	f1: 79.07 	 4729 	 5793 	 6168
wo 	p: 94.74 	r: 85.01 	f1: 89.61 	 2880 	 3040 	 3388
ni 	p: 83.18 	r: 75.69 	f1: 79.26 	 1152 	 1385 	 1522

[32m iter_2[0m
ga 	p: 84.68 	r: 74.12 	f1: 79.05 	 4572 	 5399 	 6168
wo 	p: 94.98 	r: 84.86 	f1: 89.63 	 2875 	 3027 	 3388
ni 	p: 82.91 	r: 76.48 	f1: 79.56 	 1164 	 1404 	 1522

[32m iter_3[0m
ga 	p: 82.98 	r: 75.39 	f1: 79.0 	 4650 	 5604 	 6168
wo 	p: 94.95 	r: 84.83 	f1: 89.6 	 2874 	 3027 	 3388
ni 	p: 82.95 	r: 76.41 	f1: 79.55 	 1163 	 1402 	 1522

[32m iter_4[0m
ga 	p: 83.0 	r: 75.42 	f1: 79.03 	 4652 	 5605 	 6168
wo 	p: 94.95 	r: 84.86 	f1: 89.62 	 2875 	 3028 	 3388
ni 	p: 82.89 	r: 76.41 	f1: 79.52 	 1163 	 1403 	 1522

[32m iter_5[0m
ga 	p: 82.98 	r: 75.41 	f1: 79.01 	 4651 	 5605 	 6168
wo 	p: 94.98 	r: 84.83 	f1: 89.62 	 2874 	 3026 	 3388
ni 	p: 84.93 	r: 74.77 	f1: 79.52 	 1138 	 1340 	 1522

[32m iter_6[0m
ga 	p: 83.0 	r: 75.42 	f1: 79.03 	 4652 	 5605 	 6168
wo 	p: 94.95 	r: 84.86 	f1: 89.62 	 2875 	 3028 	 3388
ni 	p: 84.86 	r: 74.77 	f1: 79.5 	 1138 	 1341 	 1522

[32m iter_7[0m
ga 	p: 82.98 	r: 75.41 	f1: 79.01 	 4651 	 5605 	 6168
wo 	p: 94.98 	r: 84.83 	f1: 89.62 	 2874 	 3026 	 3388
ni 	p: 84.93 	r: 74.77 	f1: 79.52 	 1138 	 1340 	 1522

[32m iter_8[0m
ga 	p: 83.0 	r: 75.42 	f1: 79.03 	 4652 	 5605 	 6168
wo 	p: 94.95 	r: 84.86 	f1: 89.62 	 2875 	 3028 	 3388
ni 	p: 84.86 	r: 74.77 	f1: 79.5 	 1138 	 1341 	 1522

[32m iter_9[0m
ga 	p: 82.98 	r: 75.41 	f1: 79.01 	 4651 	 5605 	 6168
wo 	p: 94.98 	r: 84.83 	f1: 89.62 	 2874 	 3026 	 3388
ni 	p: 84.93 	r: 74.77 	f1: 79.52 	 1138 	 1340 	 1522
best_thres [[0.44, 0.59, 0.13], [0.33, 0.78, 0.1], [0.56, 0.84, 0.09], [0.43, 0.82, 0.09], [0.43, 0.84, 0.09], [0.43, 0.84, 0.12], [0.43, 0.84, 0.12], [0.43, 0.84, 0.12], [0.43, 0.84, 0.12], [0.43, 0.84, 0.12]]
f [0.8224, 0.8226, 0.823, 0.823, 0.823, 0.823, 0.8231, 0.8231, 0.8231, 0.8231]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse 	current best epoch 18 	 [0.31, 0.7, 0.11] 	 lr: 2.5e-05 	 f: 82.63404788158559
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse[0m [33m epoch 28 [0m
Train...

[32m iter_0[0m
ga 	p: 80.62 	r: 74.72 	f1: 77.56 	 4609 	 5717 	 6168
wo 	p: 92.28 	r: 86.48 	f1: 89.29 	 2930 	 3175 	 3388
ni 	p: 84.17 	r: 72.67 	f1: 78.0 	 1106 	 1314 	 1522

[32m iter_1[0m
ga 	p: 79.8 	r: 75.47 	f1: 77.58 	 4655 	 5833 	 6168
wo 	p: 93.99 	r: 85.33 	f1: 89.45 	 2891 	 3076 	 3388
ni 	p: 83.09 	r: 73.92 	f1: 78.23 	 1125 	 1354 	 1522

[32m iter_2[0m
ga 	p: 79.8 	r: 75.41 	f1: 77.54 	 4651 	 5828 	 6168
wo 	p: 93.71 	r: 85.33 	f1: 89.32 	 2891 	 3085 	 3388
ni 	p: 83.35 	r: 73.65 	f1: 78.2 	 1121 	 1345 	 1522
best_thres [[0.34, 0.38, 0.13], [0.28, 0.85, 0.08], [0.28, 0.82, 0.08]]
f [0.8123, 0.8125, 0.8123]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	best in epoch 17 	 [0.45, 0.55, 0.15] 	 lr: 6.25e-05 	 f: 82.54275976360965
model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse.h5
e2e-stack model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse
gpu: 0
6 3 2
vocab size:  29396
prediction mode: model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse test [0.5, 0.5, 0.5]
model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse.h5
e2e-stack model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse
gpu: 0
6 3 2
vocab size:  29396
prediction mode: model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse test [0.5, 0.5, 0.5]
loss: tensor(394.8188) lr: 1e-05 time: 1839.95
pred_count_train 41644

Test...
loss: tensor(869.1220) lr: 5e-05 time: 3045.72
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.65 	r: 75.26 	f1: 78.33 	 4642 	 5685 	 6168
wo 	p: 91.9 	r: 86.75 	f1: 89.25 	 2939 	 3198 	 3388
ni 	p: 85.01 	r: 75.3 	f1: 79.86 	 1146 	 1348 	 1522

[32m iter_1[0m
ga 	p: 82.22 	r: 75.05 	f1: 78.47 	 4629 	 5630 	 6168
wo 	p: 92.41 	r: 86.92 	f1: 89.58 	 2945 	 3187 	 3388
ni 	p: 86.4 	r: 73.46 	f1: 79.4 	 1118 	 1294 	 1522

[32m iter_2[0m
ga 	p: 82.18 	r: 75.15 	f1: 78.51 	 4635 	 5640 	 6168
wo 	p: 93.57 	r: 85.95 	f1: 89.6 	 2912 	 3112 	 3388
ni 	p: 85.83 	r: 74.05 	f1: 79.51 	 1127 	 1313 	 1522
best_thres [[0.36, 0.34, 0.16], [0.36, 0.33, 0.15], [0.35, 0.57, 0.13]]
f [0.8191, 0.8198, 0.82]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse 	best in epoch 8 	 [0.52, 0.76, 0.16] 	 lr: 0.0001 	 f: 82.9521537813947
model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse.h5
e2e-stack model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse
gpu: 0
6 3 2
vocab size:  29396
prediction mode: model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse test [0.5, 0.5, 0.5]
model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse.h5
e2e-stack model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse
gpu: 0
6 3 2
vocab size:  29396
prediction mode: model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse test [0.5, 0.5, 0.5]
model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse.h5
e2e-stack model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse
gpu: 0
6 3 2
vocab size:  29396
prediction mode: model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse test [0.5, 0.5, 0.5]
model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse.h5
e2e-stack model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse
gpu: 0
6 3 2
vocab size:  29396
prediction mode: model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse test [0.5, 0.5, 0.5]

[32m iter_0[0m
ga 	p: 78.11 	r: 74.85 	f1: 76.45 	 4617 	 5911 	 6168
wo 	p: 91.8 	r: 84.56 	f1: 88.03 	 2865 	 3121 	 3388
ni 	p: 81.28 	r: 77.0 	f1: 79.08 	 1172 	 1442 	 1522

[32m iter_1[0m
ga 	p: 81.69 	r: 71.76 	f1: 76.4 	 4426 	 5418 	 6168
wo 	p: 90.57 	r: 85.33 	f1: 87.87 	 2891 	 3192 	 3388
ni 	p: 79.6 	r: 80.22 	f1: 79.91 	 1221 	 1534 	 1522

[32m iter_2[0m
ga 	p: 82.42 	r: 71.69 	f1: 76.68 	 4422 	 5365 	 6168
wo 	p: 92.14 	r: 84.39 	f1: 88.09 	 2859 	 3103 	 3388
ni 	p: 81.4 	r: 78.19 	f1: 79.76 	 1190 	 1462 	 1522
best_thres [[0.35, 0.48, 0.14], [0.54, 0.35, 0.1], [0.57, 0.48, 0.13]]
f [0.8031, 0.8039, 0.8047]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 17 	 [0.57, 0.56, 0.2] 	 lr: 0.00025 	 f: 80.6346610475996
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 31 [0m
Train...
loss: tensor(187.4316) lr: 1e-05 time: 4000.59
pred_count_train 41644

Test...
loss: tensor(630.3609) lr: 5e-05 time: 2903.97
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.77 	r: 75.45 	f1: 78.94 	 4654 	 5623 	 6168
wo 	p: 93.22 	r: 85.98 	f1: 89.45 	 2913 	 3125 	 3388
ni 	p: 82.83 	r: 75.43 	f1: 78.95 	 1148 	 1386 	 1522

[32m iter_1[0m
ga 	p: 82.98 	r: 75.24 	f1: 78.92 	 4641 	 5593 	 6168
wo 	p: 94.88 	r: 84.83 	f1: 89.57 	 2874 	 3029 	 3388
ni 	p: 85.61 	r: 73.85 	f1: 79.29 	 1124 	 1313 	 1522

[32m iter_2[0m
ga 	p: 83.51 	r: 74.72 	f1: 78.87 	 4609 	 5519 	 6168
wo 	p: 94.44 	r: 85.18 	f1: 89.57 	 2886 	 3056 	 3388
ni 	p: 84.17 	r: 75.1 	f1: 79.38 	 1143 	 1358 	 1522

[32m iter_3[0m
ga 	p: 82.95 	r: 75.16 	f1: 78.86 	 4636 	 5589 	 6168
wo 	p: 94.47 	r: 85.24 	f1: 89.62 	 2888 	 3057 	 3388
ni 	p: 83.58 	r: 75.56 	f1: 79.37 	 1150 	 1376 	 1522

[32m iter_4[0m
ga 	p: 82.97 	r: 75.18 	f1: 78.88 	 4637 	 5589 	 6168
wo 	p: 94.47 	r: 85.24 	f1: 89.62 	 2888 	 3057 	 3388
ni 	p: 83.59 	r: 75.62 	f1: 79.41 	 1151 	 1377 	 1522

[32m iter_5[0m
ga 	p: 82.85 	r: 75.26 	f1: 78.87 	 4642 	 5603 	 6168
wo 	p: 94.47 	r: 85.18 	f1: 89.59 	 2886 	 3055 	 3388
ni 	p: 85.06 	r: 74.44 	f1: 79.4 	 1133 	 1332 	 1522

[32m iter_6[0m
ga 	p: 82.97 	r: 75.18 	f1: 78.88 	 4637 	 5589 	 6168
wo 	p: 94.47 	r: 85.21 	f1: 89.6 	 2887 	 3056 	 3388
ni 	p: 83.59 	r: 75.62 	f1: 79.41 	 1151 	 1377 	 1522

[32m iter_7[0m
ga 	p: 82.85 	r: 75.26 	f1: 78.87 	 4642 	 5603 	 6168
wo 	p: 94.47 	r: 85.21 	f1: 89.6 	 2887 	 3056 	 3388
ni 	p: 85.06 	r: 74.44 	f1: 79.4 	 1133 	 1332 	 1522

[32m iter_8[0m
ga 	p: 82.97 	r: 75.18 	f1: 78.88 	 4637 	 5589 	 6168
wo 	p: 94.47 	r: 85.18 	f1: 89.59 	 2886 	 3055 	 3388
ni 	p: 83.59 	r: 75.62 	f1: 79.41 	 1151 	 1377 	 1522

[32m iter_9[0m
ga 	p: 82.85 	r: 75.26 	f1: 78.87 	 4642 	 5603 	 6168
wo 	p: 94.47 	r: 85.21 	f1: 89.6 	 2887 	 3056 	 3388
ni 	p: 85.06 	r: 74.44 	f1: 79.4 	 1133 	 1332 	 1522
best_thres [[0.45, 0.62, 0.13], [0.45, 0.82, 0.14], [0.5, 0.75, 0.11], [0.45, 0.75, 0.1], [0.45, 0.75, 0.1], [0.44, 0.75, 0.12], [0.45, 0.75, 0.1], [0.44, 0.75, 0.12], [0.45, 0.75, 0.1], [0.44, 0.75, 0.12]]
f [0.8217, 0.822, 0.8221, 0.8221, 0.8221, 0.8221, 0.8222, 0.8222, 0.8222, 0.8222]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse 	current best epoch 18 	 [0.31, 0.7, 0.11] 	 lr: 2.5e-05 	 f: 82.63404788158559
[34me2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse[0m [33m epoch 29 [0m
Train...

[32m iter_0[0m
ga 	p: 81.54 	r: 72.2 	f1: 76.58 	 4453 	 5461 	 6168
wo 	p: 91.95 	r: 84.3 	f1: 87.96 	 2856 	 3106 	 3388
ni 	p: 85.51 	r: 70.57 	f1: 77.32 	 1074 	 1256 	 1522

[32m iter_1[0m
ga 	p: 81.35 	r: 72.26 	f1: 76.53 	 4457 	 5479 	 6168
wo 	p: 92.52 	r: 83.62 	f1: 87.84 	 2833 	 3062 	 3388
ni 	p: 80.7 	r: 74.7 	f1: 77.58 	 1137 	 1409 	 1522

[32m iter_2[0m
ga 	p: 81.76 	r: 71.94 	f1: 76.53 	 4437 	 5427 	 6168
wo 	p: 93.6 	r: 82.88 	f1: 87.91 	 2808 	 3000 	 3388
ni 	p: 79.36 	r: 76.54 	f1: 77.93 	 1165 	 1468 	 1522
best_thres [[0.5, 0.56, 0.22], [0.48, 0.59, 0.11], [0.51, 0.71, 0.09]]
f [0.8022, 0.8018, 0.8019]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 17 	 [0.57, 0.56, 0.2] 	 lr: 0.00025 	 f: 80.6346610475996
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 32 [0m
Train...
loss: tensor(481.2083) lr: 5e-05 time: 2818.11
pred_count_train 41644

Test...
loss: tensor(148.4668) lr: 1e-05 time: 3696.15
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 78.73 	r: 73.8 	f1: 76.18 	 4552 	 5782 	 6168
wo 	p: 93.02 	r: 83.0 	f1: 87.72 	 2812 	 3023 	 3388
ni 	p: 81.61 	r: 73.46 	f1: 77.32 	 1118 	 1370 	 1522

[32m iter_1[0m
ga 	p: 78.92 	r: 73.49 	f1: 76.11 	 4533 	 5744 	 6168
wo 	p: 91.56 	r: 84.18 	f1: 87.71 	 2852 	 3115 	 3388
ni 	p: 79.53 	r: 74.05 	f1: 76.69 	 1127 	 1417 	 1522

[32m iter_2[0m
ga 	p: 79.52 	r: 72.94 	f1: 76.09 	 4499 	 5658 	 6168
wo 	p: 91.36 	r: 84.62 	f1: 87.86 	 2867 	 3138 	 3388
ni 	p: 82.01 	r: 72.21 	f1: 76.8 	 1099 	 1340 	 1522
best_thres [[0.38, 0.72, 0.14], [0.38, 0.49, 0.11], [0.43, 0.47, 0.15]]
f [0.7982, 0.7977, 0.7978]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	current best epoch 17 	 [0.57, 0.56, 0.2] 	 lr: 0.00025 	 f: 80.6346610475996
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse[0m [33m epoch 33 [0m
Train...
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             loss: tensor(369.4803) lr: 5e-05 time: 2938.3
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.63 	r: 72.1 	f1: 76.13 	 4447 	 5515 	 6168
wo 	p: 92.43 	r: 83.21 	f1: 87.57 	 2819 	 3050 	 3388
ni 	p: 84.37 	r: 71.29 	f1: 77.28 	 1085 	 1286 	 1522

[32m iter_1[0m
ga 	p: 79.91 	r: 72.55 	f1: 76.05 	 4475 	 5600 	 6168
wo 	p: 94.1 	r: 81.97 	f1: 87.62 	 2777 	 2951 	 3388
ni 	p: 83.94 	r: 71.09 	f1: 76.98 	 1082 	 1289 	 1522

[32m iter_2[0m
ga 	p: 81.37 	r: 71.95 	f1: 76.37 	 4438 	 5454 	 6168
wo 	p: 93.69 	r: 82.44 	f1: 87.71 	 2793 	 2981 	 3388
ni 	p: 84.39 	r: 71.02 	f1: 77.13 	 1081 	 1281 	 1522
best_thres [[0.54, 0.73, 0.22], [0.51, 0.81, 0.21], [0.6, 0.82, 0.21]]
f [0.798, 0.7974, 0.7981]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse 	best in epoch 17 	 [0.57, 0.56, 0.2] 	 lr: 0.00025 	 f: 80.6346610475996
model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse.h5
e2e-stack model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse
gpu: 0
6 3 2
vocab size:  29396
prediction mode: model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.4_it3_rs2016_preFalse test [0.5, 0.5, 0.5]
loss: tensor(116.1316) lr: 1e-05 time: 3698.17
pred_count_train 41644

Test...
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it10_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it10_rs2016_preFalse[0m [33m epoch 1 [0m
Train...

[32m iter_0[0m
ga 	p: 81.05 	r: 76.07 	f1: 78.48 	 4692 	 5789 	 6168
wo 	p: 93.26 	r: 85.8 	f1: 89.38 	 2907 	 3117 	 3388
ni 	p: 83.51 	r: 72.86 	f1: 77.82 	 1109 	 1328 	 1522

[32m iter_1[0m
ga 	p: 81.06 	r: 76.1 	f1: 78.5 	 4694 	 5791 	 6168
wo 	p: 93.49 	r: 85.63 	f1: 89.39 	 2901 	 3103 	 3388
ni 	p: 85.22 	r: 72.34 	f1: 78.25 	 1101 	 1292 	 1522

[32m iter_2[0m
ga 	p: 83.47 	r: 74.11 	f1: 78.51 	 4571 	 5476 	 6168
wo 	p: 93.68 	r: 85.71 	f1: 89.52 	 2904 	 3100 	 3388
ni 	p: 84.76 	r: 72.34 	f1: 78.06 	 1101 	 1299 	 1522

[32m iter_3[0m
ga 	p: 84.46 	r: 73.41 	f1: 78.55 	 4528 	 5361 	 6168
wo 	p: 93.62 	r: 85.71 	f1: 89.49 	 2904 	 3102 	 3388
ni 	p: 84.68 	r: 72.27 	f1: 77.99 	 1100 	 1299 	 1522

[32m iter_4[0m
ga 	p: 84.47 	r: 73.44 	f1: 78.57 	 4530 	 5363 	 6168
wo 	p: 93.65 	r: 85.71 	f1: 89.51 	 2904 	 3101 	 3388
ni 	p: 84.82 	r: 72.34 	f1: 78.09 	 1101 	 1298 	 1522

[32m iter_5[0m
ga 	p: 84.46 	r: 73.43 	f1: 78.56 	 4529 	 5362 	 6168
wo 	p: 93.62 	r: 85.71 	f1: 89.49 	 2904 	 3102 	 3388
ni 	p: 84.81 	r: 72.27 	f1: 78.04 	 1100 	 1297 	 1522

[32m iter_6[0m
ga 	p: 84.47 	r: 73.44 	f1: 78.57 	 4530 	 5363 	 6168
wo 	p: 93.65 	r: 85.71 	f1: 89.51 	 2904 	 3101 	 3388
ni 	p: 84.81 	r: 72.27 	f1: 78.04 	 1100 	 1297 	 1522

[32m iter_7[0m
ga 	p: 84.46 	r: 73.43 	f1: 78.56 	 4529 	 5362 	 6168
wo 	p: 93.62 	r: 85.71 	f1: 89.49 	 2904 	 3102 	 3388
ni 	p: 84.81 	r: 72.27 	f1: 78.04 	 1100 	 1297 	 1522

[32m iter_8[0m
ga 	p: 84.47 	r: 73.44 	f1: 78.57 	 4530 	 5363 	 6168
wo 	p: 93.65 	r: 85.71 	f1: 89.51 	 2904 	 3101 	 3388
ni 	p: 84.81 	r: 72.27 	f1: 78.04 	 1100 	 1297 	 1522

[32m iter_9[0m
ga 	p: 84.46 	r: 73.43 	f1: 78.56 	 4529 	 5362 	 6168
wo 	p: 93.62 	r: 85.71 	f1: 89.49 	 2904 	 3102 	 3388
ni 	p: 84.81 	r: 72.27 	f1: 78.04 	 1100 	 1297 	 1522
best_thres [[0.31, 0.63, 0.15], [0.29, 0.64, 0.13], [0.49, 0.64, 0.12], [0.59, 0.64, 0.12], [0.59, 0.64, 0.12], [0.59, 0.64, 0.12], [0.59, 0.64, 0.12], [0.59, 0.64, 0.12], [0.59, 0.64, 0.12], [0.59, 0.64, 0.12]]
f [0.8172, 0.8175, 0.8179, 0.8181, 0.8183, 0.8184, 0.8185, 0.8186, 0.8186, 0.8186]
e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse 	best in epoch 18 	 [0.31, 0.7, 0.11] 	 lr: 2.5e-05 	 f: 82.63404788158559
model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it10_rs2016_preFalse.h5
e2e-stack model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it10_rs2016_preFalse
gpu: 0
6 3 2
vocab size:  29396
loss: tensor(3191.7375) lr: 0.0002 time: 10373.81
pred_count_train 41644

Test...
loss: tensor(3171.4216) lr: 0.001 time: 10663.52
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 77.47 	r: 74.04 	f1: 75.72 	 4567 	 5895 	 6168
wo 	p: 91.17 	r: 84.15 	f1: 87.52 	 2851 	 3127 	 3388
ni 	p: 83.81 	r: 79.24 	f1: 81.46 	 1206 	 1439 	 1522

[32m iter_1[0m
ga 	p: 78.03 	r: 73.49 	f1: 75.7 	 4533 	 5809 	 6168
wo 	p: 92.37 	r: 83.26 	f1: 87.58 	 2821 	 3054 	 3388
ni 	p: 83.58 	r: 78.58 	f1: 81.0 	 1196 	 1431 	 1522

[32m iter_2[0m
ga 	p: 78.06 	r: 73.43 	f1: 75.67 	 4529 	 5802 	 6168
wo 	p: 92.2 	r: 83.35 	f1: 87.55 	 2824 	 3063 	 3388
ni 	p: 83.66 	r: 78.71 	f1: 81.11 	 1198 	 1432 	 1522

[32m iter_3[0m
ga 	p: 78.24 	r: 73.23 	f1: 75.66 	 4517 	 5773 	 6168
wo 	p: 92.19 	r: 83.32 	f1: 87.53 	 2823 	 3062 	 3388
ni 	p: 83.59 	r: 78.65 	f1: 81.04 	 1197 	 1432 	 1522

[32m iter_4[0m
ga 	p: 78.27 	r: 73.25 	f1: 75.68 	 4518 	 5772 	 6168
wo 	p: 92.2 	r: 83.35 	f1: 87.55 	 2824 	 3063 	 3388
ni 	p: 83.66 	r: 78.71 	f1: 81.11 	 1198 	 1432 	 1522

[32m iter_5[0m
ga 	p: 78.24 	r: 73.23 	f1: 75.66 	 4517 	 5773 	 6168
wo 	p: 92.19 	r: 83.32 	f1: 87.53 	 2823 	 3062 	 3388
ni 	p: 83.59 	r: 78.65 	f1: 81.04 	 1197 	 1432 	 1522

[32m iter_6[0m
ga 	p: 78.27 	r: 73.25 	f1: 75.68 	 4518 	 5772 	 6168
wo 	p: 92.2 	r: 83.35 	f1: 87.55 	 2824 	 3063 	 3388
ni 	p: 83.66 	r: 78.71 	f1: 81.11 	 1198 	 1432 	 1522

[32m iter_7[0m
ga 	p: 78.24 	r: 73.23 	f1: 75.66 	 4517 	 5773 	 6168
wo 	p: 92.19 	r: 83.32 	f1: 87.53 	 2823 	 3062 	 3388
ni 	p: 83.59 	r: 78.65 	f1: 81.04 	 1197 	 1432 	 1522

[32m iter_8[0m
ga 	p: 78.27 	r: 73.25 	f1: 75.68 	 4518 	 5772 	 6168
wo 	p: 92.2 	r: 83.35 	f1: 87.55 	 2824 	 3063 	 3388
ni 	p: 83.66 	r: 78.71 	f1: 81.11 	 1198 	 1432 	 1522

[32m iter_9[0m
ga 	p: 78.24 	r: 73.23 	f1: 75.66 	 4517 	 5773 	 6168
wo 	p: 92.19 	r: 83.32 	f1: 87.53 	 2823 	 3062 	 3388
ni 	p: 83.59 	r: 78.65 	f1: 81.04 	 1197 	 1432 	 1522
best_thres [[0.42, 0.25, 0.25], [0.45, 0.29, 0.26], [0.45, 0.28, 0.26], [0.46, 0.28, 0.26], [0.46, 0.28, 0.26], [0.46, 0.28, 0.26], [0.46, 0.28, 0.26], [0.46, 0.28, 0.26], [0.46, 0.28, 0.26], [0.46, 0.28, 0.26]]
f [0.8008, 0.8004, 0.8003, 0.8002, 0.8002, 0.8002, 0.8002, 0.8001, 0.8001, 0.8001]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it10_rs2016_preFalse 	current best epoch 1 	 [0.46, 0.28, 0.26] 	 lr: 0.0002 	 f: 80.0117921769201
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it10_rs2016_preFalse[0m [33m epoch 2 [0m
Train...

[32m iter_0[0m
ga 	p: 74.4 	r: 71.66 	f1: 73.0 	 4420 	 5941 	 6168
wo 	p: 92.61 	r: 80.28 	f1: 86.01 	 2720 	 2937 	 3388
ni 	p: 83.89 	r: 75.3 	f1: 79.36 	 1146 	 1366 	 1522

[32m iter_1[0m
ga 	p: 74.56 	r: 70.27 	f1: 72.35 	 4334 	 5813 	 6168
wo 	p: 89.73 	r: 82.76 	f1: 86.1 	 2804 	 3125 	 3388
ni 	p: 79.89 	r: 77.27 	f1: 78.56 	 1176 	 1472 	 1522

[32m iter_2[0m
ga 	p: 74.56 	r: 71.29 	f1: 72.89 	 4397 	 5897 	 6168
wo 	p: 92.77 	r: 80.34 	f1: 86.11 	 2722 	 2934 	 3388
ni 	p: 83.44 	r: 75.16 	f1: 79.09 	 1144 	 1371 	 1522

[32m iter_3[0m
ga 	p: 74.35 	r: 70.36 	f1: 72.3 	 4340 	 5837 	 6168
wo 	p: 89.81 	r: 82.76 	f1: 86.14 	 2804 	 3122 	 3388
ni 	p: 83.42 	r: 74.38 	f1: 78.64 	 1132 	 1357 	 1522

[32m iter_4[0m
ga 	p: 74.84 	r: 71.08 	f1: 72.91 	 4384 	 5858 	 6168
wo 	p: 92.77 	r: 80.34 	f1: 86.11 	 2722 	 2934 	 3388
ni 	p: 83.44 	r: 75.16 	f1: 79.09 	 1144 	 1371 	 1522

[32m iter_5[0m
ga 	p: 74.33 	r: 70.38 	f1: 72.3 	 4341 	 5840 	 6168
wo 	p: 89.78 	r: 82.73 	f1: 86.11 	 2803 	 3122 	 3388
ni 	p: 83.48 	r: 74.38 	f1: 78.67 	 1132 	 1356 	 1522

[32m iter_6[0m
ga 	p: 74.82 	r: 71.14 	f1: 72.93 	 4388 	 5865 	 6168
wo 	p: 92.8 	r: 80.31 	f1: 86.11 	 2721 	 2932 	 3388
ni 	p: 83.44 	r: 75.16 	f1: 79.09 	 1144 	 1371 	 1522

[32m iter_7[0m
ga 	p: 74.32 	r: 70.36 	f1: 72.29 	 4340 	 5840 	 6168
wo 	p: 89.81 	r: 82.73 	f1: 86.13 	 2803 	 3121 	 3388
ni 	p: 83.42 	r: 74.38 	f1: 78.64 	 1132 	 1357 	 1522

[32m iter_8[0m
ga 	p: 74.88 	r: 71.06 	f1: 72.92 	 4383 	 5853 	 6168
wo 	p: 92.81 	r: 80.34 	f1: 86.13 	 2722 	 2933 	 3388
ni 	p: 83.38 	r: 75.16 	f1: 79.06 	 1144 	 1372 	 1522

[32m iter_9[0m
ga 	p: 74.35 	r: 70.41 	f1: 72.33 	 4343 	 5841 	 6168
wo 	p: 89.78 	r: 82.73 	f1: 86.11 	 2803 	 3122 	 3388
ni 	p: 83.48 	r: 74.38 	f1: 78.67 	 1132 	 1356 	 1522
best_thres [[0.5, 0.37, 0.24], [0.45, 0.22, 0.15], [0.5, 0.37, 0.23], [0.45, 0.22, 0.23], [0.51, 0.37, 0.23], [0.45, 0.22, 0.23], [0.51, 0.37, 0.23], [0.45, 0.22, 0.23], [0.51, 0.37, 0.23], [0.45, 0.22, 0.23]]
f [0.7772, 0.7755, 0.7759, 0.7753, 0.7756, 0.7753, 0.7755, 0.7753, 0.7754, 0.7753]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it10_rs2016_preFalse 	current best epoch 1 	 [0.45, 0.22, 0.23] 	 lr: 0.001 	 f: 77.52722637725168
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it10_rs2016_preFalse[0m [33m epoch 2 [0m
Train...
loss: tensor(2031.2751) lr: 0.0002 time: 10042.89
pred_count_train 41644

Test...
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(3192.2349) lr: 0.0002 time: 3111.26
pred_count_train 41644

Test...
loss: tensor(3192.6335) lr: 0.001 time: 3126.75
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 73.38 	r: 70.57 	f1: 71.95 	 4353 	 5932 	 6168
wo 	p: 91.74 	r: 79.63 	f1: 85.26 	 2698 	 2941 	 3388
ni 	p: 81.07 	r: 76.81 	f1: 78.88 	 1169 	 1442 	 1522

[32m iter_1[0m
ga 	p: 74.14 	r: 69.08 	f1: 71.52 	 4261 	 5747 	 6168
wo 	p: 90.31 	r: 80.58 	f1: 85.17 	 2730 	 3023 	 3388
ni 	p: 71.23 	r: 75.82 	f1: 73.46 	 1154 	 1620 	 1522

[32m iter_2[0m
ga 	p: 73.22 	r: 70.69 	f1: 71.93 	 4360 	 5955 	 6168
wo 	p: 90.02 	r: 81.17 	f1: 85.36 	 2750 	 3055 	 3388
ni 	p: 82.62 	r: 74.05 	f1: 78.1 	 1127 	 1364 	 1522
best_thres [[0.51, 0.3, 0.32], [0.47, 0.22, 0.15], [0.5, 0.21, 0.37]]
f [0.7685, 0.7636, 0.7651]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 1 	 [0.5, 0.21, 0.37] 	 lr: 0.001 	 f: 76.5070825494068
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 2 [0m
Train...

[32m iter_0[0m
ga 	p: 76.01 	r: 75.31 	f1: 75.66 	 4645 	 6111 	 6168
wo 	p: 90.7 	r: 84.39 	f1: 87.43 	 2859 	 3152 	 3388
ni 	p: 83.22 	r: 78.84 	f1: 80.97 	 1200 	 1442 	 1522

[32m iter_1[0m
ga 	p: 77.23 	r: 73.9 	f1: 75.53 	 4558 	 5902 	 6168
wo 	p: 91.39 	r: 83.65 	f1: 87.35 	 2834 	 3101 	 3388
ni 	p: 85.3 	r: 76.61 	f1: 80.72 	 1166 	 1367 	 1522

[32m iter_2[0m
ga 	p: 75.76 	r: 75.13 	f1: 75.44 	 4634 	 6117 	 6168
wo 	p: 91.88 	r: 83.21 	f1: 87.33 	 2819 	 3068 	 3388
ni 	p: 86.21 	r: 76.02 	f1: 80.8 	 1157 	 1342 	 1522
best_thres [[0.41, 0.25, 0.22], [0.46, 0.26, 0.28], [0.41, 0.29, 0.3]]
f [0.7992, 0.7986, 0.7981]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse 	current best epoch 1 	 [0.41, 0.29, 0.3] 	 lr: 0.0002 	 f: 79.80751434388303
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse[0m [33m epoch 2 [0m
Train...
loss: tensor(2569.0618) lr: 0.001 time: 3201.18
pred_count_train 41644

Test...
loss: tensor(2035.4315) lr: 0.0002 time: 3187.78
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 75.67 	r: 69.83 	f1: 72.63 	 4307 	 5692 	 6168
wo 	p: 88.68 	r: 81.85 	f1: 85.13 	 2773 	 3127 	 3388
ni 	p: 78.21 	r: 78.06 	f1: 78.13 	 1188 	 1519 	 1522

[32m iter_1[0m
ga 	p: 75.83 	r: 69.39 	f1: 72.47 	 4280 	 5644 	 6168
wo 	p: 89.59 	r: 80.55 	f1: 84.83 	 2729 	 3046 	 3388
ni 	p: 77.6 	r: 77.86 	f1: 77.73 	 1185 	 1527 	 1522

[32m iter_2[0m
ga 	p: 76.09 	r: 69.88 	f1: 72.85 	 4310 	 5664 	 6168
wo 	p: 89.77 	r: 80.81 	f1: 85.06 	 2738 	 3050 	 3388
ni 	p: 77.58 	r: 78.45 	f1: 78.01 	 1194 	 1539 	 1522
best_thres [[0.47, 0.23, 0.13], [0.42, 0.27, 0.13], [0.47, 0.27, 0.13]]
f [0.7721, 0.7709, 0.7715]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 2 	 [0.47, 0.27, 0.13] 	 lr: 0.001 	 f: 77.14937072546142
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 3 [0m
Train...

[32m iter_0[0m
ga 	p: 82.51 	r: 73.56 	f1: 77.77 	 4537 	 5499 	 6168
wo 	p: 93.25 	r: 83.94 	f1: 88.35 	 2844 	 3050 	 3388
ni 	p: 84.13 	r: 81.87 	f1: 82.98 	 1246 	 1481 	 1522

[32m iter_1[0m
ga 	p: 81.4 	r: 74.43 	f1: 77.76 	 4591 	 5640 	 6168
wo 	p: 93.16 	r: 83.97 	f1: 88.33 	 2845 	 3054 	 3388
ni 	p: 83.61 	r: 81.8 	f1: 82.7 	 1245 	 1489 	 1522

[32m iter_2[0m
ga 	p: 81.23 	r: 74.71 	f1: 77.83 	 4608 	 5673 	 6168
wo 	p: 93.15 	r: 83.94 	f1: 88.31 	 2844 	 3053 	 3388
ni 	p: 83.53 	r: 82.0 	f1: 82.76 	 1248 	 1494 	 1522
best_thres [[0.49, 0.43, 0.33], [0.44, 0.41, 0.31], [0.43, 0.41, 0.31]]
f [0.8174, 0.817, 0.817]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse 	current best epoch 2 	 [0.43, 0.41, 0.31] 	 lr: 0.0002 	 f: 81.70009581101668
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse[0m [33m epoch 3 [0m
Train...
loss: tensor(2555.7878) lr: 0.001 time: 3318.14
pred_count_train 41644

Test...
loss: tensor(1646.1239) lr: 0.0002 time: 3302.74
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 75.39 	r: 70.74 	f1: 72.99 	 4363 	 5787 	 6168
wo 	p: 91.83 	r: 79.63 	f1: 85.3 	 2698 	 2938 	 3388
ni 	p: 82.06 	r: 72.73 	f1: 77.12 	 1107 	 1349 	 1522

[32m iter_1[0m
ga 	p: 75.23 	r: 68.97 	f1: 71.96 	 4254 	 5655 	 6168
wo 	p: 91.39 	r: 78.93 	f1: 84.7 	 2674 	 2926 	 3388
ni 	p: 78.18 	r: 75.1 	f1: 76.61 	 1143 	 1462 	 1522

[32m iter_2[0m
ga 	p: 75.4 	r: 69.96 	f1: 72.58 	 4315 	 5723 	 6168
wo 	p: 92.48 	r: 79.13 	f1: 85.29 	 2681 	 2899 	 3388
ni 	p: 79.04 	r: 75.56 	f1: 77.26 	 1150 	 1455 	 1522
best_thres [[0.45, 0.43, 0.18], [0.39, 0.36, 0.12], [0.45, 0.46, 0.13]]
f [0.7723, 0.7683, 0.7689]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 2 	 [0.47, 0.27, 0.13] 	 lr: 0.001 	 f: 77.14937072546142
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 4 [0m
Train...

[32m iter_0[0m
ga 	p: 80.52 	r: 75.75 	f1: 78.06 	 4672 	 5802 	 6168
wo 	p: 93.11 	r: 85.33 	f1: 89.05 	 2891 	 3105 	 3388
ni 	p: 86.02 	r: 78.84 	f1: 82.28 	 1200 	 1395 	 1522

[32m iter_1[0m
ga 	p: 80.69 	r: 75.32 	f1: 77.91 	 4646 	 5758 	 6168
wo 	p: 93.2 	r: 85.42 	f1: 89.14 	 2894 	 3105 	 3388
ni 	p: 85.3 	r: 79.3 	f1: 82.19 	 1207 	 1415 	 1522

[32m iter_2[0m
ga 	p: 80.9 	r: 75.54 	f1: 78.13 	 4659 	 5759 	 6168
wo 	p: 94.03 	r: 84.56 	f1: 89.04 	 2865 	 3047 	 3388
ni 	p: 86.12 	r: 78.65 	f1: 82.21 	 1197 	 1390 	 1522
best_thres [[0.46, 0.54, 0.18], [0.47, 0.5, 0.16], [0.47, 0.63, 0.18]]
f [0.8197, 0.8194, 0.8196]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse 	current best epoch 3 	 [0.47, 0.63, 0.18] 	 lr: 0.0002 	 f: 81.95906889548509
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse[0m [33m epoch 4 [0m
Train...
loss: tensor(2620.9111) lr: 0.001 time: 3056.21
pred_count_train 41644

Test...
loss: tensor(1304.8167) lr: 0.0002 time: 3056.21
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 75.78 	r: 67.77 	f1: 71.55 	 4180 	 5516 	 6168
wo 	p: 89.82 	r: 79.19 	f1: 84.17 	 2683 	 2987 	 3388
ni 	p: 77.94 	r: 75.89 	f1: 76.9 	 1155 	 1482 	 1522

[32m iter_1[0m
ga 	p: 72.62 	r: 69.75 	f1: 71.15 	 4302 	 5924 	 6168
wo 	p: 91.33 	r: 77.1 	f1: 83.61 	 2612 	 2860 	 3388
ni 	p: 72.83 	r: 74.84 	f1: 73.82 	 1139 	 1564 	 1522

[32m iter_2[0m
ga 	p: 72.23 	r: 70.83 	f1: 71.52 	 4369 	 6049 	 6168
wo 	p: 90.42 	r: 78.31 	f1: 83.93 	 2653 	 2934 	 3388
ni 	p: 77.91 	r: 76.02 	f1: 76.95 	 1157 	 1485 	 1522
best_thres [[0.31, 0.3, 0.22], [0.22, 0.34, 0.13], [0.21, 0.32, 0.21]]
f [0.7613, 0.7565, 0.7574]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 2 	 [0.47, 0.27, 0.13] 	 lr: 0.001 	 f: 77.14937072546142
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 5 [0m
Train...

[32m iter_0[0m
ga 	p: 80.23 	r: 74.69 	f1: 77.36 	 4607 	 5742 	 6168
wo 	p: 93.08 	r: 85.8 	f1: 89.3 	 2907 	 3123 	 3388
ni 	p: 86.88 	r: 77.46 	f1: 81.9 	 1179 	 1357 	 1522

[32m iter_1[0m
ga 	p: 80.96 	r: 74.19 	f1: 77.43 	 4576 	 5652 	 6168
wo 	p: 93.16 	r: 85.68 	f1: 89.27 	 2903 	 3116 	 3388
ni 	p: 86.43 	r: 78.25 	f1: 82.14 	 1191 	 1378 	 1522

[32m iter_2[0m
ga 	p: 80.65 	r: 74.59 	f1: 77.5 	 4601 	 5705 	 6168
wo 	p: 93.3 	r: 85.51 	f1: 89.23 	 2897 	 3105 	 3388
ni 	p: 85.87 	r: 78.65 	f1: 82.1 	 1197 	 1394 	 1522
best_thres [[0.22, 0.52, 0.21], [0.24, 0.51, 0.18], [0.23, 0.53, 0.17]]
f [0.8162, 0.8166, 0.8168]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse 	current best epoch 3 	 [0.47, 0.63, 0.18] 	 lr: 0.0002 	 f: 81.95906889548509
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse[0m [33m epoch 5 [0m
Train...
loss: tensor(2709.1953) lr: 0.001 time: 2941.9
pred_count_train 41644

Test...
loss: tensor(994.5547) lr: 0.0002 time: 2910.96
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 71.89 	r: 69.83 	f1: 70.84 	 4307 	 5991 	 6168
wo 	p: 88.79 	r: 79.52 	f1: 83.9 	 2694 	 3034 	 3388
ni 	p: 73.88 	r: 71.35 	f1: 72.59 	 1086 	 1470 	 1522

[32m iter_1[0m
ga 	p: 70.01 	r: 71.42 	f1: 70.71 	 4405 	 6292 	 6168
wo 	p: 90.37 	r: 78.66 	f1: 84.11 	 2665 	 2949 	 3388
ni 	p: 74.14 	r: 69.32 	f1: 71.65 	 1055 	 1423 	 1522

[32m iter_2[0m
ga 	p: 72.29 	r: 69.31 	f1: 70.77 	 4275 	 5914 	 6168
wo 	p: 89.87 	r: 78.81 	f1: 83.98 	 2670 	 2971 	 3388
ni 	p: 77.41 	r: 68.46 	f1: 72.66 	 1042 	 1346 	 1522
best_thres [[0.19, 0.47, 0.08], [0.14, 0.52, 0.09], [0.2, 0.56, 0.11]]
f [0.7497, 0.7486, 0.7489]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 2 	 [0.47, 0.27, 0.13] 	 lr: 0.001 	 f: 77.14937072546142
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 6 [0m
Train...

[32m iter_0[0m
ga 	p: 77.2 	r: 77.2 	f1: 77.2 	 4762 	 6168 	 6168
wo 	p: 92.11 	r: 86.54 	f1: 89.24 	 2932 	 3183 	 3388
ni 	p: 84.36 	r: 76.22 	f1: 80.08 	 1160 	 1375 	 1522

[32m iter_1[0m
ga 	p: 78.04 	r: 76.38 	f1: 77.2 	 4711 	 6037 	 6168
wo 	p: 93.84 	r: 85.36 	f1: 89.4 	 2892 	 3082 	 3388
ni 	p: 83.98 	r: 76.81 	f1: 80.23 	 1169 	 1392 	 1522

[32m iter_2[0m
ga 	p: 77.72 	r: 76.51 	f1: 77.11 	 4719 	 6072 	 6168
wo 	p: 93.64 	r: 85.63 	f1: 89.45 	 2901 	 3098 	 3388
ni 	p: 84.95 	r: 76.02 	f1: 80.24 	 1157 	 1362 	 1522
best_thres [[0.22, 0.5, 0.27], [0.25, 0.72, 0.23], [0.24, 0.72, 0.25]]
f [0.8121, 0.8124, 0.8124]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse 	current best epoch 3 	 [0.47, 0.63, 0.18] 	 lr: 0.0002 	 f: 81.95906889548509
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse[0m [33m epoch 6 [0m
Train...
loss: tensor(2841.5513) lr: 0.001 time: 3395.32
pred_count_train 41644

Test...
loss: tensor(755.2384) lr: 0.0002 time: 3405.07
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 71.79 	r: 67.61 	f1: 69.63 	 4170 	 5809 	 6168
wo 	p: 90.54 	r: 77.69 	f1: 83.62 	 2632 	 2907 	 3388
ni 	p: 76.98 	r: 71.62 	f1: 74.2 	 1090 	 1416 	 1522

[32m iter_1[0m
ga 	p: 69.69 	r: 69.62 	f1: 69.65 	 4294 	 6162 	 6168
wo 	p: 87.5 	r: 79.37 	f1: 83.24 	 2689 	 3073 	 3388
ni 	p: 71.89 	r: 75.3 	f1: 73.56 	 1146 	 1594 	 1522

[32m iter_2[0m
ga 	p: 70.09 	r: 68.79 	f1: 69.43 	 4243 	 6054 	 6168
wo 	p: 88.61 	r: 78.75 	f1: 83.39 	 2668 	 3011 	 3388
ni 	p: 77.91 	r: 70.43 	f1: 73.98 	 1072 	 1376 	 1522
best_thres [[0.39, 0.46, 0.15], [0.27, 0.29, 0.09], [0.31, 0.35, 0.16]]
f [0.7442, 0.7431, 0.7427]
load model: epoch2
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 2 	 [0.47, 0.27, 0.13] 	 lr: 0.001 	 f: 77.14937072546142
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 7 [0m
Train...

[32m iter_0[0m
ga 	p: 79.85 	r: 75.5 	f1: 77.62 	 4657 	 5832 	 6168
wo 	p: 92.17 	r: 85.8 	f1: 88.87 	 2907 	 3154 	 3388
ni 	p: 81.51 	r: 75.89 	f1: 78.6 	 1155 	 1417 	 1522

[32m iter_1[0m
ga 	p: 81.28 	r: 74.08 	f1: 77.51 	 4569 	 5621 	 6168
wo 	p: 92.62 	r: 85.57 	f1: 88.95 	 2899 	 3130 	 3388
ni 	p: 83.66 	r: 74.7 	f1: 78.93 	 1137 	 1359 	 1522

[32m iter_2[0m
ga 	p: 80.39 	r: 75.03 	f1: 77.62 	 4628 	 5757 	 6168
wo 	p: 92.45 	r: 85.63 	f1: 88.91 	 2901 	 3138 	 3388
ni 	p: 83.9 	r: 74.64 	f1: 79.0 	 1136 	 1354 	 1522
best_thres [[0.47, 0.48, 0.06], [0.55, 0.51, 0.06], [0.49, 0.51, 0.06]]
f [0.8118, 0.812, 0.8122]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse 	current best epoch 3 	 [0.47, 0.63, 0.18] 	 lr: 0.0002 	 f: 81.95906889548509
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse[0m [33m epoch 7 [0m
Train...
loss: tensor(2173.2134) lr: 0.0005 time: 2997.29
pred_count_train 41644

Test...
loss: tensor(596.2653) lr: 0.0002 time: 2970.86
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 77.63 	r: 73.07 	f1: 75.28 	 4507 	 5806 	 6168
wo 	p: 91.14 	r: 83.83 	f1: 87.33 	 2840 	 3116 	 3388
ni 	p: 82.81 	r: 75.95 	f1: 79.23 	 1156 	 1396 	 1522

[32m iter_1[0m
ga 	p: 76.87 	r: 73.93 	f1: 75.37 	 4560 	 5932 	 6168
wo 	p: 91.44 	r: 83.26 	f1: 87.16 	 2821 	 3085 	 3388
ni 	p: 80.84 	r: 77.07 	f1: 78.91 	 1173 	 1451 	 1522

[32m iter_2[0m
ga 	p: 76.58 	r: 74.22 	f1: 75.38 	 4578 	 5978 	 6168
wo 	p: 91.1 	r: 84.0 	f1: 87.41 	 2846 	 3124 	 3388
ni 	p: 84.1 	r: 74.7 	f1: 79.12 	 1137 	 1352 	 1522
best_thres [[0.39, 0.31, 0.16], [0.33, 0.3, 0.13], [0.33, 0.29, 0.18]]
f [0.7948, 0.7944, 0.7947]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 7 	 [0.33, 0.29, 0.18] 	 lr: 0.0005 	 f: 79.46769240313925
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 8 [0m
Train...

[32m iter_0[0m
ga 	p: 81.49 	r: 75.81 	f1: 78.55 	 4676 	 5738 	 6168
wo 	p: 92.59 	r: 84.8 	f1: 88.52 	 2873 	 3103 	 3388
ni 	p: 84.19 	r: 73.13 	f1: 78.27 	 1113 	 1322 	 1522

[32m iter_1[0m
ga 	p: 80.85 	r: 76.39 	f1: 78.56 	 4712 	 5828 	 6168
wo 	p: 91.03 	r: 85.63 	f1: 88.24 	 2901 	 3187 	 3388
ni 	p: 84.7 	r: 72.4 	f1: 78.07 	 1102 	 1301 	 1522

[32m iter_2[0m
ga 	p: 80.71 	r: 76.52 	f1: 78.56 	 4720 	 5848 	 6168
wo 	p: 91.66 	r: 85.6 	f1: 88.52 	 2900 	 3164 	 3388
ni 	p: 85.67 	r: 71.48 	f1: 77.94 	 1088 	 1270 	 1522
best_thres [[0.47, 0.75, 0.13], [0.41, 0.53, 0.13], [0.4, 0.68, 0.15]]
f [0.8156, 0.8152, 0.8152]
load model: epoch3
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse 	current best epoch 3 	 [0.47, 0.63, 0.18] 	 lr: 0.0002 	 f: 81.95906889548509
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse[0m [33m epoch 8 [0m
Train...
loss: tensor(1984.2236) lr: 0.0005 time: 2919.75
pred_count_train 41644

Test...
loss: tensor(1100.6798) lr: 0.0001 time: 2880.88
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 77.36 	r: 73.64 	f1: 75.45 	 4542 	 5871 	 6168
wo 	p: 91.48 	r: 84.59 	f1: 87.9 	 2866 	 3133 	 3388
ni 	p: 84.88 	r: 76.35 	f1: 80.39 	 1162 	 1369 	 1522

[32m iter_1[0m
ga 	p: 78.96 	r: 72.88 	f1: 75.79 	 4495 	 5693 	 6168
wo 	p: 92.41 	r: 83.32 	f1: 87.63 	 2823 	 3055 	 3388
ni 	p: 82.82 	r: 77.27 	f1: 79.95 	 1176 	 1420 	 1522

[32m iter_2[0m
ga 	p: 77.51 	r: 73.85 	f1: 75.63 	 4555 	 5877 	 6168
wo 	p: 91.77 	r: 84.21 	f1: 87.83 	 2853 	 3109 	 3388
ni 	p: 83.77 	r: 77.0 	f1: 80.25 	 1172 	 1399 	 1522
best_thres [[0.49, 0.45, 0.26], [0.49, 0.45, 0.23], [0.49, 0.45, 0.25]]
f [0.799, 0.7993, 0.7994]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 8 	 [0.49, 0.45, 0.25] 	 lr: 0.0005 	 f: 79.93765586034912
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 9 [0m
Train...

[32m iter_0[0m
ga 	p: 79.8 	r: 77.5 	f1: 78.63 	 4780 	 5990 	 6168
wo 	p: 93.93 	r: 85.8 	f1: 89.68 	 2907 	 3095 	 3388
ni 	p: 85.64 	r: 77.99 	f1: 81.64 	 1187 	 1386 	 1522

[32m iter_1[0m
ga 	p: 81.35 	r: 76.46 	f1: 78.83 	 4716 	 5797 	 6168
wo 	p: 93.68 	r: 86.22 	f1: 89.79 	 2921 	 3118 	 3388
ni 	p: 86.62 	r: 77.0 	f1: 81.53 	 1172 	 1353 	 1522

[32m iter_2[0m
ga 	p: 81.25 	r: 76.49 	f1: 78.8 	 4718 	 5807 	 6168
wo 	p: 93.45 	r: 86.39 	f1: 89.79 	 2927 	 3132 	 3388
ni 	p: 86.26 	r: 77.53 	f1: 81.66 	 1180 	 1368 	 1522
best_thres [[0.41, 0.68, 0.19], [0.48, 0.7, 0.19], [0.48, 0.71, 0.18]]
f [0.8236, 0.8245, 0.8248]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse 	current best epoch 8 	 [0.48, 0.71, 0.18] 	 lr: 0.0001 	 f: 82.47666459240821
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse[0m [33m epoch 9 [0m
Train...
loss: tensor(1893.0967) lr: 0.0005 time: 2947.93
pred_count_train 41644

Test...
loss: tensor(770.3642) lr: 0.0001 time: 2934.39
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 78.67 	r: 72.81 	f1: 75.63 	 4491 	 5709 	 6168
wo 	p: 91.36 	r: 83.29 	f1: 87.14 	 2822 	 3089 	 3388
ni 	p: 86.22 	r: 72.34 	f1: 78.67 	 1101 	 1277 	 1522

[32m iter_1[0m
ga 	p: 78.65 	r: 72.76 	f1: 75.59 	 4488 	 5706 	 6168
wo 	p: 90.68 	r: 83.03 	f1: 86.69 	 2813 	 3102 	 3388
ni 	p: 81.46 	r: 77.07 	f1: 79.2 	 1173 	 1440 	 1522

[32m iter_2[0m
ga 	p: 78.67 	r: 73.33 	f1: 75.91 	 4523 	 5749 	 6168
wo 	p: 92.06 	r: 82.5 	f1: 87.02 	 2795 	 3036 	 3388
ni 	p: 82.57 	r: 75.95 	f1: 79.12 	 1156 	 1400 	 1522
best_thres [[0.31, 0.32, 0.13], [0.28, 0.28, 0.08], [0.29, 0.38, 0.09]]
f [0.7955, 0.7951, 0.7958]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 8 	 [0.49, 0.45, 0.25] 	 lr: 0.0005 	 f: 79.93765586034912
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 10 [0m
Train...

[32m iter_0[0m
ga 	p: 81.83 	r: 75.15 	f1: 78.35 	 4635 	 5664 	 6168
wo 	p: 92.81 	r: 86.07 	f1: 89.31 	 2916 	 3142 	 3388
ni 	p: 86.97 	r: 74.57 	f1: 80.3 	 1135 	 1305 	 1522

[32m iter_1[0m
ga 	p: 81.85 	r: 75.39 	f1: 78.49 	 4650 	 5681 	 6168
wo 	p: 93.0 	r: 85.89 	f1: 89.3 	 2910 	 3129 	 3388
ni 	p: 85.68 	r: 75.89 	f1: 80.49 	 1155 	 1348 	 1522

[32m iter_2[0m
ga 	p: 83.66 	r: 73.96 	f1: 78.51 	 4562 	 5453 	 6168
wo 	p: 94.03 	r: 85.06 	f1: 89.32 	 2882 	 3065 	 3388
ni 	p: 85.66 	r: 76.15 	f1: 80.63 	 1159 	 1353 	 1522
best_thres [[0.51, 0.53, 0.15], [0.49, 0.52, 0.11], [0.64, 0.72, 0.11]]
f [0.8199, 0.8203, 0.8207]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse 	current best epoch 8 	 [0.48, 0.71, 0.18] 	 lr: 0.0001 	 f: 82.47666459240821
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse[0m [33m epoch 10 [0m
Train...
loss: tensor(1832.8762) lr: 0.0005 time: 2716.98
pred_count_train 41644

Test...
loss: tensor(546.3008) lr: 0.0001 time: 2700.88
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.03 	r: 73.91 	f1: 76.38 	 4559 	 5769 	 6168
wo 	p: 93.87 	r: 81.88 	f1: 87.47 	 2774 	 2955 	 3388
ni 	p: 80.77 	r: 77.0 	f1: 78.84 	 1172 	 1451 	 1522

[32m iter_1[0m
ga 	p: 79.54 	r: 73.18 	f1: 76.23 	 4514 	 5675 	 6168
wo 	p: 91.16 	r: 83.41 	f1: 87.11 	 2826 	 3100 	 3388
ni 	p: 80.62 	r: 76.81 	f1: 78.67 	 1169 	 1450 	 1522

[32m iter_2[0m
ga 	p: 78.84 	r: 73.77 	f1: 76.22 	 4550 	 5771 	 6168
wo 	p: 92.48 	r: 82.76 	f1: 87.35 	 2804 	 3032 	 3388
ni 	p: 80.34 	r: 77.86 	f1: 79.08 	 1185 	 1475 	 1522
best_thres [[0.35, 0.59, 0.19], [0.36, 0.35, 0.17], [0.35, 0.49, 0.16]]
f [0.8004, 0.7996, 0.7996]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 10 	 [0.35, 0.49, 0.16] 	 lr: 0.0005 	 f: 79.96307422706221
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 11 [0m
Train...

[32m iter_0[0m
ga 	p: 81.04 	r: 75.39 	f1: 78.11 	 4650 	 5738 	 6168
wo 	p: 91.83 	r: 87.22 	f1: 89.46 	 2955 	 3218 	 3388
ni 	p: 82.05 	r: 78.71 	f1: 80.35 	 1198 	 1460 	 1522

[32m iter_1[0m
ga 	p: 82.63 	r: 74.19 	f1: 78.18 	 4576 	 5538 	 6168
wo 	p: 92.35 	r: 86.54 	f1: 89.35 	 2932 	 3175 	 3388
ni 	p: 83.39 	r: 78.19 	f1: 80.71 	 1190 	 1427 	 1522

[32m iter_2[0m
ga 	p: 83.44 	r: 73.52 	f1: 78.17 	 4535 	 5435 	 6168
wo 	p: 92.21 	r: 86.66 	f1: 89.35 	 2936 	 3184 	 3388
ni 	p: 83.83 	r: 77.99 	f1: 80.8 	 1187 	 1416 	 1522
best_thres [[0.39, 0.26, 0.17], [0.48, 0.28, 0.17], [0.55, 0.26, 0.18]]
f [0.8191, 0.8195, 0.8197]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse 	current best epoch 8 	 [0.48, 0.71, 0.18] 	 lr: 0.0001 	 f: 82.47666459240821
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse[0m [33m epoch 11 [0m
Train...
loss: tensor(1806.8093) lr: 0.0005 time: 2876.9
pred_count_train 41644

Test...
loss: tensor(382.1754) lr: 0.0001 time: 2864.49
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 76.57 	r: 74.61 	f1: 75.58 	 4602 	 6010 	 6168
wo 	p: 91.22 	r: 84.03 	f1: 87.48 	 2847 	 3121 	 3388
ni 	p: 81.51 	r: 78.78 	f1: 80.12 	 1199 	 1471 	 1522

[32m iter_1[0m
ga 	p: 78.66 	r: 72.68 	f1: 75.55 	 4483 	 5699 	 6168
wo 	p: 92.81 	r: 81.91 	f1: 87.02 	 2775 	 2990 	 3388
ni 	p: 82.44 	r: 76.48 	f1: 79.35 	 1164 	 1412 	 1522

[32m iter_2[0m
ga 	p: 78.48 	r: 73.04 	f1: 75.66 	 4505 	 5740 	 6168
wo 	p: 92.76 	r: 82.79 	f1: 87.49 	 2805 	 3024 	 3388
ni 	p: 81.66 	r: 78.38 	f1: 79.99 	 1193 	 1461 	 1522
best_thres [[0.35, 0.68, 0.2], [0.4, 0.78, 0.22], [0.43, 0.8, 0.2]]
f [0.7978, 0.7966, 0.7971]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 10 	 [0.35, 0.49, 0.16] 	 lr: 0.0005 	 f: 79.96307422706221
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 12 [0m
Train...

[32m iter_0[0m
ga 	p: 81.17 	r: 75.26 	f1: 78.1 	 4642 	 5719 	 6168
wo 	p: 94.13 	r: 84.24 	f1: 88.91 	 2854 	 3032 	 3388
ni 	p: 83.23 	r: 75.95 	f1: 79.42 	 1156 	 1389 	 1522

[32m iter_1[0m
ga 	p: 82.52 	r: 74.24 	f1: 78.16 	 4579 	 5549 	 6168
wo 	p: 92.25 	r: 85.74 	f1: 88.88 	 2905 	 3149 	 3388
ni 	p: 83.69 	r: 75.49 	f1: 79.38 	 1149 	 1373 	 1522

[32m iter_2[0m
ga 	p: 82.15 	r: 74.63 	f1: 78.21 	 4603 	 5603 	 6168
wo 	p: 91.15 	r: 86.66 	f1: 88.85 	 2936 	 3221 	 3388
ni 	p: 83.32 	r: 75.82 	f1: 79.39 	 1154 	 1385 	 1522
best_thres [[0.36, 0.83, 0.25], [0.44, 0.83, 0.23], [0.41, 0.49, 0.21]]
f [0.8155, 0.816, 0.8162]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse 	current best epoch 8 	 [0.48, 0.71, 0.18] 	 lr: 0.0001 	 f: 82.47666459240821
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse[0m [33m epoch 12 [0m
Train...
loss: tensor(1780.1199) lr: 0.0005 time: 2647.49
pred_count_train 41644

Test...
loss: tensor(292.6564) lr: 0.0001 time: 2652.56
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.77 	r: 71.98 	f1: 75.68 	 4440 	 5566 	 6168
wo 	p: 91.46 	r: 83.12 	f1: 87.09 	 2816 	 3079 	 3388
ni 	p: 83.03 	r: 75.23 	f1: 78.94 	 1145 	 1379 	 1522

[32m iter_1[0m
ga 	p: 79.39 	r: 72.24 	f1: 75.65 	 4456 	 5613 	 6168
wo 	p: 91.55 	r: 82.5 	f1: 86.79 	 2795 	 3053 	 3388
ni 	p: 85.09 	r: 74.97 	f1: 79.71 	 1141 	 1341 	 1522

[32m iter_2[0m
ga 	p: 78.11 	r: 73.51 	f1: 75.74 	 4534 	 5805 	 6168
wo 	p: 90.99 	r: 83.12 	f1: 86.87 	 2816 	 3095 	 3388
ni 	p: 82.07 	r: 77.0 	f1: 79.46 	 1172 	 1428 	 1522
best_thres [[0.59, 0.27, 0.18], [0.55, 0.26, 0.2], [0.52, 0.24, 0.14]]
f [0.7962, 0.7961, 0.7962]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 10 	 [0.35, 0.49, 0.16] 	 lr: 0.0005 	 f: 79.96307422706221
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 13 [0m
Train...

[32m iter_0[0m
ga 	p: 80.39 	r: 76.15 	f1: 78.21 	 4697 	 5843 	 6168
wo 	p: 92.42 	r: 86.69 	f1: 89.46 	 2937 	 3178 	 3388
ni 	p: 85.77 	r: 73.26 	f1: 79.02 	 1115 	 1300 	 1522

[32m iter_1[0m
ga 	p: 80.72 	r: 75.73 	f1: 78.14 	 4671 	 5787 	 6168
wo 	p: 92.11 	r: 86.81 	f1: 89.38 	 2941 	 3193 	 3388
ni 	p: 84.84 	r: 73.92 	f1: 79.0 	 1125 	 1326 	 1522

[32m iter_2[0m
ga 	p: 80.68 	r: 75.68 	f1: 78.1 	 4668 	 5786 	 6168
wo 	p: 92.26 	r: 86.57 	f1: 89.33 	 2933 	 3179 	 3388
ni 	p: 85.59 	r: 73.39 	f1: 79.02 	 1117 	 1305 	 1522
best_thres [[0.47, 0.65, 0.16], [0.52, 0.65, 0.12], [0.53, 0.79, 0.13]]
f [0.8177, 0.8174, 0.8172]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse 	current best epoch 8 	 [0.48, 0.71, 0.18] 	 lr: 0.0001 	 f: 82.47666459240821
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse[0m [33m epoch 13 [0m
Train...
loss: tensor(1760.2836) lr: 0.0005 time: 2855.1
pred_count_train 41644

Test...
loss: tensor(663.8702) lr: 5e-05 time: 2837.07
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.63 	r: 71.48 	f1: 75.78 	 4409 	 5468 	 6168
wo 	p: 93.84 	r: 82.26 	f1: 87.67 	 2787 	 2970 	 3388
ni 	p: 78.06 	r: 78.06 	f1: 78.06 	 1188 	 1522 	 1522

[32m iter_1[0m
ga 	p: 78.25 	r: 73.22 	f1: 75.65 	 4516 	 5771 	 6168
wo 	p: 92.33 	r: 83.15 	f1: 87.5 	 2817 	 3051 	 3388
ni 	p: 79.65 	r: 77.4 	f1: 78.51 	 1178 	 1479 	 1522

[32m iter_2[0m
ga 	p: 82.28 	r: 70.23 	f1: 75.78 	 4332 	 5265 	 6168
wo 	p: 93.8 	r: 82.11 	f1: 87.57 	 2782 	 2966 	 3388
ni 	p: 80.25 	r: 76.61 	f1: 78.39 	 1166 	 1453 	 1522
best_thres [[0.39, 0.49, 0.07], [0.28, 0.35, 0.08], [0.44, 0.47, 0.09]]
f [0.797, 0.7966, 0.7969]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 10 	 [0.35, 0.49, 0.16] 	 lr: 0.0005 	 f: 79.96307422706221
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 14 [0m
Train...

[32m iter_0[0m
ga 	p: 83.02 	r: 75.15 	f1: 78.89 	 4635 	 5583 	 6168
wo 	p: 92.04 	r: 87.34 	f1: 89.63 	 2959 	 3215 	 3388
ni 	p: 85.0 	r: 77.79 	f1: 81.23 	 1184 	 1393 	 1522

[32m iter_1[0m
ga 	p: 83.34 	r: 74.68 	f1: 78.77 	 4606 	 5527 	 6168
wo 	p: 92.19 	r: 87.46 	f1: 89.76 	 2963 	 3214 	 3388
ni 	p: 87.45 	r: 76.02 	f1: 81.34 	 1157 	 1323 	 1522

[32m iter_2[0m
ga 	p: 83.84 	r: 74.43 	f1: 78.86 	 4591 	 5476 	 6168
wo 	p: 92.11 	r: 87.57 	f1: 89.79 	 2967 	 3221 	 3388
ni 	p: 87.44 	r: 75.95 	f1: 81.29 	 1156 	 1322 	 1522
best_thres [[0.47, 0.44, 0.16], [0.48, 0.43, 0.19], [0.5, 0.41, 0.19]]
f [0.8254, 0.8254, 0.8257]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse 	current best epoch 13 	 [0.5, 0.41, 0.19] 	 lr: 5e-05 	 f: 82.56597594003904
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse[0m [33m epoch 14 [0m
Train...
loss: tensor(1743.2338) lr: 0.0005 time: 2655.98
pred_count_train 41644

Test...
loss: tensor(457.5195) lr: 5e-05 time: 2677.05
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.24 	r: 72.34 	f1: 75.63 	 4462 	 5631 	 6168
wo 	p: 90.61 	r: 84.0 	f1: 87.18 	 2846 	 3141 	 3388
ni 	p: 84.52 	r: 74.97 	f1: 79.46 	 1141 	 1350 	 1522

[32m iter_1[0m
ga 	p: 78.59 	r: 72.47 	f1: 75.4 	 4470 	 5688 	 6168
wo 	p: 91.05 	r: 83.5 	f1: 87.11 	 2829 	 3107 	 3388
ni 	p: 81.97 	r: 75.3 	f1: 78.49 	 1146 	 1398 	 1522

[32m iter_2[0m
ga 	p: 78.68 	r: 72.92 	f1: 75.69 	 4498 	 5717 	 6168
wo 	p: 92.23 	r: 82.29 	f1: 86.98 	 2788 	 3023 	 3388
ni 	p: 81.92 	r: 77.99 	f1: 79.91 	 1187 	 1449 	 1522
best_thres [[0.39, 0.39, 0.3], [0.33, 0.39, 0.23], [0.35, 0.54, 0.22]]
f [0.7971, 0.7956, 0.796]
load model: epoch10
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 10 	 [0.35, 0.49, 0.16] 	 lr: 0.0005 	 f: 79.96307422706221
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 15 [0m
Train...

[32m iter_0[0m
ga 	p: 79.56 	r: 76.62 	f1: 78.06 	 4726 	 5940 	 6168
wo 	p: 92.6 	r: 86.1 	f1: 89.23 	 2917 	 3150 	 3388
ni 	p: 86.19 	r: 75.43 	f1: 80.45 	 1148 	 1332 	 1522

[32m iter_1[0m
ga 	p: 80.67 	r: 75.65 	f1: 78.08 	 4666 	 5784 	 6168
wo 	p: 92.99 	r: 85.77 	f1: 89.24 	 2906 	 3125 	 3388
ni 	p: 86.43 	r: 75.3 	f1: 80.48 	 1146 	 1326 	 1522

[32m iter_2[0m
ga 	p: 80.79 	r: 75.57 	f1: 78.09 	 4661 	 5769 	 6168
wo 	p: 93.12 	r: 85.92 	f1: 89.38 	 2911 	 3126 	 3388
ni 	p: 85.99 	r: 75.82 	f1: 80.59 	 1154 	 1342 	 1522
best_thres [[0.27, 0.49, 0.26], [0.32, 0.54, 0.22], [0.33, 0.54, 0.2]]
f [0.8178, 0.8179, 0.8182]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse 	current best epoch 13 	 [0.5, 0.41, 0.19] 	 lr: 5e-05 	 f: 82.56597594003904
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse[0m [33m epoch 15 [0m
Train...
loss: tensor(1549.8312) lr: 0.00025 time: 2800.93
pred_count_train 41644

Test...
loss: tensor(314.9839) lr: 5e-05 time: 2779.53
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.46 	r: 72.91 	f1: 76.5 	 4497 	 5589 	 6168
wo 	p: 92.48 	r: 83.44 	f1: 87.73 	 2827 	 3057 	 3388
ni 	p: 82.94 	r: 78.25 	f1: 80.53 	 1191 	 1436 	 1522

[32m iter_1[0m
ga 	p: 77.78 	r: 75.37 	f1: 76.56 	 4649 	 5977 	 6168
wo 	p: 92.88 	r: 83.15 	f1: 87.74 	 2817 	 3033 	 3388
ni 	p: 85.29 	r: 76.94 	f1: 80.9 	 1171 	 1373 	 1522

[32m iter_2[0m
ga 	p: 80.96 	r: 72.67 	f1: 76.59 	 4482 	 5536 	 6168
wo 	p: 92.5 	r: 83.74 	f1: 87.9 	 2837 	 3067 	 3388
ni 	p: 84.56 	r: 78.06 	f1: 81.18 	 1188 	 1405 	 1522
best_thres [[0.45, 0.44, 0.23], [0.31, 0.44, 0.27], [0.47, 0.43, 0.24]]
f [0.8048, 0.8049, 0.8055]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 15 	 [0.47, 0.43, 0.24] 	 lr: 0.00025 	 f: 80.55315742383098
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 16 [0m
Train...

[32m iter_0[0m
ga 	p: 80.73 	r: 75.26 	f1: 77.9 	 4642 	 5750 	 6168
wo 	p: 92.59 	r: 86.98 	f1: 89.7 	 2947 	 3183 	 3388
ni 	p: 82.49 	r: 74.9 	f1: 78.51 	 1140 	 1382 	 1522

[32m iter_1[0m
ga 	p: 79.48 	r: 76.15 	f1: 77.78 	 4697 	 5910 	 6168
wo 	p: 92.48 	r: 87.07 	f1: 89.69 	 2950 	 3190 	 3388
ni 	p: 81.23 	r: 77.33 	f1: 79.23 	 1177 	 1449 	 1522

[32m iter_2[0m
ga 	p: 79.35 	r: 76.3 	f1: 77.79 	 4706 	 5931 	 6168
wo 	p: 92.53 	r: 87.07 	f1: 89.72 	 2950 	 3188 	 3388
ni 	p: 82.06 	r: 76.94 	f1: 79.42 	 1171 	 1427 	 1522
best_thres [[0.42, 0.39, 0.19], [0.32, 0.37, 0.11], [0.3, 0.37, 0.12]]
f [0.8161, 0.816, 0.8162]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse 	current best epoch 13 	 [0.5, 0.41, 0.19] 	 lr: 5e-05 	 f: 82.56597594003904
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse[0m [33m epoch 16 [0m
Train...
loss: tensor(1314.0693) lr: 0.00025 time: 2723.93
pred_count_train 41644

Test...
loss: tensor(212.8234) lr: 5e-05 time: 2834.03
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 78.94 	r: 73.9 	f1: 76.34 	 4558 	 5774 	 6168
wo 	p: 91.44 	r: 84.83 	f1: 88.01 	 2874 	 3143 	 3388
ni 	p: 83.22 	r: 74.97 	f1: 78.88 	 1141 	 1371 	 1522

[32m iter_1[0m
ga 	p: 79.43 	r: 74.27 	f1: 76.77 	 4581 	 5767 	 6168
wo 	p: 91.73 	r: 84.47 	f1: 87.95 	 2862 	 3120 	 3388
ni 	p: 82.76 	r: 75.69 	f1: 79.07 	 1152 	 1392 	 1522

[32m iter_2[0m
ga 	p: 79.57 	r: 74.17 	f1: 76.77 	 4575 	 5750 	 6168
wo 	p: 93.92 	r: 82.94 	f1: 88.09 	 2810 	 2992 	 3388
ni 	p: 84.16 	r: 74.7 	f1: 79.15 	 1137 	 1351 	 1522
best_thres [[0.27, 0.43, 0.21], [0.27, 0.4, 0.17], [0.28, 0.69, 0.21]]
f [0.8025, 0.8037, 0.8041]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 15 	 [0.47, 0.43, 0.24] 	 lr: 0.00025 	 f: 80.55315742383098
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 17 [0m
Train...

[32m iter_0[0m
ga 	p: 81.59 	r: 74.89 	f1: 78.1 	 4619 	 5661 	 6168
wo 	p: 93.53 	r: 85.27 	f1: 89.21 	 2889 	 3089 	 3388
ni 	p: 83.42 	r: 73.39 	f1: 78.08 	 1117 	 1339 	 1522

[32m iter_1[0m
ga 	p: 80.63 	r: 75.63 	f1: 78.05 	 4665 	 5786 	 6168
wo 	p: 92.5 	r: 86.25 	f1: 89.26 	 2922 	 3159 	 3388
ni 	p: 86.02 	r: 71.55 	f1: 78.12 	 1089 	 1266 	 1522

[32m iter_2[0m
ga 	p: 81.15 	r: 75.11 	f1: 78.02 	 4633 	 5709 	 6168
wo 	p: 92.46 	r: 86.19 	f1: 89.21 	 2920 	 3158 	 3388
ni 	p: 86.26 	r: 71.75 	f1: 78.34 	 1092 	 1266 	 1522
best_thres [[0.21, 0.64, 0.15], [0.14, 0.51, 0.15], [0.17, 0.51, 0.15]]
f [0.8149, 0.815, 0.8151]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse 	current best epoch 13 	 [0.5, 0.41, 0.19] 	 lr: 5e-05 	 f: 82.56597594003904
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse[0m [33m epoch 17 [0m
Train...
loss: tensor(1172.7782) lr: 0.00025 time: 3089.66
pred_count_train 41644

Test...
loss: tensor(152.9150) lr: 5e-05 time: 3093.45
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.49 	r: 72.37 	f1: 75.76 	 4464 	 5616 	 6168
wo 	p: 91.05 	r: 84.42 	f1: 87.61 	 2860 	 3141 	 3388
ni 	p: 81.73 	r: 76.41 	f1: 78.98 	 1163 	 1423 	 1522

[32m iter_1[0m
ga 	p: 78.24 	r: 74.34 	f1: 76.24 	 4585 	 5860 	 6168
wo 	p: 92.37 	r: 83.26 	f1: 87.58 	 2821 	 3054 	 3388
ni 	p: 82.44 	r: 75.56 	f1: 78.85 	 1150 	 1395 	 1522

[32m iter_2[0m
ga 	p: 79.91 	r: 72.8 	f1: 76.19 	 4490 	 5619 	 6168
wo 	p: 91.37 	r: 84.39 	f1: 87.74 	 2859 	 3129 	 3388
ni 	p: 80.97 	r: 76.87 	f1: 78.87 	 1170 	 1445 	 1522
best_thres [[0.51, 0.47, 0.14], [0.38, 0.53, 0.15], [0.5, 0.47, 0.13]]
f [0.7985, 0.7993, 0.7999]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 15 	 [0.47, 0.43, 0.24] 	 lr: 0.00025 	 f: 80.55315742383098
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 18 [0m
Train...

[32m iter_0[0m
ga 	p: 81.11 	r: 75.73 	f1: 78.33 	 4671 	 5759 	 6168
wo 	p: 94.26 	r: 84.89 	f1: 89.33 	 2876 	 3051 	 3388
ni 	p: 79.37 	r: 73.32 	f1: 76.23 	 1116 	 1406 	 1522

[32m iter_1[0m
ga 	p: 82.31 	r: 74.66 	f1: 78.3 	 4605 	 5595 	 6168
wo 	p: 91.97 	r: 87.16 	f1: 89.5 	 2953 	 3211 	 3388
ni 	p: 83.5 	r: 71.81 	f1: 77.22 	 1093 	 1309 	 1522

[32m iter_2[0m
ga 	p: 82.32 	r: 74.74 	f1: 78.35 	 4610 	 5600 	 6168
wo 	p: 92.61 	r: 86.57 	f1: 89.49 	 2933 	 3167 	 3388
ni 	p: 83.58 	r: 71.88 	f1: 77.29 	 1094 	 1309 	 1522
best_thres [[0.29, 0.79, 0.11], [0.36, 0.47, 0.11], [0.36, 0.58, 0.1]]
f [0.8137, 0.815, 0.8155]
load model: epoch13
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse 	current best epoch 13 	 [0.5, 0.41, 0.19] 	 lr: 5e-05 	 f: 82.56597594003904
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse[0m [33m epoch 18 [0m
Train...
loss: tensor(1056.7339) lr: 0.00025 time: 2812.3
pred_count_train 41644

Test...
loss: tensor(408.8759) lr: 2.5e-05 time: 2872.42
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.54 	r: 72.62 	f1: 75.92 	 4479 	 5631 	 6168
wo 	p: 91.72 	r: 84.36 	f1: 87.88 	 2858 	 3116 	 3388
ni 	p: 83.83 	r: 75.95 	f1: 79.7 	 1156 	 1379 	 1522

[32m iter_1[0m
ga 	p: 80.01 	r: 72.55 	f1: 76.1 	 4475 	 5593 	 6168
wo 	p: 92.3 	r: 83.8 	f1: 87.84 	 2839 	 3076 	 3388
ni 	p: 82.82 	r: 76.68 	f1: 79.63 	 1167 	 1409 	 1522

[32m iter_2[0m
ga 	p: 79.05 	r: 73.83 	f1: 76.35 	 4554 	 5761 	 6168
wo 	p: 92.15 	r: 84.21 	f1: 88.0 	 2853 	 3096 	 3388
ni 	p: 82.75 	r: 77.53 	f1: 80.05 	 1180 	 1426 	 1522
best_thres [[0.61, 0.39, 0.24], [0.59, 0.42, 0.2], [0.61, 0.42, 0.19]]
f [0.8011, 0.8014, 0.8023]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 15 	 [0.47, 0.43, 0.24] 	 lr: 0.00025 	 f: 80.55315742383098
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 19 [0m
Train...

[32m iter_0[0m
ga 	p: 80.99 	r: 76.44 	f1: 78.65 	 4715 	 5822 	 6168
wo 	p: 93.24 	r: 86.33 	f1: 89.66 	 2925 	 3137 	 3388
ni 	p: 85.77 	r: 74.84 	f1: 79.93 	 1139 	 1328 	 1522

[32m iter_1[0m
ga 	p: 80.7 	r: 76.88 	f1: 78.74 	 4742 	 5876 	 6168
wo 	p: 93.51 	r: 86.33 	f1: 89.78 	 2925 	 3128 	 3388
ni 	p: 82.0 	r: 78.12 	f1: 80.01 	 1189 	 1450 	 1522

[32m iter_2[0m
ga 	p: 80.88 	r: 76.65 	f1: 78.71 	 4728 	 5846 	 6168
wo 	p: 93.51 	r: 86.36 	f1: 89.8 	 2926 	 3129 	 3388
ni 	p: 85.29 	r: 75.82 	f1: 80.28 	 1154 	 1353 	 1522
best_thres [[0.37, 0.49, 0.17], [0.33, 0.51, 0.07], [0.35, 0.51, 0.1]]
f [0.8218, 0.8222, 0.8224]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse 	current best epoch 13 	 [0.5, 0.41, 0.19] 	 lr: 5e-05 	 f: 82.56597594003904
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse[0m [33m epoch 19 [0m
Train...
loss: tensor(964.6786) lr: 0.00025 time: 2915.94
pred_count_train 41644

Test...
loss: tensor(294.7228) lr: 2.5e-05 time: 2916.79
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 78.83 	r: 73.49 	f1: 76.07 	 4533 	 5750 	 6168
wo 	p: 91.71 	r: 84.21 	f1: 87.8 	 2853 	 3111 	 3388
ni 	p: 82.21 	r: 73.78 	f1: 77.77 	 1123 	 1366 	 1522

[32m iter_1[0m
ga 	p: 79.59 	r: 73.61 	f1: 76.48 	 4540 	 5704 	 6168
wo 	p: 92.44 	r: 84.03 	f1: 88.03 	 2847 	 3080 	 3388
ni 	p: 84.79 	r: 72.14 	f1: 77.96 	 1098 	 1295 	 1522

[32m iter_2[0m
ga 	p: 78.89 	r: 74.35 	f1: 76.55 	 4586 	 5813 	 6168
wo 	p: 92.68 	r: 83.71 	f1: 87.97 	 2836 	 3060 	 3388
ni 	p: 84.65 	r: 72.47 	f1: 78.09 	 1103 	 1303 	 1522
best_thres [[0.4, 0.46, 0.14], [0.4, 0.47, 0.17], [0.38, 0.59, 0.16]]
f [0.7988, 0.8004, 0.801]
load model: epoch15
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 15 	 [0.47, 0.43, 0.24] 	 lr: 0.00025 	 f: 80.55315742383098
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 20 [0m
Train...

[32m iter_0[0m
ga 	p: 82.4 	r: 74.63 	f1: 78.32 	 4603 	 5586 	 6168
wo 	p: 92.78 	r: 86.54 	f1: 89.55 	 2932 	 3160 	 3388
ni 	p: 83.14 	r: 74.51 	f1: 78.59 	 1134 	 1364 	 1522

[32m iter_1[0m
ga 	p: 82.34 	r: 74.71 	f1: 78.34 	 4608 	 5596 	 6168
wo 	p: 93.88 	r: 85.54 	f1: 89.51 	 2898 	 3087 	 3388
ni 	p: 82.69 	r: 75.03 	f1: 78.68 	 1142 	 1381 	 1522

[32m iter_2[0m
ga 	p: 82.36 	r: 74.77 	f1: 78.38 	 4612 	 5600 	 6168
wo 	p: 93.76 	r: 85.66 	f1: 89.53 	 2902 	 3095 	 3388
ni 	p: 83.46 	r: 74.9 	f1: 78.95 	 1140 	 1366 	 1522
best_thres [[0.42, 0.46, 0.12], [0.41, 0.7, 0.08], [0.41, 0.73, 0.08]]
f [0.8183, 0.8182, 0.8184]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse 	current best epoch 13 	 [0.5, 0.41, 0.19] 	 lr: 5e-05 	 f: 82.56597594003904
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse[0m [33m epoch 20 [0m
Train...
loss: tensor(1174.8080) lr: 0.000125 time: 2659.64
pred_count_train 41644

Test...
loss: tensor(212.3707) lr: 2.5e-05 time: 2694.68
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 78.81 	r: 74.35 	f1: 76.52 	 4586 	 5819 	 6168
wo 	p: 92.05 	r: 84.03 	f1: 87.86 	 2847 	 3093 	 3388
ni 	p: 85.09 	r: 77.27 	f1: 80.99 	 1176 	 1382 	 1522

[32m iter_1[0m
ga 	p: 79.41 	r: 74.51 	f1: 76.88 	 4596 	 5788 	 6168
wo 	p: 92.41 	r: 84.03 	f1: 88.02 	 2847 	 3081 	 3388
ni 	p: 82.49 	r: 78.91 	f1: 80.66 	 1201 	 1456 	 1522

[32m iter_2[0m
ga 	p: 79.62 	r: 74.22 	f1: 76.82 	 4578 	 5750 	 6168
wo 	p: 92.58 	r: 83.65 	f1: 87.89 	 2834 	 3061 	 3388
ni 	p: 82.39 	r: 79.3 	f1: 80.82 	 1207 	 1465 	 1522
best_thres [[0.38, 0.54, 0.18], [0.37, 0.55, 0.14], [0.4, 0.65, 0.13]]
f [0.8056, 0.8067, 0.8069]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 20 	 [0.4, 0.65, 0.13] 	 lr: 0.000125 	 f: 80.68736453086747
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 21 [0m
Train...

[32m iter_0[0m
ga 	p: 81.15 	r: 74.55 	f1: 77.71 	 4598 	 5666 	 6168
wo 	p: 92.42 	r: 86.69 	f1: 89.46 	 2937 	 3178 	 3388
ni 	p: 83.92 	r: 73.39 	f1: 78.3 	 1117 	 1331 	 1522

[32m iter_1[0m
ga 	p: 81.93 	r: 74.01 	f1: 77.77 	 4565 	 5572 	 6168
wo 	p: 93.38 	r: 85.8 	f1: 89.43 	 2907 	 3113 	 3388
ni 	p: 82.42 	r: 75.16 	f1: 78.63 	 1144 	 1388 	 1522

[32m iter_2[0m
ga 	p: 80.8 	r: 74.98 	f1: 77.78 	 4625 	 5724 	 6168
wo 	p: 93.41 	r: 85.77 	f1: 89.43 	 2906 	 3111 	 3388
ni 	p: 81.73 	r: 75.82 	f1: 78.66 	 1154 	 1412 	 1522
best_thres [[0.37, 0.4, 0.11], [0.4, 0.54, 0.06], [0.32, 0.54, 0.05]]
f [0.8142, 0.8145, 0.8145]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse 	current best epoch 13 	 [0.5, 0.41, 0.19] 	 lr: 5e-05 	 f: 82.56597594003904
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse[0m [33m epoch 21 [0m
Train...
loss: tensor(903.1949) lr: 0.000125 time: 2892.77
pred_count_train 41644

Test...
loss: tensor(151.0121) lr: 2.5e-05 time: 2803.41
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.85 	r: 72.21 	f1: 76.29 	 4454 	 5509 	 6168
wo 	p: 92.14 	r: 84.12 	f1: 87.95 	 2850 	 3093 	 3388
ni 	p: 84.44 	r: 74.9 	f1: 79.39 	 1140 	 1350 	 1522

[32m iter_1[0m
ga 	p: 78.61 	r: 73.82 	f1: 76.14 	 4553 	 5792 	 6168
wo 	p: 92.77 	r: 83.68 	f1: 87.99 	 2835 	 3056 	 3388
ni 	p: 83.27 	r: 75.23 	f1: 79.05 	 1145 	 1375 	 1522

[32m iter_2[0m
ga 	p: 81.43 	r: 71.6 	f1: 76.2 	 4416 	 5423 	 6168
wo 	p: 92.38 	r: 84.06 	f1: 88.02 	 2848 	 3083 	 3388
ni 	p: 84.59 	r: 74.64 	f1: 79.3 	 1136 	 1343 	 1522
best_thres [[0.42, 0.59, 0.14], [0.29, 0.72, 0.12], [0.44, 0.73, 0.13]]
f [0.803, 0.8021, 0.8023]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 20 	 [0.4, 0.65, 0.13] 	 lr: 0.000125 	 f: 80.68736453086747
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 22 [0m
Train...

[32m iter_0[0m
ga 	p: 81.61 	r: 74.09 	f1: 77.67 	 4570 	 5600 	 6168
wo 	p: 92.84 	r: 85.33 	f1: 88.93 	 2891 	 3114 	 3388
ni 	p: 84.35 	r: 71.55 	f1: 77.43 	 1089 	 1291 	 1522

[32m iter_1[0m
ga 	p: 81.73 	r: 73.82 	f1: 77.57 	 4553 	 5571 	 6168
wo 	p: 92.67 	r: 85.77 	f1: 89.09 	 2906 	 3136 	 3388
ni 	p: 83.04 	r: 73.65 	f1: 78.06 	 1121 	 1350 	 1522

[32m iter_2[0m
ga 	p: 81.69 	r: 73.85 	f1: 77.57 	 4555 	 5576 	 6168
wo 	p: 92.76 	r: 85.8 	f1: 89.14 	 2907 	 3134 	 3388
ni 	p: 84.19 	r: 72.8 	f1: 78.08 	 1108 	 1316 	 1522
best_thres [[0.42, 0.61, 0.13], [0.44, 0.56, 0.06], [0.44, 0.56, 0.07]]
f [0.8111, 0.8115, 0.8117]
load model: epoch13
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse 	current best epoch 13 	 [0.5, 0.41, 0.19] 	 lr: 5e-05 	 f: 82.56597594003904
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse[0m [33m epoch 22 [0m
Train...
loss: tensor(716.2811) lr: 0.000125 time: 2521.49
pred_count_train 41644

Test...
loss: tensor(393.9432) lr: 1.25e-05 time: 2483.31
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.3 	r: 72.97 	f1: 76.0 	 4501 	 5676 	 6168
wo 	p: 90.54 	r: 85.01 	f1: 87.68 	 2880 	 3181 	 3388
ni 	p: 85.63 	r: 73.98 	f1: 79.38 	 1126 	 1315 	 1522

[32m iter_1[0m
ga 	p: 80.32 	r: 72.54 	f1: 76.23 	 4474 	 5570 	 6168
wo 	p: 93.6 	r: 82.44 	f1: 87.66 	 2793 	 2984 	 3388
ni 	p: 83.6 	r: 76.35 	f1: 79.81 	 1162 	 1390 	 1522

[32m iter_2[0m
ga 	p: 82.2 	r: 71.0 	f1: 76.19 	 4379 	 5327 	 6168
wo 	p: 91.61 	r: 84.47 	f1: 87.9 	 2862 	 3124 	 3388
ni 	p: 84.31 	r: 76.28 	f1: 80.1 	 1161 	 1377 	 1522
best_thres [[0.4, 0.29, 0.18], [0.43, 0.71, 0.12], [0.69, 0.36, 0.12]]
f [0.8007, 0.8013, 0.8021]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 20 	 [0.4, 0.65, 0.13] 	 lr: 0.000125 	 f: 80.68736453086747
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 23 [0m
Train...

[32m iter_0[0m
ga 	p: 83.37 	r: 74.53 	f1: 78.7 	 4597 	 5514 	 6168
wo 	p: 93.44 	r: 86.57 	f1: 89.87 	 2933 	 3139 	 3388
ni 	p: 85.1 	r: 75.43 	f1: 79.97 	 1148 	 1349 	 1522

[32m iter_1[0m
ga 	p: 80.81 	r: 77.03 	f1: 78.87 	 4751 	 5879 	 6168
wo 	p: 93.52 	r: 86.48 	f1: 89.86 	 2930 	 3133 	 3388
ni 	p: 87.11 	r: 74.57 	f1: 80.35 	 1135 	 1303 	 1522

[32m iter_2[0m
ga 	p: 80.19 	r: 77.69 	f1: 78.92 	 4792 	 5976 	 6168
wo 	p: 93.43 	r: 86.51 	f1: 89.84 	 2931 	 3137 	 3388
ni 	p: 87.42 	r: 74.44 	f1: 80.41 	 1133 	 1296 	 1522
best_thres [[0.42, 0.56, 0.15], [0.26, 0.57, 0.15], [0.23, 0.55, 0.15]]
f [0.8233, 0.8238, 0.824]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse 	current best epoch 13 	 [0.5, 0.41, 0.19] 	 lr: 5e-05 	 f: 82.56597594003904
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse[0m [33m epoch 23 [0m
Train...
loss: tensor(584.9389) lr: 0.000125 time: 2564.76
pred_count_train 41644

Test...
loss: tensor(314.0391) lr: 1.25e-05 time: 2590.79
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 78.33 	r: 73.22 	f1: 75.69 	 4516 	 5765 	 6168
wo 	p: 93.58 	r: 82.56 	f1: 87.72 	 2797 	 2989 	 3388
ni 	p: 80.38 	r: 74.84 	f1: 77.51 	 1139 	 1417 	 1522

[32m iter_1[0m
ga 	p: 78.51 	r: 73.38 	f1: 75.86 	 4526 	 5765 	 6168
wo 	p: 91.51 	r: 84.33 	f1: 87.77 	 2857 	 3122 	 3388
ni 	p: 83.28 	r: 72.67 	f1: 77.61 	 1106 	 1328 	 1522

[32m iter_2[0m
ga 	p: 81.41 	r: 71.13 	f1: 75.92 	 4387 	 5389 	 6168
wo 	p: 92.47 	r: 83.71 	f1: 87.87 	 2836 	 3067 	 3388
ni 	p: 80.48 	r: 74.24 	f1: 77.24 	 1130 	 1404 	 1522
best_thres [[0.34, 0.79, 0.13], [0.33, 0.61, 0.17], [0.62, 0.82, 0.12]]
f [0.7955, 0.7964, 0.7969]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 20 	 [0.4, 0.65, 0.13] 	 lr: 0.000125 	 f: 80.68736453086747
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 24 [0m
Train...

[32m iter_0[0m
ga 	p: 81.92 	r: 75.06 	f1: 78.34 	 4630 	 5652 	 6168
wo 	p: 93.31 	r: 86.48 	f1: 89.77 	 2930 	 3140 	 3388
ni 	p: 81.25 	r: 77.14 	f1: 79.14 	 1174 	 1445 	 1522

[32m iter_1[0m
ga 	p: 81.99 	r: 74.84 	f1: 78.25 	 4616 	 5630 	 6168
wo 	p: 93.59 	r: 86.22 	f1: 89.75 	 2921 	 3121 	 3388
ni 	p: 84.68 	r: 74.84 	f1: 79.46 	 1139 	 1345 	 1522

[32m iter_2[0m
ga 	p: 82.06 	r: 74.9 	f1: 78.32 	 4620 	 5630 	 6168
wo 	p: 93.6 	r: 86.28 	f1: 89.79 	 2923 	 3123 	 3388
ni 	p: 83.24 	r: 76.02 	f1: 79.46 	 1157 	 1390 	 1522
best_thres [[0.41, 0.54, 0.13], [0.41, 0.6, 0.15], [0.41, 0.6, 0.12]]
f [0.8195, 0.8195, 0.8197]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse 	current best epoch 13 	 [0.5, 0.41, 0.19] 	 lr: 5e-05 	 f: 82.56597594003904
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse[0m [33m epoch 24 [0m
Train...
loss: tensor(492.8534) lr: 0.000125 time: 2527.19
pred_count_train 41644

Test...
loss: tensor(256.1044) lr: 1.25e-05 time: 2439.03
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 78.49 	r: 71.89 	f1: 75.04 	 4434 	 5649 	 6168
wo 	p: 91.6 	r: 83.68 	f1: 87.46 	 2835 	 3095 	 3388
ni 	p: 81.21 	r: 75.23 	f1: 78.1 	 1145 	 1410 	 1522

[32m iter_1[0m
ga 	p: 79.65 	r: 71.85 	f1: 75.55 	 4432 	 5564 	 6168
wo 	p: 92.87 	r: 83.41 	f1: 87.89 	 2826 	 3043 	 3388
ni 	p: 82.54 	r: 73.59 	f1: 77.8 	 1120 	 1357 	 1522

[32m iter_2[0m
ga 	p: 79.2 	r: 72.42 	f1: 75.66 	 4467 	 5640 	 6168
wo 	p: 92.56 	r: 83.71 	f1: 87.91 	 2836 	 3064 	 3388
ni 	p: 83.28 	r: 73.65 	f1: 78.17 	 1121 	 1346 	 1522
best_thres [[0.52, 0.47, 0.12], [0.64, 0.63, 0.14], [0.65, 0.59, 0.14]]
f [0.7926, 0.7944, 0.7954]
load model: epoch20
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 20 	 [0.4, 0.65, 0.13] 	 lr: 0.000125 	 f: 80.68736453086747
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 25 [0m
Train...

[32m iter_0[0m
ga 	p: 81.05 	r: 75.39 	f1: 78.12 	 4650 	 5737 	 6168
wo 	p: 92.67 	r: 86.22 	f1: 89.33 	 2921 	 3152 	 3388
ni 	p: 82.61 	r: 75.23 	f1: 78.75 	 1145 	 1386 	 1522

[32m iter_1[0m
ga 	p: 81.16 	r: 75.42 	f1: 78.18 	 4652 	 5732 	 6168
wo 	p: 92.42 	r: 86.78 	f1: 89.51 	 2940 	 3181 	 3388
ni 	p: 85.56 	r: 72.8 	f1: 78.67 	 1108 	 1295 	 1522

[32m iter_2[0m
ga 	p: 80.43 	r: 76.17 	f1: 78.24 	 4698 	 5841 	 6168
wo 	p: 92.54 	r: 86.72 	f1: 89.53 	 2938 	 3175 	 3388
ni 	p: 84.27 	r: 74.24 	f1: 78.94 	 1130 	 1341 	 1522
best_thres [[0.35, 0.48, 0.13], [0.33, 0.39, 0.14], [0.28, 0.39, 0.11]]
f [0.8164, 0.8169, 0.8172]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse 	current best epoch 13 	 [0.5, 0.41, 0.19] 	 lr: 5e-05 	 f: 82.56597594003904
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse[0m [33m epoch 25 [0m
Train...
loss: tensor(833.6415) lr: 6.25e-05 time: 2562.88
pred_count_train 41644

Test...
loss: tensor(205.9998) lr: 1.25e-05 time: 2568.14
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 78.03 	r: 74.74 	f1: 76.35 	 4610 	 5908 	 6168
wo 	p: 93.67 	r: 83.0 	f1: 88.01 	 2812 	 3002 	 3388
ni 	p: 86.38 	r: 72.93 	f1: 79.09 	 1110 	 1285 	 1522

[32m iter_1[0m
ga 	p: 78.84 	r: 74.12 	f1: 76.41 	 4572 	 5799 	 6168
wo 	p: 93.58 	r: 83.0 	f1: 87.97 	 2812 	 3005 	 3388
ni 	p: 82.64 	r: 75.36 	f1: 78.83 	 1147 	 1388 	 1522

[32m iter_2[0m
ga 	p: 78.93 	r: 73.9 	f1: 76.33 	 4558 	 5775 	 6168
wo 	p: 93.96 	r: 82.67 	f1: 87.96 	 2801 	 2981 	 3388
ni 	p: 82.53 	r: 75.76 	f1: 79.0 	 1153 	 1397 	 1522
best_thres [[0.35, 0.66, 0.19], [0.37, 0.78, 0.12], [0.39, 0.85, 0.11]]
f [0.8021, 0.8022, 0.8021]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 20 	 [0.4, 0.65, 0.13] 	 lr: 0.000125 	 f: 80.68736453086747
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 26 [0m
Train...

[32m iter_0[0m
ga 	p: 81.65 	r: 74.32 	f1: 77.81 	 4584 	 5614 	 6168
wo 	p: 92.78 	r: 86.04 	f1: 89.28 	 2915 	 3142 	 3388
ni 	p: 82.98 	r: 74.31 	f1: 78.41 	 1131 	 1363 	 1522

[32m iter_1[0m
ga 	p: 80.51 	r: 75.06 	f1: 77.69 	 4630 	 5751 	 6168
wo 	p: 92.97 	r: 85.89 	f1: 89.29 	 2910 	 3130 	 3388
ni 	p: 83.08 	r: 74.51 	f1: 78.56 	 1134 	 1365 	 1522

[32m iter_2[0m
ga 	p: 80.33 	r: 75.16 	f1: 77.66 	 4636 	 5771 	 6168
wo 	p: 93.03 	r: 85.86 	f1: 89.3 	 2909 	 3127 	 3388
ni 	p: 83.44 	r: 74.18 	f1: 78.54 	 1129 	 1353 	 1522
best_thres [[0.37, 0.53, 0.11], [0.29, 0.54, 0.08], [0.28, 0.55, 0.08]]
f [0.8143, 0.8139, 0.8137]
load model: epoch13
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse 	current best epoch 13 	 [0.5, 0.41, 0.19] 	 lr: 5e-05 	 f: 82.56597594003904
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse[0m [33m epoch 26 [0m
Train...
loss: tensor(631.9997) lr: 6.25e-05 time: 2484.41
pred_count_train 41644

Test...
loss: tensor(392.4690) lr: 1e-05 time: 2509.01
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.18 	r: 72.83 	f1: 75.87 	 4492 	 5673 	 6168
wo 	p: 91.04 	r: 85.18 	f1: 88.01 	 2886 	 3170 	 3388
ni 	p: 86.06 	r: 72.21 	f1: 78.53 	 1099 	 1277 	 1522

[32m iter_1[0m
ga 	p: 81.88 	r: 70.87 	f1: 75.98 	 4371 	 5338 	 6168
wo 	p: 91.18 	r: 85.15 	f1: 88.06 	 2885 	 3164 	 3388
ni 	p: 82.57 	r: 75.95 	f1: 79.12 	 1156 	 1400 	 1522

[32m iter_2[0m
ga 	p: 78.87 	r: 73.33 	f1: 76.0 	 4523 	 5735 	 6168
wo 	p: 91.16 	r: 85.27 	f1: 88.12 	 2889 	 3169 	 3388
ni 	p: 84.97 	r: 73.52 	f1: 78.83 	 1119 	 1317 	 1522
best_thres [[0.41, 0.4, 0.21], [0.63, 0.42, 0.12], [0.38, 0.42, 0.16]]
f [0.7998, 0.8008, 0.8009]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 20 	 [0.4, 0.65, 0.13] 	 lr: 0.000125 	 f: 80.68736453086747
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 27 [0m
Train...

[32m iter_0[0m
ga 	p: 81.86 	r: 75.57 	f1: 78.59 	 4661 	 5694 	 6168
wo 	p: 93.39 	r: 86.3 	f1: 89.71 	 2924 	 3131 	 3388
ni 	p: 83.14 	r: 76.81 	f1: 79.85 	 1169 	 1406 	 1522

[32m iter_1[0m
ga 	p: 79.62 	r: 77.79 	f1: 78.69 	 4798 	 6026 	 6168
wo 	p: 93.55 	r: 86.42 	f1: 89.84 	 2928 	 3130 	 3388
ni 	p: 85.59 	r: 75.3 	f1: 80.11 	 1146 	 1339 	 1522

[32m iter_2[0m
ga 	p: 80.5 	r: 76.85 	f1: 78.63 	 4740 	 5888 	 6168
wo 	p: 93.58 	r: 86.48 	f1: 89.89 	 2930 	 3131 	 3388
ni 	p: 86.0 	r: 75.1 	f1: 80.18 	 1143 	 1329 	 1522
best_thres [[0.37, 0.56, 0.11], [0.23, 0.56, 0.12], [0.28, 0.56, 0.12]]
f [0.8216, 0.8221, 0.8223]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse 	current best epoch 13 	 [0.5, 0.41, 0.19] 	 lr: 5e-05 	 f: 82.56597594003904
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse[0m [33m epoch 27 [0m
Train...
loss: tensor(492.6852) lr: 6.25e-05 time: 2360.25
pred_count_train 41644

Test...
loss: tensor(323.3020) lr: 1e-05 time: 2360.72
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 76.75 	r: 74.08 	f1: 75.39 	 4569 	 5953 	 6168
wo 	p: 91.68 	r: 83.91 	f1: 87.63 	 2843 	 3101 	 3388
ni 	p: 83.49 	r: 73.78 	f1: 78.34 	 1123 	 1345 	 1522

[32m iter_1[0m
ga 	p: 79.6 	r: 72.39 	f1: 75.83 	 4465 	 5609 	 6168
wo 	p: 91.37 	r: 84.33 	f1: 87.71 	 2857 	 3127 	 3388
ni 	p: 86.04 	r: 70.89 	f1: 77.74 	 1079 	 1254 	 1522

[32m iter_2[0m
ga 	p: 79.48 	r: 72.91 	f1: 76.05 	 4497 	 5658 	 6168
wo 	p: 92.97 	r: 83.09 	f1: 87.75 	 2815 	 3028 	 3388
ni 	p: 85.65 	r: 71.35 	f1: 77.85 	 1086 	 1268 	 1522
best_thres [[0.24, 0.4, 0.12], [0.39, 0.36, 0.18], [0.37, 0.64, 0.16]]
f [0.7948, 0.7961, 0.797]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 20 	 [0.4, 0.65, 0.13] 	 lr: 0.000125 	 f: 80.68736453086747
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 28 [0m
Train...

[32m iter_0[0m
ga 	p: 81.23 	r: 75.71 	f1: 78.38 	 4670 	 5749 	 6168
wo 	p: 93.75 	r: 85.86 	f1: 89.63 	 2909 	 3103 	 3388
ni 	p: 82.34 	r: 75.69 	f1: 78.88 	 1152 	 1399 	 1522

[32m iter_1[0m
ga 	p: 79.82 	r: 77.14 	f1: 78.46 	 4758 	 5961 	 6168
wo 	p: 93.66 	r: 85.89 	f1: 89.61 	 2910 	 3107 	 3388
ni 	p: 82.75 	r: 75.62 	f1: 79.03 	 1151 	 1391 	 1522

[32m iter_2[0m
ga 	p: 80.23 	r: 76.73 	f1: 78.45 	 4733 	 5899 	 6168
wo 	p: 92.84 	r: 86.87 	f1: 89.75 	 2943 	 3170 	 3388
ni 	p: 81.19 	r: 77.4 	f1: 79.25 	 1178 	 1451 	 1522
best_thres [[0.33, 0.58, 0.11], [0.24, 0.58, 0.09], [0.26, 0.44, 0.07]]
f [0.8187, 0.8188, 0.8192]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse 	current best epoch 13 	 [0.5, 0.41, 0.19] 	 lr: 5e-05 	 f: 82.56597594003904
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse[0m [33m epoch 28 [0m
Train...
loss: tensor(391.5905) lr: 6.25e-05 time: 2485.32
pred_count_train 41644

Test...
loss: tensor(271.5929) lr: 1e-05 time: 2478.33
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.12 	r: 71.03 	f1: 75.3 	 4381 	 5468 	 6168
wo 	p: 90.74 	r: 84.74 	f1: 87.64 	 2871 	 3164 	 3388
ni 	p: 83.57 	r: 73.19 	f1: 78.04 	 1114 	 1333 	 1522

[32m iter_1[0m
ga 	p: 80.24 	r: 71.69 	f1: 75.73 	 4422 	 5511 	 6168
wo 	p: 91.16 	r: 84.36 	f1: 87.63 	 2858 	 3135 	 3388
ni 	p: 83.48 	r: 73.06 	f1: 77.93 	 1112 	 1332 	 1522

[32m iter_2[0m
ga 	p: 80.56 	r: 71.43 	f1: 75.72 	 4406 	 5469 	 6168
wo 	p: 92.14 	r: 83.44 	f1: 87.58 	 2827 	 3068 	 3388
ni 	p: 84.65 	r: 72.47 	f1: 78.09 	 1103 	 1303 	 1522
best_thres [[0.49, 0.36, 0.14], [0.49, 0.39, 0.13], [0.65, 0.64, 0.14]]
f [0.7951, 0.7961, 0.7964]
load model: epoch20
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 20 	 [0.4, 0.65, 0.13] 	 lr: 0.000125 	 f: 80.68736453086747
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 29 [0m
Train...

[32m iter_0[0m
ga 	p: 83.1 	r: 73.91 	f1: 78.24 	 4559 	 5486 	 6168
wo 	p: 92.66 	r: 86.78 	f1: 89.62 	 2940 	 3173 	 3388
ni 	p: 84.0 	r: 74.51 	f1: 78.97 	 1134 	 1350 	 1522

[32m iter_1[0m
ga 	p: 80.07 	r: 76.28 	f1: 78.13 	 4705 	 5876 	 6168
wo 	p: 92.01 	r: 87.34 	f1: 89.61 	 2959 	 3216 	 3388
ni 	p: 83.61 	r: 75.1 	f1: 79.13 	 1143 	 1367 	 1522

[32m iter_2[0m
ga 	p: 80.14 	r: 76.3 	f1: 78.17 	 4706 	 5872 	 6168
wo 	p: 92.96 	r: 86.51 	f1: 89.62 	 2931 	 3153 	 3388
ni 	p: 84.14 	r: 74.97 	f1: 79.29 	 1141 	 1356 	 1522
best_thres [[0.44, 0.41, 0.15], [0.26, 0.34, 0.1], [0.26, 0.44, 0.1]]
f [0.8188, 0.8183, 0.8183]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse 	current best epoch 13 	 [0.5, 0.41, 0.19] 	 lr: 5e-05 	 f: 82.56597594003904
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse[0m [33m epoch 29 [0m
Train...
loss: tensor(824.4642) lr: 5e-05 time: 2404.81
pred_count_train 41644

Test...
loss: tensor(225.9028) lr: 1e-05 time: 2315.68
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.62 	r: 73.23 	f1: 76.29 	 4517 	 5673 	 6168
wo 	p: 92.9 	r: 83.38 	f1: 87.88 	 2825 	 3041 	 3388
ni 	p: 86.1 	r: 73.65 	f1: 79.39 	 1121 	 1302 	 1522

[32m iter_1[0m
ga 	p: 78.28 	r: 75.03 	f1: 76.62 	 4628 	 5912 	 6168
wo 	p: 93.81 	r: 82.79 	f1: 87.96 	 2805 	 2990 	 3388
ni 	p: 84.17 	r: 75.1 	f1: 79.38 	 1143 	 1358 	 1522

[32m iter_2[0m
ga 	p: 78.28 	r: 74.92 	f1: 76.56 	 4621 	 5903 	 6168
wo 	p: 93.43 	r: 83.09 	f1: 87.96 	 2815 	 3013 	 3388
ni 	p: 85.25 	r: 74.84 	f1: 79.71 	 1139 	 1336 	 1522
best_thres [[0.41, 0.55, 0.22], [0.31, 0.73, 0.16], [0.32, 0.74, 0.17]]
f [0.8024, 0.8031, 0.8034]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 20 	 [0.4, 0.65, 0.13] 	 lr: 0.000125 	 f: 80.68736453086747
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 30 [0m
Train...

[32m iter_0[0m
ga 	p: 81.57 	r: 74.48 	f1: 77.86 	 4594 	 5632 	 6168
wo 	p: 92.12 	r: 86.95 	f1: 89.46 	 2946 	 3198 	 3388
ni 	p: 84.01 	r: 73.19 	f1: 78.23 	 1114 	 1326 	 1522

[32m iter_1[0m
ga 	p: 80.03 	r: 75.96 	f1: 77.94 	 4685 	 5854 	 6168
wo 	p: 93.43 	r: 85.6 	f1: 89.34 	 2900 	 3104 	 3388
ni 	p: 83.54 	r: 74.05 	f1: 78.51 	 1127 	 1349 	 1522

[32m iter_2[0m
ga 	p: 81.14 	r: 74.84 	f1: 77.86 	 4616 	 5689 	 6168
wo 	p: 93.43 	r: 85.63 	f1: 89.36 	 2901 	 3105 	 3388
ni 	p: 83.97 	r: 73.65 	f1: 78.47 	 1121 	 1335 	 1522
best_thres [[0.36, 0.34, 0.15], [0.25, 0.49, 0.11], [0.32, 0.49, 0.11]]
f [0.8151, 0.8149, 0.8148]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.6_it3_rs2016_preFalse 	best in epoch 13 	 [0.5, 0.41, 0.19] 	 lr: 5e-05 	 f: 82.56597594003904
model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse.h5
e2e-stack model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse
gpu: 0
6 3 2
vocab size:  29396
prediction mode: model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse test [0.5, 0.5, 0.5]
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(629.7394) lr: 5e-05 time: 2432.75
pred_count_train 41644

Test...
loss: tensor(3188.3105) lr: 0.0002 time: 2445.55
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.39 	r: 71.92 	f1: 75.92 	 4436 	 5518 	 6168
wo 	p: 91.66 	r: 83.74 	f1: 87.52 	 2837 	 3095 	 3388
ni 	p: 81.31 	r: 76.61 	f1: 78.89 	 1166 	 1434 	 1522

[32m iter_1[0m
ga 	p: 81.61 	r: 71.87 	f1: 76.43 	 4433 	 5432 	 6168
wo 	p: 91.38 	r: 84.53 	f1: 87.83 	 2864 	 3134 	 3388
ni 	p: 83.31 	r: 75.1 	f1: 78.99 	 1143 	 1372 	 1522

[32m iter_2[0m
ga 	p: 81.1 	r: 72.37 	f1: 76.49 	 4464 	 5504 	 6168
wo 	p: 91.2 	r: 84.45 	f1: 87.69 	 2861 	 3137 	 3388
ni 	p: 83.92 	r: 74.77 	f1: 79.08 	 1138 	 1356 	 1522
best_thres [[0.48, 0.51, 0.11], [0.54, 0.45, 0.13], [0.53, 0.45, 0.13]]
f [0.799, 0.8011, 0.8018]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 20 	 [0.4, 0.65, 0.13] 	 lr: 0.000125 	 f: 80.68736453086747
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 31 [0m
Train...

[32m iter_0[0m
ga 	p: 77.99 	r: 73.65 	f1: 75.76 	 4543 	 5825 	 6168
wo 	p: 91.15 	r: 84.24 	f1: 87.56 	 2854 	 3131 	 3388
ni 	p: 85.87 	r: 76.28 	f1: 80.79 	 1161 	 1352 	 1522

[32m iter_1[0m
ga 	p: 77.94 	r: 73.85 	f1: 75.84 	 4555 	 5844 	 6168
wo 	p: 93.74 	r: 82.14 	f1: 87.56 	 2783 	 2969 	 3388
ni 	p: 85.02 	r: 77.2 	f1: 80.92 	 1175 	 1382 	 1522

[32m iter_2[0m
ga 	p: 77.95 	r: 73.88 	f1: 75.86 	 4557 	 5846 	 6168
wo 	p: 92.14 	r: 83.35 	f1: 87.53 	 2824 	 3065 	 3388
ni 	p: 85.03 	r: 77.27 	f1: 80.96 	 1176 	 1383 	 1522
best_thres [[0.46, 0.28, 0.26], [0.46, 0.42, 0.25], [0.46, 0.32, 0.25]]
f [0.8003, 0.8003, 0.8005]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse 	current best epoch 1 	 [0.46, 0.32, 0.25] 	 lr: 0.0002 	 f: 80.04872639815089
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse[0m [33m epoch 2 [0m
Train...
loss: tensor(498.3422) lr: 5e-05 time: 2367.43
pred_count_train 41644

Test...
loss: tensor(2033.5277) lr: 0.0002 time: 2364.25
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.12 	r: 72.39 	f1: 75.61 	 4465 	 5643 	 6168
wo 	p: 93.82 	r: 82.05 	f1: 87.55 	 2780 	 2963 	 3388
ni 	p: 85.34 	r: 71.88 	f1: 78.03 	 1094 	 1282 	 1522

[32m iter_1[0m
ga 	p: 78.29 	r: 73.95 	f1: 76.05 	 4561 	 5826 	 6168
wo 	p: 92.98 	r: 82.82 	f1: 87.61 	 2806 	 3018 	 3388
ni 	p: 81.03 	r: 75.23 	f1: 78.02 	 1145 	 1413 	 1522

[32m iter_2[0m
ga 	p: 77.82 	r: 74.35 	f1: 76.05 	 4586 	 5893 	 6168
wo 	p: 93.4 	r: 82.64 	f1: 87.69 	 2800 	 2998 	 3388
ni 	p: 83.41 	r: 73.32 	f1: 78.04 	 1116 	 1338 	 1522
best_thres [[0.42, 0.68, 0.18], [0.34, 0.72, 0.09], [0.31, 0.83, 0.12]]
f [0.7955, 0.7967, 0.7972]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	current best epoch 20 	 [0.4, 0.65, 0.13] 	 lr: 0.000125 	 f: 80.68736453086747
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse[0m [33m epoch 32 [0m
Train...

[32m iter_0[0m
ga 	p: 81.2 	r: 75.0 	f1: 77.98 	 4626 	 5697 	 6168
wo 	p: 91.05 	r: 86.22 	f1: 88.57 	 2921 	 3208 	 3388
ni 	p: 83.89 	r: 82.79 	f1: 83.33 	 1260 	 1502 	 1522

[32m iter_1[0m
ga 	p: 79.97 	r: 75.88 	f1: 77.87 	 4680 	 5852 	 6168
wo 	p: 91.69 	r: 85.6 	f1: 88.54 	 2900 	 3163 	 3388
ni 	p: 83.96 	r: 81.87 	f1: 82.9 	 1246 	 1484 	 1522

[32m iter_2[0m
ga 	p: 79.91 	r: 75.84 	f1: 77.82 	 4678 	 5854 	 6168
wo 	p: 91.61 	r: 85.71 	f1: 88.56 	 2904 	 3170 	 3388
ni 	p: 85.22 	r: 80.68 	f1: 82.89 	 1228 	 1441 	 1522
best_thres [[0.45, 0.31, 0.24], [0.4, 0.36, 0.26], [0.4, 0.35, 0.29]]
f [0.8198, 0.819, 0.8186]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse 	current best epoch 2 	 [0.4, 0.35, 0.29] 	 lr: 0.0002 	 f: 81.86053711013079
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse[0m [33m epoch 3 [0m
Train...
loss: tensor(395.0058) lr: 5e-05 time: 2294.89
pred_count_train 41644

Test...
loss: tensor(1640.9635) lr: 0.0002 time: 2260.56
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 77.83 	r: 72.97 	f1: 75.32 	 4501 	 5783 	 6168
wo 	p: 92.66 	r: 82.67 	f1: 87.38 	 2801 	 3023 	 3388
ni 	p: 82.82 	r: 72.54 	f1: 77.34 	 1104 	 1333 	 1522

[32m iter_1[0m
ga 	p: 78.38 	r: 73.02 	f1: 75.61 	 4504 	 5746 	 6168
wo 	p: 91.71 	r: 83.91 	f1: 87.64 	 2843 	 3100 	 3388
ni 	p: 86.4 	r: 69.71 	f1: 77.16 	 1061 	 1228 	 1522

[32m iter_2[0m
ga 	p: 78.72 	r: 72.86 	f1: 75.68 	 4494 	 5709 	 6168
wo 	p: 91.17 	r: 84.42 	f1: 87.66 	 2860 	 3137 	 3388
ni 	p: 85.91 	r: 70.11 	f1: 77.21 	 1067 	 1242 	 1522
best_thres [[0.31, 0.68, 0.14], [0.33, 0.61, 0.2], [0.35, 0.52, 0.18]]
f [0.7924, 0.7937, 0.7944]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse 	best in epoch 20 	 [0.4, 0.65, 0.13] 	 lr: 0.000125 	 f: 80.68736453086747
model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse.h5
e2e-stack model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse
gpu: 0
6 3 2
vocab size:  29396
prediction mode: model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse test [0.5, 0.5, 0.5]

[32m iter_0[0m
ga 	p: 80.32 	r: 76.38 	f1: 78.3 	 4711 	 5865 	 6168
wo 	p: 92.59 	r: 85.98 	f1: 89.16 	 2913 	 3146 	 3388
ni 	p: 86.48 	r: 77.73 	f1: 81.87 	 1183 	 1368 	 1522

[32m iter_1[0m
ga 	p: 79.58 	r: 76.82 	f1: 78.17 	 4738 	 5954 	 6168
wo 	p: 95.06 	r: 84.09 	f1: 89.24 	 2849 	 2997 	 3388
ni 	p: 85.98 	r: 78.58 	f1: 82.11 	 1196 	 1391 	 1522

[32m iter_2[0m
ga 	p: 79.91 	r: 76.82 	f1: 78.33 	 4738 	 5929 	 6168
wo 	p: 94.19 	r: 84.77 	f1: 89.23 	 2872 	 3049 	 3388
ni 	p: 86.65 	r: 78.06 	f1: 82.13 	 1188 	 1371 	 1522
best_thres [[0.45, 0.38, 0.11], [0.41, 0.57, 0.1], [0.42, 0.52, 0.11]]
f [0.8209, 0.8205, 0.8207]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse 	current best epoch 3 	 [0.42, 0.52, 0.11] 	 lr: 0.0002 	 f: 82.07265488927594
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse[0m [33m epoch 4 [0m
Train...
loss: tensor(1309.9227) lr: 0.0002 time: 1848.25
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.43 	r: 77.06 	f1: 78.23 	 4753 	 5984 	 6168
wo 	p: 92.7 	r: 85.77 	f1: 89.1 	 2906 	 3135 	 3388
ni 	p: 84.05 	r: 81.73 	f1: 82.88 	 1244 	 1480 	 1522

[32m iter_1[0m
ga 	p: 79.93 	r: 76.64 	f1: 78.25 	 4727 	 5914 	 6168
wo 	p: 94.96 	r: 84.03 	f1: 89.16 	 2847 	 2998 	 3388
ni 	p: 85.0 	r: 80.81 	f1: 82.86 	 1230 	 1447 	 1522

[32m iter_2[0m
ga 	p: 80.2 	r: 76.3 	f1: 78.2 	 4706 	 5868 	 6168
wo 	p: 94.6 	r: 84.3 	f1: 89.15 	 2856 	 3019 	 3388
ni 	p: 84.74 	r: 81.01 	f1: 82.84 	 1233 	 1455 	 1522
best_thres [[0.17, 0.58, 0.18], [0.18, 0.77, 0.19], [0.19, 0.77, 0.19]]
f [0.8214, 0.8214, 0.8213]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse 	current best epoch 4 	 [0.19, 0.77, 0.19] 	 lr: 0.0002 	 f: 82.1334490346174
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse[0m [33m epoch 5 [0m
Train...
loss: tensor(1005.3558) lr: 0.0002 time: 1892.29
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.32 	r: 75.81 	f1: 78.47 	 4676 	 5750 	 6168
wo 	p: 92.09 	r: 86.28 	f1: 89.09 	 2923 	 3174 	 3388
ni 	p: 81.14 	r: 76.02 	f1: 78.49 	 1157 	 1426 	 1522

[32m iter_1[0m
ga 	p: 81.84 	r: 75.31 	f1: 78.44 	 4645 	 5676 	 6168
wo 	p: 92.25 	r: 86.1 	f1: 89.07 	 2917 	 3162 	 3388
ni 	p: 82.83 	r: 75.1 	f1: 78.77 	 1143 	 1380 	 1522

[32m iter_2[0m
ga 	p: 82.04 	r: 75.18 	f1: 78.46 	 4637 	 5652 	 6168
wo 	p: 91.99 	r: 86.45 	f1: 89.14 	 2929 	 3184 	 3388
ni 	p: 83.1 	r: 74.97 	f1: 78.83 	 1141 	 1373 	 1522
best_thres [[0.43, 0.34, 0.2], [0.45, 0.35, 0.22], [0.46, 0.32, 0.22]]
f [0.8172, 0.8174, 0.8176]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse 	current best epoch 4 	 [0.19, 0.77, 0.19] 	 lr: 0.0002 	 f: 82.1334490346174
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse[0m [33m epoch 6 [0m
Train...
loss: tensor(761.8506) lr: 0.0002 time: 1737.5
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.92 	r: 74.51 	f1: 77.58 	 4596 	 5680 	 6168
wo 	p: 92.26 	r: 86.22 	f1: 89.14 	 2921 	 3166 	 3388
ni 	p: 82.32 	r: 76.81 	f1: 79.47 	 1169 	 1420 	 1522

[32m iter_1[0m
ga 	p: 80.6 	r: 74.9 	f1: 77.65 	 4620 	 5732 	 6168
wo 	p: 92.75 	r: 85.74 	f1: 89.11 	 2905 	 3132 	 3388
ni 	p: 83.45 	r: 76.22 	f1: 79.67 	 1160 	 1390 	 1522

[32m iter_2[0m
ga 	p: 80.63 	r: 74.92 	f1: 77.67 	 4621 	 5731 	 6168
wo 	p: 92.61 	r: 86.13 	f1: 89.25 	 2918 	 3151 	 3388
ni 	p: 83.93 	r: 76.54 	f1: 80.07 	 1165 	 1388 	 1522
best_thres [[0.54, 0.62, 0.13], [0.51, 0.63, 0.15], [0.51, 0.63, 0.15]]
f [0.8139, 0.8141, 0.8145]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse 	current best epoch 4 	 [0.19, 0.77, 0.19] 	 lr: 0.0002 	 f: 82.1334490346174
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse[0m [33m epoch 7 [0m
Train...
loss: tensor(606.4867) lr: 0.0002 time: 1869.62
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.51 	r: 75.96 	f1: 78.17 	 4685 	 5819 	 6168
wo 	p: 92.87 	r: 86.16 	f1: 89.39 	 2919 	 3143 	 3388
ni 	p: 82.36 	r: 71.48 	f1: 76.54 	 1088 	 1321 	 1522

[32m iter_1[0m
ga 	p: 81.05 	r: 75.66 	f1: 78.27 	 4667 	 5758 	 6168
wo 	p: 95.06 	r: 84.68 	f1: 89.57 	 2869 	 3018 	 3388
ni 	p: 79.84 	r: 73.92 	f1: 76.77 	 1125 	 1409 	 1522

[32m iter_2[0m
ga 	p: 81.4 	r: 75.47 	f1: 78.32 	 4655 	 5719 	 6168
wo 	p: 94.63 	r: 84.86 	f1: 89.48 	 2875 	 3038 	 3388
ni 	p: 82.91 	r: 71.09 	f1: 76.55 	 1082 	 1305 	 1522
best_thres [[0.42, 0.57, 0.18], [0.45, 0.85, 0.12], [0.47, 0.85, 0.19]]
f [0.8138, 0.8142, 0.8144]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse 	current best epoch 4 	 [0.19, 0.77, 0.19] 	 lr: 0.0002 	 f: 82.1334490346174
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse[0m [33m epoch 8 [0m
Train...
loss: tensor(491.4017) lr: 0.0002 time: 1851.31
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.48 	r: 74.79 	f1: 77.53 	 4613 	 5732 	 6168
wo 	p: 92.81 	r: 85.77 	f1: 89.15 	 2906 	 3131 	 3388
ni 	p: 84.0 	r: 71.75 	f1: 77.39 	 1092 	 1300 	 1522

[32m iter_1[0m
ga 	p: 80.84 	r: 75.18 	f1: 77.91 	 4637 	 5736 	 6168
wo 	p: 92.95 	r: 85.6 	f1: 89.12 	 2900 	 3120 	 3388
ni 	p: 83.37 	r: 71.48 	f1: 76.97 	 1088 	 1305 	 1522

[32m iter_2[0m
ga 	p: 81.62 	r: 74.66 	f1: 77.98 	 4605 	 5642 	 6168
wo 	p: 92.76 	r: 85.89 	f1: 89.2 	 2910 	 3137 	 3388
ni 	p: 82.82 	r: 72.21 	f1: 77.15 	 1099 	 1327 	 1522
best_thres [[0.61, 0.69, 0.28], [0.61, 0.68, 0.29], [0.7, 0.66, 0.27]]
f [0.8108, 0.8115, 0.8121]
load model: epoch4
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse 	current best epoch 4 	 [0.19, 0.77, 0.19] 	 lr: 0.0002 	 f: 82.1334490346174
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse[0m [33m epoch 9 [0m
Train...
loss: tensor(791.5230) lr: 0.0001 time: 1763.3
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.15 	r: 75.05 	f1: 78.89 	 4629 	 5567 	 6168
wo 	p: 93.41 	r: 85.8 	f1: 89.45 	 2907 	 3112 	 3388
ni 	p: 88.52 	r: 73.98 	f1: 80.6 	 1126 	 1272 	 1522

[32m iter_1[0m
ga 	p: 82.99 	r: 75.52 	f1: 79.08 	 4658 	 5613 	 6168
wo 	p: 92.96 	r: 86.51 	f1: 89.62 	 2931 	 3153 	 3388
ni 	p: 84.59 	r: 77.2 	f1: 80.73 	 1175 	 1389 	 1522

[32m iter_2[0m
ga 	p: 82.81 	r: 75.5 	f1: 78.99 	 4657 	 5624 	 6168
wo 	p: 92.7 	r: 86.57 	f1: 89.53 	 2933 	 3164 	 3388
ni 	p: 86.95 	r: 75.3 	f1: 80.7 	 1146 	 1318 	 1522
best_thres [[0.53, 0.64, 0.19], [0.5, 0.55, 0.1], [0.5, 0.55, 0.14]]
f [0.8238, 0.8247, 0.8247]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse 	current best epoch 9 	 [0.5, 0.55, 0.14] 	 lr: 0.0001 	 f: 82.47013208082464
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse[0m [33m epoch 10 [0m
Train...
loss: tensor(505.2592) lr: 0.0001 time: 1855.38
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.03 	r: 75.63 	f1: 78.7 	 4665 	 5687 	 6168
wo 	p: 91.8 	r: 86.57 	f1: 89.11 	 2933 	 3195 	 3388
ni 	p: 87.57 	r: 72.21 	f1: 79.15 	 1099 	 1255 	 1522

[32m iter_1[0m
ga 	p: 83.57 	r: 74.79 	f1: 78.94 	 4613 	 5520 	 6168
wo 	p: 93.26 	r: 85.71 	f1: 89.33 	 2904 	 3114 	 3388
ni 	p: 84.71 	r: 74.24 	f1: 79.13 	 1130 	 1334 	 1522

[32m iter_2[0m
ga 	p: 81.36 	r: 76.65 	f1: 78.94 	 4728 	 5811 	 6168
wo 	p: 93.05 	r: 85.8 	f1: 89.28 	 2907 	 3124 	 3388
ni 	p: 85.51 	r: 74.05 	f1: 79.37 	 1127 	 1318 	 1522
best_thres [[0.44, 0.53, 0.36], [0.52, 0.69, 0.21], [0.37, 0.69, 0.23]]
f [0.8199, 0.8208, 0.821]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse 	current best epoch 9 	 [0.5, 0.55, 0.14] 	 lr: 0.0001 	 f: 82.47013208082464
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse[0m [33m epoch 11 [0m
Train...
loss: tensor(346.7487) lr: 0.0001 time: 1816.89
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.01 	r: 75.23 	f1: 78.47 	 4640 	 5658 	 6168
wo 	p: 92.44 	r: 86.22 	f1: 89.22 	 2921 	 3160 	 3388
ni 	p: 83.52 	r: 74.9 	f1: 78.97 	 1140 	 1365 	 1522

[32m iter_1[0m
ga 	p: 81.77 	r: 75.78 	f1: 78.66 	 4674 	 5716 	 6168
wo 	p: 92.08 	r: 86.81 	f1: 89.36 	 2941 	 3194 	 3388
ni 	p: 85.09 	r: 74.24 	f1: 79.3 	 1130 	 1328 	 1522

[32m iter_2[0m
ga 	p: 81.39 	r: 76.1 	f1: 78.66 	 4694 	 5767 	 6168
wo 	p: 92.32 	r: 86.54 	f1: 89.34 	 2932 	 3176 	 3388
ni 	p: 84.97 	r: 74.64 	f1: 79.47 	 1136 	 1337 	 1522
best_thres [[0.5, 0.71, 0.19], [0.48, 0.6, 0.18], [0.44, 0.69, 0.17]]
f [0.8185, 0.8195, 0.8198]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse 	current best epoch 9 	 [0.5, 0.55, 0.14] 	 lr: 0.0001 	 f: 82.47013208082464
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse[0m [33m epoch 12 [0m
Train...
loss: tensor(263.6018) lr: 0.0001 time: 1795.95
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.12 	r: 75.24 	f1: 78.07 	 4641 	 5721 	 6168
wo 	p: 93.04 	r: 85.63 	f1: 89.18 	 2901 	 3118 	 3388
ni 	p: 81.4 	r: 73.32 	f1: 77.15 	 1116 	 1371 	 1522

[32m iter_1[0m
ga 	p: 81.09 	r: 75.37 	f1: 78.13 	 4649 	 5733 	 6168
wo 	p: 92.76 	r: 86.19 	f1: 89.35 	 2920 	 3148 	 3388
ni 	p: 81.07 	r: 74.57 	f1: 77.69 	 1135 	 1400 	 1522

[32m iter_2[0m
ga 	p: 81.2 	r: 75.41 	f1: 78.19 	 4651 	 5728 	 6168
wo 	p: 92.84 	r: 86.16 	f1: 89.38 	 2919 	 3144 	 3388
ni 	p: 83.84 	r: 72.6 	f1: 77.82 	 1105 	 1318 	 1522
best_thres [[0.43, 0.65, 0.24], [0.42, 0.56, 0.18], [0.42, 0.56, 0.27]]
f [0.8134, 0.8142, 0.8147]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse 	current best epoch 9 	 [0.5, 0.55, 0.14] 	 lr: 0.0001 	 f: 82.47013208082464
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse[0m [33m epoch 13 [0m
Train...
loss: tensor(209.5448) lr: 0.0001 time: 1801.74
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.03 	r: 73.83 	f1: 78.16 	 4554 	 5485 	 6168
wo 	p: 91.39 	r: 86.51 	f1: 88.89 	 2931 	 3207 	 3388
ni 	p: 84.71 	r: 73.52 	f1: 78.72 	 1119 	 1321 	 1522

[32m iter_1[0m
ga 	p: 83.0 	r: 73.99 	f1: 78.24 	 4564 	 5499 	 6168
wo 	p: 92.55 	r: 85.48 	f1: 88.88 	 2896 	 3129 	 3388
ni 	p: 84.52 	r: 74.64 	f1: 79.27 	 1136 	 1344 	 1522

[32m iter_2[0m
ga 	p: 82.61 	r: 74.38 	f1: 78.28 	 4588 	 5554 	 6168
wo 	p: 92.58 	r: 85.48 	f1: 88.89 	 2896 	 3128 	 3388
ni 	p: 84.63 	r: 74.18 	f1: 79.06 	 1129 	 1334 	 1522
best_thres [[0.64, 0.56, 0.05], [0.64, 0.8, 0.03], [0.61, 0.84, 0.03]]
f [0.8159, 0.8163, 0.8164]
load model: epoch9
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse 	current best epoch 9 	 [0.5, 0.55, 0.14] 	 lr: 0.0001 	 f: 82.47013208082464
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse[0m [33m epoch 14 [0m
Train...
loss: tensor(416.6148) lr: 5e-05 time: 1803.61
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.86 	r: 74.48 	f1: 78.89 	 4594 	 5478 	 6168
wo 	p: 92.35 	r: 86.92 	f1: 89.55 	 2945 	 3189 	 3388
ni 	p: 86.11 	r: 75.76 	f1: 80.6 	 1153 	 1339 	 1522

[32m iter_1[0m
ga 	p: 82.52 	r: 75.76 	f1: 79.0 	 4673 	 5663 	 6168
wo 	p: 93.08 	r: 86.6 	f1: 89.72 	 2934 	 3152 	 3388
ni 	p: 84.64 	r: 76.74 	f1: 80.5 	 1168 	 1380 	 1522

[32m iter_2[0m
ga 	p: 82.73 	r: 75.63 	f1: 79.02 	 4665 	 5639 	 6168
wo 	p: 93.01 	r: 86.45 	f1: 89.61 	 2929 	 3149 	 3388
ni 	p: 85.77 	r: 75.62 	f1: 80.38 	 1151 	 1342 	 1522
best_thres [[0.47, 0.5, 0.23], [0.36, 0.56, 0.15], [0.37, 0.56, 0.19]]
f [0.8245, 0.8248, 0.8247]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse 	current best epoch 14 	 [0.37, 0.56, 0.19] 	 lr: 5e-05 	 f: 82.47305907338945
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse[0m [33m epoch 15 [0m
Train...
loss: tensor(252.4344) lr: 5e-05 time: 1825.65
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.48 	r: 74.37 	f1: 78.66 	 4587 	 5495 	 6168
wo 	p: 92.89 	r: 86.72 	f1: 89.7 	 2938 	 3163 	 3388
ni 	p: 83.78 	r: 75.69 	f1: 79.53 	 1152 	 1375 	 1522

[32m iter_1[0m
ga 	p: 82.27 	r: 75.54 	f1: 78.76 	 4659 	 5663 	 6168
wo 	p: 92.79 	r: 86.92 	f1: 89.76 	 2945 	 3174 	 3388
ni 	p: 85.04 	r: 75.1 	f1: 79.76 	 1143 	 1344 	 1522

[32m iter_2[0m
ga 	p: 82.35 	r: 75.58 	f1: 78.82 	 4662 	 5661 	 6168
wo 	p: 92.88 	r: 86.69 	f1: 89.68 	 2937 	 3162 	 3388
ni 	p: 85.7 	r: 74.44 	f1: 79.68 	 1133 	 1322 	 1522
best_thres [[0.55, 0.45, 0.12], [0.45, 0.41, 0.11], [0.45, 0.42, 0.12]]
f [0.822, 0.8225, 0.8226]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse 	current best epoch 14 	 [0.37, 0.56, 0.19] 	 lr: 5e-05 	 f: 82.47305907338945
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse[0m [33m epoch 16 [0m
Train...
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse
gpu: 0
6 3 2
vocab size:  29396
prediction mode: model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse test [0.5, 0.5, 0.5]
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse
gpu: 0
6 3 2
vocab size:  29396
prediction mode: model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse test [0.5, 0.5, 0.5]
loss: tensor(163.5923) lr: 5e-05 time: 1890.59
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.27 	r: 75.44 	f1: 78.7 	 4653 	 5656 	 6168
wo 	p: 92.9 	r: 86.13 	f1: 89.39 	 2918 	 3141 	 3388
ni 	p: 82.7 	r: 76.61 	f1: 79.54 	 1166 	 1410 	 1522

[32m iter_1[0m
ga 	p: 82.8 	r: 75.31 	f1: 78.88 	 4645 	 5610 	 6168
wo 	p: 93.15 	r: 86.28 	f1: 89.58 	 2923 	 3138 	 3388
ni 	p: 85.47 	r: 74.57 	f1: 79.65 	 1135 	 1328 	 1522

[32m iter_2[0m
ga 	p: 84.02 	r: 74.19 	f1: 78.8 	 4576 	 5446 	 6168
wo 	p: 92.65 	r: 86.66 	f1: 89.55 	 2936 	 3169 	 3388
ni 	p: 84.35 	r: 75.1 	f1: 79.46 	 1143 	 1355 	 1522
best_thres [[0.46, 0.57, 0.12], [0.49, 0.54, 0.15], [0.64, 0.45, 0.12]]
f [0.821, 0.8219, 0.8221]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse 	current best epoch 14 	 [0.37, 0.56, 0.19] 	 lr: 5e-05 	 f: 82.47305907338945
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse[0m [33m epoch 17 [0m
Train...
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse
gpu: 0
6 3 2
vocab size:  29396
prediction mode: model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.8_it10_rs2016_preFalse test [0.5, 0.5, 0.5]
{'ga': {'nn': 6278, 'np': 2651, 'pn': 4483, 'pp': 14460},
 'ni': {'nn': 22277, 'np': 1354, 'pn': 1037, 'pp': 1711},
 'o': {'nn': 15629, 'np': 677, 'pn': 1302, 'pp': 8996}}
ga:	prec: 84.51, recall: 76.33, f1: 80.21
o:	prec: 93.0, recall: 87.36, f1: 90.09
ni:	prec: 55.82, recall: 62.26, f1: 58.87
all:	prec: 84.31, recall: 78.67, f1: 81.4
pred_count_test 26367
loss: tensor(124.9333) lr: 5e-05 time: 2327.79
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.01 	r: 75.47 	f1: 78.14 	 4655 	 5746 	 6168
wo 	p: 92.15 	r: 86.63 	f1: 89.3 	 2935 	 3185 	 3388
ni 	p: 81.41 	r: 73.65 	f1: 77.34 	 1121 	 1377 	 1522

[32m iter_1[0m
ga 	p: 81.71 	r: 75.18 	f1: 78.31 	 4637 	 5675 	 6168
wo 	p: 92.67 	r: 86.16 	f1: 89.29 	 2919 	 3150 	 3388
ni 	p: 83.3 	r: 73.06 	f1: 77.84 	 1112 	 1335 	 1522

[32m iter_2[0m
ga 	p: 82.05 	r: 74.84 	f1: 78.28 	 4616 	 5626 	 6168
wo 	p: 92.47 	r: 86.28 	f1: 89.27 	 2923 	 3161 	 3388
ni 	p: 83.93 	r: 72.73 	f1: 77.93 	 1107 	 1319 	 1522
best_thres [[0.37, 0.64, 0.17], [0.44, 0.82, 0.14], [0.47, 0.85, 0.15]]
f [0.8146, 0.8155, 0.8157]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse 	current best epoch 14 	 [0.37, 0.56, 0.19] 	 lr: 5e-05 	 f: 82.47305907338945
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse[0m [33m epoch 18 [0m
Train...
loss: tensor(93.8891) lr: 5e-05 time: 1799.58
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.66 	r: 76.43 	f1: 78.49 	 4714 	 5844 	 6168
wo 	p: 91.93 	r: 87.1 	f1: 89.45 	 2951 	 3210 	 3388
ni 	p: 85.09 	r: 76.48 	f1: 80.55 	 1164 	 1368 	 1522

[32m iter_1[0m
ga 	p: 80.44 	r: 76.59 	f1: 78.47 	 4724 	 5873 	 6168
wo 	p: 92.47 	r: 86.63 	f1: 89.45 	 2935 	 3174 	 3388
ni 	p: 84.45 	r: 78.12 	f1: 81.16 	 1189 	 1408 	 1522

[32m iter_2[0m
ga 	p: 81.29 	r: 75.81 	f1: 78.46 	 4676 	 5752 	 6168
wo 	p: 92.47 	r: 86.63 	f1: 89.45 	 2935 	 3174 	 3388
ni 	p: 84.28 	r: 77.86 	f1: 80.94 	 1185 	 1406 	 1522
best_thres [[0.4, 0.43, 0.19], [0.37, 0.52, 0.09], [0.47, 0.51, 0.09]]
f [0.8213, 0.8216, 0.8216]
load model: epoch14
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse 	current best epoch 14 	 [0.37, 0.56, 0.19] 	 lr: 5e-05 	 f: 82.47305907338945
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse[0m [33m epoch 19 [0m
Train...
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/test.json
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/test.json
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
6 3 2
vocab size:  29396
prediction mode: model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse dev [0.5, 0.5, 0.5]
{'ga': {'nn': 3296, 'np': 1375, 'pn': 2420, 'pp': 7617},
 'ni': {'nn': 11581, 'np': 557, 'pn': 549, 'pp': 1200},
 'o': {'nn': 8139, 'np': 406, 'pn': 665, 'pp': 4820}}
ga:	prec: 84.71, recall: 75.89, f1: 80.06
o:	prec: 92.23, recall: 87.88, f1: 90.0
ni:	prec: 68.3, recall: 68.61, f1: 68.45
all:	prec: 85.36, recall: 78.96, f1: 82.04
pred_count_test 13880
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
6 3 2
vocab size:  29396
prediction mode: model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse dev [0.5, 0.5, 0.5]
{'ga': {'nn': 3285, 'np': 1387, 'pn': 2409, 'pp': 7628},
 'ni': {'nn': 11557, 'np': 582, 'pn': 514, 'pp': 1235},
 'o': {'nn': 8146, 'np': 398, 'pn': 666, 'pp': 4819}}
ga:	prec: 84.61, recall: 76.0, f1: 80.08
o:	prec: 92.37, recall: 87.86, f1: 90.06
ni:	prec: 67.97, recall: 70.61, f1: 69.27
all:	prec: 85.25, recall: 79.22, f1: 82.12
pred_count_test 13880
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
loss: tensor(222.2362) lr: 2.5e-05 time: 1739.89
pred_count_train 41644

Test...
6 3 2
vocab size:  29396
prediction mode: model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse dev [0.5, 0.5, 0.5]
{'ga': {'nn': 3290, 'np': 1385, 'pn': 2408, 'pp': 7629},
 'ni': {'nn': 11554, 'np': 584, 'pn': 513, 'pp': 1236},
 'o': {'nn': 8145, 'np': 401, 'pn': 664, 'pp': 4821}}
ga:	prec: 84.64, recall: 76.01, f1: 80.09
o:	prec: 92.32, recall: 87.89, f1: 90.05
ni:	prec: 67.91, recall: 70.67, f1: 69.26
all:	prec: 85.24, recall: 79.24, f1: 82.13
pred_count_test 13880

[32m iter_0[0m
ga 	p: 83.98 	r: 74.29 	f1: 78.84 	 4582 	 5456 	 6168
wo 	p: 92.74 	r: 86.75 	f1: 89.64 	 2939 	 3169 	 3388
ni 	p: 84.36 	r: 75.82 	f1: 79.86 	 1154 	 1368 	 1522

[32m iter_1[0m
ga 	p: 82.57 	r: 75.6 	f1: 78.93 	 4663 	 5647 	 6168
wo 	p: 93.02 	r: 86.51 	f1: 89.65 	 2931 	 3151 	 3388
ni 	p: 83.94 	r: 75.89 	f1: 79.71 	 1155 	 1376 	 1522

[32m iter_2[0m
ga 	p: 82.84 	r: 75.29 	f1: 78.89 	 4644 	 5606 	 6168
wo 	p: 93.82 	r: 85.98 	f1: 89.73 	 2913 	 3105 	 3388
ni 	p: 82.18 	r: 76.94 	f1: 79.47 	 1171 	 1425 	 1522
best_thres [[0.55, 0.45, 0.16], [0.44, 0.46, 0.12], [0.46, 0.61, 0.09]]
f [0.8234, 0.8234, 0.8232]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse 	current best epoch 14 	 [0.37, 0.56, 0.19] 	 lr: 5e-05 	 f: 82.47305907338945
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse[0m [33m epoch 20 [0m
Train...
loss: tensor(136.8060) lr: 2.5e-05 time: 1795.81
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.24 	r: 75.62 	f1: 78.33 	 4664 	 5741 	 6168
wo 	p: 93.05 	r: 86.19 	f1: 89.49 	 2920 	 3138 	 3388
ni 	p: 85.13 	r: 72.21 	f1: 78.14 	 1099 	 1291 	 1522

[32m iter_1[0m
ga 	p: 83.32 	r: 74.29 	f1: 78.55 	 4582 	 5499 	 6168
wo 	p: 92.61 	r: 86.92 	f1: 89.68 	 2945 	 3180 	 3388
ni 	p: 82.54 	r: 74.24 	f1: 78.17 	 1130 	 1369 	 1522

[32m iter_2[0m
ga 	p: 83.11 	r: 74.51 	f1: 78.58 	 4596 	 5530 	 6168
wo 	p: 92.47 	r: 87.01 	f1: 89.66 	 2948 	 3188 	 3388
ni 	p: 80.1 	r: 76.41 	f1: 78.21 	 1163 	 1452 	 1522
best_thres [[0.31, 0.58, 0.17], [0.46, 0.43, 0.09], [0.44, 0.41, 0.06]]
f [0.8173, 0.8184, 0.8188]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse 	current best epoch 14 	 [0.37, 0.56, 0.19] 	 lr: 5e-05 	 f: 82.47305907338945
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse[0m [33m epoch 21 [0m
Train...
loss: tensor(91.6214) lr: 2.5e-05 time: 1775.44
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.34 	r: 75.96 	f1: 78.55 	 4685 	 5760 	 6168
wo 	p: 92.1 	r: 86.72 	f1: 89.33 	 2938 	 3190 	 3388
ni 	p: 84.5 	r: 74.51 	f1: 79.19 	 1134 	 1342 	 1522

[32m iter_1[0m
ga 	p: 84.18 	r: 73.75 	f1: 78.62 	 4549 	 5404 	 6168
wo 	p: 91.98 	r: 86.95 	f1: 89.39 	 2946 	 3203 	 3388
ni 	p: 84.66 	r: 75.03 	f1: 79.55 	 1142 	 1349 	 1522

[32m iter_2[0m
ga 	p: 81.25 	r: 76.25 	f1: 78.67 	 4703 	 5788 	 6168
wo 	p: 92.22 	r: 86.72 	f1: 89.38 	 2938 	 3186 	 3388
ni 	p: 84.68 	r: 74.44 	f1: 79.23 	 1133 	 1338 	 1522
best_thres [[0.3, 0.46, 0.08], [0.57, 0.4, 0.06], [0.28, 0.45, 0.06]]
f [0.8196, 0.8204, 0.8204]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse 	current best epoch 14 	 [0.37, 0.56, 0.19] 	 lr: 5e-05 	 f: 82.47305907338945
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse[0m [33m epoch 22 [0m
Train...
loss: tensor(67.3739) lr: 2.5e-05 time: 1781.54
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.41 	r: 76.2 	f1: 78.72 	 4700 	 5773 	 6168
wo 	p: 92.78 	r: 86.51 	f1: 89.54 	 2931 	 3159 	 3388
ni 	p: 84.3 	r: 76.22 	f1: 80.06 	 1160 	 1376 	 1522

[32m iter_1[0m
ga 	p: 83.29 	r: 74.68 	f1: 78.75 	 4606 	 5530 	 6168
wo 	p: 92.99 	r: 86.48 	f1: 89.62 	 2930 	 3151 	 3388
ni 	p: 87.42 	r: 73.98 	f1: 80.14 	 1126 	 1288 	 1522

[32m iter_2[0m
ga 	p: 83.47 	r: 74.58 	f1: 78.77 	 4600 	 5511 	 6168
wo 	p: 92.87 	r: 86.45 	f1: 89.54 	 2929 	 3154 	 3388
ni 	p: 87.12 	r: 74.24 	f1: 80.17 	 1130 	 1297 	 1522
best_thres [[0.29, 0.42, 0.08], [0.47, 0.4, 0.1], [0.49, 0.39, 0.09]]
f [0.8221, 0.8226, 0.8228]
load model: epoch14
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse 	current best epoch 14 	 [0.37, 0.56, 0.19] 	 lr: 5e-05 	 f: 82.47305907338945
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse[0m [33m epoch 23 [0m
Train...
loss: tensor(211.7349) lr: 1.25e-05 time: 1735.83
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.76 	r: 74.53 	f1: 78.88 	 4597 	 5488 	 6168
wo 	p: 93.31 	r: 86.01 	f1: 89.51 	 2914 	 3123 	 3388
ni 	p: 83.71 	r: 77.0 	f1: 80.22 	 1172 	 1400 	 1522

[32m iter_1[0m
ga 	p: 83.04 	r: 75.34 	f1: 79.0 	 4647 	 5596 	 6168
wo 	p: 93.46 	r: 86.07 	f1: 89.61 	 2916 	 3120 	 3388
ni 	p: 83.25 	r: 77.07 	f1: 80.04 	 1173 	 1409 	 1522

[32m iter_2[0m
ga 	p: 83.25 	r: 75.18 	f1: 79.01 	 4637 	 5570 	 6168
wo 	p: 94.09 	r: 85.6 	f1: 89.64 	 2900 	 3082 	 3388
ni 	p: 83.38 	r: 76.81 	f1: 79.96 	 1169 	 1402 	 1522
best_thres [[0.51, 0.61, 0.15], [0.44, 0.59, 0.11], [0.46, 0.77, 0.11]]
f [0.8235, 0.8237, 0.8238]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse 	current best epoch 14 	 [0.37, 0.56, 0.19] 	 lr: 5e-05 	 f: 82.47305907338945
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse[0m [33m epoch 24 [0m
Train...
loss: tensor(147.2583) lr: 1.25e-05 time: 1757.07
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.61 	r: 74.01 	f1: 78.52 	 4565 	 5460 	 6168
wo 	p: 93.81 	r: 85.45 	f1: 89.43 	 2895 	 3086 	 3388
ni 	p: 84.74 	r: 74.77 	f1: 79.44 	 1138 	 1343 	 1522

[32m iter_1[0m
ga 	p: 82.93 	r: 74.84 	f1: 78.68 	 4616 	 5566 	 6168
wo 	p: 93.48 	r: 85.86 	f1: 89.51 	 2909 	 3112 	 3388
ni 	p: 82.94 	r: 75.36 	f1: 78.97 	 1147 	 1383 	 1522

[32m iter_2[0m
ga 	p: 83.41 	r: 74.42 	f1: 78.66 	 4590 	 5503 	 6168
wo 	p: 93.45 	r: 85.92 	f1: 89.53 	 2911 	 3115 	 3388
ni 	p: 81.88 	r: 76.28 	f1: 78.98 	 1161 	 1418 	 1522
best_thres [[0.47, 0.62, 0.16], [0.41, 0.53, 0.1], [0.44, 0.53, 0.08]]
f [0.8201, 0.8203, 0.8204]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse 	current best epoch 14 	 [0.37, 0.56, 0.19] 	 lr: 5e-05 	 f: 82.47305907338945
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse[0m [33m epoch 25 [0m
Train...
loss: tensor(108.4451) lr: 1.25e-05 time: 1772.89
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.01 	r: 74.53 	f1: 78.54 	 4597 	 5538 	 6168
wo 	p: 91.81 	r: 87.01 	f1: 89.35 	 2948 	 3211 	 3388
ni 	p: 83.01 	r: 75.76 	f1: 79.22 	 1153 	 1389 	 1522

[32m iter_1[0m
ga 	p: 82.41 	r: 74.92 	f1: 78.49 	 4621 	 5607 	 6168
wo 	p: 91.48 	r: 87.49 	f1: 89.44 	 2964 	 3240 	 3388
ni 	p: 85.11 	r: 73.59 	f1: 78.93 	 1120 	 1316 	 1522

[32m iter_2[0m
ga 	p: 82.66 	r: 74.76 	f1: 78.51 	 4611 	 5578 	 6168
wo 	p: 93.18 	r: 85.92 	f1: 89.4 	 2911 	 3124 	 3388
ni 	p: 85.26 	r: 73.32 	f1: 78.84 	 1116 	 1309 	 1522
best_thres [[0.44, 0.36, 0.1], [0.4, 0.26, 0.1], [0.41, 0.51, 0.1]]
f [0.8199, 0.8198, 0.8196]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse 	current best epoch 14 	 [0.37, 0.56, 0.19] 	 lr: 5e-05 	 f: 82.47305907338945
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse[0m [33m epoch 26 [0m
Train...
loss: tensor(80.8428) lr: 1.25e-05 time: 1732.18
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.35 	r: 74.09 	f1: 78.45 	 4570 	 5483 	 6168
wo 	p: 92.96 	r: 86.07 	f1: 89.38 	 2916 	 3137 	 3388
ni 	p: 84.18 	r: 73.78 	f1: 78.64 	 1123 	 1334 	 1522

[32m iter_1[0m
ga 	p: 82.57 	r: 74.74 	f1: 78.46 	 4610 	 5583 	 6168
wo 	p: 92.38 	r: 86.63 	f1: 89.41 	 2935 	 3177 	 3388
ni 	p: 84.2 	r: 73.52 	f1: 78.5 	 1119 	 1329 	 1522

[32m iter_2[0m
ga 	p: 82.61 	r: 74.84 	f1: 78.53 	 4616 	 5588 	 6168
wo 	p: 92.69 	r: 86.51 	f1: 89.5 	 2931 	 3162 	 3388
ni 	p: 84.27 	r: 73.19 	f1: 78.34 	 1114 	 1322 	 1522
best_thres [[0.44, 0.47, 0.1], [0.37, 0.33, 0.07], [0.37, 0.34, 0.07]]
f [0.8187, 0.8186, 0.8188]
load model: epoch14
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse 	current best epoch 14 	 [0.37, 0.56, 0.19] 	 lr: 5e-05 	 f: 82.47305907338945
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse[0m [33m epoch 27 [0m
Train...
loss: tensor(208.6368) lr: 1e-05 time: 1737.55
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.16 	r: 75.02 	f1: 78.88 	 4627 	 5564 	 6168
wo 	p: 93.01 	r: 86.42 	f1: 89.6 	 2928 	 3148 	 3388
ni 	p: 84.69 	r: 75.62 	f1: 79.9 	 1151 	 1359 	 1522

[32m iter_1[0m
ga 	p: 83.35 	r: 75.16 	f1: 79.05 	 4636 	 5562 	 6168
wo 	p: 93.42 	r: 86.39 	f1: 89.77 	 2927 	 3133 	 3388
ni 	p: 84.88 	r: 75.23 	f1: 79.76 	 1145 	 1349 	 1522

[32m iter_2[0m
ga 	p: 83.25 	r: 75.19 	f1: 79.02 	 4638 	 5571 	 6168
wo 	p: 93.48 	r: 86.33 	f1: 89.77 	 2925 	 3129 	 3388
ni 	p: 84.84 	r: 75.03 	f1: 79.64 	 1142 	 1346 	 1522
best_thres [[0.45, 0.55, 0.12], [0.45, 0.56, 0.1], [0.44, 0.56, 0.1]]
f [0.8233, 0.8239, 0.824]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse 	current best epoch 14 	 [0.37, 0.56, 0.19] 	 lr: 5e-05 	 f: 82.47305907338945
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse[0m [33m epoch 28 [0m
Train...
loss: tensor(152.3164) lr: 1e-05 time: 1764.94
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.8 	r: 74.06 	f1: 78.63 	 4568 	 5451 	 6168
wo 	p: 92.49 	r: 86.87 	f1: 89.59 	 2943 	 3182 	 3388
ni 	p: 84.86 	r: 75.89 	f1: 80.12 	 1155 	 1361 	 1522

[32m iter_1[0m
ga 	p: 83.77 	r: 74.3 	f1: 78.75 	 4583 	 5471 	 6168
wo 	p: 93.0 	r: 86.6 	f1: 89.68 	 2934 	 3155 	 3388
ni 	p: 84.57 	r: 74.9 	f1: 79.44 	 1140 	 1348 	 1522

[32m iter_2[0m
ga 	p: 84.53 	r: 73.72 	f1: 78.76 	 4547 	 5379 	 6168
wo 	p: 92.83 	r: 86.72 	f1: 89.67 	 2938 	 3165 	 3388
ni 	p: 83.96 	r: 74.97 	f1: 79.21 	 1141 	 1359 	 1522
best_thres [[0.45, 0.41, 0.13], [0.44, 0.45, 0.1], [0.51, 0.41, 0.09]]
f [0.8225, 0.8225, 0.8224]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse 	current best epoch 14 	 [0.37, 0.56, 0.19] 	 lr: 5e-05 	 f: 82.47305907338945
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse[0m [33m epoch 29 [0m
Train...
loss: tensor(118.5649) lr: 1e-05 time: 1759.5
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.87 	r: 74.74 	f1: 78.6 	 4610 	 5563 	 6168
wo 	p: 94.42 	r: 84.98 	f1: 89.45 	 2879 	 3049 	 3388
ni 	p: 84.55 	r: 74.05 	f1: 78.95 	 1127 	 1333 	 1522

[32m iter_1[0m
ga 	p: 82.3 	r: 75.31 	f1: 78.65 	 4645 	 5644 	 6168
wo 	p: 94.13 	r: 85.24 	f1: 89.47 	 2888 	 3068 	 3388
ni 	p: 83.13 	r: 75.1 	f1: 78.91 	 1143 	 1375 	 1522

[32m iter_2[0m
ga 	p: 82.11 	r: 75.44 	f1: 78.63 	 4653 	 5667 	 6168
wo 	p: 94.17 	r: 85.27 	f1: 89.5 	 2889 	 3068 	 3388
ni 	p: 83.3 	r: 74.7 	f1: 78.77 	 1137 	 1365 	 1522
best_thres [[0.39, 0.71, 0.13], [0.34, 0.65, 0.08], [0.33, 0.65, 0.08]]
f [0.8197, 0.8198, 0.8197]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse 	current best epoch 14 	 [0.37, 0.56, 0.19] 	 lr: 5e-05 	 f: 82.47305907338945
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse[0m [33m epoch 30 [0m
Train...
loss: tensor(90.8602) lr: 1e-05 time: 1702.02
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.69 	r: 74.35 	f1: 78.3 	 4586 	 5546 	 6168
wo 	p: 94.38 	r: 84.77 	f1: 89.32 	 2872 	 3043 	 3388
ni 	p: 82.77 	r: 75.1 	f1: 78.75 	 1143 	 1381 	 1522

[32m iter_1[0m
ga 	p: 83.39 	r: 74.24 	f1: 78.55 	 4579 	 5491 	 6168
wo 	p: 92.28 	r: 86.78 	f1: 89.44 	 2940 	 3186 	 3388
ni 	p: 85.38 	r: 73.26 	f1: 78.85 	 1115 	 1306 	 1522

[32m iter_2[0m
ga 	p: 83.34 	r: 74.19 	f1: 78.5 	 4576 	 5491 	 6168
wo 	p: 92.34 	r: 86.84 	f1: 89.5 	 2942 	 3186 	 3388
ni 	p: 85.85 	r: 72.54 	f1: 78.63 	 1104 	 1286 	 1522
best_thres [[0.38, 0.69, 0.09], [0.41, 0.29, 0.09], [0.41, 0.29, 0.1]]
f [0.8173, 0.8186, 0.8189]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse 	best in epoch 14 	 [0.37, 0.56, 0.19] 	 lr: 5e-05 	 f: 82.47305907338945
model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse.h5
e2e-stack model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/test.json
6 3 2
vocab size:  29396
prediction mode: model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.7_it3_rs2016_preFalse test [0.5, 0.5, 0.5]
{'ga': {'nn': 6331, 'np': 2565, 'pn': 4705, 'pp': 14238},
 'ni': {'nn': 22165, 'np': 1471, 'pn': 912, 'pp': 1836},
 'o': {'nn': 15639, 'np': 665, 'pn': 1334, 'pp': 8964}}
ga:	prec: 84.73, recall: 75.16, f1: 79.66
o:	prec: 93.09, recall: 87.05, f1: 89.97
ni:	prec: 55.52, recall: 66.81, f1: 60.64
all:	prec: 84.19, recall: 78.27, f1: 81.12
pred_count_test 26367
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(3183.9587) lr: 0.0002 time: 1693.23
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 76.99 	r: 74.32 	f1: 75.63 	 4584 	 5954 	 6168
wo 	p: 92.04 	r: 84.0 	f1: 87.84 	 2846 	 3092 	 3388
ni 	p: 83.79 	r: 78.12 	f1: 80.86 	 1189 	 1419 	 1522

[32m iter_1[0m
ga 	p: 76.78 	r: 74.3 	f1: 75.52 	 4583 	 5969 	 6168
wo 	p: 92.09 	r: 83.83 	f1: 87.76 	 2840 	 3084 	 3388
ni 	p: 82.39 	r: 78.98 	f1: 80.64 	 1202 	 1459 	 1522

[32m iter_2[0m
ga 	p: 76.81 	r: 74.3 	f1: 75.53 	 4583 	 5967 	 6168
wo 	p: 92.31 	r: 83.62 	f1: 87.75 	 2833 	 3069 	 3388
ni 	p: 82.01 	r: 79.37 	f1: 80.67 	 1208 	 1473 	 1522
best_thres [[0.43, 0.28, 0.24], [0.43, 0.28, 0.22], [0.43, 0.3, 0.21]]
f [0.8002, 0.7996, 0.7994]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse 	current best epoch 1 	 [0.43, 0.3, 0.21] 	 lr: 0.0002 	 f: 79.93819530284301
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse[0m [33m epoch 2 [0m
Train...
loss: tensor(2033.9325) lr: 0.0002 time: 1692.55
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.55 	r: 75.15 	f1: 77.76 	 4635 	 5754 	 6168
wo 	p: 92.82 	r: 84.33 	f1: 88.37 	 2857 	 3078 	 3388
ni 	p: 84.49 	r: 81.27 	f1: 82.85 	 1237 	 1464 	 1522

[32m iter_1[0m
ga 	p: 80.65 	r: 74.82 	f1: 77.63 	 4615 	 5722 	 6168
wo 	p: 92.95 	r: 84.0 	f1: 88.25 	 2846 	 3062 	 3388
ni 	p: 84.18 	r: 81.47 	f1: 82.8 	 1240 	 1473 	 1522

[32m iter_2[0m
ga 	p: 81.0 	r: 74.56 	f1: 77.65 	 4599 	 5678 	 6168
wo 	p: 92.28 	r: 84.62 	f1: 88.28 	 2867 	 3107 	 3388
ni 	p: 84.19 	r: 81.54 	f1: 82.84 	 1241 	 1474 	 1522
best_thres [[0.41, 0.36, 0.23], [0.42, 0.38, 0.23], [0.44, 0.31, 0.23]]
f [0.8168, 0.8162, 0.8162]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse 	current best epoch 2 	 [0.44, 0.31, 0.23] 	 lr: 0.0002 	 f: 81.61946101239734
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse[0m [33m epoch 3 [0m
Train...
loss: tensor(1640.1147) lr: 0.0002 time: 1692.31
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.59 	r: 72.62 	f1: 77.28 	 4479 	 5423 	 6168
wo 	p: 92.23 	r: 84.45 	f1: 88.17 	 2861 	 3102 	 3388
ni 	p: 83.57 	r: 78.19 	f1: 80.79 	 1190 	 1424 	 1522

[32m iter_1[0m
ga 	p: 82.26 	r: 73.07 	f1: 77.39 	 4507 	 5479 	 6168
wo 	p: 91.76 	r: 84.83 	f1: 88.16 	 2874 	 3132 	 3388
ni 	p: 84.19 	r: 78.38 	f1: 81.18 	 1193 	 1417 	 1522

[32m iter_2[0m
ga 	p: 81.36 	r: 73.74 	f1: 77.36 	 4548 	 5590 	 6168
wo 	p: 92.04 	r: 84.59 	f1: 88.16 	 2866 	 3114 	 3388
ni 	p: 84.65 	r: 77.92 	f1: 81.15 	 1186 	 1401 	 1522
best_thres [[0.56, 0.42, 0.11], [0.54, 0.38, 0.12], [0.5, 0.4, 0.13]]
f [0.8113, 0.8119, 0.8119]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse 	current best epoch 2 	 [0.44, 0.31, 0.23] 	 lr: 0.0002 	 f: 81.61946101239734
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse[0m [33m epoch 4 [0m
Train...
loss: tensor(1299.7458) lr: 0.0002 time: 1600.3
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 78.96 	r: 76.25 	f1: 77.58 	 4703 	 5956 	 6168
wo 	p: 92.85 	r: 85.8 	f1: 89.19 	 2907 	 3131 	 3388
ni 	p: 86.2 	r: 79.24 	f1: 82.57 	 1206 	 1399 	 1522

[32m iter_1[0m
ga 	p: 78.87 	r: 76.26 	f1: 77.55 	 4704 	 5964 	 6168
wo 	p: 93.16 	r: 85.66 	f1: 89.25 	 2902 	 3115 	 3388
ni 	p: 87.11 	r: 79.04 	f1: 82.88 	 1203 	 1381 	 1522

[32m iter_2[0m
ga 	p: 78.57 	r: 76.57 	f1: 77.56 	 4723 	 6011 	 6168
wo 	p: 93.07 	r: 85.63 	f1: 89.19 	 2901 	 3117 	 3388
ni 	p: 87.49 	r: 78.58 	f1: 82.8 	 1196 	 1367 	 1522
best_thres [[0.21, 0.48, 0.24], [0.21, 0.5, 0.27], [0.2, 0.5, 0.28]]
f [0.8177, 0.8178, 0.8178]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse 	current best epoch 4 	 [0.2, 0.5, 0.28] 	 lr: 0.0002 	 f: 81.77812137611133
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse[0m [33m epoch 5 [0m
Train...
loss: tensor(993.9985) lr: 0.0002 time: 1431.7
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.12 	r: 74.9 	f1: 78.34 	 4620 	 5626 	 6168
wo 	p: 92.72 	r: 86.87 	f1: 89.7 	 2943 	 3174 	 3388
ni 	p: 84.87 	r: 73.0 	f1: 78.49 	 1111 	 1309 	 1522

[32m iter_1[0m
ga 	p: 83.05 	r: 74.09 	f1: 78.31 	 4570 	 5503 	 6168
wo 	p: 94.41 	r: 85.24 	f1: 89.59 	 2888 	 3059 	 3388
ni 	p: 86.54 	r: 71.81 	f1: 78.49 	 1093 	 1263 	 1522

[32m iter_2[0m
ga 	p: 83.09 	r: 74.22 	f1: 78.4 	 4578 	 5510 	 6168
wo 	p: 94.44 	r: 85.24 	f1: 89.61 	 2888 	 3058 	 3388
ni 	p: 86.12 	r: 72.14 	f1: 78.51 	 1098 	 1275 	 1522
best_thres [[0.56, 0.41, 0.14], [0.61, 0.66, 0.16], [0.61, 0.66, 0.15]]
f [0.8188, 0.8185, 0.8186]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse 	current best epoch 5 	 [0.61, 0.66, 0.15] 	 lr: 0.0002 	 f: 81.85554903112156
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse[0m [33m epoch 6 [0m
Train...
loss: tensor(751.5342) lr: 0.0002 time: 1431.28
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.82 	r: 72.67 	f1: 77.85 	 4482 	 5347 	 6168
wo 	p: 92.07 	r: 86.69 	f1: 89.3 	 2937 	 3190 	 3388
ni 	p: 86.41 	r: 76.02 	f1: 80.88 	 1157 	 1339 	 1522

[32m iter_1[0m
ga 	p: 82.07 	r: 74.08 	f1: 77.87 	 4569 	 5567 	 6168
wo 	p: 92.37 	r: 86.81 	f1: 89.5 	 2941 	 3184 	 3388
ni 	p: 82.88 	r: 78.25 	f1: 80.5 	 1191 	 1437 	 1522

[32m iter_2[0m
ga 	p: 82.88 	r: 73.52 	f1: 77.92 	 4535 	 5472 	 6168
wo 	p: 92.3 	r: 86.72 	f1: 89.42 	 2938 	 3183 	 3388
ni 	p: 83.12 	r: 77.66 	f1: 80.3 	 1182 	 1422 	 1522
best_thres [[0.64, 0.69, 0.14], [0.53, 0.67, 0.09], [0.58, 0.69, 0.09]]
f [0.8186, 0.8184, 0.8184]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse 	current best epoch 5 	 [0.61, 0.66, 0.15] 	 lr: 0.0002 	 f: 81.85554903112156
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse[0m [33m epoch 7 [0m
Train...
loss: tensor(587.3842) lr: 0.0002 time: 1410.04
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 78.74 	r: 77.06 	f1: 77.89 	 4753 	 6036 	 6168
wo 	p: 92.35 	r: 85.89 	f1: 89.0 	 2910 	 3151 	 3388
ni 	p: 82.14 	r: 74.64 	f1: 78.21 	 1136 	 1383 	 1522

[32m iter_1[0m
ga 	p: 82.52 	r: 73.95 	f1: 78.0 	 4561 	 5527 	 6168
wo 	p: 91.95 	r: 85.68 	f1: 88.71 	 2903 	 3157 	 3388
ni 	p: 83.91 	r: 73.32 	f1: 78.26 	 1116 	 1330 	 1522

[32m iter_2[0m
ga 	p: 80.75 	r: 75.37 	f1: 77.97 	 4649 	 5757 	 6168
wo 	p: 93.26 	r: 84.95 	f1: 88.91 	 2878 	 3086 	 3388
ni 	p: 82.71 	r: 74.18 	f1: 78.21 	 1129 	 1365 	 1522
best_thres [[0.32, 0.74, 0.17], [0.62, 0.68, 0.22], [0.49, 0.82, 0.18]]
f [0.8129, 0.8132, 0.8133]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse 	current best epoch 5 	 [0.61, 0.66, 0.15] 	 lr: 0.0002 	 f: 81.85554903112156
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse[0m [33m epoch 8 [0m
Train...
loss: tensor(480.3376) lr: 0.0002 time: 1431.14
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.9 	r: 74.97 	f1: 77.82 	 4624 	 5716 	 6168
wo 	p: 92.8 	r: 86.04 	f1: 89.29 	 2915 	 3141 	 3388
ni 	p: 84.97 	r: 70.96 	f1: 77.34 	 1080 	 1271 	 1522

[32m iter_1[0m
ga 	p: 80.86 	r: 74.59 	f1: 77.6 	 4601 	 5690 	 6168
wo 	p: 93.11 	r: 85.36 	f1: 89.07 	 2892 	 3106 	 3388
ni 	p: 86.65 	r: 70.37 	f1: 77.66 	 1071 	 1236 	 1522

[32m iter_2[0m
ga 	p: 80.9 	r: 74.77 	f1: 77.72 	 4612 	 5701 	 6168
wo 	p: 93.59 	r: 85.33 	f1: 89.27 	 2891 	 3089 	 3388
ni 	p: 86.73 	r: 69.97 	f1: 77.45 	 1065 	 1228 	 1522
best_thres [[0.57, 0.74, 0.28], [0.57, 0.74, 0.32], [0.57, 0.81, 0.34]]
f [0.8129, 0.8121, 0.8122]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse 	current best epoch 5 	 [0.61, 0.66, 0.15] 	 lr: 0.0002 	 f: 81.85554903112156
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse[0m [33m epoch 9 [0m
Train...
loss: tensor(421.4629) lr: 0.0002 time: 1429.63
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.29 	r: 73.98 	f1: 77.46 	 4563 	 5613 	 6168
wo 	p: 92.76 	r: 85.45 	f1: 88.95 	 2895 	 3121 	 3388
ni 	p: 87.15 	r: 70.43 	f1: 77.91 	 1072 	 1230 	 1522

[32m iter_1[0m
ga 	p: 81.39 	r: 73.25 	f1: 77.11 	 4518 	 5551 	 6168
wo 	p: 92.5 	r: 85.54 	f1: 88.88 	 2898 	 3133 	 3388
ni 	p: 86.46 	r: 70.5 	f1: 77.67 	 1073 	 1241 	 1522

[32m iter_2[0m
ga 	p: 81.24 	r: 73.46 	f1: 77.16 	 4531 	 5577 	 6168
wo 	p: 92.64 	r: 85.45 	f1: 88.9 	 2895 	 3125 	 3388
ni 	p: 84.92 	r: 71.42 	f1: 77.59 	 1087 	 1280 	 1522
best_thres [[0.65, 0.63, 0.18], [0.66, 0.57, 0.13], [0.66, 0.6, 0.1]]
f [0.8108, 0.8096, 0.8092]
load model: epoch5
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse 	current best epoch 5 	 [0.61, 0.66, 0.15] 	 lr: 0.0002 	 f: 81.85554903112156
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse[0m [33m epoch 10 [0m
Train...
loss: tensor(558.6610) lr: 0.0001 time: 1428.59
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.15 	r: 76.59 	f1: 78.81 	 4724 	 5821 	 6168
wo 	p: 94.03 	r: 85.95 	f1: 89.81 	 2912 	 3097 	 3388
ni 	p: 85.41 	r: 75.76 	f1: 80.29 	 1153 	 1350 	 1522

[32m iter_1[0m
ga 	p: 81.32 	r: 76.8 	f1: 79.0 	 4737 	 5825 	 6168
wo 	p: 94.31 	r: 85.63 	f1: 89.76 	 2901 	 3076 	 3388
ni 	p: 86.07 	r: 75.1 	f1: 80.21 	 1143 	 1328 	 1522

[32m iter_2[0m
ga 	p: 81.36 	r: 76.73 	f1: 78.98 	 4733 	 5817 	 6168
wo 	p: 94.11 	r: 85.77 	f1: 89.75 	 2906 	 3088 	 3388
ni 	p: 85.19 	r: 75.56 	f1: 80.08 	 1150 	 1350 	 1522
best_thres [[0.37, 0.75, 0.27], [0.37, 0.78, 0.3], [0.37, 0.76, 0.27]]
f [0.8235, 0.8239, 0.8239]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse 	current best epoch 10 	 [0.37, 0.76, 0.27] 	 lr: 0.0001 	 f: 82.3898977901416
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse[0m [33m epoch 11 [0m
Train...
loss: tensor(321.4834) lr: 0.0001 time: 1403.22
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.22 	r: 74.89 	f1: 78.38 	 4619 	 5618 	 6168
wo 	p: 93.39 	r: 86.3 	f1: 89.71 	 2924 	 3131 	 3388
ni 	p: 86.83 	r: 73.19 	f1: 79.43 	 1114 	 1283 	 1522

[32m iter_1[0m
ga 	p: 82.84 	r: 74.42 	f1: 78.4 	 4590 	 5541 	 6168
wo 	p: 93.62 	r: 85.77 	f1: 89.53 	 2906 	 3104 	 3388
ni 	p: 87.65 	r: 73.19 	f1: 79.77 	 1114 	 1271 	 1522

[32m iter_2[0m
ga 	p: 82.56 	r: 74.58 	f1: 78.36 	 4600 	 5572 	 6168
wo 	p: 93.2 	r: 86.19 	f1: 89.56 	 2920 	 3133 	 3388
ni 	p: 87.37 	r: 73.19 	f1: 79.66 	 1114 	 1275 	 1522
best_thres [[0.63, 0.75, 0.18], [0.69, 0.77, 0.19], [0.67, 0.72, 0.18]]
f [0.8202, 0.8202, 0.8201]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse 	current best epoch 10 	 [0.37, 0.76, 0.27] 	 lr: 0.0001 	 f: 82.3898977901416
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse[0m [33m epoch 12 [0m
Train...
loss: tensor(224.0911) lr: 0.0001 time: 1429.51
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.05 	r: 75.11 	f1: 77.97 	 4633 	 5716 	 6168
wo 	p: 92.5 	r: 85.89 	f1: 89.07 	 2910 	 3146 	 3388
ni 	p: 80.91 	r: 73.78 	f1: 77.18 	 1123 	 1388 	 1522

[32m iter_1[0m
ga 	p: 81.88 	r: 74.5 	f1: 78.01 	 4595 	 5612 	 6168
wo 	p: 92.94 	r: 85.54 	f1: 89.09 	 2898 	 3118 	 3388
ni 	p: 81.45 	r: 73.0 	f1: 76.99 	 1111 	 1364 	 1522

[32m iter_2[0m
ga 	p: 81.48 	r: 74.82 	f1: 78.01 	 4615 	 5664 	 6168
wo 	p: 92.96 	r: 85.39 	f1: 89.02 	 2893 	 3112 	 3388
ni 	p: 80.39 	r: 74.05 	f1: 77.09 	 1127 	 1402 	 1522
best_thres [[0.54, 0.79, 0.2], [0.63, 0.82, 0.19], [0.58, 0.85, 0.15]]
f [0.8126, 0.8127, 0.8126]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse 	current best epoch 10 	 [0.37, 0.76, 0.27] 	 lr: 0.0001 	 f: 82.3898977901416
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse[0m [33m epoch 13 [0m
Train...
loss: tensor(176.1747) lr: 0.0001 time: 1430.11
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.95 	r: 73.36 	f1: 77.86 	 4525 	 5455 	 6168
wo 	p: 93.38 	r: 85.74 	f1: 89.4 	 2905 	 3111 	 3388
ni 	p: 82.96 	r: 73.92 	f1: 78.18 	 1125 	 1356 	 1522

[32m iter_1[0m
ga 	p: 81.7 	r: 74.38 	f1: 77.87 	 4588 	 5616 	 6168
wo 	p: 92.76 	r: 86.25 	f1: 89.39 	 2922 	 3150 	 3388
ni 	p: 83.7 	r: 72.86 	f1: 77.91 	 1109 	 1325 	 1522

[32m iter_2[0m
ga 	p: 81.27 	r: 74.85 	f1: 77.93 	 4617 	 5681 	 6168
wo 	p: 92.71 	r: 86.3 	f1: 89.39 	 2924 	 3154 	 3388
ni 	p: 83.82 	r: 72.86 	f1: 77.96 	 1109 	 1323 	 1522
best_thres [[0.66, 0.7, 0.02], [0.5, 0.54, 0.02], [0.45, 0.54, 0.02]]
f [0.8148, 0.8145, 0.8146]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse 	current best epoch 10 	 [0.37, 0.76, 0.27] 	 lr: 0.0001 	 f: 82.3898977901416
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse[0m [33m epoch 14 [0m
Train...
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 1 [0m
Train...
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 1 [0m
Train...
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 1 [0m
Train...
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 1 [0m
Train...
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 1 [0m
Train...
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(143.9410) lr: 0.0001 time: 2361.09
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.94 	r: 75.05 	f1: 78.34 	 4629 	 5649 	 6168
wo 	p: 92.49 	r: 86.48 	f1: 89.38 	 2930 	 3168 	 3388
ni 	p: 81.69 	r: 73.0 	f1: 77.1 	 1111 	 1360 	 1522

[32m iter_1[0m
ga 	p: 82.12 	r: 74.84 	f1: 78.31 	 4616 	 5621 	 6168
wo 	p: 92.71 	r: 86.3 	f1: 89.39 	 2924 	 3154 	 3388
ni 	p: 84.48 	r: 71.88 	f1: 77.67 	 1094 	 1295 	 1522

[32m iter_2[0m
ga 	p: 82.09 	r: 75.03 	f1: 78.4 	 4628 	 5638 	 6168
wo 	p: 92.9 	r: 86.07 	f1: 89.35 	 2916 	 3139 	 3388
ni 	p: 83.84 	r: 72.27 	f1: 77.63 	 1100 	 1312 	 1522
best_thres [[0.59, 0.46, 0.16], [0.6, 0.43, 0.21], [0.6, 0.48, 0.18]]
f [0.8158, 0.8162, 0.8164]
load model: epoch10
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse 	current best epoch 10 	 [0.37, 0.76, 0.27] 	 lr: 0.0001 	 f: 82.3898977901416
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse[0m [33m epoch 15 [0m
Train...
loss: tensor(3203.8760) lr: 0.0002 time: 5591.96
pred_count_train 41644

Test...
loss: tensor(3201.5171) lr: 0.0002 time: 5720.35
pred_count_train 41644

Test...
loss: tensor(3200.1440) lr: 0.0002 time: 5742.52
pred_count_train 41644

Test...
loss: tensor(3201.5171) lr: 0.0002 time: 6094.71
pred_count_train 41644

Test...
loss: tensor(3203.8760) lr: 0.0002 time: 6066.08
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 77.62 	r: 73.9 	f1: 75.71 	 4558 	 5872 	 6168
wo 	p: 92.65 	r: 83.38 	f1: 87.77 	 2825 	 3049 	 3388
ni 	p: 84.7 	r: 77.46 	f1: 80.92 	 1179 	 1392 	 1522

[32m iter_1[0m
ga 	p: 77.12 	r: 73.85 	f1: 75.45 	 4555 	 5906 	 6168
wo 	p: 91.02 	r: 84.71 	f1: 87.75 	 2870 	 3153 	 3388
ni 	p: 84.69 	r: 76.68 	f1: 80.48 	 1167 	 1378 	 1522

[32m iter_2[0m
ga 	p: 77.14 	r: 73.96 	f1: 75.52 	 4562 	 5914 	 6168
wo 	p: 90.83 	r: 84.83 	f1: 87.73 	 2874 	 3164 	 3388
ni 	p: 85.0 	r: 77.07 	f1: 80.84 	 1173 	 1380 	 1522
best_thres [[0.44, 0.43, 0.29], [0.44, 0.32, 0.28], [0.44, 0.31, 0.28]]
f [0.8005, 0.7996, 0.7996]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 1 	 [0.44, 0.31, 0.28] 	 lr: 0.0002 	 f: 79.95717078923683
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 2 [0m
Train...

[32m iter_0[0m
ga 	p: 76.78 	r: 75.49 	f1: 76.13 	 4656 	 6064 	 6168
wo 	p: 92.43 	r: 83.94 	f1: 87.98 	 2844 	 3077 	 3388
ni 	p: 82.67 	r: 78.65 	f1: 80.61 	 1197 	 1448 	 1522

[32m iter_1[0m
ga 	p: 76.93 	r: 74.94 	f1: 75.92 	 4622 	 6008 	 6168
wo 	p: 92.38 	r: 84.5 	f1: 88.27 	 2863 	 3099 	 3388
ni 	p: 82.81 	r: 78.52 	f1: 80.61 	 1195 	 1443 	 1522

[32m iter_2[0m
ga 	p: 76.98 	r: 75.08 	f1: 76.02 	 4631 	 6016 	 6168
wo 	p: 92.24 	r: 84.59 	f1: 88.25 	 2866 	 3107 	 3388
ni 	p: 81.73 	r: 79.37 	f1: 80.53 	 1208 	 1478 	 1522
best_thres [[0.43, 0.39, 0.21], [0.44, 0.35, 0.21], [0.44, 0.34, 0.19]]
f [0.8028, 0.8027, 0.8028]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 1 	 [0.44, 0.34, 0.19] 	 lr: 0.0002 	 f: 80.28442146089205
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 2 [0m
Train...

[32m iter_0[0m
ga 	p: 77.3 	r: 74.82 	f1: 76.04 	 4615 	 5970 	 6168
wo 	p: 92.08 	r: 84.12 	f1: 87.92 	 2850 	 3095 	 3388
ni 	p: 81.96 	r: 78.52 	f1: 80.2 	 1195 	 1458 	 1522

[32m iter_1[0m
ga 	p: 77.86 	r: 74.53 	f1: 76.16 	 4597 	 5904 	 6168
wo 	p: 92.32 	r: 84.03 	f1: 87.98 	 2847 	 3084 	 3388
ni 	p: 81.22 	r: 78.45 	f1: 79.81 	 1194 	 1470 	 1522

[32m iter_2[0m
ga 	p: 77.82 	r: 74.51 	f1: 76.13 	 4596 	 5906 	 6168
wo 	p: 92.29 	r: 84.03 	f1: 87.97 	 2847 	 3085 	 3388
ni 	p: 81.35 	r: 78.52 	f1: 79.91 	 1195 	 1469 	 1522
best_thres [[0.4, 0.39, 0.24], [0.42, 0.39, 0.24], [0.42, 0.39, 0.24]]
f [0.8018, 0.802, 0.802]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 1 	 [0.42, 0.39, 0.24] 	 lr: 0.0002 	 f: 80.20409741012755
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 2 [0m
Train...

[32m iter_0[0m
ga 	p: 76.78 	r: 75.49 	f1: 76.13 	 4656 	 6064 	 6168
wo 	p: 92.43 	r: 83.94 	f1: 87.98 	 2844 	 3077 	 3388
ni 	p: 82.67 	r: 78.65 	f1: 80.61 	 1197 	 1448 	 1522

[32m iter_1[0m
ga 	p: 76.93 	r: 74.94 	f1: 75.92 	 4622 	 6008 	 6168
wo 	p: 92.38 	r: 84.5 	f1: 88.27 	 2863 	 3099 	 3388
ni 	p: 82.81 	r: 78.52 	f1: 80.61 	 1195 	 1443 	 1522

[32m iter_2[0m
ga 	p: 76.98 	r: 75.08 	f1: 76.02 	 4631 	 6016 	 6168
wo 	p: 92.24 	r: 84.59 	f1: 88.25 	 2866 	 3107 	 3388
ni 	p: 81.73 	r: 79.37 	f1: 80.53 	 1208 	 1478 	 1522
best_thres [[0.43, 0.39, 0.21], [0.44, 0.35, 0.21], [0.44, 0.34, 0.19]]
f [0.8028, 0.8027, 0.8028]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 1 	 [0.44, 0.34, 0.19] 	 lr: 0.0002 	 f: 80.28442146089205
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 2 [0m
Train...

[32m iter_0[0m
ga 	p: 77.62 	r: 73.9 	f1: 75.71 	 4558 	 5872 	 6168
wo 	p: 92.65 	r: 83.38 	f1: 87.77 	 2825 	 3049 	 3388
ni 	p: 84.7 	r: 77.46 	f1: 80.92 	 1179 	 1392 	 1522

[32m iter_1[0m
ga 	p: 77.12 	r: 73.85 	f1: 75.45 	 4555 	 5906 	 6168
wo 	p: 91.02 	r: 84.71 	f1: 87.75 	 2870 	 3153 	 3388
ni 	p: 84.69 	r: 76.68 	f1: 80.48 	 1167 	 1378 	 1522

[32m iter_2[0m
ga 	p: 77.14 	r: 73.96 	f1: 75.52 	 4562 	 5914 	 6168
wo 	p: 90.83 	r: 84.83 	f1: 87.73 	 2874 	 3164 	 3388
ni 	p: 85.0 	r: 77.07 	f1: 80.84 	 1173 	 1380 	 1522
best_thres [[0.44, 0.43, 0.29], [0.44, 0.32, 0.28], [0.44, 0.31, 0.28]]
f [0.8005, 0.7996, 0.7996]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 1 	 [0.44, 0.31, 0.28] 	 lr: 0.0002 	 f: 79.95717078923683
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 2 [0m
Train...
loss: tensor(252.1461) lr: 5e-05 time: 5376.36
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.0 	r: 75.99 	f1: 78.88 	 4687 	 5716 	 6168
wo 	p: 93.86 	r: 85.77 	f1: 89.64 	 2906 	 3096 	 3388
ni 	p: 85.55 	r: 73.92 	f1: 79.31 	 1125 	 1315 	 1522

[32m iter_1[0m
ga 	p: 81.88 	r: 75.88 	f1: 78.76 	 4680 	 5716 	 6168
wo 	p: 94.08 	r: 85.77 	f1: 89.73 	 2906 	 3089 	 3388
ni 	p: 84.2 	r: 75.3 	f1: 79.5 	 1146 	 1361 	 1522

[32m iter_2[0m
ga 	p: 81.33 	r: 76.28 	f1: 78.73 	 4705 	 5785 	 6168
wo 	p: 93.93 	r: 85.92 	f1: 89.75 	 2911 	 3099 	 3388
ni 	p: 84.57 	r: 74.9 	f1: 79.44 	 1140 	 1348 	 1522
best_thres [[0.36, 0.84, 0.23], [0.35, 0.84, 0.19], [0.31, 0.82, 0.19]]
f [0.8223, 0.8222, 0.822]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse 	current best epoch 10 	 [0.37, 0.76, 0.27] 	 lr: 0.0001 	 f: 82.3898977901416
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse[0m [33m epoch 16 [0m
Train...
loss: tensor(2034.0045) lr: 0.0002 time: 6084.95
pred_count_train 41644

Test...
loss: tensor(2031.1133) lr: 0.0002 time: 6163.64
pred_count_train 41644

Test...
loss: tensor(2029.5548) lr: 0.0002 time: 6165.5
pred_count_train 41644

Test...
loss: tensor(2031.1133) lr: 0.0002 time: 6178.93
pred_count_train 41644

Test...
loss: tensor(2034.0045) lr: 0.0002 time: 6117.55
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 78.56 	r: 77.09 	f1: 77.82 	 4755 	 6053 	 6168
wo 	p: 92.98 	r: 84.77 	f1: 88.68 	 2872 	 3089 	 3388
ni 	p: 85.84 	r: 79.63 	f1: 82.62 	 1212 	 1412 	 1522

[32m iter_1[0m
ga 	p: 78.68 	r: 76.35 	f1: 77.5 	 4709 	 5985 	 6168
wo 	p: 91.7 	r: 85.8 	f1: 88.66 	 2907 	 3170 	 3388
ni 	p: 85.83 	r: 79.96 	f1: 82.79 	 1217 	 1418 	 1522

[32m iter_2[0m
ga 	p: 79.14 	r: 76.33 	f1: 77.71 	 4708 	 5949 	 6168
wo 	p: 91.65 	r: 85.86 	f1: 88.66 	 2909 	 3174 	 3388
ni 	p: 86.27 	r: 80.09 	f1: 83.07 	 1219 	 1413 	 1522
best_thres [[0.25, 0.35, 0.24], [0.26, 0.27, 0.2], [0.27, 0.27, 0.21]]
f [0.8172, 0.8166, 0.8169]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 2 	 [0.27, 0.27, 0.21] 	 lr: 0.0002 	 f: 81.6925281600074
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 3 [0m
Train...

[32m iter_0[0m
ga 	p: 81.38 	r: 74.74 	f1: 77.92 	 4610 	 5665 	 6168
wo 	p: 93.23 	r: 85.42 	f1: 89.16 	 2894 	 3104 	 3388
ni 	p: 86.31 	r: 80.35 	f1: 83.23 	 1223 	 1417 	 1522

[32m iter_1[0m
ga 	p: 81.14 	r: 75.0 	f1: 77.95 	 4626 	 5701 	 6168
wo 	p: 92.15 	r: 86.25 	f1: 89.1 	 2922 	 3171 	 3388
ni 	p: 87.64 	r: 79.17 	f1: 83.19 	 1205 	 1375 	 1522

[32m iter_2[0m
ga 	p: 81.1 	r: 75.0 	f1: 77.93 	 4626 	 5704 	 6168
wo 	p: 92.92 	r: 85.6 	f1: 89.11 	 2900 	 3121 	 3388
ni 	p: 87.51 	r: 79.17 	f1: 83.13 	 1205 	 1377 	 1522
best_thres [[0.32, 0.36, 0.19], [0.3, 0.28, 0.2], [0.3, 0.33, 0.2]]
f [0.8208, 0.8209, 0.8208]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 2 	 [0.3, 0.33, 0.2] 	 lr: 0.0002 	 f: 82.07737713131566
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 3 [0m
Train...

[32m iter_0[0m
ga 	p: 78.51 	r: 77.12 	f1: 77.81 	 4757 	 6059 	 6168
wo 	p: 92.82 	r: 85.45 	f1: 88.98 	 2895 	 3119 	 3388
ni 	p: 84.67 	r: 80.22 	f1: 82.39 	 1221 	 1442 	 1522

[32m iter_1[0m
ga 	p: 79.0 	r: 76.93 	f1: 77.95 	 4745 	 6006 	 6168
wo 	p: 90.6 	r: 87.34 	f1: 88.94 	 2959 	 3266 	 3388
ni 	p: 83.2 	r: 82.0 	f1: 82.59 	 1248 	 1500 	 1522

[32m iter_2[0m
ga 	p: 78.86 	r: 76.88 	f1: 77.86 	 4742 	 6013 	 6168
wo 	p: 92.73 	r: 85.45 	f1: 88.94 	 2895 	 3122 	 3388
ni 	p: 83.29 	r: 82.19 	f1: 82.74 	 1251 	 1502 	 1522
best_thres [[0.25, 0.4, 0.19], [0.26, 0.26, 0.15], [0.26, 0.39, 0.15]]
f [0.8179, 0.8186, 0.8186]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 2 	 [0.26, 0.39, 0.15] 	 lr: 0.0002 	 f: 81.86261740955825
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 3 [0m
Train...

[32m iter_0[0m
ga 	p: 81.38 	r: 74.74 	f1: 77.92 	 4610 	 5665 	 6168
wo 	p: 93.23 	r: 85.42 	f1: 89.16 	 2894 	 3104 	 3388
ni 	p: 86.31 	r: 80.35 	f1: 83.23 	 1223 	 1417 	 1522

[32m iter_1[0m
ga 	p: 81.14 	r: 75.0 	f1: 77.95 	 4626 	 5701 	 6168
wo 	p: 92.15 	r: 86.25 	f1: 89.1 	 2922 	 3171 	 3388
ni 	p: 87.64 	r: 79.17 	f1: 83.19 	 1205 	 1375 	 1522

[32m iter_2[0m
ga 	p: 81.1 	r: 75.0 	f1: 77.93 	 4626 	 5704 	 6168
wo 	p: 92.92 	r: 85.6 	f1: 89.11 	 2900 	 3121 	 3388
ni 	p: 87.51 	r: 79.17 	f1: 83.13 	 1205 	 1377 	 1522
best_thres [[0.32, 0.36, 0.19], [0.3, 0.28, 0.2], [0.3, 0.33, 0.2]]
f [0.8208, 0.8209, 0.8208]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 2 	 [0.3, 0.33, 0.2] 	 lr: 0.0002 	 f: 82.07737713131566
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 3 [0m
Train...
loss: tensor(128.1012) lr: 5e-05 time: 5426.93
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 78.56 	r: 77.09 	f1: 77.82 	 4755 	 6053 	 6168
wo 	p: 92.98 	r: 84.77 	f1: 88.68 	 2872 	 3089 	 3388
ni 	p: 85.84 	r: 79.63 	f1: 82.62 	 1212 	 1412 	 1522

[32m iter_1[0m
ga 	p: 78.68 	r: 76.35 	f1: 77.5 	 4709 	 5985 	 6168
wo 	p: 91.7 	r: 85.8 	f1: 88.66 	 2907 	 3170 	 3388
ni 	p: 85.83 	r: 79.96 	f1: 82.79 	 1217 	 1418 	 1522

[32m iter_2[0m
ga 	p: 79.14 	r: 76.33 	f1: 77.71 	 4708 	 5949 	 6168
wo 	p: 91.65 	r: 85.86 	f1: 88.66 	 2909 	 3174 	 3388
ni 	p: 86.27 	r: 80.09 	f1: 83.07 	 1219 	 1413 	 1522
best_thres [[0.25, 0.35, 0.24], [0.26, 0.27, 0.2], [0.27, 0.27, 0.21]]
f [0.8172, 0.8166, 0.8169]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 2 	 [0.27, 0.27, 0.21] 	 lr: 0.0002 	 f: 81.6925281600074
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 3 [0m
Train...

[32m iter_0[0m
ga 	p: 81.38 	r: 76.12 	f1: 78.66 	 4695 	 5769 	 6168
wo 	p: 92.38 	r: 86.57 	f1: 89.38 	 2933 	 3175 	 3388
ni 	p: 83.14 	r: 76.81 	f1: 79.85 	 1169 	 1406 	 1522

[32m iter_1[0m
ga 	p: 81.22 	r: 75.96 	f1: 78.5 	 4685 	 5768 	 6168
wo 	p: 92.31 	r: 86.75 	f1: 89.44 	 2939 	 3184 	 3388
ni 	p: 84.72 	r: 75.43 	f1: 79.81 	 1148 	 1355 	 1522

[32m iter_2[0m
ga 	p: 81.75 	r: 75.47 	f1: 78.49 	 4655 	 5694 	 6168
wo 	p: 92.42 	r: 86.78 	f1: 89.51 	 2940 	 3181 	 3388
ni 	p: 84.77 	r: 75.36 	f1: 79.79 	 1147 	 1353 	 1522
best_thres [[0.24, 0.39, 0.12], [0.24, 0.33, 0.16], [0.28, 0.33, 0.16]]
f [0.8211, 0.8207, 0.8207]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse 	current best epoch 10 	 [0.37, 0.76, 0.27] 	 lr: 0.0001 	 f: 82.3898977901416
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse[0m [33m epoch 17 [0m
Train...
loss: tensor(1641.4999) lr: 0.0002 time: 5885.31
pred_count_train 41644

Test...
loss: tensor(1637.8877) lr: 0.0002 time: 6006.32
pred_count_train 41644

Test...
loss: tensor(1634.7312) lr: 0.0002 time: 6010.84
pred_count_train 41644

Test...
loss: tensor(1637.8877) lr: 0.0002 time: 6042.88
pred_count_train 41644

Test...
loss: tensor(1641.4999) lr: 0.0002 time: 6008.08
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.53 	r: 77.22 	f1: 78.36 	 4763 	 5989 	 6168
wo 	p: 93.4 	r: 85.15 	f1: 89.08 	 2885 	 3089 	 3388
ni 	p: 87.0 	r: 80.88 	f1: 83.83 	 1231 	 1415 	 1522

[32m iter_1[0m
ga 	p: 81.48 	r: 76.31 	f1: 78.81 	 4707 	 5777 	 6168
wo 	p: 92.54 	r: 86.07 	f1: 89.19 	 2916 	 3151 	 3388
ni 	p: 86.68 	r: 81.21 	f1: 83.85 	 1236 	 1426 	 1522

[32m iter_2[0m
ga 	p: 81.24 	r: 76.44 	f1: 78.77 	 4715 	 5804 	 6168
wo 	p: 93.04 	r: 85.68 	f1: 89.21 	 2903 	 3120 	 3388
ni 	p: 86.89 	r: 81.41 	f1: 84.06 	 1239 	 1426 	 1522
best_thres [[0.28, 0.68, 0.26], [0.34, 0.6, 0.22], [0.33, 0.66, 0.22]]
f [0.8232, 0.825, 0.8255]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 3 	 [0.33, 0.66, 0.22] 	 lr: 0.0002 	 f: 82.55342924989523
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 4 [0m
Train...

[32m iter_0[0m
ga 	p: 82.0 	r: 75.63 	f1: 78.69 	 4665 	 5689 	 6168
wo 	p: 92.71 	r: 86.39 	f1: 89.44 	 2927 	 3157 	 3388
ni 	p: 85.16 	r: 83.31 	f1: 84.22 	 1268 	 1489 	 1522

[32m iter_1[0m
ga 	p: 82.49 	r: 75.71 	f1: 78.96 	 4670 	 5661 	 6168
wo 	p: 93.35 	r: 85.83 	f1: 89.44 	 2908 	 3115 	 3388
ni 	p: 86.75 	r: 81.73 	f1: 84.17 	 1244 	 1434 	 1522

[32m iter_2[0m
ga 	p: 82.0 	r: 75.84 	f1: 78.8 	 4678 	 5705 	 6168
wo 	p: 92.72 	r: 86.45 	f1: 89.48 	 2929 	 3159 	 3388
ni 	p: 86.71 	r: 82.33 	f1: 84.46 	 1253 	 1445 	 1522
best_thres [[0.38, 0.49, 0.13], [0.39, 0.53, 0.16], [0.38, 0.49, 0.15]]
f [0.8275, 0.8282, 0.8283]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 3 	 [0.38, 0.49, 0.15] 	 lr: 0.0002 	 f: 82.82985894395206
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 4 [0m
Train...

[32m iter_0[0m
ga 	p: 82.63 	r: 74.79 	f1: 78.51 	 4613 	 5583 	 6168
wo 	p: 92.98 	r: 86.36 	f1: 89.55 	 2926 	 3147 	 3388
ni 	p: 86.38 	r: 81.27 	f1: 83.75 	 1237 	 1432 	 1522

[32m iter_1[0m
ga 	p: 82.66 	r: 75.19 	f1: 78.75 	 4638 	 5611 	 6168
wo 	p: 93.33 	r: 85.92 	f1: 89.47 	 2911 	 3119 	 3388
ni 	p: 86.06 	r: 81.54 	f1: 83.74 	 1241 	 1442 	 1522

[32m iter_2[0m
ga 	p: 82.56 	r: 75.15 	f1: 78.68 	 4635 	 5614 	 6168
wo 	p: 93.03 	r: 86.25 	f1: 89.51 	 2922 	 3141 	 3388
ni 	p: 86.04 	r: 81.41 	f1: 83.66 	 1239 	 1440 	 1522
best_thres [[0.39, 0.55, 0.14], [0.39, 0.59, 0.14], [0.39, 0.56, 0.14]]
f [0.8264, 0.8268, 0.8269]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 3 	 [0.39, 0.56, 0.14] 	 lr: 0.0002 	 f: 82.68745197057854
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 4 [0m
Train...
loss: tensor(84.7841) lr: 5e-05 time: 5493.51
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.0 	r: 75.63 	f1: 78.69 	 4665 	 5689 	 6168
wo 	p: 92.71 	r: 86.39 	f1: 89.44 	 2927 	 3157 	 3388
ni 	p: 85.16 	r: 83.31 	f1: 84.22 	 1268 	 1489 	 1522

[32m iter_1[0m
ga 	p: 82.49 	r: 75.71 	f1: 78.96 	 4670 	 5661 	 6168
wo 	p: 93.35 	r: 85.83 	f1: 89.44 	 2908 	 3115 	 3388
ni 	p: 86.75 	r: 81.73 	f1: 84.17 	 1244 	 1434 	 1522

[32m iter_2[0m
ga 	p: 82.0 	r: 75.84 	f1: 78.8 	 4678 	 5705 	 6168
wo 	p: 92.72 	r: 86.45 	f1: 89.48 	 2929 	 3159 	 3388
ni 	p: 86.71 	r: 82.33 	f1: 84.46 	 1253 	 1445 	 1522
best_thres [[0.38, 0.49, 0.13], [0.39, 0.53, 0.16], [0.38, 0.49, 0.15]]
f [0.8275, 0.8282, 0.8283]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 3 	 [0.38, 0.49, 0.15] 	 lr: 0.0002 	 f: 82.82985894395206
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 4 [0m
Train...

[32m iter_0[0m
ga 	p: 79.53 	r: 77.22 	f1: 78.36 	 4763 	 5989 	 6168
wo 	p: 93.4 	r: 85.15 	f1: 89.08 	 2885 	 3089 	 3388
ni 	p: 87.0 	r: 80.88 	f1: 83.83 	 1231 	 1415 	 1522

[32m iter_1[0m
ga 	p: 81.48 	r: 76.31 	f1: 78.81 	 4707 	 5777 	 6168
wo 	p: 92.54 	r: 86.07 	f1: 89.19 	 2916 	 3151 	 3388
ni 	p: 86.68 	r: 81.21 	f1: 83.85 	 1236 	 1426 	 1522

[32m iter_2[0m
ga 	p: 81.24 	r: 76.44 	f1: 78.77 	 4715 	 5804 	 6168
wo 	p: 93.04 	r: 85.68 	f1: 89.21 	 2903 	 3120 	 3388
ni 	p: 86.89 	r: 81.41 	f1: 84.06 	 1239 	 1426 	 1522
best_thres [[0.28, 0.68, 0.26], [0.34, 0.6, 0.22], [0.33, 0.66, 0.22]]
f [0.8232, 0.825, 0.8255]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 3 	 [0.33, 0.66, 0.22] 	 lr: 0.0002 	 f: 82.55342924989523
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 4 [0m
Train...

[32m iter_0[0m
ga 	p: 82.23 	r: 75.57 	f1: 78.76 	 4661 	 5668 	 6168
wo 	p: 92.56 	r: 86.25 	f1: 89.29 	 2922 	 3157 	 3388
ni 	p: 86.08 	r: 71.94 	f1: 78.38 	 1095 	 1272 	 1522

[32m iter_1[0m
ga 	p: 82.7 	r: 75.19 	f1: 78.77 	 4638 	 5608 	 6168
wo 	p: 93.19 	r: 85.98 	f1: 89.44 	 2913 	 3126 	 3388
ni 	p: 80.9 	r: 75.69 	f1: 78.21 	 1152 	 1424 	 1522

[32m iter_2[0m
ga 	p: 82.71 	r: 75.23 	f1: 78.79 	 4640 	 5610 	 6168
wo 	p: 92.79 	r: 86.22 	f1: 89.38 	 2921 	 3148 	 3388
ni 	p: 81.02 	r: 75.43 	f1: 78.12 	 1148 	 1417 	 1522
best_thres [[0.37, 0.63, 0.17], [0.42, 0.7, 0.05], [0.42, 0.62, 0.05]]
f [0.8196, 0.8196, 0.8196]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse 	current best epoch 10 	 [0.37, 0.76, 0.27] 	 lr: 0.0001 	 f: 82.3898977901416
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse[0m [33m epoch 18 [0m
Train...
loss: tensor(1287.3573) lr: 0.0002 time: 5782.48
pred_count_train 41644

Test...
loss: tensor(1293.2733) lr: 0.0002 time: 5905.6
pred_count_train 41644

Test...
loss: tensor(1292.1790) lr: 0.0002 time: 5916.27
pred_count_train 41644

Test...
loss: tensor(1293.2733) lr: 0.0002 time: 5980.1
pred_count_train 41644

Test...
loss: tensor(1287.3573) lr: 0.0002 time: 5935.94
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.51 	r: 74.9 	f1: 78.97 	 4620 	 5532 	 6168
wo 	p: 93.38 	r: 86.13 	f1: 89.61 	 2918 	 3125 	 3388
ni 	p: 88.74 	r: 77.14 	f1: 82.53 	 1174 	 1323 	 1522

[32m iter_1[0m
ga 	p: 80.3 	r: 76.67 	f1: 78.44 	 4729 	 5889 	 6168
wo 	p: 93.91 	r: 85.51 	f1: 89.51 	 2897 	 3085 	 3388
ni 	p: 84.79 	r: 79.11 	f1: 81.85 	 1204 	 1420 	 1522

[32m iter_2[0m
ga 	p: 81.58 	r: 76.02 	f1: 78.7 	 4689 	 5748 	 6168
wo 	p: 92.77 	r: 86.39 	f1: 89.47 	 2927 	 3155 	 3388
ni 	p: 88.3 	r: 77.86 	f1: 82.75 	 1185 	 1342 	 1522
best_thres [[0.57, 0.39, 0.34], [0.43, 0.42, 0.19], [0.5, 0.32, 0.27]]
f [0.8274, 0.8249, 0.8251]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 3 	 [0.33, 0.66, 0.22] 	 lr: 0.0002 	 f: 82.55342924989523
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 5 [0m
Train...

[32m iter_0[0m
ga 	p: 81.34 	r: 76.8 	f1: 79.0 	 4737 	 5824 	 6168
wo 	p: 93.79 	r: 85.21 	f1: 89.3 	 2887 	 3078 	 3388
ni 	p: 85.63 	r: 79.11 	f1: 82.24 	 1204 	 1406 	 1522

[32m iter_1[0m
ga 	p: 81.57 	r: 76.56 	f1: 78.98 	 4722 	 5789 	 6168
wo 	p: 91.05 	r: 87.34 	f1: 89.15 	 2959 	 3250 	 3388
ni 	p: 86.92 	r: 79.89 	f1: 83.26 	 1216 	 1399 	 1522

[32m iter_2[0m
ga 	p: 81.19 	r: 76.96 	f1: 79.02 	 4747 	 5847 	 6168
wo 	p: 93.81 	r: 85.04 	f1: 89.21 	 2881 	 3071 	 3388
ni 	p: 86.9 	r: 79.76 	f1: 83.18 	 1214 	 1397 	 1522
best_thres [[0.52, 0.47, 0.2], [0.53, 0.22, 0.18], [0.54, 0.48, 0.18]]
f [0.8256, 0.8263, 0.8264]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 3 	 [0.38, 0.49, 0.15] 	 lr: 0.0002 	 f: 82.82985894395206
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 5 [0m
Train...

[32m iter_0[0m
ga 	p: 82.9 	r: 74.14 	f1: 78.28 	 4573 	 5516 	 6168
wo 	p: 92.93 	r: 86.16 	f1: 89.42 	 2919 	 3141 	 3388
ni 	p: 87.99 	r: 78.91 	f1: 83.2 	 1201 	 1365 	 1522

[32m iter_1[0m
ga 	p: 84.28 	r: 73.44 	f1: 78.49 	 4530 	 5375 	 6168
wo 	p: 92.81 	r: 86.13 	f1: 89.34 	 2918 	 3144 	 3388
ni 	p: 87.85 	r: 79.3 	f1: 83.36 	 1207 	 1374 	 1522

[32m iter_2[0m
ga 	p: 84.38 	r: 73.39 	f1: 78.51 	 4527 	 5365 	 6168
wo 	p: 92.79 	r: 86.22 	f1: 89.38 	 2921 	 3148 	 3388
ni 	p: 87.47 	r: 79.83 	f1: 83.48 	 1215 	 1389 	 1522
best_thres [[0.53, 0.46, 0.33], [0.58, 0.43, 0.32], [0.59, 0.43, 0.3]]
f [0.824, 0.8247, 0.8251]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 3 	 [0.39, 0.56, 0.14] 	 lr: 0.0002 	 f: 82.68745197057854
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 5 [0m
Train...
loss: tensor(64.1094) lr: 5e-05 time: 5691.44
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.34 	r: 76.8 	f1: 79.0 	 4737 	 5824 	 6168
wo 	p: 93.79 	r: 85.21 	f1: 89.3 	 2887 	 3078 	 3388
ni 	p: 85.63 	r: 79.11 	f1: 82.24 	 1204 	 1406 	 1522

[32m iter_1[0m
ga 	p: 81.57 	r: 76.56 	f1: 78.98 	 4722 	 5789 	 6168
wo 	p: 91.05 	r: 87.34 	f1: 89.15 	 2959 	 3250 	 3388
ni 	p: 86.92 	r: 79.89 	f1: 83.26 	 1216 	 1399 	 1522

[32m iter_2[0m
ga 	p: 81.19 	r: 76.96 	f1: 79.02 	 4747 	 5847 	 6168
wo 	p: 93.81 	r: 85.04 	f1: 89.21 	 2881 	 3071 	 3388
ni 	p: 86.9 	r: 79.76 	f1: 83.18 	 1214 	 1397 	 1522
best_thres [[0.52, 0.47, 0.2], [0.53, 0.22, 0.18], [0.54, 0.48, 0.18]]
f [0.8256, 0.8263, 0.8264]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 3 	 [0.38, 0.49, 0.15] 	 lr: 0.0002 	 f: 82.82985894395206
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 5 [0m
Train...

[32m iter_0[0m
ga 	p: 83.51 	r: 74.9 	f1: 78.97 	 4620 	 5532 	 6168
wo 	p: 93.38 	r: 86.13 	f1: 89.61 	 2918 	 3125 	 3388
ni 	p: 88.74 	r: 77.14 	f1: 82.53 	 1174 	 1323 	 1522

[32m iter_1[0m
ga 	p: 80.3 	r: 76.67 	f1: 78.44 	 4729 	 5889 	 6168
wo 	p: 93.91 	r: 85.51 	f1: 89.51 	 2897 	 3085 	 3388
ni 	p: 84.79 	r: 79.11 	f1: 81.85 	 1204 	 1420 	 1522

[32m iter_2[0m
ga 	p: 81.58 	r: 76.02 	f1: 78.7 	 4689 	 5748 	 6168
wo 	p: 92.77 	r: 86.39 	f1: 89.47 	 2927 	 3155 	 3388
ni 	p: 88.3 	r: 77.86 	f1: 82.75 	 1185 	 1342 	 1522
best_thres [[0.57, 0.39, 0.34], [0.43, 0.42, 0.19], [0.5, 0.32, 0.27]]
f [0.8274, 0.8249, 0.8251]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 3 	 [0.33, 0.66, 0.22] 	 lr: 0.0002 	 f: 82.55342924989523
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 5 [0m
Train...

[32m iter_0[0m
ga 	p: 81.95 	r: 75.15 	f1: 78.4 	 4635 	 5656 	 6168
wo 	p: 92.89 	r: 86.39 	f1: 89.52 	 2927 	 3151 	 3388
ni 	p: 85.3 	r: 75.1 	f1: 79.87 	 1143 	 1340 	 1522

[32m iter_1[0m
ga 	p: 82.21 	r: 75.15 	f1: 78.52 	 4635 	 5638 	 6168
wo 	p: 93.53 	r: 86.16 	f1: 89.69 	 2919 	 3121 	 3388
ni 	p: 81.93 	r: 77.46 	f1: 79.64 	 1179 	 1439 	 1522

[32m iter_2[0m
ga 	p: 82.77 	r: 74.79 	f1: 78.58 	 4613 	 5573 	 6168
wo 	p: 93.39 	r: 86.3 	f1: 89.71 	 2924 	 3131 	 3388
ni 	p: 83.18 	r: 76.35 	f1: 79.62 	 1162 	 1397 	 1522
best_thres [[0.42, 0.74, 0.13], [0.43, 0.8, 0.05], [0.49, 0.78, 0.07]]
f [0.8203, 0.8206, 0.8209]
load model: epoch10
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse 	current best epoch 10 	 [0.37, 0.76, 0.27] 	 lr: 0.0001 	 f: 82.3898977901416
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse[0m [33m epoch 19 [0m
Train...
loss: tensor(985.2929) lr: 0.0002 time: 5674.11
pred_count_train 41644

Test...
loss: tensor(988.1452) lr: 0.0002 time: 5815.86
pred_count_train 41644

Test...
loss: tensor(988.4592) lr: 0.0002 time: 5811.1
pred_count_train 41644

Test...
loss: tensor(988.1452) lr: 0.0002 time: 5806.88
pred_count_train 41644

Test...
loss: tensor(985.2929) lr: 0.0002 time: 5794.94
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.01 	r: 75.18 	f1: 78.45 	 4637 	 5654 	 6168
wo 	p: 92.94 	r: 85.51 	f1: 89.07 	 2897 	 3117 	 3388
ni 	p: 86.77 	r: 77.99 	f1: 82.15 	 1187 	 1368 	 1522

[32m iter_1[0m
ga 	p: 81.32 	r: 75.89 	f1: 78.51 	 4681 	 5756 	 6168
wo 	p: 93.22 	r: 85.66 	f1: 89.28 	 2902 	 3113 	 3388
ni 	p: 87.61 	r: 78.52 	f1: 82.81 	 1195 	 1364 	 1522

[32m iter_2[0m
ga 	p: 81.87 	r: 75.47 	f1: 78.54 	 4655 	 5686 	 6168
wo 	p: 92.81 	r: 86.07 	f1: 89.31 	 2916 	 3142 	 3388
ni 	p: 87.63 	r: 78.65 	f1: 82.89 	 1197 	 1366 	 1522
best_thres [[0.45, 0.49, 0.37], [0.37, 0.46, 0.31], [0.41, 0.43, 0.31]]
f [0.8221, 0.8229, 0.8234]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 3 	 [0.33, 0.66, 0.22] 	 lr: 0.0002 	 f: 82.55342924989523
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 6 [0m
Train...

[32m iter_0[0m
ga 	p: 81.36 	r: 75.88 	f1: 78.52 	 4680 	 5752 	 6168
wo 	p: 93.02 	r: 86.57 	f1: 89.68 	 2933 	 3153 	 3388
ni 	p: 84.99 	r: 78.12 	f1: 81.41 	 1189 	 1399 	 1522

[32m iter_1[0m
ga 	p: 81.05 	r: 76.02 	f1: 78.46 	 4689 	 5785 	 6168
wo 	p: 93.73 	r: 86.01 	f1: 89.7 	 2914 	 3109 	 3388
ni 	p: 84.43 	r: 78.71 	f1: 81.47 	 1198 	 1419 	 1522

[32m iter_2[0m
ga 	p: 81.2 	r: 75.86 	f1: 78.44 	 4679 	 5762 	 6168
wo 	p: 93.5 	r: 86.16 	f1: 89.68 	 2919 	 3122 	 3388
ni 	p: 86.39 	r: 77.99 	f1: 81.98 	 1187 	 1374 	 1522
best_thres [[0.34, 0.58, 0.4], [0.32, 0.65, 0.32], [0.33, 0.67, 0.38]]
f [0.8233, 0.8231, 0.8232]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 3 	 [0.38, 0.49, 0.15] 	 lr: 0.0002 	 f: 82.82985894395206
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 6 [0m
Train...

[32m iter_0[0m
ga 	p: 80.08 	r: 76.41 	f1: 78.2 	 4713 	 5885 	 6168
wo 	p: 92.56 	r: 86.69 	f1: 89.53 	 2937 	 3173 	 3388
ni 	p: 86.55 	r: 78.65 	f1: 82.41 	 1197 	 1383 	 1522

[32m iter_1[0m
ga 	p: 79.99 	r: 76.69 	f1: 78.3 	 4730 	 5913 	 6168
wo 	p: 92.06 	r: 86.95 	f1: 89.44 	 2946 	 3200 	 3388
ni 	p: 86.8 	r: 78.65 	f1: 82.52 	 1197 	 1379 	 1522

[32m iter_2[0m
ga 	p: 80.14 	r: 76.49 	f1: 78.27 	 4718 	 5887 	 6168
wo 	p: 92.12 	r: 87.01 	f1: 89.5 	 2948 	 3200 	 3388
ni 	p: 87.85 	r: 77.92 	f1: 82.59 	 1186 	 1350 	 1522
best_thres [[0.35, 0.52, 0.44], [0.34, 0.45, 0.44], [0.36, 0.45, 0.48]]
f [0.8223, 0.8225, 0.8226]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 3 	 [0.39, 0.56, 0.14] 	 lr: 0.0002 	 f: 82.68745197057854
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 6 [0m
Train...
loss: tensor(236.2672) lr: 2.5e-05 time: 5559.8
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.36 	r: 75.88 	f1: 78.52 	 4680 	 5752 	 6168
wo 	p: 93.02 	r: 86.57 	f1: 89.68 	 2933 	 3153 	 3388
ni 	p: 84.99 	r: 78.12 	f1: 81.41 	 1189 	 1399 	 1522

[32m iter_1[0m
ga 	p: 81.05 	r: 76.02 	f1: 78.46 	 4689 	 5785 	 6168
wo 	p: 93.73 	r: 86.01 	f1: 89.7 	 2914 	 3109 	 3388
ni 	p: 84.43 	r: 78.71 	f1: 81.47 	 1198 	 1419 	 1522

[32m iter_2[0m
ga 	p: 81.2 	r: 75.86 	f1: 78.44 	 4679 	 5762 	 6168
wo 	p: 93.5 	r: 86.16 	f1: 89.68 	 2919 	 3122 	 3388
ni 	p: 86.39 	r: 77.99 	f1: 81.98 	 1187 	 1374 	 1522
best_thres [[0.34, 0.58, 0.4], [0.32, 0.65, 0.32], [0.33, 0.67, 0.38]]
f [0.8233, 0.8231, 0.8232]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 3 	 [0.38, 0.49, 0.15] 	 lr: 0.0002 	 f: 82.82985894395206
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 6 [0m
Train...

[32m iter_0[0m
ga 	p: 82.01 	r: 75.18 	f1: 78.45 	 4637 	 5654 	 6168
wo 	p: 92.94 	r: 85.51 	f1: 89.07 	 2897 	 3117 	 3388
ni 	p: 86.77 	r: 77.99 	f1: 82.15 	 1187 	 1368 	 1522

[32m iter_1[0m
ga 	p: 81.32 	r: 75.89 	f1: 78.51 	 4681 	 5756 	 6168
wo 	p: 93.22 	r: 85.66 	f1: 89.28 	 2902 	 3113 	 3388
ni 	p: 87.61 	r: 78.52 	f1: 82.81 	 1195 	 1364 	 1522

[32m iter_2[0m
ga 	p: 81.87 	r: 75.47 	f1: 78.54 	 4655 	 5686 	 6168
wo 	p: 92.81 	r: 86.07 	f1: 89.31 	 2916 	 3142 	 3388
ni 	p: 87.63 	r: 78.65 	f1: 82.89 	 1197 	 1366 	 1522
best_thres [[0.45, 0.49, 0.37], [0.37, 0.46, 0.31], [0.41, 0.43, 0.31]]
f [0.8221, 0.8229, 0.8234]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 3 	 [0.33, 0.66, 0.22] 	 lr: 0.0002 	 f: 82.55342924989523
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 6 [0m
Train...

[32m iter_0[0m
ga 	p: 82.91 	r: 75.42 	f1: 78.99 	 4652 	 5611 	 6168
wo 	p: 93.19 	r: 86.84 	f1: 89.9 	 2942 	 3157 	 3388
ni 	p: 82.2 	r: 77.07 	f1: 79.55 	 1173 	 1427 	 1522

[32m iter_1[0m
ga 	p: 82.73 	r: 75.44 	f1: 78.92 	 4653 	 5624 	 6168
wo 	p: 93.23 	r: 86.63 	f1: 89.81 	 2935 	 3148 	 3388
ni 	p: 84.06 	r: 75.89 	f1: 79.77 	 1155 	 1374 	 1522

[32m iter_2[0m
ga 	p: 82.93 	r: 75.39 	f1: 78.98 	 4650 	 5607 	 6168
wo 	p: 93.31 	r: 86.51 	f1: 89.78 	 2931 	 3141 	 3388
ni 	p: 83.5 	r: 76.15 	f1: 79.66 	 1159 	 1388 	 1522
best_thres [[0.48, 0.65, 0.08], [0.46, 0.66, 0.11], [0.47, 0.67, 0.1]]
f [0.8242, 0.8241, 0.824]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse 	current best epoch 19 	 [0.47, 0.67, 0.1] 	 lr: 2.5e-05 	 f: 82.40335263926167
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse[0m [33m epoch 20 [0m
Train...
loss: tensor(743.2130) lr: 0.0002 time: 5557.14
pred_count_train 41644

Test...
loss: tensor(741.5734) lr: 0.0002 time: 5753.08
pred_count_train 41644

Test...
loss: tensor(736.2643) lr: 0.0002 time: 5782.65
pred_count_train 41644

Test...
loss: tensor(741.5734) lr: 0.0002 time: 5798.9
pred_count_train 41644

Test...
loss: tensor(743.2130) lr: 0.0002 time: 5687.72
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.68 	r: 75.1 	f1: 77.79 	 4632 	 5741 	 6168
wo 	p: 92.89 	r: 85.98 	f1: 89.3 	 2913 	 3136 	 3388
ni 	p: 86.04 	r: 77.33 	f1: 81.45 	 1177 	 1368 	 1522

[32m iter_1[0m
ga 	p: 79.73 	r: 76.04 	f1: 77.84 	 4690 	 5882 	 6168
wo 	p: 93.43 	r: 85.15 	f1: 89.1 	 2885 	 3088 	 3388
ni 	p: 87.67 	r: 76.15 	f1: 81.5 	 1159 	 1322 	 1522

[32m iter_2[0m
ga 	p: 79.69 	r: 76.17 	f1: 77.89 	 4698 	 5895 	 6168
wo 	p: 93.41 	r: 85.36 	f1: 89.2 	 2892 	 3096 	 3388
ni 	p: 86.81 	r: 76.54 	f1: 81.35 	 1165 	 1342 	 1522
best_thres [[0.43, 0.48, 0.17], [0.36, 0.57, 0.16], [0.35, 0.61, 0.15]]
f [0.8181, 0.8177, 0.8178]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 3 	 [0.33, 0.66, 0.22] 	 lr: 0.0002 	 f: 82.55342924989523
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 7 [0m
Train...

[32m iter_0[0m
ga 	p: 82.95 	r: 73.28 	f1: 77.82 	 4520 	 5449 	 6168
wo 	p: 92.65 	r: 85.24 	f1: 88.79 	 2888 	 3117 	 3388
ni 	p: 85.49 	r: 76.68 	f1: 80.85 	 1167 	 1365 	 1522

[32m iter_1[0m
ga 	p: 82.15 	r: 74.08 	f1: 77.9 	 4569 	 5562 	 6168
wo 	p: 91.63 	r: 86.28 	f1: 88.87 	 2923 	 3190 	 3388
ni 	p: 89.03 	r: 75.16 	f1: 81.51 	 1144 	 1285 	 1522

[32m iter_2[0m
ga 	p: 82.0 	r: 74.43 	f1: 78.03 	 4591 	 5599 	 6168
wo 	p: 92.42 	r: 85.63 	f1: 88.89 	 2901 	 3139 	 3388
ni 	p: 88.91 	r: 75.3 	f1: 81.54 	 1146 	 1289 	 1522
best_thres [[0.58, 0.72, 0.23], [0.52, 0.65, 0.29], [0.55, 0.77, 0.27]]
f [0.8163, 0.8172, 0.8176]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 3 	 [0.38, 0.49, 0.15] 	 lr: 0.0002 	 f: 82.82985894395206
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 7 [0m
Train...

[32m iter_0[0m
ga 	p: 81.0 	r: 75.19 	f1: 77.99 	 4638 	 5726 	 6168
wo 	p: 92.24 	r: 86.36 	f1: 89.21 	 2926 	 3172 	 3388
ni 	p: 87.26 	r: 75.62 	f1: 81.03 	 1151 	 1319 	 1522

[32m iter_1[0m
ga 	p: 80.7 	r: 75.57 	f1: 78.05 	 4661 	 5776 	 6168
wo 	p: 91.74 	r: 86.84 	f1: 89.22 	 2942 	 3207 	 3388
ni 	p: 87.73 	r: 75.16 	f1: 80.96 	 1144 	 1304 	 1522

[32m iter_2[0m
ga 	p: 80.64 	r: 75.57 	f1: 78.02 	 4661 	 5780 	 6168
wo 	p: 91.54 	r: 86.87 	f1: 89.14 	 2943 	 3215 	 3388
ni 	p: 87.31 	r: 75.49 	f1: 80.97 	 1149 	 1316 	 1522
best_thres [[0.41, 0.55, 0.15], [0.4, 0.46, 0.15], [0.4, 0.45, 0.15]]
f [0.8185, 0.8187, 0.8186]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 3 	 [0.39, 0.56, 0.14] 	 lr: 0.0002 	 f: 82.68745197057854
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 7 [0m
Train...
loss: tensor(126.0667) lr: 2.5e-05 time: 5424.62
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.95 	r: 73.28 	f1: 77.82 	 4520 	 5449 	 6168
wo 	p: 92.65 	r: 85.24 	f1: 88.79 	 2888 	 3117 	 3388
ni 	p: 85.49 	r: 76.68 	f1: 80.85 	 1167 	 1365 	 1522

[32m iter_1[0m
ga 	p: 82.15 	r: 74.08 	f1: 77.9 	 4569 	 5562 	 6168
wo 	p: 91.63 	r: 86.28 	f1: 88.87 	 2923 	 3190 	 3388
ni 	p: 89.03 	r: 75.16 	f1: 81.51 	 1144 	 1285 	 1522

[32m iter_2[0m
ga 	p: 82.0 	r: 74.43 	f1: 78.03 	 4591 	 5599 	 6168
wo 	p: 92.42 	r: 85.63 	f1: 88.89 	 2901 	 3139 	 3388
ni 	p: 88.91 	r: 75.3 	f1: 81.54 	 1146 	 1289 	 1522
best_thres [[0.58, 0.72, 0.23], [0.52, 0.65, 0.29], [0.55, 0.77, 0.27]]
f [0.8163, 0.8172, 0.8176]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 3 	 [0.38, 0.49, 0.15] 	 lr: 0.0002 	 f: 82.82985894395206
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 7 [0m
Train...

[32m iter_0[0m
ga 	p: 80.68 	r: 75.1 	f1: 77.79 	 4632 	 5741 	 6168
wo 	p: 92.89 	r: 85.98 	f1: 89.3 	 2913 	 3136 	 3388
ni 	p: 86.04 	r: 77.33 	f1: 81.45 	 1177 	 1368 	 1522

[32m iter_1[0m
ga 	p: 79.73 	r: 76.04 	f1: 77.84 	 4690 	 5882 	 6168
wo 	p: 93.43 	r: 85.15 	f1: 89.1 	 2885 	 3088 	 3388
ni 	p: 87.67 	r: 76.15 	f1: 81.5 	 1159 	 1322 	 1522

[32m iter_2[0m
ga 	p: 79.69 	r: 76.17 	f1: 77.89 	 4698 	 5895 	 6168
wo 	p: 93.41 	r: 85.36 	f1: 89.2 	 2892 	 3096 	 3388
ni 	p: 86.81 	r: 76.54 	f1: 81.35 	 1165 	 1342 	 1522
best_thres [[0.43, 0.48, 0.17], [0.36, 0.57, 0.16], [0.35, 0.61, 0.15]]
f [0.8181, 0.8177, 0.8178]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 3 	 [0.33, 0.66, 0.22] 	 lr: 0.0002 	 f: 82.55342924989523
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 7 [0m
Train...

[32m iter_0[0m
ga 	p: 81.39 	r: 76.2 	f1: 78.71 	 4700 	 5775 	 6168
wo 	p: 93.64 	r: 86.07 	f1: 89.7 	 2916 	 3114 	 3388
ni 	p: 84.44 	r: 73.46 	f1: 78.57 	 1118 	 1324 	 1522

[32m iter_1[0m
ga 	p: 81.55 	r: 76.12 	f1: 78.74 	 4695 	 5757 	 6168
wo 	p: 93.65 	r: 86.22 	f1: 89.78 	 2921 	 3119 	 3388
ni 	p: 82.04 	r: 75.03 	f1: 78.38 	 1142 	 1392 	 1522

[32m iter_2[0m
ga 	p: 81.61 	r: 76.1 	f1: 78.76 	 4694 	 5752 	 6168
wo 	p: 93.71 	r: 86.16 	f1: 89.77 	 2919 	 3115 	 3388
ni 	p: 84.26 	r: 73.52 	f1: 78.53 	 1119 	 1328 	 1522
best_thres [[0.33, 0.73, 0.11], [0.32, 0.71, 0.07], [0.32, 0.72, 0.1]]
f [0.8204, 0.8205, 0.8207]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse 	current best epoch 19 	 [0.47, 0.67, 0.1] 	 lr: 2.5e-05 	 f: 82.40335263926167
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse[0m [33m epoch 21 [0m
Train...
loss: tensor(567.2493) lr: 0.0002 time: 5616.35
pred_count_train 41644

Test...
loss: tensor(577.8354) lr: 0.0002 time: 5806.16
pred_count_train 41644

Test...
loss: tensor(565.9879) lr: 0.0002 time: 5801.74
pred_count_train 41644

Test...
loss: tensor(577.8354) lr: 0.0002 time: 5709.7
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.35 	r: 74.64 	f1: 76.93 	 4604 	 5802 	 6168
wo 	p: 92.67 	r: 83.97 	f1: 88.11 	 2845 	 3070 	 3388
ni 	p: 80.96 	r: 73.46 	f1: 77.02 	 1118 	 1381 	 1522

[32m iter_1[0m
ga 	p: 78.49 	r: 75.6 	f1: 77.02 	 4663 	 5941 	 6168
wo 	p: 92.81 	r: 84.21 	f1: 88.3 	 2853 	 3074 	 3388
ni 	p: 87.05 	r: 68.92 	f1: 76.93 	 1049 	 1205 	 1522

[32m iter_2[0m
ga 	p: 78.71 	r: 75.78 	f1: 77.22 	 4674 	 5938 	 6168
wo 	p: 92.37 	r: 84.74 	f1: 88.39 	 2871 	 3108 	 3388
ni 	p: 87.87 	r: 68.99 	f1: 77.29 	 1050 	 1195 	 1522
best_thres [[0.46, 0.72, 0.19], [0.34, 0.78, 0.27], [0.36, 0.8, 0.29]]
f [0.8032, 0.8038, 0.8046]
load model: epoch3
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 3 	 [0.33, 0.66, 0.22] 	 lr: 0.0002 	 f: 82.55342924989523
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 8 [0m
Train...
loss: tensor(567.2493) lr: 0.0002 time: 5630.51
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.18 	r: 72.73 	f1: 77.61 	 4486 	 5393 	 6168
wo 	p: 93.65 	r: 84.95 	f1: 89.09 	 2878 	 3073 	 3388
ni 	p: 81.13 	r: 72.01 	f1: 76.3 	 1096 	 1351 	 1522

[32m iter_1[0m
ga 	p: 80.25 	r: 75.15 	f1: 77.61 	 4635 	 5776 	 6168
wo 	p: 93.88 	r: 84.68 	f1: 89.04 	 2869 	 3056 	 3388
ni 	p: 81.04 	r: 70.76 	f1: 75.55 	 1077 	 1329 	 1522

[32m iter_2[0m
ga 	p: 79.04 	r: 76.31 	f1: 77.65 	 4707 	 5955 	 6168
wo 	p: 93.52 	r: 84.83 	f1: 88.96 	 2874 	 3073 	 3388
ni 	p: 79.87 	r: 71.68 	f1: 75.55 	 1091 	 1366 	 1522
best_thres [[0.65, 0.59, 0.09], [0.46, 0.63, 0.07], [0.37, 0.65, 0.06]]
f [0.8098, 0.8089, 0.8085]
load model: epoch3
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 3 	 [0.38, 0.49, 0.15] 	 lr: 0.0002 	 f: 82.82985894395206
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 8 [0m
Train...
loss: tensor(75.9816) lr: 2.5e-05 time: 5366.68
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.01 	r: 73.93 	f1: 78.21 	 4560 	 5493 	 6168
wo 	p: 92.61 	r: 85.06 	f1: 88.68 	 2882 	 3112 	 3388
ni 	p: 84.14 	r: 75.62 	f1: 79.65 	 1151 	 1368 	 1522

[32m iter_1[0m
ga 	p: 80.96 	r: 75.29 	f1: 78.02 	 4644 	 5736 	 6168
wo 	p: 92.87 	r: 84.92 	f1: 88.71 	 2877 	 3098 	 3388
ni 	p: 83.52 	r: 75.23 	f1: 79.16 	 1145 	 1371 	 1522

[32m iter_2[0m
ga 	p: 82.69 	r: 73.83 	f1: 78.01 	 4554 	 5507 	 6168
wo 	p: 92.56 	r: 85.21 	f1: 88.74 	 2887 	 3119 	 3388
ni 	p: 83.53 	r: 75.3 	f1: 79.2 	 1146 	 1372 	 1522
best_thres [[0.45, 0.68, 0.16], [0.31, 0.67, 0.15], [0.44, 0.66, 0.15]]
f [0.8164, 0.8154, 0.8152]
load model: epoch3
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 3 	 [0.39, 0.56, 0.14] 	 lr: 0.0002 	 f: 82.68745197057854
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 8 [0m
Train...

[32m iter_0[0m
ga 	p: 83.18 	r: 72.73 	f1: 77.61 	 4486 	 5393 	 6168
wo 	p: 93.65 	r: 84.95 	f1: 89.09 	 2878 	 3073 	 3388
ni 	p: 81.13 	r: 72.01 	f1: 76.3 	 1096 	 1351 	 1522

[32m iter_1[0m
ga 	p: 80.25 	r: 75.15 	f1: 77.61 	 4635 	 5776 	 6168
wo 	p: 93.88 	r: 84.68 	f1: 89.04 	 2869 	 3056 	 3388
ni 	p: 81.04 	r: 70.76 	f1: 75.55 	 1077 	 1329 	 1522

[32m iter_2[0m
ga 	p: 79.04 	r: 76.31 	f1: 77.65 	 4707 	 5955 	 6168
wo 	p: 93.52 	r: 84.83 	f1: 88.96 	 2874 	 3073 	 3388
ni 	p: 79.87 	r: 71.68 	f1: 75.55 	 1091 	 1366 	 1522
best_thres [[0.65, 0.59, 0.09], [0.46, 0.63, 0.07], [0.37, 0.65, 0.06]]
f [0.8098, 0.8089, 0.8085]
load model: epoch3
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 3 	 [0.38, 0.49, 0.15] 	 lr: 0.0002 	 f: 82.82985894395206
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 8 [0m
Train...

[32m iter_0[0m
ga 	p: 79.35 	r: 74.64 	f1: 76.93 	 4604 	 5802 	 6168
wo 	p: 92.67 	r: 83.97 	f1: 88.11 	 2845 	 3070 	 3388
ni 	p: 80.96 	r: 73.46 	f1: 77.02 	 1118 	 1381 	 1522

[32m iter_1[0m
ga 	p: 78.49 	r: 75.6 	f1: 77.02 	 4663 	 5941 	 6168
wo 	p: 92.81 	r: 84.21 	f1: 88.3 	 2853 	 3074 	 3388
ni 	p: 87.05 	r: 68.92 	f1: 76.93 	 1049 	 1205 	 1522

[32m iter_2[0m
ga 	p: 78.71 	r: 75.78 	f1: 77.22 	 4674 	 5938 	 6168
wo 	p: 92.37 	r: 84.74 	f1: 88.39 	 2871 	 3108 	 3388
ni 	p: 87.87 	r: 68.99 	f1: 77.29 	 1050 	 1195 	 1522
best_thres [[0.46, 0.72, 0.19], [0.34, 0.78, 0.27], [0.36, 0.8, 0.29]]
f [0.8032, 0.8038, 0.8046]
load model: epoch3
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 3 	 [0.33, 0.66, 0.22] 	 lr: 0.0002 	 f: 82.55342924989523
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 8 [0m
Train...

[32m iter_0[0m
ga 	p: 80.39 	r: 76.7 	f1: 78.5 	 4731 	 5885 	 6168
wo 	p: 93.3 	r: 86.3 	f1: 89.67 	 2924 	 3134 	 3388
ni 	p: 85.77 	r: 73.65 	f1: 79.25 	 1121 	 1307 	 1522

[32m iter_1[0m
ga 	p: 82.45 	r: 75.02 	f1: 78.56 	 4627 	 5612 	 6168
wo 	p: 93.69 	r: 86.33 	f1: 89.86 	 2925 	 3122 	 3388
ni 	p: 85.34 	r: 74.18 	f1: 79.37 	 1129 	 1323 	 1522

[32m iter_2[0m
ga 	p: 82.48 	r: 75.03 	f1: 78.58 	 4628 	 5611 	 6168
wo 	p: 93.69 	r: 86.39 	f1: 89.9 	 2927 	 3124 	 3388
ni 	p: 85.78 	r: 73.72 	f1: 79.29 	 1122 	 1308 	 1522
best_thres [[0.29, 0.67, 0.14], [0.49, 0.67, 0.13], [0.49, 0.67, 0.14]]
f [0.82, 0.8208, 0.821]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse 	current best epoch 19 	 [0.47, 0.67, 0.1] 	 lr: 2.5e-05 	 f: 82.40335263926167
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse[0m [33m epoch 22 [0m
Train...
loss: tensor(1087.0262) lr: 0.0001 time: 5630.08
pred_count_train 41644

Test...
loss: tensor(1093.0525) lr: 0.0001 time: 5804.82
pred_count_train 41644

Test...
loss: tensor(1092.0839) lr: 0.0001 time: 5807.97
pred_count_train 41644

Test...
loss: tensor(1093.0525) lr: 0.0001 time: 5729.36
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 84.36 	r: 75.13 	f1: 79.48 	 4634 	 5493 	 6168
wo 	p: 92.57 	r: 87.1 	f1: 89.75 	 2951 	 3188 	 3388
ni 	p: 86.42 	r: 78.58 	f1: 82.31 	 1196 	 1384 	 1522

[32m iter_1[0m
ga 	p: 82.58 	r: 76.41 	f1: 79.38 	 4713 	 5707 	 6168
wo 	p: 93.08 	r: 86.54 	f1: 89.69 	 2932 	 3150 	 3388
ni 	p: 87.79 	r: 77.46 	f1: 82.3 	 1179 	 1343 	 1522

[32m iter_2[0m
ga 	p: 82.32 	r: 76.7 	f1: 79.41 	 4731 	 5747 	 6168
wo 	p: 93.72 	r: 85.89 	f1: 89.63 	 2910 	 3105 	 3388
ni 	p: 85.72 	r: 79.3 	f1: 82.39 	 1207 	 1408 	 1522
best_thres [[0.56, 0.37, 0.2], [0.46, 0.4, 0.18], [0.45, 0.56, 0.13]]
f [0.8306, 0.83, 0.8298]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 8 	 [0.45, 0.56, 0.13] 	 lr: 0.0001 	 f: 82.97808936777554
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 9 [0m
Train...
loss: tensor(1087.0262) lr: 0.0001 time: 5625.91
pred_count_train 41644

Test...
loss: tensor(47.6610) lr: 2.5e-05 time: 5353.61
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.91 	r: 77.84 	f1: 79.34 	 4801 	 5934 	 6168
wo 	p: 93.46 	r: 86.42 	f1: 89.8 	 2928 	 3133 	 3388
ni 	p: 88.58 	r: 78.52 	f1: 83.25 	 1195 	 1349 	 1522

[32m iter_1[0m
ga 	p: 82.57 	r: 76.73 	f1: 79.55 	 4733 	 5732 	 6168
wo 	p: 93.08 	r: 86.54 	f1: 89.69 	 2932 	 3150 	 3388
ni 	p: 88.38 	r: 79.43 	f1: 83.67 	 1209 	 1368 	 1522

[32m iter_2[0m
ga 	p: 82.17 	r: 77.17 	f1: 79.59 	 4760 	 5793 	 6168
wo 	p: 92.77 	r: 86.69 	f1: 89.62 	 2937 	 3166 	 3388
ni 	p: 88.35 	r: 79.24 	f1: 83.55 	 1206 	 1365 	 1522
best_thres [[0.38, 0.57, 0.25], [0.45, 0.54, 0.19], [0.43, 0.54, 0.19]]
f [0.8304, 0.8313, 0.8315]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 8 	 [0.43, 0.54, 0.19] 	 lr: 0.0001 	 f: 83.14960139511709
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 9 [0m
Train...

[32m iter_0[0m
ga 	p: 83.74 	r: 75.13 	f1: 79.2 	 4634 	 5534 	 6168
wo 	p: 93.03 	r: 87.01 	f1: 89.92 	 2948 	 3169 	 3388
ni 	p: 86.31 	r: 79.96 	f1: 83.02 	 1217 	 1410 	 1522

[32m iter_1[0m
ga 	p: 83.56 	r: 75.29 	f1: 79.21 	 4644 	 5558 	 6168
wo 	p: 93.28 	r: 86.89 	f1: 89.98 	 2944 	 3156 	 3388
ni 	p: 86.15 	r: 80.95 	f1: 83.47 	 1232 	 1430 	 1522

[32m iter_2[0m
ga 	p: 83.9 	r: 75.02 	f1: 79.21 	 4627 	 5515 	 6168
wo 	p: 93.35 	r: 86.66 	f1: 89.88 	 2936 	 3145 	 3388
ni 	p: 86.13 	r: 80.81 	f1: 83.39 	 1230 	 1428 	 1522
best_thres [[0.5, 0.48, 0.14], [0.49, 0.48, 0.13], [0.51, 0.51, 0.13]]
f [0.8304, 0.8308, 0.8308]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 8 	 [0.51, 0.51, 0.13] 	 lr: 0.0001 	 f: 83.08403718208844
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 9 [0m
Train...

[32m iter_0[0m
ga 	p: 80.91 	r: 77.84 	f1: 79.34 	 4801 	 5934 	 6168
wo 	p: 93.46 	r: 86.42 	f1: 89.8 	 2928 	 3133 	 3388
ni 	p: 88.58 	r: 78.52 	f1: 83.25 	 1195 	 1349 	 1522

[32m iter_1[0m
ga 	p: 82.57 	r: 76.73 	f1: 79.55 	 4733 	 5732 	 6168
wo 	p: 93.08 	r: 86.54 	f1: 89.69 	 2932 	 3150 	 3388
ni 	p: 88.38 	r: 79.43 	f1: 83.67 	 1209 	 1368 	 1522

[32m iter_2[0m
ga 	p: 82.17 	r: 77.17 	f1: 79.59 	 4760 	 5793 	 6168
wo 	p: 92.77 	r: 86.69 	f1: 89.62 	 2937 	 3166 	 3388
ni 	p: 88.35 	r: 79.24 	f1: 83.55 	 1206 	 1365 	 1522
best_thres [[0.38, 0.57, 0.25], [0.45, 0.54, 0.19], [0.43, 0.54, 0.19]]
f [0.8304, 0.8313, 0.8315]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 8 	 [0.43, 0.54, 0.19] 	 lr: 0.0001 	 f: 83.14960139511709
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 9 [0m
Train...

[32m iter_0[0m
ga 	p: 84.36 	r: 75.13 	f1: 79.48 	 4634 	 5493 	 6168
wo 	p: 92.57 	r: 87.1 	f1: 89.75 	 2951 	 3188 	 3388
ni 	p: 86.42 	r: 78.58 	f1: 82.31 	 1196 	 1384 	 1522

[32m iter_1[0m
ga 	p: 82.58 	r: 76.41 	f1: 79.38 	 4713 	 5707 	 6168
wo 	p: 93.08 	r: 86.54 	f1: 89.69 	 2932 	 3150 	 3388
ni 	p: 87.79 	r: 77.46 	f1: 82.3 	 1179 	 1343 	 1522

[32m iter_2[0m
ga 	p: 82.32 	r: 76.7 	f1: 79.41 	 4731 	 5747 	 6168
wo 	p: 93.72 	r: 85.89 	f1: 89.63 	 2910 	 3105 	 3388
ni 	p: 85.72 	r: 79.3 	f1: 82.39 	 1207 	 1408 	 1522
best_thres [[0.56, 0.37, 0.2], [0.46, 0.4, 0.18], [0.45, 0.56, 0.13]]
f [0.8306, 0.83, 0.8298]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 8 	 [0.45, 0.56, 0.13] 	 lr: 0.0001 	 f: 82.97808936777554
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 9 [0m
Train...

[32m iter_0[0m
ga 	p: 82.27 	r: 75.31 	f1: 78.64 	 4645 	 5646 	 6168
wo 	p: 92.76 	r: 86.57 	f1: 89.56 	 2933 	 3162 	 3388
ni 	p: 85.48 	r: 74.24 	f1: 79.47 	 1130 	 1322 	 1522

[32m iter_1[0m
ga 	p: 82.29 	r: 75.18 	f1: 78.57 	 4637 	 5635 	 6168
wo 	p: 93.3 	r: 86.28 	f1: 89.65 	 2923 	 3133 	 3388
ni 	p: 85.98 	r: 73.32 	f1: 79.15 	 1116 	 1298 	 1522

[32m iter_2[0m
ga 	p: 83.02 	r: 74.68 	f1: 78.63 	 4606 	 5548 	 6168
wo 	p: 93.05 	r: 86.57 	f1: 89.69 	 2933 	 3152 	 3388
ni 	p: 85.99 	r: 73.39 	f1: 79.19 	 1117 	 1299 	 1522
best_thres [[0.33, 0.55, 0.07], [0.32, 0.59, 0.08], [0.4, 0.55, 0.08]]
f [0.8212, 0.8209, 0.8211]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse 	current best epoch 19 	 [0.47, 0.67, 0.1] 	 lr: 2.5e-05 	 f: 82.40335263926167
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse[0m [33m epoch 23 [0m
Train...
loss: tensor(757.2229) lr: 0.0001 time: 5401.38
pred_count_train 41644

Test...
loss: tensor(754.5905) lr: 0.0001 time: 5809.89
pred_count_train 41644

Test...
loss: tensor(753.2182) lr: 0.0001 time: 5847.18
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.92 	r: 74.84 	f1: 78.22 	 4616 	 5635 	 6168
wo 	p: 92.32 	r: 86.54 	f1: 89.34 	 2932 	 3176 	 3388
ni 	p: 86.8 	r: 75.62 	f1: 80.83 	 1151 	 1326 	 1522

[32m iter_1[0m
ga 	p: 82.78 	r: 74.12 	f1: 78.21 	 4572 	 5523 	 6168
wo 	p: 92.58 	r: 86.48 	f1: 89.42 	 2930 	 3165 	 3388
ni 	p: 85.92 	r: 76.15 	f1: 80.74 	 1159 	 1349 	 1522

[32m iter_2[0m
ga 	p: 83.1 	r: 74.08 	f1: 78.33 	 4569 	 5498 	 6168
wo 	p: 92.87 	r: 86.1 	f1: 89.36 	 2917 	 3141 	 3388
ni 	p: 85.59 	r: 76.48 	f1: 80.78 	 1164 	 1360 	 1522
best_thres [[0.44, 0.51, 0.29], [0.48, 0.64, 0.21], [0.51, 0.73, 0.19]]
f [0.8201, 0.8202, 0.8204]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 8 	 [0.45, 0.56, 0.13] 	 lr: 0.0001 	 f: 82.97808936777554
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 10 [0m
Train...
loss: tensor(754.5905) lr: 0.0001 time: 5866.69
pred_count_train 41644

Test...
loss: tensor(757.2229) lr: 0.0001 time: 5813.43
pred_count_train 41644

Test...
loss: tensor(33.3519) lr: 2.5e-05 time: 5657.5
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.2 	r: 76.2 	f1: 78.62 	 4700 	 5788 	 6168
wo 	p: 92.48 	r: 86.42 	f1: 89.35 	 2928 	 3166 	 3388
ni 	p: 84.04 	r: 77.86 	f1: 80.83 	 1185 	 1410 	 1522

[32m iter_1[0m
ga 	p: 82.12 	r: 75.58 	f1: 78.72 	 4662 	 5677 	 6168
wo 	p: 92.8 	r: 86.33 	f1: 89.45 	 2925 	 3152 	 3388
ni 	p: 82.06 	r: 80.55 	f1: 81.3 	 1226 	 1494 	 1522

[32m iter_2[0m
ga 	p: 81.78 	r: 75.91 	f1: 78.74 	 4682 	 5725 	 6168
wo 	p: 92.03 	r: 86.87 	f1: 89.37 	 2943 	 3198 	 3388
ni 	p: 81.6 	r: 81.01 	f1: 81.31 	 1233 	 1511 	 1522
best_thres [[0.43, 0.5, 0.2], [0.46, 0.59, 0.11], [0.45, 0.46, 0.1]]
f [0.822, 0.8228, 0.8231]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 8 	 [0.43, 0.54, 0.19] 	 lr: 0.0001 	 f: 83.14960139511709
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 10 [0m
Train...

[32m iter_0[0m
ga 	p: 79.61 	r: 77.56 	f1: 78.57 	 4784 	 6009 	 6168
wo 	p: 92.11 	r: 87.22 	f1: 89.6 	 2955 	 3208 	 3388
ni 	p: 87.42 	r: 76.22 	f1: 81.43 	 1160 	 1327 	 1522

[32m iter_1[0m
ga 	p: 81.44 	r: 76.54 	f1: 78.91 	 4721 	 5797 	 6168
wo 	p: 94.0 	r: 85.54 	f1: 89.57 	 2898 	 3083 	 3388
ni 	p: 82.78 	r: 79.89 	f1: 81.31 	 1216 	 1469 	 1522

[32m iter_2[0m
ga 	p: 81.32 	r: 76.46 	f1: 78.82 	 4716 	 5799 	 6168
wo 	p: 92.9 	r: 86.54 	f1: 89.61 	 2932 	 3156 	 3388
ni 	p: 82.91 	r: 79.7 	f1: 81.27 	 1213 	 1463 	 1522
best_thres [[0.35, 0.52, 0.29], [0.45, 0.69, 0.15], [0.45, 0.57, 0.15]]
f [0.8231, 0.8239, 0.8241]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 8 	 [0.51, 0.51, 0.13] 	 lr: 0.0001 	 f: 83.08403718208844
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 10 [0m
Train...

[32m iter_0[0m
ga 	p: 81.2 	r: 76.2 	f1: 78.62 	 4700 	 5788 	 6168
wo 	p: 92.48 	r: 86.42 	f1: 89.35 	 2928 	 3166 	 3388
ni 	p: 84.04 	r: 77.86 	f1: 80.83 	 1185 	 1410 	 1522

[32m iter_1[0m
ga 	p: 82.12 	r: 75.58 	f1: 78.72 	 4662 	 5677 	 6168
wo 	p: 92.8 	r: 86.33 	f1: 89.45 	 2925 	 3152 	 3388
ni 	p: 82.06 	r: 80.55 	f1: 81.3 	 1226 	 1494 	 1522

[32m iter_2[0m
ga 	p: 81.78 	r: 75.91 	f1: 78.74 	 4682 	 5725 	 6168
wo 	p: 92.03 	r: 86.87 	f1: 89.37 	 2943 	 3198 	 3388
ni 	p: 81.6 	r: 81.01 	f1: 81.31 	 1233 	 1511 	 1522
best_thres [[0.43, 0.5, 0.2], [0.46, 0.59, 0.11], [0.45, 0.46, 0.1]]
f [0.822, 0.8228, 0.8231]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 8 	 [0.43, 0.54, 0.19] 	 lr: 0.0001 	 f: 83.14960139511709
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 10 [0m
Train...

[32m iter_0[0m
ga 	p: 81.92 	r: 74.84 	f1: 78.22 	 4616 	 5635 	 6168
wo 	p: 92.32 	r: 86.54 	f1: 89.34 	 2932 	 3176 	 3388
ni 	p: 86.8 	r: 75.62 	f1: 80.83 	 1151 	 1326 	 1522

[32m iter_1[0m
ga 	p: 82.78 	r: 74.12 	f1: 78.21 	 4572 	 5523 	 6168
wo 	p: 92.58 	r: 86.48 	f1: 89.42 	 2930 	 3165 	 3388
ni 	p: 85.92 	r: 76.15 	f1: 80.74 	 1159 	 1349 	 1522

[32m iter_2[0m
ga 	p: 83.1 	r: 74.08 	f1: 78.33 	 4569 	 5498 	 6168
wo 	p: 92.87 	r: 86.1 	f1: 89.36 	 2917 	 3141 	 3388
ni 	p: 85.59 	r: 76.48 	f1: 80.78 	 1164 	 1360 	 1522
best_thres [[0.44, 0.51, 0.29], [0.48, 0.64, 0.21], [0.51, 0.73, 0.19]]
f [0.8201, 0.8202, 0.8204]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 8 	 [0.45, 0.56, 0.13] 	 lr: 0.0001 	 f: 82.97808936777554
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 10 [0m
Train...

[32m iter_0[0m
ga 	p: 81.08 	r: 75.52 	f1: 78.2 	 4658 	 5745 	 6168
wo 	p: 93.32 	r: 86.22 	f1: 89.63 	 2921 	 3130 	 3388
ni 	p: 84.71 	r: 73.92 	f1: 78.95 	 1125 	 1328 	 1522

[32m iter_1[0m
ga 	p: 81.55 	r: 75.34 	f1: 78.32 	 4647 	 5698 	 6168
wo 	p: 93.44 	r: 86.22 	f1: 89.68 	 2921 	 3126 	 3388
ni 	p: 85.1 	r: 73.52 	f1: 78.89 	 1119 	 1315 	 1522

[32m iter_2[0m
ga 	p: 81.61 	r: 75.41 	f1: 78.39 	 4651 	 5699 	 6168
wo 	p: 93.27 	r: 86.28 	f1: 89.64 	 2923 	 3134 	 3388
ni 	p: 85.47 	r: 73.06 	f1: 78.78 	 1112 	 1301 	 1522
best_thres [[0.26, 0.83, 0.09], [0.3, 0.84, 0.09], [0.3, 0.82, 0.1]]
f [0.818, 0.8184, 0.8186]
load model: epoch19
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse 	current best epoch 19 	 [0.47, 0.67, 0.1] 	 lr: 2.5e-05 	 f: 82.40335263926167
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse[0m [33m epoch 24 [0m
Train...
loss: tensor(515.6434) lr: 0.0001 time: 5186.9
pred_count_train 41644

Test...
loss: tensor(533.4177) lr: 0.0001 time: 5525.15
pred_count_train 41644

Test...
loss: tensor(529.4289) lr: 0.0001 time: 5558.04
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.24 	r: 74.4 	f1: 77.67 	 4589 	 5649 	 6168
wo 	p: 93.27 	r: 84.21 	f1: 88.51 	 2853 	 3059 	 3388
ni 	p: 84.52 	r: 76.74 	f1: 80.44 	 1168 	 1382 	 1522

[32m iter_1[0m
ga 	p: 81.28 	r: 74.9 	f1: 77.96 	 4620 	 5684 	 6168
wo 	p: 93.24 	r: 84.68 	f1: 88.75 	 2869 	 3077 	 3388
ni 	p: 85.78 	r: 75.69 	f1: 80.42 	 1152 	 1343 	 1522

[32m iter_2[0m
ga 	p: 82.19 	r: 74.32 	f1: 78.06 	 4584 	 5577 	 6168
wo 	p: 93.18 	r: 84.68 	f1: 88.73 	 2869 	 3079 	 3388
ni 	p: 85.13 	r: 77.14 	f1: 80.94 	 1174 	 1379 	 1522
best_thres [[0.49, 0.48, 0.26], [0.51, 0.47, 0.23], [0.7, 0.49, 0.17]]
f [0.8135, 0.8147, 0.8155]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 8 	 [0.45, 0.56, 0.13] 	 lr: 0.0001 	 f: 82.97808936777554
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 11 [0m
Train...
loss: tensor(533.4177) lr: 0.0001 time: 5659.9
pred_count_train 41644

Test...
loss: tensor(515.6434) lr: 0.0001 time: 5658.81
pred_count_train 41644

Test...
loss: tensor(120.8124) lr: 1.25e-05 time: 5576.09
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.15 	r: 75.18 	f1: 78.05 	 4637 	 5714 	 6168
wo 	p: 93.73 	r: 85.15 	f1: 89.24 	 2885 	 3078 	 3388
ni 	p: 84.11 	r: 72.67 	f1: 77.97 	 1106 	 1315 	 1522

[32m iter_1[0m
ga 	p: 80.45 	r: 76.12 	f1: 78.22 	 4695 	 5836 	 6168
wo 	p: 92.95 	r: 85.95 	f1: 89.31 	 2912 	 3133 	 3388
ni 	p: 84.67 	r: 72.6 	f1: 78.17 	 1105 	 1305 	 1522

[32m iter_2[0m
ga 	p: 80.13 	r: 76.3 	f1: 78.17 	 4706 	 5873 	 6168
wo 	p: 93.73 	r: 85.15 	f1: 89.24 	 2885 	 3078 	 3388
ni 	p: 84.45 	r: 72.8 	f1: 78.19 	 1108 	 1312 	 1522
best_thres [[0.45, 0.44, 0.25], [0.38, 0.3, 0.19], [0.36, 0.45, 0.18]]
f [0.8145, 0.8153, 0.8153]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 8 	 [0.43, 0.54, 0.19] 	 lr: 0.0001 	 f: 83.14960139511709
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 11 [0m
Train...

[32m iter_0[0m
ga 	p: 80.33 	r: 76.78 	f1: 78.51 	 4736 	 5896 	 6168
wo 	p: 93.14 	r: 86.22 	f1: 89.55 	 2921 	 3136 	 3388
ni 	p: 88.92 	r: 72.73 	f1: 80.01 	 1107 	 1245 	 1522

[32m iter_1[0m
ga 	p: 80.22 	r: 77.48 	f1: 78.83 	 4779 	 5957 	 6168
wo 	p: 92.85 	r: 86.22 	f1: 89.41 	 2921 	 3146 	 3388
ni 	p: 89.44 	r: 72.86 	f1: 80.3 	 1109 	 1240 	 1522

[32m iter_2[0m
ga 	p: 79.89 	r: 77.66 	f1: 78.76 	 4790 	 5996 	 6168
wo 	p: 93.15 	r: 85.92 	f1: 89.39 	 2911 	 3125 	 3388
ni 	p: 89.64 	r: 72.8 	f1: 80.35 	 1108 	 1236 	 1522
best_thres [[0.36, 0.48, 0.33], [0.34, 0.44, 0.34], [0.32, 0.47, 0.34]]
f [0.8208, 0.8216, 0.8217]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 8 	 [0.51, 0.51, 0.13] 	 lr: 0.0001 	 f: 83.08403718208844
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 11 [0m
Train...

[32m iter_0[0m
ga 	p: 81.15 	r: 75.18 	f1: 78.05 	 4637 	 5714 	 6168
wo 	p: 93.73 	r: 85.15 	f1: 89.24 	 2885 	 3078 	 3388
ni 	p: 84.11 	r: 72.67 	f1: 77.97 	 1106 	 1315 	 1522

[32m iter_1[0m
ga 	p: 80.45 	r: 76.12 	f1: 78.22 	 4695 	 5836 	 6168
wo 	p: 92.95 	r: 85.95 	f1: 89.31 	 2912 	 3133 	 3388
ni 	p: 84.67 	r: 72.6 	f1: 78.17 	 1105 	 1305 	 1522

[32m iter_2[0m
ga 	p: 80.13 	r: 76.3 	f1: 78.17 	 4706 	 5873 	 6168
wo 	p: 93.73 	r: 85.15 	f1: 89.24 	 2885 	 3078 	 3388
ni 	p: 84.45 	r: 72.8 	f1: 78.19 	 1108 	 1312 	 1522
best_thres [[0.45, 0.44, 0.25], [0.38, 0.3, 0.19], [0.36, 0.45, 0.18]]
f [0.8145, 0.8153, 0.8153]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 8 	 [0.43, 0.54, 0.19] 	 lr: 0.0001 	 f: 83.14960139511709
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 11 [0m
Train...

[32m iter_0[0m
ga 	p: 81.24 	r: 74.4 	f1: 77.67 	 4589 	 5649 	 6168
wo 	p: 93.27 	r: 84.21 	f1: 88.51 	 2853 	 3059 	 3388
ni 	p: 84.52 	r: 76.74 	f1: 80.44 	 1168 	 1382 	 1522

[32m iter_1[0m
ga 	p: 81.28 	r: 74.9 	f1: 77.96 	 4620 	 5684 	 6168
wo 	p: 93.24 	r: 84.68 	f1: 88.75 	 2869 	 3077 	 3388
ni 	p: 85.78 	r: 75.69 	f1: 80.42 	 1152 	 1343 	 1522

[32m iter_2[0m
ga 	p: 82.19 	r: 74.32 	f1: 78.06 	 4584 	 5577 	 6168
wo 	p: 93.18 	r: 84.68 	f1: 88.73 	 2869 	 3079 	 3388
ni 	p: 85.13 	r: 77.14 	f1: 80.94 	 1174 	 1379 	 1522
best_thres [[0.49, 0.48, 0.26], [0.51, 0.47, 0.23], [0.7, 0.49, 0.17]]
f [0.8135, 0.8147, 0.8155]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 8 	 [0.45, 0.56, 0.13] 	 lr: 0.0001 	 f: 82.97808936777554
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 11 [0m
Train...

[32m iter_0[0m
ga 	p: 83.89 	r: 74.32 	f1: 78.82 	 4584 	 5464 	 6168
wo 	p: 93.76 	r: 85.98 	f1: 89.7 	 2913 	 3107 	 3388
ni 	p: 84.28 	r: 74.7 	f1: 79.21 	 1137 	 1349 	 1522

[32m iter_1[0m
ga 	p: 81.54 	r: 76.12 	f1: 78.74 	 4695 	 5758 	 6168
wo 	p: 92.96 	r: 86.57 	f1: 89.65 	 2933 	 3155 	 3388
ni 	p: 85.25 	r: 74.05 	f1: 79.25 	 1127 	 1322 	 1522

[32m iter_2[0m
ga 	p: 84.73 	r: 73.51 	f1: 78.72 	 4534 	 5351 	 6168
wo 	p: 92.93 	r: 86.57 	f1: 89.64 	 2933 	 3156 	 3388
ni 	p: 86.83 	r: 72.8 	f1: 79.2 	 1108 	 1276 	 1522
best_thres [[0.58, 0.71, 0.14], [0.36, 0.57, 0.17], [0.64, 0.57, 0.21]]
f [0.8224, 0.822, 0.822]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse 	current best epoch 19 	 [0.47, 0.67, 0.1] 	 lr: 2.5e-05 	 f: 82.40335263926167
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse[0m [33m epoch 25 [0m
Train...
loss: tensor(364.9572) lr: 0.0001 time: 5462.15
pred_count_train 41644

Test...
loss: tensor(369.1731) lr: 0.0001 time: 5843.82
pred_count_train 41644

Test...
loss: tensor(363.3878) lr: 0.0001 time: 5941.39
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.89 	r: 74.14 	f1: 77.83 	 4573 	 5584 	 6168
wo 	p: 92.43 	r: 85.77 	f1: 88.98 	 2906 	 3144 	 3388
ni 	p: 84.82 	r: 76.02 	f1: 80.18 	 1157 	 1364 	 1522

[32m iter_1[0m
ga 	p: 82.16 	r: 73.93 	f1: 77.83 	 4560 	 5550 	 6168
wo 	p: 92.22 	r: 85.74 	f1: 88.87 	 2905 	 3150 	 3388
ni 	p: 86.21 	r: 75.56 	f1: 80.53 	 1150 	 1334 	 1522

[32m iter_2[0m
ga 	p: 79.97 	r: 75.73 	f1: 77.79 	 4671 	 5841 	 6168
wo 	p: 91.91 	r: 85.86 	f1: 88.78 	 2909 	 3165 	 3388
ni 	p: 84.67 	r: 77.27 	f1: 80.8 	 1176 	 1389 	 1522
best_thres [[0.52, 0.6, 0.27], [0.65, 0.76, 0.2], [0.36, 0.77, 0.14]]
f [0.8159, 0.816, 0.8158]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 8 	 [0.45, 0.56, 0.13] 	 lr: 0.0001 	 f: 82.97808936777554
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 12 [0m
Train...
loss: tensor(369.1731) lr: 0.0001 time: 6138.8
pred_count_train 41644

Test...
loss: tensor(364.9572) lr: 0.0001 time: 6050.89
pred_count_train 41644

Test...
loss: tensor(80.3648) lr: 1.25e-05 time: 5903.23
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.58 	r: 76.77 	f1: 78.15 	 4735 	 5950 	 6168
wo 	p: 92.99 	r: 85.39 	f1: 89.03 	 2893 	 3111 	 3388
ni 	p: 83.53 	r: 77.66 	f1: 80.49 	 1182 	 1415 	 1522

[32m iter_1[0m
ga 	p: 79.91 	r: 77.01 	f1: 78.43 	 4750 	 5944 	 6168
wo 	p: 91.25 	r: 86.81 	f1: 88.97 	 2941 	 3223 	 3388
ni 	p: 84.38 	r: 76.28 	f1: 80.12 	 1161 	 1376 	 1522

[32m iter_2[0m
ga 	p: 80.05 	r: 76.75 	f1: 78.36 	 4734 	 5914 	 6168
wo 	p: 92.03 	r: 86.22 	f1: 89.03 	 2921 	 3174 	 3388
ni 	p: 82.95 	r: 77.33 	f1: 80.04 	 1177 	 1419 	 1522
best_thres [[0.31, 0.76, 0.18], [0.29, 0.68, 0.15], [0.3, 0.84, 0.11]]
f [0.8175, 0.8182, 0.8182]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 8 	 [0.43, 0.54, 0.19] 	 lr: 0.0001 	 f: 83.14960139511709
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 12 [0m
Train...

[32m iter_0[0m
ga 	p: 80.77 	r: 75.47 	f1: 78.03 	 4655 	 5763 	 6168
wo 	p: 94.25 	r: 85.6 	f1: 89.71 	 2900 	 3077 	 3388
ni 	p: 86.09 	r: 76.02 	f1: 80.74 	 1157 	 1344 	 1522

[32m iter_1[0m
ga 	p: 81.86 	r: 74.85 	f1: 78.2 	 4617 	 5640 	 6168
wo 	p: 93.62 	r: 85.74 	f1: 89.51 	 2905 	 3103 	 3388
ni 	p: 85.48 	r: 76.61 	f1: 80.8 	 1166 	 1364 	 1522

[32m iter_2[0m
ga 	p: 81.36 	r: 75.15 	f1: 78.13 	 4635 	 5697 	 6168
wo 	p: 93.44 	r: 85.71 	f1: 89.41 	 2904 	 3108 	 3388
ni 	p: 85.32 	r: 77.14 	f1: 81.02 	 1174 	 1376 	 1522
best_thres [[0.47, 0.83, 0.21], [0.56, 0.82, 0.19], [0.52, 0.84, 0.18]]
f [0.8195, 0.8198, 0.8198]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 8 	 [0.51, 0.51, 0.13] 	 lr: 0.0001 	 f: 83.08403718208844
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 12 [0m
Train...

[32m iter_0[0m
ga 	p: 79.58 	r: 76.77 	f1: 78.15 	 4735 	 5950 	 6168
wo 	p: 92.99 	r: 85.39 	f1: 89.03 	 2893 	 3111 	 3388
ni 	p: 83.53 	r: 77.66 	f1: 80.49 	 1182 	 1415 	 1522

[32m iter_1[0m
ga 	p: 79.91 	r: 77.01 	f1: 78.43 	 4750 	 5944 	 6168
wo 	p: 91.25 	r: 86.81 	f1: 88.97 	 2941 	 3223 	 3388
ni 	p: 84.38 	r: 76.28 	f1: 80.12 	 1161 	 1376 	 1522

[32m iter_2[0m
ga 	p: 80.05 	r: 76.75 	f1: 78.36 	 4734 	 5914 	 6168
wo 	p: 92.03 	r: 86.22 	f1: 89.03 	 2921 	 3174 	 3388
ni 	p: 82.95 	r: 77.33 	f1: 80.04 	 1177 	 1419 	 1522
best_thres [[0.31, 0.76, 0.18], [0.29, 0.68, 0.15], [0.3, 0.84, 0.11]]
f [0.8175, 0.8182, 0.8182]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 8 	 [0.43, 0.54, 0.19] 	 lr: 0.0001 	 f: 83.14960139511709
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 12 [0m
Train...

[32m iter_0[0m
ga 	p: 81.89 	r: 74.14 	f1: 77.83 	 4573 	 5584 	 6168
wo 	p: 92.43 	r: 85.77 	f1: 88.98 	 2906 	 3144 	 3388
ni 	p: 84.82 	r: 76.02 	f1: 80.18 	 1157 	 1364 	 1522

[32m iter_1[0m
ga 	p: 82.16 	r: 73.93 	f1: 77.83 	 4560 	 5550 	 6168
wo 	p: 92.22 	r: 85.74 	f1: 88.87 	 2905 	 3150 	 3388
ni 	p: 86.21 	r: 75.56 	f1: 80.53 	 1150 	 1334 	 1522

[32m iter_2[0m
ga 	p: 79.97 	r: 75.73 	f1: 77.79 	 4671 	 5841 	 6168
wo 	p: 91.91 	r: 85.86 	f1: 88.78 	 2909 	 3165 	 3388
ni 	p: 84.67 	r: 77.27 	f1: 80.8 	 1176 	 1389 	 1522
best_thres [[0.52, 0.6, 0.27], [0.65, 0.76, 0.2], [0.36, 0.77, 0.14]]
f [0.8159, 0.816, 0.8158]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 8 	 [0.45, 0.56, 0.13] 	 lr: 0.0001 	 f: 82.97808936777554
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 12 [0m
Train...

[32m iter_0[0m
ga 	p: 84.01 	r: 73.93 	f1: 78.65 	 4560 	 5428 	 6168
wo 	p: 93.21 	r: 86.33 	f1: 89.64 	 2925 	 3138 	 3388
ni 	p: 85.45 	r: 74.11 	f1: 79.38 	 1128 	 1320 	 1522

[32m iter_1[0m
ga 	p: 81.42 	r: 75.89 	f1: 78.56 	 4681 	 5749 	 6168
wo 	p: 93.22 	r: 86.48 	f1: 89.73 	 2930 	 3143 	 3388
ni 	p: 84.85 	r: 74.31 	f1: 79.23 	 1131 	 1333 	 1522

[32m iter_2[0m
ga 	p: 82.82 	r: 74.71 	f1: 78.55 	 4608 	 5564 	 6168
wo 	p: 93.09 	r: 86.63 	f1: 89.74 	 2935 	 3153 	 3388
ni 	p: 84.63 	r: 74.51 	f1: 79.25 	 1134 	 1340 	 1522
best_thres [[0.64, 0.72, 0.11], [0.36, 0.67, 0.1], [0.51, 0.65, 0.09]]
f [0.8217, 0.8212, 0.8212]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse 	current best epoch 19 	 [0.47, 0.67, 0.1] 	 lr: 2.5e-05 	 f: 82.40335263926167
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse[0m [33m epoch 26 [0m
Train...
loss: tensor(276.5278) lr: 0.0001 time: 5606.82
pred_count_train 41644

Test...
loss: tensor(274.2504) lr: 0.0001 time: 6027.96
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.79 	r: 74.95 	f1: 77.76 	 4623 	 5722 	 6168
wo 	p: 91.49 	r: 86.33 	f1: 88.84 	 2925 	 3197 	 3388
ni 	p: 83.17 	r: 74.38 	f1: 78.53 	 1132 	 1361 	 1522

[32m iter_1[0m
ga 	p: 79.92 	r: 75.94 	f1: 77.88 	 4684 	 5861 	 6168
wo 	p: 91.55 	r: 86.3 	f1: 88.85 	 2924 	 3194 	 3388
ni 	p: 84.08 	r: 73.92 	f1: 78.67 	 1125 	 1338 	 1522

[32m iter_2[0m
ga 	p: 80.06 	r: 75.91 	f1: 77.93 	 4682 	 5848 	 6168
wo 	p: 91.5 	r: 86.1 	f1: 88.72 	 2917 	 3188 	 3388
ni 	p: 85.23 	r: 72.8 	f1: 78.53 	 1108 	 1300 	 1522
best_thres [[0.38, 0.4, 0.13], [0.29, 0.37, 0.08], [0.3, 0.39, 0.09]]
f [0.8128, 0.8131, 0.8132]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 8 	 [0.45, 0.56, 0.13] 	 lr: 0.0001 	 f: 82.97808936777554
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 13 [0m
Train...
loss: tensor(268.9569) lr: 0.0001 time: 6153.76
pred_count_train 41644

Test...
loss: tensor(274.2504) lr: 0.0001 time: 6121.33
pred_count_train 41644

Test...
loss: tensor(276.5278) lr: 0.0001 time: 5983.15
pred_count_train 41644

Test...
loss: tensor(53.6173) lr: 1.25e-05 time: 5882.56
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.28 	r: 75.5 	f1: 77.82 	 4657 	 5801 	 6168
wo 	p: 92.99 	r: 86.19 	f1: 89.46 	 2920 	 3140 	 3388
ni 	p: 85.29 	r: 76.54 	f1: 80.68 	 1165 	 1366 	 1522

[32m iter_1[0m
ga 	p: 80.07 	r: 76.02 	f1: 77.99 	 4689 	 5856 	 6168
wo 	p: 92.73 	r: 86.63 	f1: 89.58 	 2935 	 3165 	 3388
ni 	p: 84.25 	r: 76.28 	f1: 80.07 	 1161 	 1378 	 1522

[32m iter_2[0m
ga 	p: 80.12 	r: 76.13 	f1: 78.08 	 4696 	 5861 	 6168
wo 	p: 92.6 	r: 86.84 	f1: 89.63 	 2942 	 3177 	 3388
ni 	p: 84.07 	r: 76.61 	f1: 80.17 	 1166 	 1387 	 1522
best_thres [[0.48, 0.66, 0.15], [0.44, 0.8, 0.08], [0.44, 0.76, 0.07]]
f [0.8176, 0.8178, 0.8182]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 8 	 [0.43, 0.54, 0.19] 	 lr: 0.0001 	 f: 83.14960139511709
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 13 [0m
Train...

[32m iter_0[0m
ga 	p: 79.38 	r: 76.1 	f1: 77.71 	 4694 	 5913 	 6168
wo 	p: 91.8 	r: 87.51 	f1: 89.6 	 2965 	 3230 	 3388
ni 	p: 87.03 	r: 71.42 	f1: 78.46 	 1087 	 1249 	 1522

[32m iter_1[0m
ga 	p: 80.95 	r: 74.81 	f1: 77.76 	 4614 	 5700 	 6168
wo 	p: 93.41 	r: 86.13 	f1: 89.62 	 2918 	 3124 	 3388
ni 	p: 86.16 	r: 73.65 	f1: 79.42 	 1121 	 1301 	 1522

[32m iter_2[0m
ga 	p: 80.94 	r: 75.0 	f1: 77.86 	 4626 	 5715 	 6168
wo 	p: 92.66 	r: 86.84 	f1: 89.65 	 2942 	 3175 	 3388
ni 	p: 85.69 	r: 73.59 	f1: 79.18 	 1120 	 1307 	 1522
best_thres [[0.47, 0.36, 0.27], [0.62, 0.62, 0.17], [0.61, 0.47, 0.15]]
f [0.8147, 0.8155, 0.8159]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 8 	 [0.51, 0.51, 0.13] 	 lr: 0.0001 	 f: 83.08403718208844
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 13 [0m
Train...

[32m iter_0[0m
ga 	p: 80.28 	r: 75.5 	f1: 77.82 	 4657 	 5801 	 6168
wo 	p: 92.99 	r: 86.19 	f1: 89.46 	 2920 	 3140 	 3388
ni 	p: 85.29 	r: 76.54 	f1: 80.68 	 1165 	 1366 	 1522

[32m iter_1[0m
ga 	p: 80.07 	r: 76.02 	f1: 77.99 	 4689 	 5856 	 6168
wo 	p: 92.73 	r: 86.63 	f1: 89.58 	 2935 	 3165 	 3388
ni 	p: 84.25 	r: 76.28 	f1: 80.07 	 1161 	 1378 	 1522

[32m iter_2[0m
ga 	p: 80.12 	r: 76.13 	f1: 78.08 	 4696 	 5861 	 6168
wo 	p: 92.6 	r: 86.84 	f1: 89.63 	 2942 	 3177 	 3388
ni 	p: 84.07 	r: 76.61 	f1: 80.17 	 1166 	 1387 	 1522
best_thres [[0.48, 0.66, 0.15], [0.44, 0.8, 0.08], [0.44, 0.76, 0.07]]
f [0.8176, 0.8178, 0.8182]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 8 	 [0.43, 0.54, 0.19] 	 lr: 0.0001 	 f: 83.14960139511709
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 13 [0m
Train...

[32m iter_0[0m
ga 	p: 80.79 	r: 74.95 	f1: 77.76 	 4623 	 5722 	 6168
wo 	p: 91.49 	r: 86.33 	f1: 88.84 	 2925 	 3197 	 3388
ni 	p: 83.17 	r: 74.38 	f1: 78.53 	 1132 	 1361 	 1522

[32m iter_1[0m
ga 	p: 79.92 	r: 75.94 	f1: 77.88 	 4684 	 5861 	 6168
wo 	p: 91.55 	r: 86.3 	f1: 88.85 	 2924 	 3194 	 3388
ni 	p: 84.08 	r: 73.92 	f1: 78.67 	 1125 	 1338 	 1522

[32m iter_2[0m
ga 	p: 80.06 	r: 75.91 	f1: 77.93 	 4682 	 5848 	 6168
wo 	p: 91.5 	r: 86.1 	f1: 88.72 	 2917 	 3188 	 3388
ni 	p: 85.23 	r: 72.8 	f1: 78.53 	 1108 	 1300 	 1522
best_thres [[0.38, 0.4, 0.13], [0.29, 0.37, 0.08], [0.3, 0.39, 0.09]]
f [0.8128, 0.8131, 0.8132]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 8 	 [0.45, 0.56, 0.13] 	 lr: 0.0001 	 f: 82.97808936777554
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 13 [0m
Train...

[32m iter_0[0m
ga 	p: 83.56 	r: 73.91 	f1: 78.44 	 4559 	 5456 	 6168
wo 	p: 93.61 	r: 85.98 	f1: 89.63 	 2913 	 3112 	 3388
ni 	p: 86.13 	r: 72.6 	f1: 78.79 	 1105 	 1283 	 1522

[32m iter_1[0m
ga 	p: 83.53 	r: 74.09 	f1: 78.53 	 4570 	 5471 	 6168
wo 	p: 93.67 	r: 85.98 	f1: 89.66 	 2913 	 3110 	 3388
ni 	p: 85.68 	r: 73.13 	f1: 78.91 	 1113 	 1299 	 1522

[32m iter_2[0m
ga 	p: 83.61 	r: 74.12 	f1: 78.58 	 4572 	 5468 	 6168
wo 	p: 93.64 	r: 85.98 	f1: 89.64 	 2913 	 3111 	 3388
ni 	p: 85.51 	r: 72.86 	f1: 78.68 	 1109 	 1297 	 1522
best_thres [[0.59, 0.69, 0.12], [0.58, 0.67, 0.11], [0.58, 0.67, 0.11]]
f [0.8196, 0.82, 0.8201]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse 	current best epoch 19 	 [0.47, 0.67, 0.1] 	 lr: 2.5e-05 	 f: 82.40335263926167
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse[0m [33m epoch 27 [0m
Train...
loss: tensor(649.0913) lr: 5e-05 time: 5299.71
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.52 	r: 75.34 	f1: 79.22 	 4647 	 5564 	 6168
wo 	p: 93.81 	r: 85.48 	f1: 89.45 	 2896 	 3087 	 3388
ni 	p: 89.27 	r: 74.9 	f1: 81.46 	 1140 	 1277 	 1522

[32m iter_1[0m
ga 	p: 81.67 	r: 76.65 	f1: 79.08 	 4728 	 5789 	 6168
wo 	p: 94.32 	r: 85.27 	f1: 89.57 	 2889 	 3063 	 3388
ni 	p: 86.01 	r: 77.53 	f1: 81.55 	 1180 	 1372 	 1522

[32m iter_2[0m
ga 	p: 82.92 	r: 75.54 	f1: 79.05 	 4659 	 5619 	 6168
wo 	p: 93.91 	r: 85.54 	f1: 89.53 	 2898 	 3086 	 3388
ni 	p: 88.49 	r: 75.76 	f1: 81.63 	 1153 	 1303 	 1522
best_thres [[0.57, 0.59, 0.22], [0.49, 0.77, 0.12], [0.7, 0.76, 0.15]]
f [0.8267, 0.8263, 0.8263]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 8 	 [0.45, 0.56, 0.13] 	 lr: 0.0001 	 f: 82.97808936777554
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 14 [0m
Train...
loss: tensor(653.9401) lr: 5e-05 time: 5863.49
pred_count_train 41644

Test...
loss: tensor(649.9597) lr: 5e-05 time: 6025.67
pred_count_train 41644

Test...
loss: tensor(653.9401) lr: 5e-05 time: 6141.5
pred_count_train 41644

Test...
loss: tensor(649.0913) lr: 5e-05 time: 6063.83
pred_count_train 41644

Test...
loss: tensor(34.4242) lr: 1.25e-05 time: 6021.98
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.11 	r: 78.6 	f1: 79.35 	 4848 	 6052 	 6168
wo 	p: 93.09 	r: 86.75 	f1: 89.81 	 2939 	 3157 	 3388
ni 	p: 85.78 	r: 76.08 	f1: 80.64 	 1158 	 1350 	 1522

[32m iter_1[0m
ga 	p: 81.46 	r: 77.01 	f1: 79.17 	 4750 	 5831 	 6168
wo 	p: 93.33 	r: 86.3 	f1: 89.68 	 2924 	 3133 	 3388
ni 	p: 85.6 	r: 76.54 	f1: 80.82 	 1165 	 1361 	 1522

[32m iter_2[0m
ga 	p: 81.36 	r: 76.99 	f1: 79.12 	 4749 	 5837 	 6168
wo 	p: 93.35 	r: 86.22 	f1: 89.64 	 2921 	 3129 	 3388
ni 	p: 86.45 	r: 75.43 	f1: 80.56 	 1148 	 1328 	 1522
best_thres [[0.36, 0.45, 0.15], [0.45, 0.47, 0.11], [0.45, 0.48, 0.12]]
f [0.8268, 0.8264, 0.826]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 8 	 [0.43, 0.54, 0.19] 	 lr: 0.0001 	 f: 83.14960139511709
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 14 [0m
Train...

[32m iter_0[0m
ga 	p: 81.34 	r: 76.54 	f1: 78.87 	 4721 	 5804 	 6168
wo 	p: 94.4 	r: 85.6 	f1: 89.78 	 2900 	 3072 	 3388
ni 	p: 87.42 	r: 76.68 	f1: 81.69 	 1167 	 1335 	 1522

[32m iter_1[0m
ga 	p: 81.11 	r: 77.08 	f1: 79.04 	 4754 	 5861 	 6168
wo 	p: 92.92 	r: 86.75 	f1: 89.73 	 2939 	 3163 	 3388
ni 	p: 86.57 	r: 78.78 	f1: 82.49 	 1199 	 1385 	 1522

[32m iter_2[0m
ga 	p: 80.77 	r: 77.45 	f1: 79.08 	 4777 	 5914 	 6168
wo 	p: 92.92 	r: 86.75 	f1: 89.73 	 2939 	 3163 	 3388
ni 	p: 87.56 	r: 78.12 	f1: 82.57 	 1189 	 1358 	 1522
best_thres [[0.52, 0.58, 0.18], [0.49, 0.39, 0.15], [0.46, 0.39, 0.16]]
f [0.8256, 0.8266, 0.827]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 8 	 [0.51, 0.51, 0.13] 	 lr: 0.0001 	 f: 83.08403718208844
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 14 [0m
Train...

[32m iter_0[0m
ga 	p: 80.11 	r: 78.6 	f1: 79.35 	 4848 	 6052 	 6168
wo 	p: 93.09 	r: 86.75 	f1: 89.81 	 2939 	 3157 	 3388
ni 	p: 85.78 	r: 76.08 	f1: 80.64 	 1158 	 1350 	 1522

[32m iter_1[0m
ga 	p: 81.46 	r: 77.01 	f1: 79.17 	 4750 	 5831 	 6168
wo 	p: 93.33 	r: 86.3 	f1: 89.68 	 2924 	 3133 	 3388
ni 	p: 85.6 	r: 76.54 	f1: 80.82 	 1165 	 1361 	 1522

[32m iter_2[0m
ga 	p: 81.36 	r: 76.99 	f1: 79.12 	 4749 	 5837 	 6168
wo 	p: 93.35 	r: 86.22 	f1: 89.64 	 2921 	 3129 	 3388
ni 	p: 86.45 	r: 75.43 	f1: 80.56 	 1148 	 1328 	 1522
best_thres [[0.36, 0.45, 0.15], [0.45, 0.47, 0.11], [0.45, 0.48, 0.12]]
f [0.8268, 0.8264, 0.826]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 8 	 [0.43, 0.54, 0.19] 	 lr: 0.0001 	 f: 83.14960139511709
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 14 [0m
Train...

[32m iter_0[0m
ga 	p: 83.52 	r: 75.34 	f1: 79.22 	 4647 	 5564 	 6168
wo 	p: 93.81 	r: 85.48 	f1: 89.45 	 2896 	 3087 	 3388
ni 	p: 89.27 	r: 74.9 	f1: 81.46 	 1140 	 1277 	 1522

[32m iter_1[0m
ga 	p: 81.67 	r: 76.65 	f1: 79.08 	 4728 	 5789 	 6168
wo 	p: 94.32 	r: 85.27 	f1: 89.57 	 2889 	 3063 	 3388
ni 	p: 86.01 	r: 77.53 	f1: 81.55 	 1180 	 1372 	 1522

[32m iter_2[0m
ga 	p: 82.92 	r: 75.54 	f1: 79.05 	 4659 	 5619 	 6168
wo 	p: 93.91 	r: 85.54 	f1: 89.53 	 2898 	 3086 	 3388
ni 	p: 88.49 	r: 75.76 	f1: 81.63 	 1153 	 1303 	 1522
best_thres [[0.57, 0.59, 0.22], [0.49, 0.77, 0.12], [0.7, 0.76, 0.15]]
f [0.8267, 0.8263, 0.8263]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 8 	 [0.45, 0.56, 0.13] 	 lr: 0.0001 	 f: 82.97808936777554
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 14 [0m
Train...

[32m iter_0[0m
ga 	p: 83.82 	r: 73.85 	f1: 78.52 	 4555 	 5434 	 6168
wo 	p: 92.83 	r: 86.81 	f1: 89.72 	 2941 	 3168 	 3388
ni 	p: 85.26 	r: 73.32 	f1: 78.84 	 1116 	 1309 	 1522

[32m iter_1[0m
ga 	p: 82.28 	r: 75.11 	f1: 78.53 	 4633 	 5631 	 6168
wo 	p: 92.36 	r: 87.04 	f1: 89.62 	 2949 	 3193 	 3388
ni 	p: 85.06 	r: 73.72 	f1: 78.99 	 1122 	 1319 	 1522

[32m iter_2[0m
ga 	p: 82.32 	r: 75.13 	f1: 78.56 	 4634 	 5629 	 6168
wo 	p: 92.36 	r: 87.1 	f1: 89.66 	 2951 	 3195 	 3388
ni 	p: 83.69 	r: 74.51 	f1: 78.83 	 1134 	 1355 	 1522
best_thres [[0.56, 0.5, 0.08], [0.36, 0.42, 0.07], [0.36, 0.42, 0.05]]
f [0.8206, 0.8205, 0.8204]
load model: epoch19
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse 	current best epoch 19 	 [0.47, 0.67, 0.1] 	 lr: 2.5e-05 	 f: 82.40335263926167
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse[0m [33m epoch 28 [0m
Train...
loss: tensor(440.5735) lr: 5e-05 time: 5445.77
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.02 	r: 74.69 	f1: 78.18 	 4607 	 5617 	 6168
wo 	p: 92.49 	r: 86.92 	f1: 89.62 	 2945 	 3184 	 3388
ni 	p: 86.31 	r: 71.22 	f1: 78.04 	 1084 	 1256 	 1522

[32m iter_1[0m
ga 	p: 82.8 	r: 74.79 	f1: 78.59 	 4613 	 5571 	 6168
wo 	p: 93.44 	r: 85.8 	f1: 89.46 	 2907 	 3111 	 3388
ni 	p: 82.24 	r: 73.32 	f1: 77.53 	 1116 	 1357 	 1522

[32m iter_2[0m
ga 	p: 82.62 	r: 74.9 	f1: 78.57 	 4620 	 5592 	 6168
wo 	p: 93.19 	r: 86.04 	f1: 89.47 	 2915 	 3128 	 3388
ni 	p: 82.14 	r: 74.05 	f1: 77.89 	 1127 	 1372 	 1522
best_thres [[0.49, 0.46, 0.17], [0.6, 0.8, 0.07], [0.65, 0.81, 0.06]]
f [0.8172, 0.8176, 0.8178]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 8 	 [0.45, 0.56, 0.13] 	 lr: 0.0001 	 f: 82.97808936777554
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 15 [0m
Train...
loss: tensor(445.6561) lr: 5e-05 time: 5618.73
pred_count_train 41644

Test...
loss: tensor(442.4736) lr: 5e-05 time: 5792.73
pred_count_train 41644

Test...
loss: tensor(445.6561) lr: 5e-05 time: 5927.3
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.36 	r: 74.71 	f1: 78.8 	 4608 	 5528 	 6168
wo 	p: 93.17 	r: 86.16 	f1: 89.53 	 2919 	 3133 	 3388
ni 	p: 83.35 	r: 74.97 	f1: 78.93 	 1141 	 1369 	 1522

[32m iter_1[0m
ga 	p: 82.26 	r: 75.68 	f1: 78.83 	 4668 	 5675 	 6168
wo 	p: 93.16 	r: 86.04 	f1: 89.46 	 2915 	 3129 	 3388
ni 	p: 85.39 	r: 72.6 	f1: 78.48 	 1105 	 1294 	 1522

[32m iter_2[0m
ga 	p: 82.87 	r: 75.37 	f1: 78.94 	 4649 	 5610 	 6168
wo 	p: 92.72 	r: 86.42 	f1: 89.46 	 2928 	 3158 	 3388
ni 	p: 86.3 	r: 72.01 	f1: 78.51 	 1096 	 1270 	 1522
best_thres [[0.54, 0.6, 0.11], [0.48, 0.83, 0.11], [0.6, 0.8, 0.11]]
f [0.8213, 0.8209, 0.8211]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 8 	 [0.43, 0.54, 0.19] 	 lr: 0.0001 	 f: 83.14960139511709
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 15 [0m
Train...
loss: tensor(440.5735) lr: 5e-05 time: 5903.6
pred_count_train 41644

Test...
loss: tensor(120.2751) lr: 1e-05 time: 5886.91
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.09 	r: 74.81 	f1: 78.28 	 4614 	 5621 	 6168
wo 	p: 93.09 	r: 86.25 	f1: 89.54 	 2922 	 3139 	 3388
ni 	p: 86.53 	r: 74.31 	f1: 79.96 	 1131 	 1307 	 1522

[32m iter_1[0m
ga 	p: 82.17 	r: 75.15 	f1: 78.5 	 4635 	 5641 	 6168
wo 	p: 93.17 	r: 86.19 	f1: 89.54 	 2920 	 3134 	 3388
ni 	p: 85.96 	r: 74.84 	f1: 80.01 	 1139 	 1325 	 1522

[32m iter_2[0m
ga 	p: 83.96 	r: 73.74 	f1: 78.52 	 4548 	 5417 	 6168
wo 	p: 92.89 	r: 86.36 	f1: 89.51 	 2926 	 3150 	 3388
ni 	p: 86.08 	r: 74.77 	f1: 80.03 	 1138 	 1322 	 1522
best_thres [[0.59, 0.69, 0.17], [0.57, 0.67, 0.14], [0.68, 0.63, 0.14]]
f [0.8198, 0.8204, 0.8208]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 8 	 [0.51, 0.51, 0.13] 	 lr: 0.0001 	 f: 83.08403718208844
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 15 [0m
Train...

[32m iter_0[0m
ga 	p: 83.36 	r: 74.71 	f1: 78.8 	 4608 	 5528 	 6168
wo 	p: 93.17 	r: 86.16 	f1: 89.53 	 2919 	 3133 	 3388
ni 	p: 83.35 	r: 74.97 	f1: 78.93 	 1141 	 1369 	 1522

[32m iter_1[0m
ga 	p: 82.26 	r: 75.68 	f1: 78.83 	 4668 	 5675 	 6168
wo 	p: 93.16 	r: 86.04 	f1: 89.46 	 2915 	 3129 	 3388
ni 	p: 85.39 	r: 72.6 	f1: 78.48 	 1105 	 1294 	 1522

[32m iter_2[0m
ga 	p: 82.87 	r: 75.37 	f1: 78.94 	 4649 	 5610 	 6168
wo 	p: 92.72 	r: 86.42 	f1: 89.46 	 2928 	 3158 	 3388
ni 	p: 86.3 	r: 72.01 	f1: 78.51 	 1096 	 1270 	 1522
best_thres [[0.54, 0.6, 0.11], [0.48, 0.83, 0.11], [0.6, 0.8, 0.11]]
f [0.8213, 0.8209, 0.8211]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 8 	 [0.43, 0.54, 0.19] 	 lr: 0.0001 	 f: 83.14960139511709
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 15 [0m
Train...

[32m iter_0[0m
ga 	p: 82.02 	r: 74.69 	f1: 78.18 	 4607 	 5617 	 6168
wo 	p: 92.49 	r: 86.92 	f1: 89.62 	 2945 	 3184 	 3388
ni 	p: 86.31 	r: 71.22 	f1: 78.04 	 1084 	 1256 	 1522

[32m iter_1[0m
ga 	p: 82.8 	r: 74.79 	f1: 78.59 	 4613 	 5571 	 6168
wo 	p: 93.44 	r: 85.8 	f1: 89.46 	 2907 	 3111 	 3388
ni 	p: 82.24 	r: 73.32 	f1: 77.53 	 1116 	 1357 	 1522

[32m iter_2[0m
ga 	p: 82.62 	r: 74.9 	f1: 78.57 	 4620 	 5592 	 6168
wo 	p: 93.19 	r: 86.04 	f1: 89.47 	 2915 	 3128 	 3388
ni 	p: 82.14 	r: 74.05 	f1: 77.89 	 1127 	 1372 	 1522
best_thres [[0.49, 0.46, 0.17], [0.6, 0.8, 0.07], [0.65, 0.81, 0.06]]
f [0.8172, 0.8176, 0.8178]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 8 	 [0.45, 0.56, 0.13] 	 lr: 0.0001 	 f: 82.97808936777554
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 15 [0m
Train...

[32m iter_0[0m
ga 	p: 83.73 	r: 74.48 	f1: 78.83 	 4594 	 5487 	 6168
wo 	p: 93.35 	r: 86.57 	f1: 89.83 	 2933 	 3142 	 3388
ni 	p: 84.88 	r: 74.9 	f1: 79.58 	 1140 	 1343 	 1522

[32m iter_1[0m
ga 	p: 82.48 	r: 75.42 	f1: 78.79 	 4652 	 5640 	 6168
wo 	p: 93.37 	r: 86.48 	f1: 89.79 	 2930 	 3138 	 3388
ni 	p: 84.66 	r: 75.03 	f1: 79.55 	 1142 	 1349 	 1522

[32m iter_2[0m
ga 	p: 82.31 	r: 75.37 	f1: 78.69 	 4649 	 5648 	 6168
wo 	p: 93.04 	r: 86.75 	f1: 89.78 	 2939 	 3159 	 3388
ni 	p: 82.86 	r: 76.54 	f1: 79.58 	 1165 	 1406 	 1522
best_thres [[0.55, 0.62, 0.12], [0.41, 0.62, 0.11], [0.4, 0.58, 0.08]]
f [0.8235, 0.8231, 0.8228]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse 	current best epoch 19 	 [0.47, 0.67, 0.1] 	 lr: 2.5e-05 	 f: 82.40335263926167
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse[0m [33m epoch 29 [0m
Train...
loss: tensor(294.7841) lr: 5e-05 time: 5494.8
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.39 	r: 75.24 	f1: 78.2 	 4641 	 5702 	 6168
wo 	p: 92.05 	r: 86.42 	f1: 89.15 	 2928 	 3181 	 3388
ni 	p: 88.25 	r: 73.52 	f1: 80.22 	 1119 	 1268 	 1522

[32m iter_1[0m
ga 	p: 80.09 	r: 76.65 	f1: 78.34 	 4728 	 5903 	 6168
wo 	p: 92.12 	r: 86.57 	f1: 89.26 	 2933 	 3184 	 3388
ni 	p: 87.95 	r: 73.85 	f1: 80.29 	 1124 	 1278 	 1522

[32m iter_2[0m
ga 	p: 81.35 	r: 75.52 	f1: 78.33 	 4658 	 5726 	 6168
wo 	p: 92.17 	r: 86.51 	f1: 89.25 	 2931 	 3180 	 3388
ni 	p: 88.18 	r: 74.05 	f1: 80.5 	 1127 	 1278 	 1522
best_thres [[0.37, 0.38, 0.24], [0.26, 0.37, 0.17], [0.34, 0.37, 0.15]]
f [0.8185, 0.8189, 0.8193]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 8 	 [0.45, 0.56, 0.13] 	 lr: 0.0001 	 f: 82.97808936777554
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 16 [0m
Train...
loss: tensor(299.2853) lr: 5e-05 time: 5630.2
pred_count_train 41644

Test...
loss: tensor(300.3770) lr: 5e-05 time: 5792.46
pred_count_train 41644

Test...
loss: tensor(299.2853) lr: 5e-05 time: 5884.88
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.28 	r: 75.58 	f1: 78.79 	 4662 	 5666 	 6168
wo 	p: 93.32 	r: 85.36 	f1: 89.16 	 2892 	 3099 	 3388
ni 	p: 81.99 	r: 76.28 	f1: 79.03 	 1161 	 1416 	 1522

[32m iter_1[0m
ga 	p: 82.45 	r: 75.71 	f1: 78.94 	 4670 	 5664 	 6168
wo 	p: 93.85 	r: 85.15 	f1: 89.29 	 2885 	 3074 	 3388
ni 	p: 84.11 	r: 74.44 	f1: 78.98 	 1133 	 1347 	 1522

[32m iter_2[0m
ga 	p: 80.92 	r: 76.95 	f1: 78.88 	 4746 	 5865 	 6168
wo 	p: 92.94 	r: 85.92 	f1: 89.29 	 2911 	 3132 	 3388
ni 	p: 83.64 	r: 74.9 	f1: 79.03 	 1140 	 1363 	 1522
best_thres [[0.47, 0.56, 0.17], [0.51, 0.84, 0.14], [0.35, 0.65, 0.12]]
f [0.8199, 0.8205, 0.8205]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 8 	 [0.43, 0.54, 0.19] 	 lr: 0.0001 	 f: 83.14960139511709
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 16 [0m
Train...
loss: tensor(84.8452) lr: 1e-05 time: 5820.33
pred_count_train 41644

Test...
loss: tensor(294.7841) lr: 5e-05 time: 5828.13
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.01 	r: 75.16 	f1: 78.44 	 4636 	 5653 	 6168
wo 	p: 93.37 	r: 86.48 	f1: 89.79 	 2930 	 3138 	 3388
ni 	p: 86.74 	r: 75.62 	f1: 80.8 	 1151 	 1327 	 1522

[32m iter_1[0m
ga 	p: 82.61 	r: 74.56 	f1: 78.38 	 4599 	 5567 	 6168
wo 	p: 93.7 	r: 85.98 	f1: 89.67 	 2913 	 3109 	 3388
ni 	p: 86.37 	r: 76.22 	f1: 80.98 	 1160 	 1343 	 1522

[32m iter_2[0m
ga 	p: 82.65 	r: 74.69 	f1: 78.47 	 4607 	 5574 	 6168
wo 	p: 93.67 	r: 86.01 	f1: 89.68 	 2914 	 3111 	 3388
ni 	p: 87.81 	r: 75.23 	f1: 81.03 	 1145 	 1304 	 1522
best_thres [[0.53, 0.57, 0.19], [0.58, 0.62, 0.15], [0.57, 0.62, 0.18]]
f [0.8225, 0.8223, 0.8224]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 8 	 [0.51, 0.51, 0.13] 	 lr: 0.0001 	 f: 83.08403718208844
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 16 [0m
Train...

[32m iter_0[0m
ga 	p: 82.28 	r: 75.58 	f1: 78.79 	 4662 	 5666 	 6168
wo 	p: 93.32 	r: 85.36 	f1: 89.16 	 2892 	 3099 	 3388
ni 	p: 81.99 	r: 76.28 	f1: 79.03 	 1161 	 1416 	 1522

[32m iter_1[0m
ga 	p: 82.45 	r: 75.71 	f1: 78.94 	 4670 	 5664 	 6168
wo 	p: 93.85 	r: 85.15 	f1: 89.29 	 2885 	 3074 	 3388
ni 	p: 84.11 	r: 74.44 	f1: 78.98 	 1133 	 1347 	 1522

[32m iter_2[0m
ga 	p: 80.92 	r: 76.95 	f1: 78.88 	 4746 	 5865 	 6168
wo 	p: 92.94 	r: 85.92 	f1: 89.29 	 2911 	 3132 	 3388
ni 	p: 83.64 	r: 74.9 	f1: 79.03 	 1140 	 1363 	 1522
best_thres [[0.47, 0.56, 0.17], [0.51, 0.84, 0.14], [0.35, 0.65, 0.12]]
f [0.8199, 0.8205, 0.8205]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 8 	 [0.43, 0.54, 0.19] 	 lr: 0.0001 	 f: 83.14960139511709
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 16 [0m
Train...

[32m iter_0[0m
ga 	p: 82.58 	r: 74.81 	f1: 78.5 	 4614 	 5587 	 6168
wo 	p: 92.35 	r: 87.28 	f1: 89.74 	 2957 	 3202 	 3388
ni 	p: 84.76 	r: 74.18 	f1: 79.12 	 1129 	 1332 	 1522

[32m iter_1[0m
ga 	p: 83.38 	r: 74.21 	f1: 78.53 	 4577 	 5489 	 6168
wo 	p: 93.17 	r: 86.54 	f1: 89.73 	 2932 	 3147 	 3388
ni 	p: 83.73 	r: 74.7 	f1: 78.96 	 1137 	 1358 	 1522

[32m iter_2[0m
ga 	p: 83.35 	r: 74.35 	f1: 78.59 	 4586 	 5502 	 6168
wo 	p: 92.7 	r: 86.95 	f1: 89.73 	 2946 	 3178 	 3388
ni 	p: 82.9 	r: 75.16 	f1: 78.84 	 1144 	 1380 	 1522
best_thres [[0.47, 0.42, 0.12], [0.54, 0.54, 0.11], [0.53, 0.45, 0.09]]
f [0.8208, 0.8207, 0.8208]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse 	current best epoch 19 	 [0.47, 0.67, 0.1] 	 lr: 2.5e-05 	 f: 82.40335263926167
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse[0m [33m epoch 30 [0m
Train...

[32m iter_0[0m
ga 	p: 81.39 	r: 75.24 	f1: 78.2 	 4641 	 5702 	 6168
wo 	p: 92.05 	r: 86.42 	f1: 89.15 	 2928 	 3181 	 3388
ni 	p: 88.25 	r: 73.52 	f1: 80.22 	 1119 	 1268 	 1522

[32m iter_1[0m
ga 	p: 80.09 	r: 76.65 	f1: 78.34 	 4728 	 5903 	 6168
wo 	p: 92.12 	r: 86.57 	f1: 89.26 	 2933 	 3184 	 3388
ni 	p: 87.95 	r: 73.85 	f1: 80.29 	 1124 	 1278 	 1522

[32m iter_2[0m
ga 	p: 81.35 	r: 75.52 	f1: 78.33 	 4658 	 5726 	 6168
wo 	p: 92.17 	r: 86.51 	f1: 89.25 	 2931 	 3180 	 3388
ni 	p: 88.18 	r: 74.05 	f1: 80.5 	 1127 	 1278 	 1522
best_thres [[0.37, 0.38, 0.24], [0.26, 0.37, 0.17], [0.34, 0.37, 0.15]]
f [0.8185, 0.8189, 0.8193]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 8 	 [0.45, 0.56, 0.13] 	 lr: 0.0001 	 f: 82.97808936777554
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 16 [0m
Train...
loss: tensor(203.2768) lr: 5e-05 time: 5244.68
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.03 	r: 74.25 	f1: 77.5 	 4580 	 5652 	 6168
wo 	p: 93.77 	r: 84.89 	f1: 89.11 	 2876 	 3067 	 3388
ni 	p: 85.43 	r: 73.59 	f1: 79.07 	 1120 	 1311 	 1522

[32m iter_1[0m
ga 	p: 81.49 	r: 74.46 	f1: 77.82 	 4593 	 5636 	 6168
wo 	p: 91.69 	r: 86.92 	f1: 89.24 	 2945 	 3212 	 3388
ni 	p: 88.59 	r: 72.4 	f1: 79.68 	 1102 	 1244 	 1522

[32m iter_2[0m
ga 	p: 81.79 	r: 74.43 	f1: 77.94 	 4591 	 5613 	 6168
wo 	p: 92.05 	r: 86.48 	f1: 89.18 	 2930 	 3183 	 3388
ni 	p: 87.2 	r: 73.39 	f1: 79.7 	 1117 	 1281 	 1522
best_thres [[0.41, 0.6, 0.2], [0.41, 0.29, 0.19], [0.44, 0.35, 0.14]]
f [0.8126, 0.8144, 0.8152]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 8 	 [0.45, 0.56, 0.13] 	 lr: 0.0001 	 f: 82.97808936777554
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 17 [0m
Train...
loss: tensor(208.8857) lr: 5e-05 time: 5529.14
pred_count_train 41644

Test...
loss: tensor(196.5113) lr: 5e-05 time: 5716.73
pred_count_train 41644

Test...
loss: tensor(208.8857) lr: 5e-05 time: 5909.41
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.64 	r: 75.1 	f1: 78.69 	 4632 	 5605 	 6168
wo 	p: 93.52 	r: 85.21 	f1: 89.17 	 2887 	 3087 	 3388
ni 	p: 81.79 	r: 75.23 	f1: 78.37 	 1145 	 1400 	 1522

[32m iter_1[0m
ga 	p: 81.91 	r: 75.66 	f1: 78.66 	 4667 	 5698 	 6168
wo 	p: 92.76 	r: 85.86 	f1: 89.18 	 2909 	 3136 	 3388
ni 	p: 85.05 	r: 71.75 	f1: 77.83 	 1092 	 1284 	 1522

[32m iter_2[0m
ga 	p: 82.05 	r: 75.45 	f1: 78.61 	 4654 	 5672 	 6168
wo 	p: 92.21 	r: 86.28 	f1: 89.14 	 2923 	 3170 	 3388
ni 	p: 84.53 	r: 72.14 	f1: 77.84 	 1098 	 1299 	 1522
best_thres [[0.49, 0.65, 0.15], [0.42, 0.73, 0.16], [0.45, 0.52, 0.14]]
f [0.8185, 0.8182, 0.818]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 8 	 [0.43, 0.54, 0.19] 	 lr: 0.0001 	 f: 83.14960139511709
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 17 [0m
Train...
loss: tensor(59.8594) lr: 1e-05 time: 5950.94
pred_count_train 41644

Test...
loss: tensor(203.2768) lr: 5e-05 time: 5952.46
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.41 	r: 74.95 	f1: 77.59 	 4623 	 5749 	 6168
wo 	p: 92.58 	r: 86.16 	f1: 89.25 	 2919 	 3153 	 3388
ni 	p: 86.64 	r: 73.26 	f1: 79.39 	 1115 	 1287 	 1522

[32m iter_1[0m
ga 	p: 80.57 	r: 75.15 	f1: 77.76 	 4635 	 5753 	 6168
wo 	p: 93.25 	r: 85.63 	f1: 89.28 	 2901 	 3111 	 3388
ni 	p: 88.22 	r: 71.35 	f1: 78.9 	 1086 	 1231 	 1522

[32m iter_2[0m
ga 	p: 81.2 	r: 74.66 	f1: 77.79 	 4605 	 5671 	 6168
wo 	p: 93.28 	r: 85.57 	f1: 89.25 	 2899 	 3108 	 3388
ni 	p: 85.55 	r: 73.92 	f1: 79.31 	 1125 	 1315 	 1522
best_thres [[0.52, 0.56, 0.34], [0.51, 0.64, 0.38], [0.56, 0.64, 0.24]]
f [0.8141, 0.8143, 0.8146]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 8 	 [0.51, 0.51, 0.13] 	 lr: 0.0001 	 f: 83.08403718208844
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 17 [0m
Train...

[32m iter_0[0m
ga 	p: 82.64 	r: 75.1 	f1: 78.69 	 4632 	 5605 	 6168
wo 	p: 93.52 	r: 85.21 	f1: 89.17 	 2887 	 3087 	 3388
ni 	p: 81.79 	r: 75.23 	f1: 78.37 	 1145 	 1400 	 1522

[32m iter_1[0m
ga 	p: 81.91 	r: 75.66 	f1: 78.66 	 4667 	 5698 	 6168
wo 	p: 92.76 	r: 85.86 	f1: 89.18 	 2909 	 3136 	 3388
ni 	p: 85.05 	r: 71.75 	f1: 77.83 	 1092 	 1284 	 1522

[32m iter_2[0m
ga 	p: 82.05 	r: 75.45 	f1: 78.61 	 4654 	 5672 	 6168
wo 	p: 92.21 	r: 86.28 	f1: 89.14 	 2923 	 3170 	 3388
ni 	p: 84.53 	r: 72.14 	f1: 77.84 	 1098 	 1299 	 1522
best_thres [[0.49, 0.65, 0.15], [0.42, 0.73, 0.16], [0.45, 0.52, 0.14]]
f [0.8185, 0.8182, 0.818]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 8 	 [0.43, 0.54, 0.19] 	 lr: 0.0001 	 f: 83.14960139511709
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 17 [0m
Train...

[32m iter_0[0m
ga 	p: 82.29 	r: 75.32 	f1: 78.65 	 4646 	 5646 	 6168
wo 	p: 93.15 	r: 86.69 	f1: 89.8 	 2937 	 3153 	 3388
ni 	p: 83.78 	r: 74.31 	f1: 78.76 	 1131 	 1350 	 1522

[32m iter_1[0m
ga 	p: 82.33 	r: 75.16 	f1: 78.58 	 4636 	 5631 	 6168
wo 	p: 92.72 	r: 86.87 	f1: 89.7 	 2943 	 3174 	 3388
ni 	p: 85.6 	r: 72.67 	f1: 78.61 	 1106 	 1292 	 1522

[32m iter_2[0m
ga 	p: 82.69 	r: 74.9 	f1: 78.6 	 4620 	 5587 	 6168
wo 	p: 92.61 	r: 86.92 	f1: 89.68 	 2945 	 3180 	 3388
ni 	p: 85.23 	r: 72.8 	f1: 78.53 	 1108 	 1300 	 1522
best_thres [[0.44, 0.55, 0.08], [0.44, 0.44, 0.11], [0.48, 0.44, 0.1]]
f [0.821, 0.8207, 0.8206]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse 	current best epoch 19 	 [0.47, 0.67, 0.1] 	 lr: 2.5e-05 	 f: 82.40335263926167
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse[0m [33m epoch 31 [0m
Train...
loss: tensor(615.7096) lr: 2.5e-05 time: 5431.03
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.03 	r: 74.25 	f1: 77.5 	 4580 	 5652 	 6168
wo 	p: 93.77 	r: 84.89 	f1: 89.11 	 2876 	 3067 	 3388
ni 	p: 85.43 	r: 73.59 	f1: 79.07 	 1120 	 1311 	 1522

[32m iter_1[0m
ga 	p: 81.49 	r: 74.46 	f1: 77.82 	 4593 	 5636 	 6168
wo 	p: 91.69 	r: 86.92 	f1: 89.24 	 2945 	 3212 	 3388
ni 	p: 88.59 	r: 72.4 	f1: 79.68 	 1102 	 1244 	 1522

[32m iter_2[0m
ga 	p: 81.79 	r: 74.43 	f1: 77.94 	 4591 	 5613 	 6168
wo 	p: 92.05 	r: 86.48 	f1: 89.18 	 2930 	 3183 	 3388
ni 	p: 87.2 	r: 73.39 	f1: 79.7 	 1117 	 1281 	 1522
best_thres [[0.41, 0.6, 0.2], [0.41, 0.29, 0.19], [0.44, 0.35, 0.14]]
f [0.8126, 0.8144, 0.8152]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 8 	 [0.45, 0.56, 0.13] 	 lr: 0.0001 	 f: 82.97808936777554
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 17 [0m
Train...

[32m iter_0[0m
ga 	p: 80.0 	r: 78.13 	f1: 79.05 	 4819 	 6024 	 6168
wo 	p: 94.14 	r: 85.86 	f1: 89.81 	 2909 	 3090 	 3388
ni 	p: 88.46 	r: 75.03 	f1: 81.19 	 1142 	 1291 	 1522

[32m iter_1[0m
ga 	p: 79.25 	r: 78.87 	f1: 79.06 	 4865 	 6139 	 6168
wo 	p: 93.84 	r: 85.92 	f1: 89.71 	 2911 	 3102 	 3388
ni 	p: 87.49 	r: 75.36 	f1: 80.97 	 1147 	 1311 	 1522

[32m iter_2[0m
ga 	p: 80.81 	r: 77.2 	f1: 78.97 	 4762 	 5893 	 6168
wo 	p: 93.64 	r: 86.07 	f1: 89.7 	 2916 	 3114 	 3388
ni 	p: 86.17 	r: 76.54 	f1: 81.07 	 1165 	 1352 	 1522
best_thres [[0.31, 0.61, 0.23], [0.24, 0.76, 0.17], [0.34, 0.76, 0.14]]
f [0.8258, 0.8254, 0.8253]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 8 	 [0.45, 0.56, 0.13] 	 lr: 0.0001 	 f: 82.97808936777554
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 18 [0m
Train...
loss: tensor(621.1550) lr: 2.5e-05 time: 5350.23
pred_count_train 41644

Test...
loss: tensor(621.0928) lr: 2.5e-05 time: 5504.52
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.31 	r: 76.2 	f1: 79.14 	 4700 	 5710 	 6168
wo 	p: 93.97 	r: 86.42 	f1: 90.04 	 2928 	 3116 	 3388
ni 	p: 85.33 	r: 76.41 	f1: 80.62 	 1163 	 1363 	 1522

[32m iter_1[0m
ga 	p: 82.7 	r: 75.88 	f1: 79.14 	 4680 	 5659 	 6168
wo 	p: 93.8 	r: 86.25 	f1: 89.87 	 2922 	 3115 	 3388
ni 	p: 86.07 	r: 76.35 	f1: 80.92 	 1162 	 1350 	 1522

[32m iter_2[0m
ga 	p: 82.46 	r: 75.86 	f1: 79.02 	 4679 	 5674 	 6168
wo 	p: 93.3 	r: 86.66 	f1: 89.85 	 2936 	 3147 	 3388
ni 	p: 85.3 	r: 76.61 	f1: 80.72 	 1166 	 1367 	 1522
best_thres [[0.4, 0.57, 0.17], [0.41, 0.73, 0.13], [0.4, 0.67, 0.12]]
f [0.8267, 0.8267, 0.8264]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 8 	 [0.43, 0.54, 0.19] 	 lr: 0.0001 	 f: 83.14960139511709
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 18 [0m
Train...
loss: tensor(621.1550) lr: 2.5e-05 time: 5672.34
pred_count_train 41644

Test...
loss: tensor(41.8414) lr: 1e-05 time: 5666.4
pred_count_train 41644

Test...
loss: tensor(615.7096) lr: 2.5e-05 time: 5655.16
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.08 	r: 76.75 	f1: 78.85 	 4734 	 5839 	 6168
wo 	p: 94.32 	r: 86.25 	f1: 90.1 	 2922 	 3098 	 3388
ni 	p: 84.98 	r: 78.45 	f1: 81.59 	 1194 	 1405 	 1522

[32m iter_1[0m
ga 	p: 80.5 	r: 77.79 	f1: 79.12 	 4798 	 5960 	 6168
wo 	p: 94.58 	r: 85.98 	f1: 90.07 	 2913 	 3080 	 3388
ni 	p: 85.26 	r: 78.32 	f1: 81.64 	 1192 	 1398 	 1522

[32m iter_2[0m
ga 	p: 82.12 	r: 76.35 	f1: 79.13 	 4709 	 5734 	 6168
wo 	p: 94.52 	r: 86.01 	f1: 90.06 	 2914 	 3083 	 3388
ni 	p: 85.62 	r: 78.25 	f1: 81.77 	 1191 	 1391 	 1522
best_thres [[0.41, 0.67, 0.13], [0.35, 0.69, 0.13], [0.45, 0.69, 0.13]]
f [0.8263, 0.827, 0.8273]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 8 	 [0.51, 0.51, 0.13] 	 lr: 0.0001 	 f: 83.08403718208844
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 18 [0m
Train...

[32m iter_0[0m
ga 	p: 82.31 	r: 76.2 	f1: 79.14 	 4700 	 5710 	 6168
wo 	p: 93.97 	r: 86.42 	f1: 90.04 	 2928 	 3116 	 3388
ni 	p: 85.33 	r: 76.41 	f1: 80.62 	 1163 	 1363 	 1522

[32m iter_1[0m
ga 	p: 82.7 	r: 75.88 	f1: 79.14 	 4680 	 5659 	 6168
wo 	p: 93.8 	r: 86.25 	f1: 89.87 	 2922 	 3115 	 3388
ni 	p: 86.07 	r: 76.35 	f1: 80.92 	 1162 	 1350 	 1522

[32m iter_2[0m
ga 	p: 82.46 	r: 75.86 	f1: 79.02 	 4679 	 5674 	 6168
wo 	p: 93.3 	r: 86.66 	f1: 89.85 	 2936 	 3147 	 3388
ni 	p: 85.3 	r: 76.61 	f1: 80.72 	 1166 	 1367 	 1522
best_thres [[0.4, 0.57, 0.17], [0.41, 0.73, 0.13], [0.4, 0.67, 0.12]]
f [0.8267, 0.8267, 0.8264]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 8 	 [0.43, 0.54, 0.19] 	 lr: 0.0001 	 f: 83.14960139511709
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 18 [0m
Train...
loss: tensor(455.1139) lr: 2.5e-05 time: 5474.62
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.38 	r: 75.7 	f1: 78.44 	 4669 	 5737 	 6168
wo 	p: 93.16 	r: 86.42 	f1: 89.66 	 2928 	 3143 	 3388
ni 	p: 84.34 	r: 74.64 	f1: 79.19 	 1136 	 1347 	 1522

[32m iter_1[0m
ga 	p: 83.83 	r: 73.69 	f1: 78.43 	 4545 	 5422 	 6168
wo 	p: 93.53 	r: 86.22 	f1: 89.73 	 2921 	 3123 	 3388
ni 	p: 84.27 	r: 74.24 	f1: 78.94 	 1130 	 1341 	 1522

[32m iter_2[0m
ga 	p: 82.49 	r: 74.79 	f1: 78.45 	 4613 	 5592 	 6168
wo 	p: 93.44 	r: 86.19 	f1: 89.67 	 2920 	 3125 	 3388
ni 	p: 82.98 	r: 74.97 	f1: 78.77 	 1141 	 1375 	 1522
best_thres [[0.32, 0.6, 0.08], [0.57, 0.63, 0.08], [0.43, 0.63, 0.06]]
f [0.8198, 0.8199, 0.8198]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.9_it3_rs2016_preFalse 	best in epoch 19 	 [0.47, 0.67, 0.1] 	 lr: 2.5e-05 	 f: 82.40335263926167
model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse.h5
e2e-stack model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/test.json

[32m iter_0[0m
ga 	p: 80.0 	r: 78.13 	f1: 79.05 	 4819 	 6024 	 6168
wo 	p: 94.14 	r: 85.86 	f1: 89.81 	 2909 	 3090 	 3388
ni 	p: 88.46 	r: 75.03 	f1: 81.19 	 1142 	 1291 	 1522

[32m iter_1[0m
ga 	p: 79.25 	r: 78.87 	f1: 79.06 	 4865 	 6139 	 6168
wo 	p: 93.84 	r: 85.92 	f1: 89.71 	 2911 	 3102 	 3388
ni 	p: 87.49 	r: 75.36 	f1: 80.97 	 1147 	 1311 	 1522

[32m iter_2[0m
ga 	p: 80.81 	r: 77.2 	f1: 78.97 	 4762 	 5893 	 6168
wo 	p: 93.64 	r: 86.07 	f1: 89.7 	 2916 	 3114 	 3388
ni 	p: 86.17 	r: 76.54 	f1: 81.07 	 1165 	 1352 	 1522
best_thres [[0.31, 0.61, 0.23], [0.24, 0.76, 0.17], [0.34, 0.76, 0.14]]
f [0.8258, 0.8254, 0.8253]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 8 	 [0.45, 0.56, 0.13] 	 lr: 0.0001 	 f: 82.97808936777554
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 18 [0m
Train...

[32m iter_0[0m
ga 	p: 82.71 	r: 75.16 	f1: 78.76 	 4636 	 5605 	 6168
wo 	p: 94.21 	r: 85.06 	f1: 89.41 	 2882 	 3059 	 3388
ni 	p: 85.64 	r: 75.23 	f1: 80.1 	 1145 	 1337 	 1522

[32m iter_1[0m
ga 	p: 81.73 	r: 75.96 	f1: 78.74 	 4685 	 5732 	 6168
wo 	p: 93.55 	r: 85.6 	f1: 89.4 	 2900 	 3100 	 3388
ni 	p: 85.74 	r: 75.43 	f1: 80.25 	 1148 	 1339 	 1522

[32m iter_2[0m
ga 	p: 82.27 	r: 75.54 	f1: 78.76 	 4659 	 5663 	 6168
wo 	p: 91.99 	r: 87.07 	f1: 89.46 	 2950 	 3207 	 3388
ni 	p: 87.94 	r: 74.24 	f1: 80.51 	 1130 	 1285 	 1522
best_thres [[0.42, 0.69, 0.16], [0.35, 0.84, 0.11], [0.39, 0.39, 0.14]]
f [0.822, 0.822, 0.8224]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 8 	 [0.45, 0.56, 0.13] 	 lr: 0.0001 	 f: 82.97808936777554
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 19 [0m
Train...
loss: tensor(461.7027) lr: 2.5e-05 time: 5533.18
pred_count_train 41644

Test...
6 3 2
vocab size:  29396
prediction mode: model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse test [0.5, 0.5, 0.5]
{'ga': {'nn': 6217, 'np': 2765, 'pn': 4239, 'pp': 14704},
 'ni': {'nn': 22154, 'np': 1480, 'pn': 937, 'pp': 1811},
 'o': {'nn': 15684, 'np': 594, 'pn': 1403, 'pp': 8895}}
ga:	prec: 84.17, recall: 77.62, f1: 80.76
o:	prec: 93.74, recall: 86.38, f1: 89.91
ni:	prec: 55.03, recall: 65.9, f1: 59.98
all:	prec: 84.0, recall: 79.43, f1: 81.65
pred_count_test 26367
loss: tensor(465.1862) lr: 2.5e-05 time: 5574.67
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.43 	r: 75.24 	f1: 79.12 	 4641 	 5563 	 6168
wo 	p: 93.64 	r: 86.01 	f1: 89.66 	 2914 	 3112 	 3388
ni 	p: 85.09 	r: 75.36 	f1: 79.93 	 1147 	 1348 	 1522

[32m iter_1[0m
ga 	p: 83.37 	r: 75.6 	f1: 79.3 	 4663 	 5593 	 6168
wo 	p: 93.43 	r: 86.04 	f1: 89.58 	 2915 	 3120 	 3388
ni 	p: 84.85 	r: 75.82 	f1: 80.08 	 1154 	 1360 	 1522

[32m iter_2[0m
ga 	p: 83.15 	r: 75.62 	f1: 79.21 	 4664 	 5609 	 6168
wo 	p: 94.03 	r: 85.54 	f1: 89.58 	 2898 	 3082 	 3388
ni 	p: 85.65 	r: 75.69 	f1: 80.36 	 1152 	 1345 	 1522
best_thres [[0.48, 0.51, 0.2], [0.49, 0.58, 0.14], [0.48, 0.79, 0.14]]
f [0.8248, 0.8252, 0.8253]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 8 	 [0.43, 0.54, 0.19] 	 lr: 0.0001 	 f: 83.14960139511709
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 19 [0m
Train...
loss: tensor(461.7027) lr: 2.5e-05 time: 5342.97
pred_count_train 41644

Test...
loss: tensor(455.1139) lr: 2.5e-05 time: 5202.63
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.19 	r: 74.59 	f1: 78.66 	 4601 	 5531 	 6168
wo 	p: 92.82 	r: 87.04 	f1: 89.84 	 2949 	 3177 	 3388
ni 	p: 84.7 	r: 78.91 	f1: 81.7 	 1201 	 1418 	 1522

[32m iter_1[0m
ga 	p: 82.29 	r: 75.39 	f1: 78.69 	 4650 	 5651 	 6168
wo 	p: 93.26 	r: 86.63 	f1: 89.82 	 2935 	 3147 	 3388
ni 	p: 85.69 	r: 78.32 	f1: 81.84 	 1192 	 1391 	 1522

[32m iter_2[0m
ga 	p: 83.67 	r: 74.21 	f1: 78.66 	 4577 	 5470 	 6168
wo 	p: 93.3 	r: 86.69 	f1: 89.87 	 2937 	 3148 	 3388
ni 	p: 86.01 	r: 78.38 	f1: 82.02 	 1193 	 1387 	 1522
best_thres [[0.52, 0.48, 0.15], [0.45, 0.5, 0.15], [0.53, 0.5, 0.15]]
f [0.8254, 0.8254, 0.8256]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 8 	 [0.51, 0.51, 0.13] 	 lr: 0.0001 	 f: 83.08403718208844
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 19 [0m
Train...

[32m iter_0[0m
ga 	p: 83.43 	r: 75.24 	f1: 79.12 	 4641 	 5563 	 6168
wo 	p: 93.64 	r: 86.01 	f1: 89.66 	 2914 	 3112 	 3388
ni 	p: 85.09 	r: 75.36 	f1: 79.93 	 1147 	 1348 	 1522

[32m iter_1[0m
ga 	p: 83.37 	r: 75.6 	f1: 79.3 	 4663 	 5593 	 6168
wo 	p: 93.43 	r: 86.04 	f1: 89.58 	 2915 	 3120 	 3388
ni 	p: 84.85 	r: 75.82 	f1: 80.08 	 1154 	 1360 	 1522

[32m iter_2[0m
ga 	p: 83.15 	r: 75.62 	f1: 79.21 	 4664 	 5609 	 6168
wo 	p: 94.03 	r: 85.54 	f1: 89.58 	 2898 	 3082 	 3388
ni 	p: 85.65 	r: 75.69 	f1: 80.36 	 1152 	 1345 	 1522
best_thres [[0.48, 0.51, 0.2], [0.49, 0.58, 0.14], [0.48, 0.79, 0.14]]
f [0.8248, 0.8252, 0.8253]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 8 	 [0.43, 0.54, 0.19] 	 lr: 0.0001 	 f: 83.14960139511709
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 19 [0m
Train...
loss: tensor(337.6075) lr: 2.5e-05 time: 4692.62
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.71 	r: 75.16 	f1: 78.76 	 4636 	 5605 	 6168
wo 	p: 94.21 	r: 85.06 	f1: 89.41 	 2882 	 3059 	 3388
ni 	p: 85.64 	r: 75.23 	f1: 80.1 	 1145 	 1337 	 1522

[32m iter_1[0m
ga 	p: 81.73 	r: 75.96 	f1: 78.74 	 4685 	 5732 	 6168
wo 	p: 93.55 	r: 85.6 	f1: 89.4 	 2900 	 3100 	 3388
ni 	p: 85.74 	r: 75.43 	f1: 80.25 	 1148 	 1339 	 1522

[32m iter_2[0m
ga 	p: 82.27 	r: 75.54 	f1: 78.76 	 4659 	 5663 	 6168
wo 	p: 91.99 	r: 87.07 	f1: 89.46 	 2950 	 3207 	 3388
ni 	p: 87.94 	r: 74.24 	f1: 80.51 	 1130 	 1285 	 1522
best_thres [[0.42, 0.69, 0.16], [0.35, 0.84, 0.11], [0.39, 0.39, 0.14]]
f [0.822, 0.822, 0.8224]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 8 	 [0.45, 0.56, 0.13] 	 lr: 0.0001 	 f: 82.97808936777554
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 19 [0m
Train...

[32m iter_0[0m
ga 	p: 81.62 	r: 75.11 	f1: 78.23 	 4633 	 5676 	 6168
wo 	p: 93.15 	r: 85.95 	f1: 89.41 	 2912 	 3126 	 3388
ni 	p: 84.11 	r: 74.44 	f1: 78.98 	 1133 	 1347 	 1522

[32m iter_1[0m
ga 	p: 82.54 	r: 74.81 	f1: 78.48 	 4614 	 5590 	 6168
wo 	p: 93.53 	r: 85.83 	f1: 89.52 	 2908 	 3109 	 3388
ni 	p: 82.8 	r: 75.3 	f1: 78.87 	 1146 	 1384 	 1522

[32m iter_2[0m
ga 	p: 81.5 	r: 75.65 	f1: 78.47 	 4666 	 5725 	 6168
wo 	p: 93.38 	r: 85.83 	f1: 89.45 	 2908 	 3114 	 3388
ni 	p: 83.01 	r: 75.1 	f1: 78.85 	 1143 	 1377 	 1522
best_thres [[0.41, 0.51, 0.19], [0.47, 0.76, 0.12], [0.37, 0.8, 0.11]]
f [0.8176, 0.8184, 0.8185]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 8 	 [0.45, 0.56, 0.13] 	 lr: 0.0001 	 f: 82.97808936777554
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 20 [0m
Train...
loss: tensor(342.7108) lr: 2.5e-05 time: 4823.05
pred_count_train 41644

Test...
loss: tensor(352.0665) lr: 2.5e-05 time: 4944.31
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.45 	r: 75.34 	f1: 78.74 	 4647 	 5636 	 6168
wo 	p: 93.5 	r: 85.39 	f1: 89.26 	 2893 	 3094 	 3388
ni 	p: 84.64 	r: 74.24 	f1: 79.1 	 1130 	 1335 	 1522

[32m iter_1[0m
ga 	p: 83.4 	r: 75.03 	f1: 79.0 	 4628 	 5549 	 6168
wo 	p: 93.99 	r: 85.01 	f1: 89.27 	 2880 	 3064 	 3388
ni 	p: 84.77 	r: 74.24 	f1: 79.16 	 1130 	 1333 	 1522

[32m iter_2[0m
ga 	p: 83.52 	r: 74.79 	f1: 78.92 	 4613 	 5523 	 6168
wo 	p: 93.73 	r: 85.18 	f1: 89.25 	 2886 	 3079 	 3388
ni 	p: 85.05 	r: 74.38 	f1: 79.36 	 1132 	 1331 	 1522
best_thres [[0.43, 0.51, 0.21], [0.5, 0.79, 0.15], [0.65, 0.78, 0.14]]
f [0.8201, 0.8209, 0.8211]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 8 	 [0.43, 0.54, 0.19] 	 lr: 0.0001 	 f: 83.14960139511709
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 20 [0m
Train...
loss: tensor(342.7108) lr: 2.5e-05 time: 4990.06
pred_count_train 41644

Test...
loss: tensor(337.6075) lr: 2.5e-05 time: 4945.06
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.97 	r: 74.64 	f1: 78.59 	 4604 	 5549 	 6168
wo 	p: 93.31 	r: 86.45 	f1: 89.75 	 2929 	 3139 	 3388
ni 	p: 84.81 	r: 77.4 	f1: 80.93 	 1178 	 1389 	 1522

[32m iter_1[0m
ga 	p: 82.09 	r: 75.37 	f1: 78.59 	 4649 	 5663 	 6168
wo 	p: 93.13 	r: 86.45 	f1: 89.67 	 2929 	 3145 	 3388
ni 	p: 86.84 	r: 76.28 	f1: 81.22 	 1161 	 1337 	 1522

[32m iter_2[0m
ga 	p: 80.56 	r: 76.78 	f1: 78.63 	 4736 	 5879 	 6168
wo 	p: 93.08 	r: 86.54 	f1: 89.69 	 2932 	 3150 	 3388
ni 	p: 86.14 	r: 76.74 	f1: 81.17 	 1168 	 1356 	 1522
best_thres [[0.59, 0.52, 0.19], [0.51, 0.49, 0.22], [0.39, 0.48, 0.2]]
f [0.8235, 0.8235, 0.8235]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 8 	 [0.51, 0.51, 0.13] 	 lr: 0.0001 	 f: 83.08403718208844
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 20 [0m
Train...
loss: tensor(246.0995) lr: 2.5e-05 time: 4710.4
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.45 	r: 75.34 	f1: 78.74 	 4647 	 5636 	 6168
wo 	p: 93.5 	r: 85.39 	f1: 89.26 	 2893 	 3094 	 3388
ni 	p: 84.64 	r: 74.24 	f1: 79.1 	 1130 	 1335 	 1522

[32m iter_1[0m
ga 	p: 83.4 	r: 75.03 	f1: 79.0 	 4628 	 5549 	 6168
wo 	p: 93.99 	r: 85.01 	f1: 89.27 	 2880 	 3064 	 3388
ni 	p: 84.77 	r: 74.24 	f1: 79.16 	 1130 	 1333 	 1522

[32m iter_2[0m
ga 	p: 83.52 	r: 74.79 	f1: 78.92 	 4613 	 5523 	 6168
wo 	p: 93.73 	r: 85.18 	f1: 89.25 	 2886 	 3079 	 3388
ni 	p: 85.05 	r: 74.38 	f1: 79.36 	 1132 	 1331 	 1522
best_thres [[0.43, 0.51, 0.21], [0.5, 0.79, 0.15], [0.65, 0.78, 0.14]]
f [0.8201, 0.8209, 0.8211]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 8 	 [0.43, 0.54, 0.19] 	 lr: 0.0001 	 f: 83.14960139511709
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 20 [0m
Train...

[32m iter_0[0m
ga 	p: 81.62 	r: 75.11 	f1: 78.23 	 4633 	 5676 	 6168
wo 	p: 93.15 	r: 85.95 	f1: 89.41 	 2912 	 3126 	 3388
ni 	p: 84.11 	r: 74.44 	f1: 78.98 	 1133 	 1347 	 1522

[32m iter_1[0m
ga 	p: 82.54 	r: 74.81 	f1: 78.48 	 4614 	 5590 	 6168
wo 	p: 93.53 	r: 85.83 	f1: 89.52 	 2908 	 3109 	 3388
ni 	p: 82.8 	r: 75.3 	f1: 78.87 	 1146 	 1384 	 1522

[32m iter_2[0m
ga 	p: 81.5 	r: 75.65 	f1: 78.47 	 4666 	 5725 	 6168
wo 	p: 93.38 	r: 85.83 	f1: 89.45 	 2908 	 3114 	 3388
ni 	p: 83.01 	r: 75.1 	f1: 78.85 	 1143 	 1377 	 1522
best_thres [[0.41, 0.51, 0.19], [0.47, 0.76, 0.12], [0.37, 0.8, 0.11]]
f [0.8176, 0.8184, 0.8185]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 8 	 [0.45, 0.56, 0.13] 	 lr: 0.0001 	 f: 82.97808936777554
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 20 [0m
Train...

[32m iter_0[0m
ga 	p: 81.63 	r: 74.37 	f1: 77.83 	 4587 	 5619 	 6168
wo 	p: 93.09 	r: 85.48 	f1: 89.12 	 2896 	 3111 	 3388
ni 	p: 82.6 	r: 76.74 	f1: 79.56 	 1168 	 1414 	 1522

[32m iter_1[0m
ga 	p: 82.73 	r: 73.87 	f1: 78.05 	 4556 	 5507 	 6168
wo 	p: 92.23 	r: 86.48 	f1: 89.26 	 2930 	 3177 	 3388
ni 	p: 83.97 	r: 75.03 	f1: 79.25 	 1142 	 1360 	 1522

[32m iter_2[0m
ga 	p: 81.26 	r: 75.08 	f1: 78.05 	 4631 	 5699 	 6168
wo 	p: 93.83 	r: 85.3 	f1: 89.36 	 2890 	 3080 	 3388
ni 	p: 87.53 	r: 72.86 	f1: 79.53 	 1109 	 1267 	 1522
best_thres [[0.44, 0.45, 0.13], [0.69, 0.29, 0.09], [0.39, 0.84, 0.13]]
f [0.8153, 0.8161, 0.8164]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 8 	 [0.45, 0.56, 0.13] 	 lr: 0.0001 	 f: 82.97808936777554
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 21 [0m
Train...
loss: tensor(253.8247) lr: 2.5e-05 time: 4709.82
pred_count_train 41644

Test...
loss: tensor(256.5805) lr: 2.5e-05 time: 4802.32
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.44 	r: 74.74 	f1: 78.4 	 4610 	 5592 	 6168
wo 	p: 92.47 	r: 85.86 	f1: 89.04 	 2909 	 3146 	 3388
ni 	p: 86.02 	r: 73.98 	f1: 79.55 	 1126 	 1309 	 1522

[32m iter_1[0m
ga 	p: 81.54 	r: 75.99 	f1: 78.67 	 4687 	 5748 	 6168
wo 	p: 93.38 	r: 85.3 	f1: 89.16 	 2890 	 3095 	 3388
ni 	p: 85.35 	r: 74.24 	f1: 79.41 	 1130 	 1324 	 1522

[32m iter_2[0m
ga 	p: 82.0 	r: 75.28 	f1: 78.5 	 4643 	 5662 	 6168
wo 	p: 93.32 	r: 85.42 	f1: 89.2 	 2894 	 3101 	 3388
ni 	p: 84.93 	r: 74.44 	f1: 79.34 	 1133 	 1334 	 1522
best_thres [[0.42, 0.45, 0.18], [0.31, 0.81, 0.11], [0.37, 0.83, 0.09]]
f [0.8185, 0.8191, 0.819]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 8 	 [0.43, 0.54, 0.19] 	 lr: 0.0001 	 f: 83.14960139511709
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 21 [0m
Train...
loss: tensor(253.8247) lr: 2.5e-05 time: 4823.96
pred_count_train 41644

Test...
loss: tensor(246.0995) lr: 2.5e-05 time: 4826.87
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.34 	r: 74.92 	f1: 78.46 	 4621 	 5612 	 6168
wo 	p: 92.93 	r: 86.95 	f1: 89.84 	 2946 	 3170 	 3388
ni 	p: 87.07 	r: 74.31 	f1: 80.18 	 1131 	 1299 	 1522

[32m iter_1[0m
ga 	p: 82.76 	r: 74.58 	f1: 78.46 	 4600 	 5558 	 6168
wo 	p: 93.28 	r: 86.1 	f1: 89.55 	 2917 	 3127 	 3388
ni 	p: 83.38 	r: 77.79 	f1: 80.49 	 1184 	 1420 	 1522

[32m iter_2[0m
ga 	p: 82.42 	r: 74.85 	f1: 78.45 	 4617 	 5602 	 6168
wo 	p: 93.35 	r: 86.19 	f1: 89.63 	 2920 	 3128 	 3388
ni 	p: 83.32 	r: 77.46 	f1: 80.29 	 1179 	 1415 	 1522
best_thres [[0.47, 0.47, 0.21], [0.5, 0.54, 0.1], [0.48, 0.54, 0.1]]
f [0.8222, 0.8218, 0.8217]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 8 	 [0.51, 0.51, 0.13] 	 lr: 0.0001 	 f: 83.08403718208844
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 21 [0m
Train...
loss: tensor(612.6396) lr: 1.25e-05 time: 4716.22
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.44 	r: 74.74 	f1: 78.4 	 4610 	 5592 	 6168
wo 	p: 92.47 	r: 85.86 	f1: 89.04 	 2909 	 3146 	 3388
ni 	p: 86.02 	r: 73.98 	f1: 79.55 	 1126 	 1309 	 1522

[32m iter_1[0m
ga 	p: 81.54 	r: 75.99 	f1: 78.67 	 4687 	 5748 	 6168
wo 	p: 93.38 	r: 85.3 	f1: 89.16 	 2890 	 3095 	 3388
ni 	p: 85.35 	r: 74.24 	f1: 79.41 	 1130 	 1324 	 1522

[32m iter_2[0m
ga 	p: 82.0 	r: 75.28 	f1: 78.5 	 4643 	 5662 	 6168
wo 	p: 93.32 	r: 85.42 	f1: 89.2 	 2894 	 3101 	 3388
ni 	p: 84.93 	r: 74.44 	f1: 79.34 	 1133 	 1334 	 1522
best_thres [[0.42, 0.45, 0.18], [0.31, 0.81, 0.11], [0.37, 0.83, 0.09]]
f [0.8185, 0.8191, 0.819]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 8 	 [0.43, 0.54, 0.19] 	 lr: 0.0001 	 f: 83.14960139511709
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 21 [0m
Train...

[32m iter_0[0m
ga 	p: 81.63 	r: 74.37 	f1: 77.83 	 4587 	 5619 	 6168
wo 	p: 93.09 	r: 85.48 	f1: 89.12 	 2896 	 3111 	 3388
ni 	p: 82.6 	r: 76.74 	f1: 79.56 	 1168 	 1414 	 1522

[32m iter_1[0m
ga 	p: 82.73 	r: 73.87 	f1: 78.05 	 4556 	 5507 	 6168
wo 	p: 92.23 	r: 86.48 	f1: 89.26 	 2930 	 3177 	 3388
ni 	p: 83.97 	r: 75.03 	f1: 79.25 	 1142 	 1360 	 1522

[32m iter_2[0m
ga 	p: 81.26 	r: 75.08 	f1: 78.05 	 4631 	 5699 	 6168
wo 	p: 93.83 	r: 85.3 	f1: 89.36 	 2890 	 3080 	 3388
ni 	p: 87.53 	r: 72.86 	f1: 79.53 	 1109 	 1267 	 1522
best_thres [[0.44, 0.45, 0.13], [0.69, 0.29, 0.09], [0.39, 0.84, 0.13]]
f [0.8153, 0.8161, 0.8164]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 8 	 [0.45, 0.56, 0.13] 	 lr: 0.0001 	 f: 82.97808936777554
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 21 [0m
Train...

[32m iter_0[0m
ga 	p: 82.87 	r: 76.02 	f1: 79.3 	 4689 	 5658 	 6168
wo 	p: 93.25 	r: 87.22 	f1: 90.13 	 2955 	 3169 	 3388
ni 	p: 89.55 	r: 74.31 	f1: 81.22 	 1131 	 1263 	 1522

[32m iter_1[0m
ga 	p: 82.79 	r: 76.18 	f1: 79.35 	 4699 	 5676 	 6168
wo 	p: 94.29 	r: 86.25 	f1: 90.09 	 2922 	 3099 	 3388
ni 	p: 87.66 	r: 76.08 	f1: 81.46 	 1158 	 1321 	 1522

[32m iter_2[0m
ga 	p: 83.04 	r: 75.99 	f1: 79.36 	 4687 	 5644 	 6168
wo 	p: 94.23 	r: 86.28 	f1: 90.08 	 2923 	 3102 	 3388
ni 	p: 86.06 	r: 77.46 	f1: 81.54 	 1179 	 1370 	 1522
best_thres [[0.43, 0.41, 0.29], [0.42, 0.67, 0.18], [0.44, 0.7, 0.14]]
f [0.8291, 0.8292, 0.8292]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 8 	 [0.45, 0.56, 0.13] 	 lr: 0.0001 	 f: 82.97808936777554
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 22 [0m
Train...
loss: tensor(619.2207) lr: 1.25e-05 time: 4656.85
pred_count_train 41644

Test...
loss: tensor(618.8445) lr: 1.25e-05 time: 4747.75
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.95 	r: 78.83 	f1: 79.39 	 4862 	 6081 	 6168
wo 	p: 94.44 	r: 86.22 	f1: 90.14 	 2921 	 3093 	 3388
ni 	p: 85.4 	r: 77.99 	f1: 81.52 	 1187 	 1390 	 1522

[32m iter_1[0m
ga 	p: 84.34 	r: 75.11 	f1: 79.46 	 4633 	 5493 	 6168
wo 	p: 92.8 	r: 87.07 	f1: 89.84 	 2950 	 3179 	 3388
ni 	p: 87.29 	r: 76.74 	f1: 81.68 	 1168 	 1338 	 1522

[32m iter_2[0m
ga 	p: 84.34 	r: 75.18 	f1: 79.5 	 4637 	 5498 	 6168
wo 	p: 92.88 	r: 86.98 	f1: 89.83 	 2947 	 3173 	 3388
ni 	p: 86.18 	r: 77.86 	f1: 81.81 	 1185 	 1375 	 1522
best_thres [[0.28, 0.61, 0.21], [0.54, 0.48, 0.19], [0.59, 0.52, 0.16]]
f [0.8289, 0.8294, 0.8297]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 8 	 [0.43, 0.54, 0.19] 	 lr: 0.0001 	 f: 83.14960139511709
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 22 [0m
Train...
loss: tensor(619.2207) lr: 1.25e-05 time: 4805.22
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.09 	r: 76.69 	f1: 79.3 	 4730 	 5762 	 6168
wo 	p: 93.98 	r: 86.63 	f1: 90.16 	 2935 	 3123 	 3388
ni 	p: 86.24 	r: 79.04 	f1: 82.48 	 1203 	 1395 	 1522

[32m iter_1[0m
ga 	p: 81.69 	r: 77.01 	f1: 79.28 	 4750 	 5815 	 6168
wo 	p: 93.9 	r: 86.75 	f1: 90.18 	 2939 	 3130 	 3388
ni 	p: 85.12 	r: 80.03 	f1: 82.49 	 1218 	 1431 	 1522

[32m iter_2[0m
ga 	p: 81.76 	r: 77.03 	f1: 79.32 	 4751 	 5811 	 6168
wo 	p: 93.2 	r: 87.43 	f1: 90.22 	 2962 	 3178 	 3388
ni 	p: 85.26 	r: 80.16 	f1: 82.63 	 1220 	 1431 	 1522
best_thres [[0.43, 0.58, 0.18], [0.4, 0.56, 0.14], [0.4, 0.48, 0.14]]
f [0.8304, 0.8304, 0.8306]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 8 	 [0.51, 0.51, 0.13] 	 lr: 0.0001 	 f: 83.08403718208844
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 22 [0m
Train...
loss: tensor(612.6396) lr: 1.25e-05 time: 4894.52
pred_count_train 41644

Test...
loss: tensor(501.3188) lr: 1.25e-05 time: 4765.85
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.95 	r: 78.83 	f1: 79.39 	 4862 	 6081 	 6168
wo 	p: 94.44 	r: 86.22 	f1: 90.14 	 2921 	 3093 	 3388
ni 	p: 85.4 	r: 77.99 	f1: 81.52 	 1187 	 1390 	 1522

[32m iter_1[0m
ga 	p: 84.34 	r: 75.11 	f1: 79.46 	 4633 	 5493 	 6168
wo 	p: 92.8 	r: 87.07 	f1: 89.84 	 2950 	 3179 	 3388
ni 	p: 87.29 	r: 76.74 	f1: 81.68 	 1168 	 1338 	 1522

[32m iter_2[0m
ga 	p: 84.34 	r: 75.18 	f1: 79.5 	 4637 	 5498 	 6168
wo 	p: 92.88 	r: 86.98 	f1: 89.83 	 2947 	 3173 	 3388
ni 	p: 86.18 	r: 77.86 	f1: 81.81 	 1185 	 1375 	 1522
best_thres [[0.28, 0.61, 0.21], [0.54, 0.48, 0.19], [0.59, 0.52, 0.16]]
f [0.8289, 0.8294, 0.8297]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 8 	 [0.43, 0.54, 0.19] 	 lr: 0.0001 	 f: 83.14960139511709
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 22 [0m
Train...

[32m iter_0[0m
ga 	p: 82.87 	r: 76.02 	f1: 79.3 	 4689 	 5658 	 6168
wo 	p: 93.25 	r: 87.22 	f1: 90.13 	 2955 	 3169 	 3388
ni 	p: 89.55 	r: 74.31 	f1: 81.22 	 1131 	 1263 	 1522

[32m iter_1[0m
ga 	p: 82.79 	r: 76.18 	f1: 79.35 	 4699 	 5676 	 6168
wo 	p: 94.29 	r: 86.25 	f1: 90.09 	 2922 	 3099 	 3388
ni 	p: 87.66 	r: 76.08 	f1: 81.46 	 1158 	 1321 	 1522

[32m iter_2[0m
ga 	p: 83.04 	r: 75.99 	f1: 79.36 	 4687 	 5644 	 6168
wo 	p: 94.23 	r: 86.28 	f1: 90.08 	 2923 	 3102 	 3388
ni 	p: 86.06 	r: 77.46 	f1: 81.54 	 1179 	 1370 	 1522
best_thres [[0.43, 0.41, 0.29], [0.42, 0.67, 0.18], [0.44, 0.7, 0.14]]
f [0.8291, 0.8292, 0.8292]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 8 	 [0.45, 0.56, 0.13] 	 lr: 0.0001 	 f: 82.97808936777554
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 22 [0m
Train...

[32m iter_0[0m
ga 	p: 81.87 	r: 76.43 	f1: 79.05 	 4714 	 5758 	 6168
wo 	p: 92.79 	r: 86.95 	f1: 89.78 	 2946 	 3175 	 3388
ni 	p: 87.69 	r: 75.36 	f1: 81.06 	 1147 	 1308 	 1522

[32m iter_1[0m
ga 	p: 81.67 	r: 76.72 	f1: 79.12 	 4732 	 5794 	 6168
wo 	p: 92.46 	r: 87.28 	f1: 89.8 	 2957 	 3198 	 3388
ni 	p: 86.44 	r: 76.22 	f1: 81.01 	 1160 	 1342 	 1522

[32m iter_2[0m
ga 	p: 81.55 	r: 76.83 	f1: 79.12 	 4739 	 5811 	 6168
wo 	p: 92.69 	r: 87.19 	f1: 89.86 	 2954 	 3187 	 3388
ni 	p: 86.95 	r: 76.15 	f1: 81.19 	 1159 	 1333 	 1522
best_thres [[0.37, 0.4, 0.21], [0.34, 0.34, 0.14], [0.33, 0.35, 0.14]]
f [0.8262, 0.8264, 0.8266]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 8 	 [0.45, 0.56, 0.13] 	 lr: 0.0001 	 f: 82.97808936777554
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 23 [0m
Train...
loss: tensor(505.0524) lr: 1.25e-05 time: 4575.85
pred_count_train 41644

Test...
loss: tensor(511.5809) lr: 1.25e-05 time: 4621.98
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.3 	r: 75.68 	f1: 79.31 	 4668 	 5604 	 6168
wo 	p: 94.16 	r: 86.1 	f1: 89.95 	 2917 	 3098 	 3388
ni 	p: 87.48 	r: 74.84 	f1: 80.67 	 1139 	 1302 	 1522

[32m iter_1[0m
ga 	p: 83.41 	r: 75.65 	f1: 79.34 	 4666 	 5594 	 6168
wo 	p: 93.8 	r: 86.13 	f1: 89.8 	 2918 	 3111 	 3388
ni 	p: 86.51 	r: 75.82 	f1: 80.81 	 1154 	 1334 	 1522

[32m iter_2[0m
ga 	p: 82.39 	r: 76.69 	f1: 79.44 	 4730 	 5741 	 6168
wo 	p: 93.65 	r: 86.25 	f1: 89.8 	 2922 	 3120 	 3388
ni 	p: 87.29 	r: 75.36 	f1: 80.89 	 1147 	 1314 	 1522
best_thres [[0.43, 0.58, 0.23], [0.43, 0.7, 0.16], [0.35, 0.7, 0.16]]
f [0.8276, 0.8276, 0.8277]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 8 	 [0.43, 0.54, 0.19] 	 lr: 0.0001 	 f: 83.14960139511709
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 23 [0m
Train...
loss: tensor(505.0524) lr: 1.25e-05 time: 4722.63
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.28 	r: 76.13 	f1: 79.09 	 4696 	 5707 	 6168
wo 	p: 93.86 	r: 86.63 	f1: 90.1 	 2935 	 3127 	 3388
ni 	p: 87.77 	r: 76.87 	f1: 81.96 	 1170 	 1333 	 1522

[32m iter_1[0m
ga 	p: 82.99 	r: 75.7 	f1: 79.18 	 4669 	 5626 	 6168
wo 	p: 93.71 	r: 86.57 	f1: 90.0 	 2933 	 3130 	 3388
ni 	p: 85.21 	r: 79.11 	f1: 82.04 	 1204 	 1413 	 1522

[32m iter_2[0m
ga 	p: 82.77 	r: 75.79 	f1: 79.13 	 4675 	 5648 	 6168
wo 	p: 93.74 	r: 86.6 	f1: 90.03 	 2934 	 3130 	 3388
ni 	p: 84.43 	r: 79.43 	f1: 81.86 	 1209 	 1432 	 1522
best_thres [[0.47, 0.6, 0.18], [0.5, 0.57, 0.12], [0.49, 0.57, 0.11]]
f [0.8285, 0.8287, 0.8286]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 8 	 [0.51, 0.51, 0.13] 	 lr: 0.0001 	 f: 83.08403718208844
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 23 [0m
Train...
loss: tensor(501.3188) lr: 1.25e-05 time: 4696.37
pred_count_train 41644

Test...
loss: tensor(418.1548) lr: 1.25e-05 time: 4598.46
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.3 	r: 75.68 	f1: 79.31 	 4668 	 5604 	 6168
wo 	p: 94.16 	r: 86.1 	f1: 89.95 	 2917 	 3098 	 3388
ni 	p: 87.48 	r: 74.84 	f1: 80.67 	 1139 	 1302 	 1522

[32m iter_1[0m
ga 	p: 83.41 	r: 75.65 	f1: 79.34 	 4666 	 5594 	 6168
wo 	p: 93.8 	r: 86.13 	f1: 89.8 	 2918 	 3111 	 3388
ni 	p: 86.51 	r: 75.82 	f1: 80.81 	 1154 	 1334 	 1522

[32m iter_2[0m
ga 	p: 82.39 	r: 76.69 	f1: 79.44 	 4730 	 5741 	 6168
wo 	p: 93.65 	r: 86.25 	f1: 89.8 	 2922 	 3120 	 3388
ni 	p: 87.29 	r: 75.36 	f1: 80.89 	 1147 	 1314 	 1522
best_thres [[0.43, 0.58, 0.23], [0.43, 0.7, 0.16], [0.35, 0.7, 0.16]]
f [0.8276, 0.8276, 0.8277]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 8 	 [0.43, 0.54, 0.19] 	 lr: 0.0001 	 f: 83.14960139511709
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 23 [0m
Train...

[32m iter_0[0m
ga 	p: 81.87 	r: 76.43 	f1: 79.05 	 4714 	 5758 	 6168
wo 	p: 92.79 	r: 86.95 	f1: 89.78 	 2946 	 3175 	 3388
ni 	p: 87.69 	r: 75.36 	f1: 81.06 	 1147 	 1308 	 1522

[32m iter_1[0m
ga 	p: 81.67 	r: 76.72 	f1: 79.12 	 4732 	 5794 	 6168
wo 	p: 92.46 	r: 87.28 	f1: 89.8 	 2957 	 3198 	 3388
ni 	p: 86.44 	r: 76.22 	f1: 81.01 	 1160 	 1342 	 1522

[32m iter_2[0m
ga 	p: 81.55 	r: 76.83 	f1: 79.12 	 4739 	 5811 	 6168
wo 	p: 92.69 	r: 87.19 	f1: 89.86 	 2954 	 3187 	 3388
ni 	p: 86.95 	r: 76.15 	f1: 81.19 	 1159 	 1333 	 1522
best_thres [[0.37, 0.4, 0.21], [0.34, 0.34, 0.14], [0.33, 0.35, 0.14]]
f [0.8262, 0.8264, 0.8266]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 8 	 [0.45, 0.56, 0.13] 	 lr: 0.0001 	 f: 82.97808936777554
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 23 [0m
Train...

[32m iter_0[0m
ga 	p: 81.64 	r: 76.12 	f1: 78.78 	 4695 	 5751 	 6168
wo 	p: 92.64 	r: 86.95 	f1: 89.71 	 2946 	 3180 	 3388
ni 	p: 87.59 	r: 74.64 	f1: 80.6 	 1136 	 1297 	 1522

[32m iter_1[0m
ga 	p: 82.26 	r: 75.62 	f1: 78.8 	 4664 	 5670 	 6168
wo 	p: 92.7 	r: 87.31 	f1: 89.92 	 2958 	 3191 	 3388
ni 	p: 85.49 	r: 76.28 	f1: 80.62 	 1161 	 1358 	 1522

[32m iter_2[0m
ga 	p: 82.39 	r: 75.63 	f1: 78.87 	 4665 	 5662 	 6168
wo 	p: 92.64 	r: 87.37 	f1: 89.93 	 2960 	 3195 	 3388
ni 	p: 88.8 	r: 73.98 	f1: 80.72 	 1126 	 1268 	 1522
best_thres [[0.35, 0.4, 0.22], [0.37, 0.34, 0.13], [0.37, 0.33, 0.17]]
f [0.8239, 0.8244, 0.8247]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 8 	 [0.45, 0.56, 0.13] 	 lr: 0.0001 	 f: 82.97808936777554
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 24 [0m
Train...
loss: tensor(424.9417) lr: 1.25e-05 time: 4618.9
pred_count_train 41644

Test...
loss: tensor(430.6076) lr: 1.25e-05 time: 4799.26
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.99 	r: 76.12 	f1: 78.95 	 4695 	 5726 	 6168
wo 	p: 93.51 	r: 86.33 	f1: 89.78 	 2925 	 3128 	 3388
ni 	p: 88.79 	r: 72.86 	f1: 80.04 	 1109 	 1249 	 1522

[32m iter_1[0m
ga 	p: 82.45 	r: 76.15 	f1: 79.17 	 4697 	 5697 	 6168
wo 	p: 94.0 	r: 85.48 	f1: 89.53 	 2896 	 3081 	 3388
ni 	p: 83.74 	r: 77.14 	f1: 80.3 	 1174 	 1402 	 1522

[32m iter_2[0m
ga 	p: 82.62 	r: 76.01 	f1: 79.18 	 4688 	 5674 	 6168
wo 	p: 93.8 	r: 85.68 	f1: 89.56 	 2903 	 3095 	 3388
ni 	p: 84.21 	r: 77.46 	f1: 80.7 	 1179 	 1400 	 1522
best_thres [[0.4, 0.49, 0.28], [0.4, 0.77, 0.12], [0.41, 0.78, 0.11]]
f [0.8242, 0.8245, 0.8249]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 8 	 [0.43, 0.54, 0.19] 	 lr: 0.0001 	 f: 83.14960139511709
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 24 [0m
Train...
loss: tensor(424.9417) lr: 1.25e-05 time: 4842.74
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.64 	r: 76.26 	f1: 78.86 	 4704 	 5762 	 6168
wo 	p: 93.62 	r: 86.19 	f1: 89.75 	 2920 	 3119 	 3388
ni 	p: 85.98 	r: 77.79 	f1: 81.68 	 1184 	 1377 	 1522

[32m iter_1[0m
ga 	p: 80.5 	r: 77.35 	f1: 78.89 	 4771 	 5927 	 6168
wo 	p: 94.33 	r: 85.51 	f1: 89.7 	 2897 	 3071 	 3388
ni 	p: 87.89 	r: 76.28 	f1: 81.67 	 1161 	 1321 	 1522

[32m iter_2[0m
ga 	p: 80.48 	r: 77.32 	f1: 78.87 	 4769 	 5926 	 6168
wo 	p: 94.57 	r: 85.3 	f1: 89.7 	 2890 	 3056 	 3388
ni 	p: 87.08 	r: 76.61 	f1: 81.51 	 1166 	 1339 	 1522
best_thres [[0.43, 0.59, 0.17], [0.35, 0.66, 0.2], [0.35, 0.69, 0.18]]
f [0.8256, 0.8255, 0.8252]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 8 	 [0.51, 0.51, 0.13] 	 lr: 0.0001 	 f: 83.08403718208844
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 24 [0m
Train...
loss: tensor(418.1548) lr: 1.25e-05 time: 4862.51
pred_count_train 41644

Test...
loss: tensor(348.0447) lr: 1.25e-05 time: 4726.19
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.99 	r: 76.12 	f1: 78.95 	 4695 	 5726 	 6168
wo 	p: 93.51 	r: 86.33 	f1: 89.78 	 2925 	 3128 	 3388
ni 	p: 88.79 	r: 72.86 	f1: 80.04 	 1109 	 1249 	 1522

[32m iter_1[0m
ga 	p: 82.45 	r: 76.15 	f1: 79.17 	 4697 	 5697 	 6168
wo 	p: 94.0 	r: 85.48 	f1: 89.53 	 2896 	 3081 	 3388
ni 	p: 83.74 	r: 77.14 	f1: 80.3 	 1174 	 1402 	 1522

[32m iter_2[0m
ga 	p: 82.62 	r: 76.01 	f1: 79.18 	 4688 	 5674 	 6168
wo 	p: 93.8 	r: 85.68 	f1: 89.56 	 2903 	 3095 	 3388
ni 	p: 84.21 	r: 77.46 	f1: 80.7 	 1179 	 1400 	 1522
best_thres [[0.4, 0.49, 0.28], [0.4, 0.77, 0.12], [0.41, 0.78, 0.11]]
f [0.8242, 0.8245, 0.8249]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 8 	 [0.43, 0.54, 0.19] 	 lr: 0.0001 	 f: 83.14960139511709
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 24 [0m
Train...

[32m iter_0[0m
ga 	p: 81.64 	r: 76.12 	f1: 78.78 	 4695 	 5751 	 6168
wo 	p: 92.64 	r: 86.95 	f1: 89.71 	 2946 	 3180 	 3388
ni 	p: 87.59 	r: 74.64 	f1: 80.6 	 1136 	 1297 	 1522

[32m iter_1[0m
ga 	p: 82.26 	r: 75.62 	f1: 78.8 	 4664 	 5670 	 6168
wo 	p: 92.7 	r: 87.31 	f1: 89.92 	 2958 	 3191 	 3388
ni 	p: 85.49 	r: 76.28 	f1: 80.62 	 1161 	 1358 	 1522

[32m iter_2[0m
ga 	p: 82.39 	r: 75.63 	f1: 78.87 	 4665 	 5662 	 6168
wo 	p: 92.64 	r: 87.37 	f1: 89.93 	 2960 	 3195 	 3388
ni 	p: 88.8 	r: 73.98 	f1: 80.72 	 1126 	 1268 	 1522
best_thres [[0.35, 0.4, 0.22], [0.37, 0.34, 0.13], [0.37, 0.33, 0.17]]
f [0.8239, 0.8244, 0.8247]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 8 	 [0.45, 0.56, 0.13] 	 lr: 0.0001 	 f: 82.97808936777554
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 24 [0m
Train...

[32m iter_0[0m
ga 	p: 81.41 	r: 75.97 	f1: 78.6 	 4686 	 5756 	 6168
wo 	p: 92.91 	r: 86.3 	f1: 89.49 	 2924 	 3147 	 3388
ni 	p: 83.49 	r: 75.1 	f1: 79.07 	 1143 	 1369 	 1522

[32m iter_1[0m
ga 	p: 80.89 	r: 76.7 	f1: 78.74 	 4731 	 5849 	 6168
wo 	p: 92.79 	r: 86.6 	f1: 89.59 	 2934 	 3162 	 3388
ni 	p: 84.36 	r: 74.77 	f1: 79.28 	 1138 	 1349 	 1522

[32m iter_2[0m
ga 	p: 81.03 	r: 76.64 	f1: 78.77 	 4727 	 5834 	 6168
wo 	p: 92.9 	r: 86.51 	f1: 89.59 	 2931 	 3155 	 3388
ni 	p: 87.43 	r: 72.67 	f1: 79.37 	 1106 	 1265 	 1522
best_thres [[0.33, 0.4, 0.16], [0.27, 0.37, 0.11], [0.27, 0.37, 0.14]]
f [0.82, 0.8206, 0.821]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 8 	 [0.45, 0.56, 0.13] 	 lr: 0.0001 	 f: 82.97808936777554
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 25 [0m
Train...
loss: tensor(356.1748) lr: 1.25e-05 time: 4430.76
pred_count_train 41644

Test...
loss: tensor(365.0047) lr: 1.25e-05 time: 4496.11
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.0 	r: 75.18 	f1: 78.89 	 4637 	 5587 	 6168
wo 	p: 92.45 	r: 86.69 	f1: 89.47 	 2937 	 3177 	 3388
ni 	p: 84.91 	r: 73.92 	f1: 79.03 	 1125 	 1325 	 1522

[32m iter_1[0m
ga 	p: 83.03 	r: 75.36 	f1: 79.01 	 4648 	 5598 	 6168
wo 	p: 92.59 	r: 86.28 	f1: 89.32 	 2923 	 3157 	 3388
ni 	p: 87.72 	r: 71.81 	f1: 78.97 	 1093 	 1246 	 1522

[32m iter_2[0m
ga 	p: 83.08 	r: 75.39 	f1: 79.05 	 4650 	 5597 	 6168
wo 	p: 93.72 	r: 85.51 	f1: 89.43 	 2897 	 3091 	 3388
ni 	p: 86.35 	r: 73.13 	f1: 79.19 	 1113 	 1289 	 1522
best_thres [[0.41, 0.42, 0.2], [0.4, 0.45, 0.2], [0.4, 0.8, 0.15]]
f [0.8219, 0.822, 0.8222]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 8 	 [0.43, 0.54, 0.19] 	 lr: 0.0001 	 f: 83.14960139511709
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 25 [0m
Train...
loss: tensor(356.1748) lr: 1.25e-05 time: 4624.51
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.79 	r: 74.77 	f1: 78.58 	 4612 	 5571 	 6168
wo 	p: 94.66 	r: 85.27 	f1: 89.72 	 2889 	 3052 	 3388
ni 	p: 83.7 	r: 77.27 	f1: 80.36 	 1176 	 1405 	 1522

[32m iter_1[0m
ga 	p: 83.39 	r: 74.22 	f1: 78.54 	 4578 	 5490 	 6168
wo 	p: 94.44 	r: 85.18 	f1: 89.57 	 2886 	 3056 	 3388
ni 	p: 84.75 	r: 77.07 	f1: 80.73 	 1173 	 1384 	 1522

[32m iter_2[0m
ga 	p: 83.37 	r: 74.22 	f1: 78.53 	 4578 	 5491 	 6168
wo 	p: 94.76 	r: 84.92 	f1: 89.57 	 2877 	 3036 	 3388
ni 	p: 86.1 	r: 75.69 	f1: 80.56 	 1152 	 1338 	 1522
best_thres [[0.47, 0.73, 0.14], [0.5, 0.7, 0.13], [0.5, 0.74, 0.16]]
f [0.8222, 0.8222, 0.8221]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 8 	 [0.51, 0.51, 0.13] 	 lr: 0.0001 	 f: 83.08403718208844
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 25 [0m
Train...
loss: tensor(348.0447) lr: 1.25e-05 time: 4758.52
pred_count_train 41644

Test...
loss: tensor(613.0255) lr: 1e-05 time: 4733.88
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.0 	r: 75.18 	f1: 78.89 	 4637 	 5587 	 6168
wo 	p: 92.45 	r: 86.69 	f1: 89.47 	 2937 	 3177 	 3388
ni 	p: 84.91 	r: 73.92 	f1: 79.03 	 1125 	 1325 	 1522

[32m iter_1[0m
ga 	p: 83.03 	r: 75.36 	f1: 79.01 	 4648 	 5598 	 6168
wo 	p: 92.59 	r: 86.28 	f1: 89.32 	 2923 	 3157 	 3388
ni 	p: 87.72 	r: 71.81 	f1: 78.97 	 1093 	 1246 	 1522

[32m iter_2[0m
ga 	p: 83.08 	r: 75.39 	f1: 79.05 	 4650 	 5597 	 6168
wo 	p: 93.72 	r: 85.51 	f1: 89.43 	 2897 	 3091 	 3388
ni 	p: 86.35 	r: 73.13 	f1: 79.19 	 1113 	 1289 	 1522
best_thres [[0.41, 0.42, 0.2], [0.4, 0.45, 0.2], [0.4, 0.8, 0.15]]
f [0.8219, 0.822, 0.8222]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 8 	 [0.43, 0.54, 0.19] 	 lr: 0.0001 	 f: 83.14960139511709
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 25 [0m
Train...

[32m iter_0[0m
ga 	p: 81.41 	r: 75.97 	f1: 78.6 	 4686 	 5756 	 6168
wo 	p: 92.91 	r: 86.3 	f1: 89.49 	 2924 	 3147 	 3388
ni 	p: 83.49 	r: 75.1 	f1: 79.07 	 1143 	 1369 	 1522

[32m iter_1[0m
ga 	p: 80.89 	r: 76.7 	f1: 78.74 	 4731 	 5849 	 6168
wo 	p: 92.79 	r: 86.6 	f1: 89.59 	 2934 	 3162 	 3388
ni 	p: 84.36 	r: 74.77 	f1: 79.28 	 1138 	 1349 	 1522

[32m iter_2[0m
ga 	p: 81.03 	r: 76.64 	f1: 78.77 	 4727 	 5834 	 6168
wo 	p: 92.9 	r: 86.51 	f1: 89.59 	 2931 	 3155 	 3388
ni 	p: 87.43 	r: 72.67 	f1: 79.37 	 1106 	 1265 	 1522
best_thres [[0.33, 0.4, 0.16], [0.27, 0.37, 0.11], [0.27, 0.37, 0.14]]
f [0.82, 0.8206, 0.821]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 8 	 [0.45, 0.56, 0.13] 	 lr: 0.0001 	 f: 82.97808936777554
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 25 [0m
Train...

[32m iter_0[0m
ga 	p: 82.31 	r: 76.48 	f1: 79.28 	 4717 	 5731 	 6168
wo 	p: 93.49 	r: 86.95 	f1: 90.11 	 2946 	 3151 	 3388
ni 	p: 86.99 	r: 76.41 	f1: 81.36 	 1163 	 1337 	 1522

[32m iter_1[0m
ga 	p: 83.14 	r: 75.63 	f1: 79.21 	 4665 	 5611 	 6168
wo 	p: 94.03 	r: 86.48 	f1: 90.1 	 2930 	 3116 	 3388
ni 	p: 89.1 	r: 75.16 	f1: 81.54 	 1144 	 1284 	 1522

[32m iter_2[0m
ga 	p: 79.82 	r: 78.7 	f1: 79.26 	 4854 	 6081 	 6168
wo 	p: 93.57 	r: 86.81 	f1: 90.06 	 2941 	 3143 	 3388
ni 	p: 89.11 	r: 75.3 	f1: 81.62 	 1146 	 1286 	 1522
best_thres [[0.4, 0.49, 0.19], [0.46, 0.66, 0.18], [0.26, 0.59, 0.17]]
f [0.8288, 0.8288, 0.8286]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 8 	 [0.45, 0.56, 0.13] 	 lr: 0.0001 	 f: 82.97808936777554
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 26 [0m
Train...
loss: tensor(620.2277) lr: 1e-05 time: 4678.69
pred_count_train 41644

Test...
loss: tensor(621.2711) lr: 1e-05 time: 4692.97
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.91 	r: 77.87 	f1: 79.36 	 4803 	 5936 	 6168
wo 	p: 93.24 	r: 87.13 	f1: 90.08 	 2952 	 3166 	 3388
ni 	p: 86.44 	r: 76.22 	f1: 81.01 	 1160 	 1342 	 1522

[32m iter_1[0m
ga 	p: 84.27 	r: 75.03 	f1: 79.38 	 4628 	 5492 	 6168
wo 	p: 92.83 	r: 87.19 	f1: 89.92 	 2954 	 3182 	 3388
ni 	p: 84.63 	r: 78.12 	f1: 81.24 	 1189 	 1405 	 1522

[32m iter_2[0m
ga 	p: 82.34 	r: 76.8 	f1: 79.47 	 4737 	 5753 	 6168
wo 	p: 93.61 	r: 86.48 	f1: 89.9 	 2930 	 3130 	 3388
ni 	p: 84.98 	r: 78.06 	f1: 81.37 	 1188 	 1398 	 1522
best_thres [[0.33, 0.51, 0.2], [0.58, 0.49, 0.12], [0.4, 0.7, 0.12]]
f [0.8285, 0.8288, 0.8289]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 8 	 [0.43, 0.54, 0.19] 	 lr: 0.0001 	 f: 83.14960139511709
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 26 [0m
Train...
loss: tensor(620.2277) lr: 1e-05 time: 4735.26
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.3 	r: 76.73 	f1: 79.42 	 4733 	 5751 	 6168
wo 	p: 94.04 	r: 86.6 	f1: 90.17 	 2934 	 3120 	 3388
ni 	p: 83.96 	r: 80.49 	f1: 82.19 	 1225 	 1459 	 1522

[32m iter_1[0m
ga 	p: 82.73 	r: 76.48 	f1: 79.48 	 4717 	 5702 	 6168
wo 	p: 92.9 	r: 87.72 	f1: 90.24 	 2972 	 3199 	 3388
ni 	p: 86.24 	r: 79.04 	f1: 82.48 	 1203 	 1395 	 1522

[32m iter_2[0m
ga 	p: 82.69 	r: 76.43 	f1: 79.43 	 4714 	 5701 	 6168
wo 	p: 92.8 	r: 87.84 	f1: 90.25 	 2976 	 3207 	 3388
ni 	p: 85.26 	r: 80.16 	f1: 82.63 	 1220 	 1431 	 1522
best_thres [[0.44, 0.61, 0.12], [0.46, 0.47, 0.14], [0.46, 0.45, 0.11]]
f [0.8307, 0.8314, 0.8316]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 25 	 [0.46, 0.45, 0.11] 	 lr: 1e-05 	 f: 83.1601738344834
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 26 [0m
Train...
loss: tensor(613.0255) lr: 1e-05 time: 4722.2
pred_count_train 41644

Test...
loss: tensor(516.2535) lr: 1e-05 time: 4626.55
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.91 	r: 77.87 	f1: 79.36 	 4803 	 5936 	 6168
wo 	p: 93.24 	r: 87.13 	f1: 90.08 	 2952 	 3166 	 3388
ni 	p: 86.44 	r: 76.22 	f1: 81.01 	 1160 	 1342 	 1522

[32m iter_1[0m
ga 	p: 84.27 	r: 75.03 	f1: 79.38 	 4628 	 5492 	 6168
wo 	p: 92.83 	r: 87.19 	f1: 89.92 	 2954 	 3182 	 3388
ni 	p: 84.63 	r: 78.12 	f1: 81.24 	 1189 	 1405 	 1522

[32m iter_2[0m
ga 	p: 82.34 	r: 76.8 	f1: 79.47 	 4737 	 5753 	 6168
wo 	p: 93.61 	r: 86.48 	f1: 89.9 	 2930 	 3130 	 3388
ni 	p: 84.98 	r: 78.06 	f1: 81.37 	 1188 	 1398 	 1522
best_thres [[0.33, 0.51, 0.2], [0.58, 0.49, 0.12], [0.4, 0.7, 0.12]]
f [0.8285, 0.8288, 0.8289]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 8 	 [0.43, 0.54, 0.19] 	 lr: 0.0001 	 f: 83.14960139511709
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 26 [0m
Train...

[32m iter_0[0m
ga 	p: 82.31 	r: 76.48 	f1: 79.28 	 4717 	 5731 	 6168
wo 	p: 93.49 	r: 86.95 	f1: 90.11 	 2946 	 3151 	 3388
ni 	p: 86.99 	r: 76.41 	f1: 81.36 	 1163 	 1337 	 1522

[32m iter_1[0m
ga 	p: 83.14 	r: 75.63 	f1: 79.21 	 4665 	 5611 	 6168
wo 	p: 94.03 	r: 86.48 	f1: 90.1 	 2930 	 3116 	 3388
ni 	p: 89.1 	r: 75.16 	f1: 81.54 	 1144 	 1284 	 1522

[32m iter_2[0m
ga 	p: 79.82 	r: 78.7 	f1: 79.26 	 4854 	 6081 	 6168
wo 	p: 93.57 	r: 86.81 	f1: 90.06 	 2941 	 3143 	 3388
ni 	p: 89.11 	r: 75.3 	f1: 81.62 	 1146 	 1286 	 1522
best_thres [[0.4, 0.49, 0.19], [0.46, 0.66, 0.18], [0.26, 0.59, 0.17]]
f [0.8288, 0.8288, 0.8286]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 8 	 [0.45, 0.56, 0.13] 	 lr: 0.0001 	 f: 82.97808936777554
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 26 [0m
Train...

[32m iter_0[0m
ga 	p: 83.04 	r: 75.66 	f1: 79.18 	 4667 	 5620 	 6168
wo 	p: 93.84 	r: 86.28 	f1: 89.9 	 2923 	 3115 	 3388
ni 	p: 89.5 	r: 73.92 	f1: 80.96 	 1125 	 1257 	 1522

[32m iter_1[0m
ga 	p: 81.24 	r: 77.09 	f1: 79.11 	 4755 	 5853 	 6168
wo 	p: 94.24 	r: 86.01 	f1: 89.94 	 2914 	 3092 	 3388
ni 	p: 87.32 	r: 76.02 	f1: 81.28 	 1157 	 1325 	 1522

[32m iter_2[0m
ga 	p: 81.28 	r: 77.06 	f1: 79.11 	 4753 	 5848 	 6168
wo 	p: 93.71 	r: 86.25 	f1: 89.82 	 2922 	 3118 	 3388
ni 	p: 88.06 	r: 75.56 	f1: 81.33 	 1150 	 1306 	 1522
best_thres [[0.47, 0.5, 0.25], [0.36, 0.69, 0.15], [0.36, 0.63, 0.16]]
f [0.8272, 0.8271, 0.8269]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 8 	 [0.45, 0.56, 0.13] 	 lr: 0.0001 	 f: 82.97808936777554
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 27 [0m
Train...
loss: tensor(520.4966) lr: 1e-05 time: 4329.23
pred_count_train 41644

Test...
loss: tensor(525.6151) lr: 1e-05 time: 4440.4
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.48 	r: 76.23 	f1: 79.23 	 4702 	 5701 	 6168
wo 	p: 93.11 	r: 87.01 	f1: 89.96 	 2948 	 3166 	 3388
ni 	p: 86.95 	r: 75.3 	f1: 80.7 	 1146 	 1318 	 1522

[32m iter_1[0m
ga 	p: 85.05 	r: 74.17 	f1: 79.24 	 4575 	 5379 	 6168
wo 	p: 93.71 	r: 86.25 	f1: 89.82 	 2922 	 3118 	 3388
ni 	p: 89.67 	r: 73.59 	f1: 80.84 	 1120 	 1249 	 1522

[32m iter_2[0m
ga 	p: 84.59 	r: 74.74 	f1: 79.36 	 4610 	 5450 	 6168
wo 	p: 93.42 	r: 86.36 	f1: 89.75 	 2926 	 3132 	 3388
ni 	p: 89.19 	r: 73.72 	f1: 80.72 	 1122 	 1258 	 1522
best_thres [[0.41, 0.48, 0.21], [0.69, 0.64, 0.21], [0.69, 0.63, 0.19]]
f [0.8274, 0.8275, 0.8276]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 8 	 [0.43, 0.54, 0.19] 	 lr: 0.0001 	 f: 83.14960139511709
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 27 [0m
Train...
loss: tensor(520.4966) lr: 1e-05 time: 4665.0
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.06 	r: 77.14 	f1: 79.05 	 4758 	 5870 	 6168
wo 	p: 94.7 	r: 86.01 	f1: 90.15 	 2914 	 3077 	 3388
ni 	p: 83.36 	r: 80.62 	f1: 81.96 	 1227 	 1472 	 1522

[32m iter_1[0m
ga 	p: 82.42 	r: 76.15 	f1: 79.16 	 4697 	 5699 	 6168
wo 	p: 93.18 	r: 87.04 	f1: 90.0 	 2949 	 3165 	 3388
ni 	p: 85.36 	r: 79.3 	f1: 82.22 	 1207 	 1414 	 1522

[32m iter_2[0m
ga 	p: 81.94 	r: 76.41 	f1: 79.08 	 4713 	 5752 	 6168
wo 	p: 93.96 	r: 86.33 	f1: 89.99 	 2925 	 3113 	 3388
ni 	p: 86.07 	r: 78.78 	f1: 82.26 	 1199 	 1393 	 1522
best_thres [[0.39, 0.65, 0.12], [0.46, 0.48, 0.13], [0.44, 0.57, 0.14]]
f [0.8279, 0.8285, 0.8285]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 25 	 [0.46, 0.45, 0.11] 	 lr: 1e-05 	 f: 83.1601738344834
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 27 [0m
Train...
loss: tensor(516.2535) lr: 1e-05 time: 4889.54
pred_count_train 41644

Test...
loss: tensor(443.8319) lr: 1e-05 time: 4847.02
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.48 	r: 76.23 	f1: 79.23 	 4702 	 5701 	 6168
wo 	p: 93.11 	r: 87.01 	f1: 89.96 	 2948 	 3166 	 3388
ni 	p: 86.95 	r: 75.3 	f1: 80.7 	 1146 	 1318 	 1522

[32m iter_1[0m
ga 	p: 85.05 	r: 74.17 	f1: 79.24 	 4575 	 5379 	 6168
wo 	p: 93.71 	r: 86.25 	f1: 89.82 	 2922 	 3118 	 3388
ni 	p: 89.67 	r: 73.59 	f1: 80.84 	 1120 	 1249 	 1522

[32m iter_2[0m
ga 	p: 84.59 	r: 74.74 	f1: 79.36 	 4610 	 5450 	 6168
wo 	p: 93.42 	r: 86.36 	f1: 89.75 	 2926 	 3132 	 3388
ni 	p: 89.19 	r: 73.72 	f1: 80.72 	 1122 	 1258 	 1522
best_thres [[0.41, 0.48, 0.21], [0.69, 0.64, 0.21], [0.69, 0.63, 0.19]]
f [0.8274, 0.8275, 0.8276]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 8 	 [0.43, 0.54, 0.19] 	 lr: 0.0001 	 f: 83.14960139511709
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 27 [0m
Train...

[32m iter_0[0m
ga 	p: 83.04 	r: 75.66 	f1: 79.18 	 4667 	 5620 	 6168
wo 	p: 93.84 	r: 86.28 	f1: 89.9 	 2923 	 3115 	 3388
ni 	p: 89.5 	r: 73.92 	f1: 80.96 	 1125 	 1257 	 1522

[32m iter_1[0m
ga 	p: 81.24 	r: 77.09 	f1: 79.11 	 4755 	 5853 	 6168
wo 	p: 94.24 	r: 86.01 	f1: 89.94 	 2914 	 3092 	 3388
ni 	p: 87.32 	r: 76.02 	f1: 81.28 	 1157 	 1325 	 1522

[32m iter_2[0m
ga 	p: 81.28 	r: 77.06 	f1: 79.11 	 4753 	 5848 	 6168
wo 	p: 93.71 	r: 86.25 	f1: 89.82 	 2922 	 3118 	 3388
ni 	p: 88.06 	r: 75.56 	f1: 81.33 	 1150 	 1306 	 1522
best_thres [[0.47, 0.5, 0.25], [0.36, 0.69, 0.15], [0.36, 0.63, 0.16]]
f [0.8272, 0.8271, 0.8269]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 8 	 [0.45, 0.56, 0.13] 	 lr: 0.0001 	 f: 82.97808936777554
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 27 [0m
Train...

[32m iter_0[0m
ga 	p: 81.6 	r: 76.52 	f1: 78.98 	 4720 	 5784 	 6168
wo 	p: 92.86 	r: 86.78 	f1: 89.72 	 2940 	 3166 	 3388
ni 	p: 88.12 	r: 73.59 	f1: 80.2 	 1120 	 1271 	 1522

[32m iter_1[0m
ga 	p: 80.29 	r: 77.68 	f1: 78.96 	 4791 	 5967 	 6168
wo 	p: 92.29 	r: 87.25 	f1: 89.7 	 2956 	 3203 	 3388
ni 	p: 85.11 	r: 76.22 	f1: 80.42 	 1160 	 1363 	 1522

[32m iter_2[0m
ga 	p: 80.12 	r: 77.8 	f1: 78.94 	 4799 	 5990 	 6168
wo 	p: 92.4 	r: 87.19 	f1: 89.72 	 2954 	 3197 	 3388
ni 	p: 87.22 	r: 74.9 	f1: 80.59 	 1140 	 1307 	 1522
best_thres [[0.36, 0.42, 0.25], [0.27, 0.33, 0.13], [0.26, 0.34, 0.15]]
f [0.8245, 0.8244, 0.8244]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 8 	 [0.45, 0.56, 0.13] 	 lr: 0.0001 	 f: 82.97808936777554
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 28 [0m
Train...
loss: tensor(451.1586) lr: 1e-05 time: 4563.37
pred_count_train 41644

Test...
loss: tensor(456.5672) lr: 1e-05 time: 4554.07
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.34 	r: 76.13 	f1: 79.12 	 4696 	 5703 	 6168
wo 	p: 93.85 	r: 86.04 	f1: 89.78 	 2915 	 3106 	 3388
ni 	p: 86.53 	r: 74.31 	f1: 79.96 	 1131 	 1307 	 1522

[32m iter_1[0m
ga 	p: 82.75 	r: 75.75 	f1: 79.09 	 4672 	 5646 	 6168
wo 	p: 94.1 	r: 85.66 	f1: 89.68 	 2902 	 3084 	 3388
ni 	p: 84.13 	r: 76.28 	f1: 80.01 	 1161 	 1380 	 1522

[32m iter_2[0m
ga 	p: 81.23 	r: 77.19 	f1: 79.16 	 4761 	 5861 	 6168
wo 	p: 93.9 	r: 85.8 	f1: 89.67 	 2907 	 3096 	 3388
ni 	p: 83.58 	r: 76.94 	f1: 80.12 	 1171 	 1401 	 1522
best_thres [[0.4, 0.57, 0.24], [0.43, 0.82, 0.14], [0.31, 0.82, 0.12]]
f [0.825, 0.8247, 0.8247]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 8 	 [0.43, 0.54, 0.19] 	 lr: 0.0001 	 f: 83.14960139511709
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 28 [0m
Train...
loss: tensor(451.1586) lr: 1e-05 time: 4649.04
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.1 	r: 77.01 	f1: 79.0 	 4750 	 5857 	 6168
wo 	p: 94.05 	r: 86.25 	f1: 89.98 	 2922 	 3107 	 3388
ni 	p: 85.59 	r: 77.66 	f1: 81.43 	 1182 	 1381 	 1522

[32m iter_1[0m
ga 	p: 81.94 	r: 76.18 	f1: 78.95 	 4699 	 5735 	 6168
wo 	p: 93.49 	r: 86.48 	f1: 89.85 	 2930 	 3134 	 3388
ni 	p: 85.85 	r: 78.12 	f1: 81.8 	 1189 	 1385 	 1522

[32m iter_2[0m
ga 	p: 81.09 	r: 76.88 	f1: 78.93 	 4742 	 5848 	 6168
wo 	p: 93.58 	r: 86.45 	f1: 89.87 	 2929 	 3130 	 3388
ni 	p: 87.39 	r: 76.48 	f1: 81.57 	 1164 	 1332 	 1522
best_thres [[0.39, 0.6, 0.18], [0.43, 0.54, 0.16], [0.38, 0.55, 0.19]]
f [0.8266, 0.8267, 0.8265]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 25 	 [0.46, 0.45, 0.11] 	 lr: 1e-05 	 f: 83.1601738344834
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 28 [0m
Train...
loss: tensor(443.8319) lr: 1e-05 time: 4699.29
pred_count_train 41644

Test...
loss: tensor(380.8162) lr: 1e-05 time: 4634.43
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.34 	r: 76.13 	f1: 79.12 	 4696 	 5703 	 6168
wo 	p: 93.85 	r: 86.04 	f1: 89.78 	 2915 	 3106 	 3388
ni 	p: 86.53 	r: 74.31 	f1: 79.96 	 1131 	 1307 	 1522

[32m iter_1[0m
ga 	p: 82.75 	r: 75.75 	f1: 79.09 	 4672 	 5646 	 6168
wo 	p: 94.1 	r: 85.66 	f1: 89.68 	 2902 	 3084 	 3388
ni 	p: 84.13 	r: 76.28 	f1: 80.01 	 1161 	 1380 	 1522

[32m iter_2[0m
ga 	p: 81.23 	r: 77.19 	f1: 79.16 	 4761 	 5861 	 6168
wo 	p: 93.9 	r: 85.8 	f1: 89.67 	 2907 	 3096 	 3388
ni 	p: 83.58 	r: 76.94 	f1: 80.12 	 1171 	 1401 	 1522
best_thres [[0.4, 0.57, 0.24], [0.43, 0.82, 0.14], [0.31, 0.82, 0.12]]
f [0.825, 0.8247, 0.8247]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 8 	 [0.43, 0.54, 0.19] 	 lr: 0.0001 	 f: 83.14960139511709
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 28 [0m
Train...

[32m iter_0[0m
ga 	p: 81.6 	r: 76.52 	f1: 78.98 	 4720 	 5784 	 6168
wo 	p: 92.86 	r: 86.78 	f1: 89.72 	 2940 	 3166 	 3388
ni 	p: 88.12 	r: 73.59 	f1: 80.2 	 1120 	 1271 	 1522

[32m iter_1[0m
ga 	p: 80.29 	r: 77.68 	f1: 78.96 	 4791 	 5967 	 6168
wo 	p: 92.29 	r: 87.25 	f1: 89.7 	 2956 	 3203 	 3388
ni 	p: 85.11 	r: 76.22 	f1: 80.42 	 1160 	 1363 	 1522

[32m iter_2[0m
ga 	p: 80.12 	r: 77.8 	f1: 78.94 	 4799 	 5990 	 6168
wo 	p: 92.4 	r: 87.19 	f1: 89.72 	 2954 	 3197 	 3388
ni 	p: 87.22 	r: 74.9 	f1: 80.59 	 1140 	 1307 	 1522
best_thres [[0.36, 0.42, 0.25], [0.27, 0.33, 0.13], [0.26, 0.34, 0.15]]
f [0.8245, 0.8244, 0.8244]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	current best epoch 8 	 [0.45, 0.56, 0.13] 	 lr: 0.0001 	 f: 82.97808936777554
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 28 [0m
Train...

[32m iter_0[0m
ga 	p: 81.35 	r: 76.18 	f1: 78.68 	 4699 	 5776 	 6168
wo 	p: 93.41 	r: 85.71 	f1: 89.4 	 2904 	 3109 	 3388
ni 	p: 84.88 	r: 74.9 	f1: 79.58 	 1140 	 1343 	 1522

[32m iter_1[0m
ga 	p: 80.91 	r: 76.61 	f1: 78.7 	 4725 	 5840 	 6168
wo 	p: 92.42 	r: 86.69 	f1: 89.46 	 2937 	 3178 	 3388
ni 	p: 84.86 	r: 74.77 	f1: 79.5 	 1138 	 1341 	 1522

[32m iter_2[0m
ga 	p: 80.99 	r: 76.65 	f1: 78.76 	 4728 	 5838 	 6168
wo 	p: 93.03 	r: 86.22 	f1: 89.49 	 2921 	 3140 	 3388
ni 	p: 85.68 	r: 74.31 	f1: 79.59 	 1131 	 1320 	 1522
best_thres [[0.35, 0.47, 0.15], [0.3, 0.34, 0.1], [0.3, 0.41, 0.1]]
f [0.8207, 0.8209, 0.8211]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	best in epoch 8 	 [0.45, 0.56, 0.13] 	 lr: 0.0001 	 f: 82.97808936777554
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(387.4171) lr: 1e-05 time: 4421.65
pred_count_train 41644

Test...
loss: tensor(397.5383) lr: 1e-05 time: 4460.91
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.37 	r: 76.56 	f1: 78.89 	 4722 	 5803 	 6168
wo 	p: 93.01 	r: 86.39 	f1: 89.58 	 2927 	 3147 	 3388
ni 	p: 84.7 	r: 76.02 	f1: 80.12 	 1157 	 1366 	 1522

[32m iter_1[0m
ga 	p: 82.68 	r: 75.13 	f1: 78.72 	 4634 	 5605 	 6168
wo 	p: 93.85 	r: 85.51 	f1: 89.48 	 2897 	 3087 	 3388
ni 	p: 86.79 	r: 74.24 	f1: 80.03 	 1130 	 1302 	 1522

[32m iter_2[0m
ga 	p: 82.5 	r: 75.34 	f1: 78.76 	 4647 	 5633 	 6168
wo 	p: 93.82 	r: 85.57 	f1: 89.5 	 2899 	 3090 	 3388
ni 	p: 86.55 	r: 74.84 	f1: 80.27 	 1139 	 1316 	 1522
best_thres [[0.35, 0.46, 0.17], [0.44, 0.73, 0.16], [0.42, 0.76, 0.14]]
f [0.8232, 0.8226, 0.8226]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	best in epoch 8 	 [0.43, 0.54, 0.19] 	 lr: 0.0001 	 f: 83.14960139511709
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(387.4171) lr: 1e-05 time: 4667.31
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.72 	r: 75.88 	f1: 78.69 	 4680 	 5727 	 6168
wo 	p: 93.87 	r: 85.95 	f1: 89.74 	 2912 	 3102 	 3388
ni 	p: 86.11 	r: 76.15 	f1: 80.82 	 1159 	 1346 	 1522

[32m iter_1[0m
ga 	p: 81.02 	r: 76.61 	f1: 78.75 	 4725 	 5832 	 6168
wo 	p: 94.99 	r: 84.98 	f1: 89.7 	 2879 	 3031 	 3388
ni 	p: 83.29 	r: 78.91 	f1: 81.04 	 1201 	 1442 	 1522

[32m iter_2[0m
ga 	p: 80.75 	r: 76.83 	f1: 78.74 	 4739 	 5869 	 6168
wo 	p: 94.78 	r: 85.18 	f1: 89.72 	 2886 	 3045 	 3388
ni 	p: 85.36 	r: 77.0 	f1: 80.97 	 1172 	 1373 	 1522
best_thres [[0.42, 0.6, 0.16], [0.36, 0.72, 0.09], [0.34, 0.68, 0.12]]
f [0.8235, 0.8235, 0.8235]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 25 	 [0.46, 0.45, 0.11] 	 lr: 1e-05 	 f: 83.1601738344834
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 29 [0m
Train...
loss: tensor(380.8162) lr: 1e-05 time: 4771.97
pred_count_train 41644

Test...
loss: tensor(3233.4871) lr: 0.0002 time: 4789.09
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.37 	r: 76.56 	f1: 78.89 	 4722 	 5803 	 6168
wo 	p: 93.01 	r: 86.39 	f1: 89.58 	 2927 	 3147 	 3388
ni 	p: 84.7 	r: 76.02 	f1: 80.12 	 1157 	 1366 	 1522

[32m iter_1[0m
ga 	p: 82.68 	r: 75.13 	f1: 78.72 	 4634 	 5605 	 6168
wo 	p: 93.85 	r: 85.51 	f1: 89.48 	 2897 	 3087 	 3388
ni 	p: 86.79 	r: 74.24 	f1: 80.03 	 1130 	 1302 	 1522

[32m iter_2[0m
ga 	p: 82.5 	r: 75.34 	f1: 78.76 	 4647 	 5633 	 6168
wo 	p: 93.82 	r: 85.57 	f1: 89.5 	 2899 	 3090 	 3388
ni 	p: 86.55 	r: 74.84 	f1: 80.27 	 1139 	 1316 	 1522
best_thres [[0.35, 0.46, 0.17], [0.44, 0.73, 0.16], [0.42, 0.76, 0.14]]
f [0.8232, 0.8226, 0.8226]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	best in epoch 8 	 [0.43, 0.54, 0.19] 	 lr: 0.0001 	 f: 83.14960139511709
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 1 [0m
Train...

[32m iter_0[0m
ga 	p: 81.35 	r: 76.18 	f1: 78.68 	 4699 	 5776 	 6168
wo 	p: 93.41 	r: 85.71 	f1: 89.4 	 2904 	 3109 	 3388
ni 	p: 84.88 	r: 74.9 	f1: 79.58 	 1140 	 1343 	 1522

[32m iter_1[0m
ga 	p: 80.91 	r: 76.61 	f1: 78.7 	 4725 	 5840 	 6168
wo 	p: 92.42 	r: 86.69 	f1: 89.46 	 2937 	 3178 	 3388
ni 	p: 84.86 	r: 74.77 	f1: 79.5 	 1138 	 1341 	 1522

loss: tensor(3234.0623) lr: 0.0002 time: 4537.36
pred_count_train 41644

Test...
[32m iter_2[0m
ga 	p: 80.99 	r: 76.65 	f1: 78.76 	 4728 	 5838 	 6168
wo 	p: 93.03 	r: 86.22 	f1: 89.49 	 2921 	 3140 	 3388
ni 	p: 85.68 	r: 74.31 	f1: 79.59 	 1131 	 1320 	 1522
best_thres [[0.35, 0.47, 0.15], [0.3, 0.34, 0.1], [0.3, 0.41, 0.1]]
f [0.8207, 0.8209, 0.8211]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse 	best in epoch 8 	 [0.45, 0.56, 0.13] 	 lr: 0.0001 	 f: 82.97808936777554
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 1 [0m
Train...

[32m iter_0[0m
ga 	p: 78.29 	r: 73.61 	f1: 75.88 	 4540 	 5799 	 6168
wo 	p: 92.37 	r: 84.3 	f1: 88.15 	 2856 	 3092 	 3388
ni 	p: 82.06 	r: 80.55 	f1: 81.3 	 1226 	 1494 	 1522

[32m iter_1[0m
ga 	p: 78.45 	r: 73.56 	f1: 75.93 	 4537 	 5783 	 6168
wo 	p: 92.21 	r: 84.24 	f1: 88.05 	 2854 	 3095 	 3388
ni 	p: 81.79 	r: 80.55 	f1: 81.17 	 1226 	 1499 	 1522

[32m iter_2[0m
ga 	p: 78.28 	r: 73.8 	f1: 75.97 	 4552 	 5815 	 6168
wo 	p: 91.99 	r: 84.36 	f1: 88.01 	 2858 	 3107 	 3388
ni 	p: 81.75 	r: 80.62 	f1: 81.18 	 1227 	 1501 	 1522
best_thres [[0.39, 0.29, 0.12], [0.39, 0.26, 0.11], [0.39, 0.25, 0.11]]
f [0.8034, 0.8033, 0.8034]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 1 	 [0.39, 0.25, 0.11] 	 lr: 0.0002 	 f: 80.33654667101322
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 2 [0m
Train...
loss: tensor(343.9477) lr: 1e-05 time: 4508.72
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 77.4 	r: 74.08 	f1: 75.7 	 4569 	 5903 	 6168
wo 	p: 92.29 	r: 84.45 	f1: 88.19 	 2861 	 3100 	 3388
ni 	p: 81.79 	r: 80.29 	f1: 81.03 	 1222 	 1494 	 1522

[32m iter_1[0m
ga 	p: 79.34 	r: 72.15 	f1: 75.57 	 4450 	 5609 	 6168
wo 	p: 92.34 	r: 83.97 	f1: 87.96 	 2845 	 3081 	 3388
ni 	p: 80.72 	r: 81.14 	f1: 80.93 	 1235 	 1530 	 1522

[32m iter_2[0m
ga 	p: 77.61 	r: 73.77 	f1: 75.64 	 4550 	 5863 	 6168
wo 	p: 92.41 	r: 84.06 	f1: 88.04 	 2848 	 3082 	 3388
ni 	p: 80.97 	r: 81.08 	f1: 81.02 	 1234 	 1524 	 1522
best_thres [[0.36, 0.21, 0.1], [0.42, 0.2, 0.08], [0.37, 0.2, 0.08]]
f [0.802, 0.8015, 0.8014]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 1 	 [0.37, 0.2, 0.08] 	 lr: 0.0002 	 f: 80.14281279105869
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 2 [0m
Train...
loss: tensor(3234.0623) lr: 0.0002 time: 4568.44
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.85 	r: 76.3 	f1: 78.51 	 4706 	 5821 	 6168
wo 	p: 93.38 	r: 86.54 	f1: 89.83 	 2932 	 3140 	 3388
ni 	p: 84.54 	r: 76.54 	f1: 80.34 	 1165 	 1378 	 1522

[32m iter_1[0m
ga 	p: 80.8 	r: 76.54 	f1: 78.61 	 4721 	 5843 	 6168
wo 	p: 93.8 	r: 86.16 	f1: 89.82 	 2919 	 3112 	 3388
ni 	p: 84.1 	r: 77.14 	f1: 80.47 	 1174 	 1396 	 1522

[32m iter_2[0m
ga 	p: 81.21 	r: 76.12 	f1: 78.58 	 4695 	 5781 	 6168
wo 	p: 93.4 	r: 86.48 	f1: 89.81 	 2930 	 3137 	 3388
ni 	p: 83.31 	r: 77.73 	f1: 80.42 	 1183 	 1420 	 1522
best_thres [[0.35, 0.6, 0.16], [0.33, 0.62, 0.13], [0.36, 0.59, 0.11]]
f [0.8221, 0.8223, 0.8224]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	best in epoch 25 	 [0.46, 0.45, 0.11] 	 lr: 1e-05 	 f: 83.1601738344834
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(3233.4871) lr: 0.0002 time: 4695.27
pred_count_train 41644

Test...
loss: tensor(2042.9691) lr: 0.0002 time: 4674.41
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 77.4 	r: 74.08 	f1: 75.7 	 4569 	 5903 	 6168
wo 	p: 92.29 	r: 84.45 	f1: 88.19 	 2861 	 3100 	 3388
ni 	p: 81.79 	r: 80.29 	f1: 81.03 	 1222 	 1494 	 1522

[32m iter_1[0m
ga 	p: 79.34 	r: 72.15 	f1: 75.57 	 4450 	 5609 	 6168
wo 	p: 92.34 	r: 83.97 	f1: 87.96 	 2845 	 3081 	 3388
ni 	p: 80.72 	r: 81.14 	f1: 80.93 	 1235 	 1530 	 1522

[32m iter_2[0m
ga 	p: 77.61 	r: 73.77 	f1: 75.64 	 4550 	 5863 	 6168
wo 	p: 92.41 	r: 84.06 	f1: 88.04 	 2848 	 3082 	 3388
ni 	p: 80.97 	r: 81.08 	f1: 81.02 	 1234 	 1524 	 1522
best_thres [[0.36, 0.21, 0.1], [0.42, 0.2, 0.08], [0.37, 0.2, 0.08]]
f [0.802, 0.8015, 0.8014]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 1 	 [0.37, 0.2, 0.08] 	 lr: 0.0002 	 f: 80.14281279105869
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 2 [0m
Train...
loss: tensor(2044.2390) lr: 0.0002 time: 4481.94
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 78.29 	r: 73.61 	f1: 75.88 	 4540 	 5799 	 6168
wo 	p: 92.37 	r: 84.3 	f1: 88.15 	 2856 	 3092 	 3388
ni 	p: 82.06 	r: 80.55 	f1: 81.3 	 1226 	 1494 	 1522

[32m iter_1[0m
ga 	p: 78.45 	r: 73.56 	f1: 75.93 	 4537 	 5783 	 6168
wo 	p: 92.21 	r: 84.24 	f1: 88.05 	 2854 	 3095 	 3388
ni 	p: 81.79 	r: 80.55 	f1: 81.17 	 1226 	 1499 	 1522

[32m iter_2[0m
ga 	p: 78.28 	r: 73.8 	f1: 75.97 	 4552 	 5815 	 6168
wo 	p: 91.99 	r: 84.36 	f1: 88.01 	 2858 	 3107 	 3388
ni 	p: 81.75 	r: 80.62 	f1: 81.18 	 1227 	 1501 	 1522
best_thres [[0.39, 0.29, 0.12], [0.39, 0.26, 0.11], [0.39, 0.25, 0.11]]
f [0.8034, 0.8033, 0.8034]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 1 	 [0.39, 0.25, 0.11] 	 lr: 0.0002 	 f: 80.33654667101322
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 2 [0m
Train...

[32m iter_0[0m
ga 	p: 78.39 	r: 74.58 	f1: 76.44 	 4600 	 5868 	 6168
wo 	p: 93.13 	r: 84.45 	f1: 88.58 	 2861 	 3072 	 3388
ni 	p: 85.71 	r: 80.42 	f1: 82.98 	 1224 	 1428 	 1522

[32m iter_1[0m
ga 	p: 78.87 	r: 74.09 	f1: 76.41 	 4570 	 5794 	 6168
wo 	p: 91.99 	r: 85.39 	f1: 88.57 	 2893 	 3145 	 3388
ni 	p: 84.3 	r: 82.19 	f1: 83.23 	 1251 	 1484 	 1522

[32m iter_2[0m
ga 	p: 79.0 	r: 74.06 	f1: 76.45 	 4568 	 5782 	 6168
wo 	p: 91.89 	r: 85.33 	f1: 88.49 	 2891 	 3146 	 3388
ni 	p: 85.09 	r: 81.34 	f1: 83.17 	 1238 	 1455 	 1522
best_thres [[0.3, 0.46, 0.31], [0.33, 0.37, 0.24], [0.33, 0.37, 0.27]]
f [0.8099, 0.8103, 0.8103]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 2 	 [0.33, 0.37, 0.27] 	 lr: 0.0002 	 f: 81.0334119985095
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 3 [0m
Train...
loss: tensor(3231.3213) lr: 0.0002 time: 4431.56
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.37 	r: 73.87 	f1: 76.52 	 4556 	 5740 	 6168
wo 	p: 91.44 	r: 85.48 	f1: 88.36 	 2896 	 3167 	 3388
ni 	p: 86.9 	r: 78.91 	f1: 82.71 	 1201 	 1382 	 1522

[32m iter_1[0m
ga 	p: 78.82 	r: 74.32 	f1: 76.5 	 4584 	 5816 	 6168
wo 	p: 91.75 	r: 85.01 	f1: 88.25 	 2880 	 3139 	 3388
ni 	p: 84.59 	r: 80.81 	f1: 82.66 	 1230 	 1454 	 1522

[32m iter_2[0m
ga 	p: 79.45 	r: 74.03 	f1: 76.64 	 4566 	 5747 	 6168
wo 	p: 91.58 	r: 85.04 	f1: 88.18 	 2881 	 3146 	 3388
ni 	p: 85.1 	r: 80.29 	f1: 82.62 	 1222 	 1436 	 1522
best_thres [[0.37, 0.36, 0.45], [0.35, 0.37, 0.34], [0.37, 0.36, 0.36]]
f [0.8099, 0.8096, 0.8097]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 2 	 [0.37, 0.36, 0.36] 	 lr: 0.0002 	 f: 80.9697950545432
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 3 [0m
Train...
loss: tensor(2044.2390) lr: 0.0002 time: 4485.94
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 77.6 	r: 74.82 	f1: 76.19 	 4615 	 5947 	 6168
wo 	p: 93.64 	r: 82.94 	f1: 87.96 	 2810 	 3001 	 3388
ni 	p: 81.55 	r: 80.42 	f1: 80.98 	 1224 	 1501 	 1522

[32m iter_1[0m
ga 	p: 77.88 	r: 74.27 	f1: 76.03 	 4581 	 5882 	 6168
wo 	p: 92.85 	r: 83.62 	f1: 88.0 	 2833 	 3051 	 3388
ni 	p: 81.68 	r: 80.55 	f1: 81.11 	 1226 	 1501 	 1522

[32m iter_2[0m
ga 	p: 77.63 	r: 74.48 	f1: 76.02 	 4594 	 5918 	 6168
wo 	p: 93.38 	r: 83.23 	f1: 88.01 	 2820 	 3020 	 3388
ni 	p: 81.52 	r: 80.55 	f1: 81.03 	 1226 	 1504 	 1522
best_thres [[0.33, 0.4, 0.11], [0.35, 0.34, 0.11], [0.34, 0.37, 0.11]]
f [0.8035, 0.8034, 0.8033]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 1 	 [0.34, 0.37, 0.11] 	 lr: 0.0002 	 f: 80.3265230254496
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 2 [0m
Train...
loss: tensor(2042.9691) lr: 0.0002 time: 4683.4
pred_count_train 41644

Test...
loss: tensor(1646.1849) lr: 0.0002 time: 4665.34
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.37 	r: 73.87 	f1: 76.52 	 4556 	 5740 	 6168
wo 	p: 91.44 	r: 85.48 	f1: 88.36 	 2896 	 3167 	 3388
ni 	p: 86.9 	r: 78.91 	f1: 82.71 	 1201 	 1382 	 1522

[32m iter_1[0m
ga 	p: 78.82 	r: 74.32 	f1: 76.5 	 4584 	 5816 	 6168
wo 	p: 91.75 	r: 85.01 	f1: 88.25 	 2880 	 3139 	 3388
ni 	p: 84.59 	r: 80.81 	f1: 82.66 	 1230 	 1454 	 1522

[32m iter_2[0m
ga 	p: 79.45 	r: 74.03 	f1: 76.64 	 4566 	 5747 	 6168
wo 	p: 91.58 	r: 85.04 	f1: 88.18 	 2881 	 3146 	 3388
ni 	p: 85.1 	r: 80.29 	f1: 82.62 	 1222 	 1436 	 1522
best_thres [[0.37, 0.36, 0.45], [0.35, 0.37, 0.34], [0.37, 0.36, 0.36]]
f [0.8099, 0.8096, 0.8097]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 2 	 [0.37, 0.36, 0.36] 	 lr: 0.0002 	 f: 80.9697950545432
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 3 [0m
Train...
loss: tensor(1650.1674) lr: 0.0002 time: 4590.64
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 85.15 	r: 72.41 	f1: 78.26 	 4466 	 5245 	 6168
wo 	p: 93.9 	r: 85.48 	f1: 89.49 	 2896 	 3084 	 3388
ni 	p: 85.47 	r: 81.93 	f1: 83.66 	 1247 	 1459 	 1522

[32m iter_1[0m
ga 	p: 81.91 	r: 75.24 	f1: 78.44 	 4641 	 5666 	 6168
wo 	p: 93.94 	r: 85.57 	f1: 89.56 	 2899 	 3086 	 3388
ni 	p: 85.34 	r: 81.47 	f1: 83.36 	 1240 	 1453 	 1522

[32m iter_2[0m
ga 	p: 81.62 	r: 75.79 	f1: 78.6 	 4675 	 5728 	 6168
wo 	p: 93.79 	r: 85.66 	f1: 89.54 	 2902 	 3094 	 3388
ni 	p: 83.6 	r: 83.05 	f1: 83.32 	 1264 	 1512 	 1522
best_thres [[0.67, 0.51, 0.2], [0.53, 0.48, 0.19], [0.53, 0.5, 0.15]]
f [0.8252, 0.8251, 0.8253]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 3 	 [0.53, 0.5, 0.15] 	 lr: 0.0002 	 f: 82.53488774563019
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 4 [0m
Train...

[32m iter_0[0m
ga 	p: 78.39 	r: 74.58 	f1: 76.44 	 4600 	 5868 	 6168
wo 	p: 93.13 	r: 84.45 	f1: 88.58 	 2861 	 3072 	 3388
ni 	p: 85.71 	r: 80.42 	f1: 82.98 	 1224 	 1428 	 1522

[32m iter_1[0m
ga 	p: 78.87 	r: 74.09 	f1: 76.41 	 4570 	 5794 	 6168
wo 	p: 91.99 	r: 85.39 	f1: 88.57 	 2893 	 3145 	 3388
ni 	p: 84.3 	r: 82.19 	f1: 83.23 	 1251 	 1484 	 1522

[32m iter_2[0m
ga 	p: 79.0 	r: 74.06 	f1: 76.45 	 4568 	 5782 	 6168
wo 	p: 91.89 	r: 85.33 	f1: 88.49 	 2891 	 3146 	 3388
ni 	p: 85.09 	r: 81.34 	f1: 83.17 	 1238 	 1455 	 1522
best_thres [[0.3, 0.46, 0.31], [0.33, 0.37, 0.24], [0.33, 0.37, 0.27]]
f [0.8099, 0.8103, 0.8103]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 2 	 [0.33, 0.37, 0.27] 	 lr: 0.0002 	 f: 81.0334119985095
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 3 [0m
Train...
loss: tensor(2037.2333) lr: 0.0002 time: 4464.4
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.82 	r: 76.83 	f1: 78.3 	 4739 	 5937 	 6168
wo 	p: 93.24 	r: 86.25 	f1: 89.6 	 2922 	 3134 	 3388
ni 	p: 87.62 	r: 80.95 	f1: 84.15 	 1232 	 1406 	 1522

[32m iter_1[0m
ga 	p: 79.31 	r: 76.9 	f1: 78.09 	 4743 	 5980 	 6168
wo 	p: 93.4 	r: 85.98 	f1: 89.53 	 2913 	 3119 	 3388
ni 	p: 86.04 	r: 82.59 	f1: 84.28 	 1257 	 1461 	 1522

[32m iter_2[0m
ga 	p: 81.33 	r: 75.28 	f1: 78.18 	 4643 	 5709 	 6168
wo 	p: 94.55 	r: 85.01 	f1: 89.52 	 2880 	 3046 	 3388
ni 	p: 86.55 	r: 82.46 	f1: 84.45 	 1255 	 1450 	 1522
best_thres [[0.44, 0.47, 0.31], [0.41, 0.47, 0.24], [0.51, 0.59, 0.25]]
f [0.8251, 0.8245, 0.8246]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 3 	 [0.51, 0.59, 0.25] 	 lr: 0.0002 	 f: 82.46169117190892
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 4 [0m
Train...
loss: tensor(1650.1674) lr: 0.0002 time: 4487.55
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.42 	r: 74.77 	f1: 77.03 	 4612 	 5807 	 6168
wo 	p: 92.76 	r: 85.12 	f1: 88.78 	 2884 	 3109 	 3388
ni 	p: 86.5 	r: 79.96 	f1: 83.1 	 1217 	 1407 	 1522

[32m iter_1[0m
ga 	p: 79.53 	r: 74.5 	f1: 76.93 	 4595 	 5778 	 6168
wo 	p: 92.45 	r: 85.27 	f1: 88.71 	 2889 	 3125 	 3388
ni 	p: 86.55 	r: 79.89 	f1: 83.09 	 1216 	 1405 	 1522

[32m iter_2[0m
ga 	p: 79.55 	r: 74.63 	f1: 77.01 	 4603 	 5786 	 6168
wo 	p: 92.42 	r: 85.3 	f1: 88.72 	 2890 	 3127 	 3388
ni 	p: 86.56 	r: 79.96 	f1: 83.13 	 1217 	 1406 	 1522
best_thres [[0.34, 0.4, 0.38], [0.35, 0.37, 0.39], [0.35, 0.37, 0.39]]
f [0.8143, 0.8139, 0.814]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 2 	 [0.35, 0.37, 0.39] 	 lr: 0.0002 	 f: 81.40034899663468
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 3 [0m
Train...
loss: tensor(1303.2003) lr: 0.0002 time: 4694.78
pred_count_train 41644

Test...
loss: tensor(1646.1849) lr: 0.0002 time: 4710.21
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.82 	r: 76.83 	f1: 78.3 	 4739 	 5937 	 6168
wo 	p: 93.24 	r: 86.25 	f1: 89.6 	 2922 	 3134 	 3388
ni 	p: 87.62 	r: 80.95 	f1: 84.15 	 1232 	 1406 	 1522

[32m iter_1[0m
ga 	p: 79.31 	r: 76.9 	f1: 78.09 	 4743 	 5980 	 6168
wo 	p: 93.4 	r: 85.98 	f1: 89.53 	 2913 	 3119 	 3388
ni 	p: 86.04 	r: 82.59 	f1: 84.28 	 1257 	 1461 	 1522

[32m iter_2[0m
ga 	p: 81.33 	r: 75.28 	f1: 78.18 	 4643 	 5709 	 6168
wo 	p: 94.55 	r: 85.01 	f1: 89.52 	 2880 	 3046 	 3388
ni 	p: 86.55 	r: 82.46 	f1: 84.45 	 1255 	 1450 	 1522
best_thres [[0.44, 0.47, 0.31], [0.41, 0.47, 0.24], [0.51, 0.59, 0.25]]
f [0.8251, 0.8245, 0.8246]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 3 	 [0.51, 0.59, 0.25] 	 lr: 0.0002 	 f: 82.46169117190892
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 4 [0m
Train...
loss: tensor(1320.0560) lr: 0.0002 time: 4471.06
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.02 	r: 77.69 	f1: 78.35 	 4792 	 6064 	 6168
wo 	p: 93.64 	r: 86.07 	f1: 89.7 	 2916 	 3114 	 3388
ni 	p: 87.64 	r: 80.16 	f1: 83.73 	 1220 	 1392 	 1522

[32m iter_1[0m
ga 	p: 79.19 	r: 77.76 	f1: 78.47 	 4796 	 6056 	 6168
wo 	p: 92.92 	r: 86.42 	f1: 89.55 	 2928 	 3151 	 3388
ni 	p: 88.88 	r: 79.83 	f1: 84.11 	 1215 	 1367 	 1522

[32m iter_2[0m
ga 	p: 79.17 	r: 77.71 	f1: 78.43 	 4793 	 6054 	 6168
wo 	p: 93.95 	r: 85.74 	f1: 89.66 	 2905 	 3092 	 3388
ni 	p: 88.89 	r: 79.89 	f1: 84.15 	 1216 	 1368 	 1522
best_thres [[0.3, 0.53, 0.35], [0.3, 0.5, 0.36], [0.3, 0.66, 0.36]]
f [0.8248, 0.8253, 0.8254]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 4 	 [0.3, 0.66, 0.36] 	 lr: 0.0002 	 f: 82.54022067435123
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 5 [0m
Train...

[32m iter_0[0m
ga 	p: 85.15 	r: 72.41 	f1: 78.26 	 4466 	 5245 	 6168
wo 	p: 93.9 	r: 85.48 	f1: 89.49 	 2896 	 3084 	 3388
ni 	p: 85.47 	r: 81.93 	f1: 83.66 	 1247 	 1459 	 1522

[32m iter_1[0m
ga 	p: 81.91 	r: 75.24 	f1: 78.44 	 4641 	 5666 	 6168
wo 	p: 93.94 	r: 85.57 	f1: 89.56 	 2899 	 3086 	 3388
ni 	p: 85.34 	r: 81.47 	f1: 83.36 	 1240 	 1453 	 1522

[32m iter_2[0m
ga 	p: 81.62 	r: 75.79 	f1: 78.6 	 4675 	 5728 	 6168
wo 	p: 93.79 	r: 85.66 	f1: 89.54 	 2902 	 3094 	 3388
ni 	p: 83.6 	r: 83.05 	f1: 83.32 	 1264 	 1512 	 1522
best_thres [[0.67, 0.51, 0.2], [0.53, 0.48, 0.19], [0.53, 0.5, 0.15]]
f [0.8252, 0.8251, 0.8253]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 3 	 [0.53, 0.5, 0.15] 	 lr: 0.0002 	 f: 82.53488774563019
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 4 [0m
Train...
loss: tensor(1646.8387) lr: 0.0002 time: 4346.79
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.73 	r: 76.52 	f1: 78.09 	 4720 	 5920 	 6168
wo 	p: 93.73 	r: 85.57 	f1: 89.46 	 2899 	 3093 	 3388
ni 	p: 88.83 	r: 79.96 	f1: 84.16 	 1217 	 1370 	 1522

[32m iter_1[0m
ga 	p: 80.57 	r: 75.5 	f1: 77.95 	 4657 	 5780 	 6168
wo 	p: 92.29 	r: 86.57 	f1: 89.34 	 2933 	 3178 	 3388
ni 	p: 86.45 	r: 82.13 	f1: 84.23 	 1250 	 1446 	 1522

[32m iter_2[0m
ga 	p: 81.42 	r: 75.02 	f1: 78.09 	 4627 	 5683 	 6168
wo 	p: 93.18 	r: 85.95 	f1: 89.42 	 2912 	 3125 	 3388
ni 	p: 88.18 	r: 80.88 	f1: 84.37 	 1231 	 1396 	 1522
best_thres [[0.37, 0.5, 0.42], [0.39, 0.39, 0.29], [0.44, 0.48, 0.35]]
f [0.8234, 0.8232, 0.8235]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 3 	 [0.51, 0.59, 0.25] 	 lr: 0.0002 	 f: 82.46169117190892
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 5 [0m
Train...
loss: tensor(1320.0560) lr: 0.0002 time: 4306.08
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.74 	r: 73.9 	f1: 78.51 	 4558 	 5443 	 6168
wo 	p: 93.54 	r: 85.01 	f1: 89.07 	 2880 	 3079 	 3388
ni 	p: 84.84 	r: 83.11 	f1: 83.97 	 1265 	 1491 	 1522

[32m iter_1[0m
ga 	p: 82.58 	r: 74.61 	f1: 78.39 	 4602 	 5573 	 6168
wo 	p: 93.76 	r: 84.68 	f1: 88.99 	 2869 	 3060 	 3388
ni 	p: 84.71 	r: 82.98 	f1: 83.84 	 1263 	 1491 	 1522

[32m iter_2[0m
ga 	p: 82.65 	r: 74.74 	f1: 78.49 	 4610 	 5578 	 6168
wo 	p: 94.43 	r: 84.12 	f1: 88.98 	 2850 	 3018 	 3388
ni 	p: 84.44 	r: 83.44 	f1: 83.94 	 1270 	 1504 	 1522
best_thres [[0.58, 0.46, 0.15], [0.53, 0.46, 0.15], [0.53, 0.54, 0.14]]
f [0.8253, 0.8246, 0.8245]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 3 	 [0.53, 0.54, 0.14] 	 lr: 0.0002 	 f: 82.45340391674938
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 4 [0m
Train...
loss: tensor(989.3631) lr: 0.0002 time: 4640.96
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.73 	r: 76.52 	f1: 78.09 	 4720 	 5920 	 6168
wo 	p: 93.73 	r: 85.57 	f1: 89.46 	 2899 	 3093 	 3388
ni 	p: 88.83 	r: 79.96 	f1: 84.16 	 1217 	 1370 	 1522

[32m iter_1[0m
ga 	p: 80.57 	r: 75.5 	f1: 77.95 	 4657 	 5780 	 6168
wo 	p: 92.29 	r: 86.57 	f1: 89.34 	 2933 	 3178 	 3388
ni 	p: 86.45 	r: 82.13 	f1: 84.23 	 1250 	 1446 	 1522

[32m iter_2[0m
ga 	p: 81.42 	r: 75.02 	f1: 78.09 	 4627 	 5683 	 6168
wo 	p: 93.18 	r: 85.95 	f1: 89.42 	 2912 	 3125 	 3388
ni 	p: 88.18 	r: 80.88 	f1: 84.37 	 1231 	 1396 	 1522
best_thres [[0.37, 0.5, 0.42], [0.39, 0.39, 0.29], [0.44, 0.48, 0.35]]
f [0.8234, 0.8232, 0.8235]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 3 	 [0.51, 0.59, 0.25] 	 lr: 0.0002 	 f: 82.46169117190892
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 5 [0m
Train...
loss: tensor(1303.2003) lr: 0.0002 time: 4660.09
pred_count_train 41644

Test...
loss: tensor(995.4302) lr: 0.0002 time: 4606.25
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.13 	r: 76.43 	f1: 78.23 	 4714 	 5883 	 6168
wo 	p: 91.42 	r: 86.78 	f1: 89.04 	 2940 	 3216 	 3388
ni 	p: 84.98 	r: 78.45 	f1: 81.59 	 1194 	 1405 	 1522

[32m iter_1[0m
ga 	p: 80.0 	r: 77.16 	f1: 78.55 	 4759 	 5949 	 6168
wo 	p: 93.75 	r: 84.95 	f1: 89.13 	 2878 	 3070 	 3388
ni 	p: 86.62 	r: 77.86 	f1: 82.01 	 1185 	 1368 	 1522

[32m iter_2[0m
ga 	p: 79.87 	r: 77.06 	f1: 78.44 	 4753 	 5951 	 6168
wo 	p: 93.71 	r: 84.92 	f1: 89.1 	 2877 	 3070 	 3388
ni 	p: 86.5 	r: 77.86 	f1: 81.95 	 1185 	 1370 	 1522
best_thres [[0.24, 0.36, 0.29], [0.22, 0.64, 0.31], [0.22, 0.68, 0.31]]
f [0.8199, 0.821, 0.821]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 4 	 [0.3, 0.66, 0.36] 	 lr: 0.0002 	 f: 82.54022067435123
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 6 [0m
Train...

[32m iter_0[0m
ga 	p: 79.02 	r: 77.69 	f1: 78.35 	 4792 	 6064 	 6168
wo 	p: 93.64 	r: 86.07 	f1: 89.7 	 2916 	 3114 	 3388
ni 	p: 87.64 	r: 80.16 	f1: 83.73 	 1220 	 1392 	 1522

[32m iter_1[0m
ga 	p: 79.19 	r: 77.76 	f1: 78.47 	 4796 	 6056 	 6168
wo 	p: 92.92 	r: 86.42 	f1: 89.55 	 2928 	 3151 	 3388
ni 	p: 88.88 	r: 79.83 	f1: 84.11 	 1215 	 1367 	 1522

[32m iter_2[0m
ga 	p: 79.17 	r: 77.71 	f1: 78.43 	 4793 	 6054 	 6168
wo 	p: 93.95 	r: 85.74 	f1: 89.66 	 2905 	 3092 	 3388
ni 	p: 88.89 	r: 79.89 	f1: 84.15 	 1216 	 1368 	 1522
best_thres [[0.3, 0.53, 0.35], [0.3, 0.5, 0.36], [0.3, 0.66, 0.36]]
f [0.8248, 0.8253, 0.8254]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 4 	 [0.3, 0.66, 0.36] 	 lr: 0.0002 	 f: 82.54022067435123
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 5 [0m
Train...
loss: tensor(1303.5570) lr: 0.0002 time: 4501.34
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.67 	r: 76.67 	f1: 78.62 	 4729 	 5862 	 6168
wo 	p: 92.92 	r: 86.36 	f1: 89.52 	 2926 	 3149 	 3388
ni 	p: 84.98 	r: 78.45 	f1: 81.59 	 1194 	 1405 	 1522

[32m iter_1[0m
ga 	p: 81.06 	r: 76.25 	f1: 78.58 	 4703 	 5802 	 6168
wo 	p: 93.13 	r: 85.98 	f1: 89.41 	 2913 	 3128 	 3388
ni 	p: 86.01 	r: 77.53 	f1: 81.55 	 1180 	 1372 	 1522

[32m iter_2[0m
ga 	p: 80.75 	r: 76.49 	f1: 78.56 	 4718 	 5843 	 6168
wo 	p: 93.21 	r: 85.95 	f1: 89.43 	 2912 	 3124 	 3388
ni 	p: 85.88 	r: 77.53 	f1: 81.49 	 1180 	 1374 	 1522
best_thres [[0.33, 0.5, 0.22], [0.34, 0.55, 0.22], [0.33, 0.59, 0.22]]
f [0.8234, 0.8231, 0.823]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 3 	 [0.51, 0.59, 0.25] 	 lr: 0.0002 	 f: 82.46169117190892
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 6 [0m
Train...
loss: tensor(995.4302) lr: 0.0002 time: 4455.06
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.88 	r: 74.53 	f1: 78.03 	 4597 	 5614 	 6168
wo 	p: 93.05 	r: 86.16 	f1: 89.47 	 2919 	 3137 	 3388
ni 	p: 86.94 	r: 82.26 	f1: 84.54 	 1252 	 1440 	 1522

[32m iter_1[0m
ga 	p: 81.44 	r: 75.18 	f1: 78.18 	 4637 	 5694 	 6168
wo 	p: 93.44 	r: 85.8 	f1: 89.46 	 2907 	 3111 	 3388
ni 	p: 87.38 	r: 82.33 	f1: 84.78 	 1253 	 1434 	 1522

[32m iter_2[0m
ga 	p: 82.69 	r: 74.17 	f1: 78.2 	 4575 	 5533 	 6168
wo 	p: 93.47 	r: 85.77 	f1: 89.46 	 2906 	 3109 	 3388
ni 	p: 87.64 	r: 82.0 	f1: 84.73 	 1248 	 1424 	 1522
best_thres [[0.46, 0.53, 0.31], [0.43, 0.56, 0.31], [0.48, 0.56, 0.32]]
f [0.8245, 0.8249, 0.8252]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 4 	 [0.48, 0.56, 0.32] 	 lr: 0.0002 	 f: 82.51686803703123
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 5 [0m
Train...

[32m iter_0[0m
ga 	p: 80.67 	r: 76.67 	f1: 78.62 	 4729 	 5862 	 6168
wo 	p: 92.92 	r: 86.36 	f1: 89.52 	 2926 	 3149 	 3388
ni 	p: 84.98 	r: 78.45 	f1: 81.59 	 1194 	 1405 	 1522

[32m iter_1[0m
ga 	p: 81.06 	r: 76.25 	f1: 78.58 	 4703 	 5802 	 6168
wo 	p: 93.13 	r: 85.98 	f1: 89.41 	 2913 	 3128 	 3388
ni 	p: 86.01 	r: 77.53 	f1: 81.55 	 1180 	 1372 	 1522

[32m iter_2[0m
ga 	p: 80.75 	r: 76.49 	f1: 78.56 	 4718 	 5843 	 6168
wo 	p: 93.21 	r: 85.95 	f1: 89.43 	 2912 	 3124 	 3388
ni 	p: 85.88 	r: 77.53 	f1: 81.49 	 1180 	 1374 	 1522
best_thres [[0.33, 0.5, 0.22], [0.34, 0.55, 0.22], [0.33, 0.59, 0.22]]
f [0.8234, 0.8231, 0.823]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 3 	 [0.51, 0.59, 0.25] 	 lr: 0.0002 	 f: 82.46169117190892
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 6 [0m
Train...
loss: tensor(746.0405) lr: 0.0002 time: 4605.23
pred_count_train 41644

Test...
loss: tensor(989.3631) lr: 0.0002 time: 4635.42
pred_count_train 41644

Test...
loss: tensor(761.6190) lr: 0.0002 time: 4507.97
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.75 	r: 74.24 	f1: 77.81 	 4579 	 5601 	 6168
wo 	p: 94.71 	r: 84.56 	f1: 89.35 	 2865 	 3025 	 3388
ni 	p: 82.76 	r: 77.6 	f1: 80.09 	 1181 	 1427 	 1522

[32m iter_1[0m
ga 	p: 81.48 	r: 74.74 	f1: 77.96 	 4610 	 5658 	 6168
wo 	p: 93.07 	r: 85.66 	f1: 89.21 	 2902 	 3118 	 3388
ni 	p: 82.67 	r: 78.65 	f1: 80.61 	 1197 	 1448 	 1522

[32m iter_2[0m
ga 	p: 81.46 	r: 74.68 	f1: 77.92 	 4606 	 5654 	 6168
wo 	p: 94.33 	r: 84.98 	f1: 89.41 	 2879 	 3052 	 3388
ni 	p: 84.93 	r: 77.0 	f1: 80.77 	 1172 	 1380 	 1522
best_thres [[0.56, 0.61, 0.21], [0.56, 0.39, 0.17], [0.61, 0.6, 0.21]]
f [0.8163, 0.817, 0.8174]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 4 	 [0.3, 0.66, 0.36] 	 lr: 0.0002 	 f: 82.54022067435123
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 7 [0m
Train...

[32m iter_0[0m
ga 	p: 80.13 	r: 76.43 	f1: 78.23 	 4714 	 5883 	 6168
wo 	p: 91.42 	r: 86.78 	f1: 89.04 	 2940 	 3216 	 3388
ni 	p: 84.98 	r: 78.45 	f1: 81.59 	 1194 	 1405 	 1522

[32m iter_1[0m
ga 	p: 80.0 	r: 77.16 	f1: 78.55 	 4759 	 5949 	 6168
wo 	p: 93.75 	r: 84.95 	f1: 89.13 	 2878 	 3070 	 3388
ni 	p: 86.62 	r: 77.86 	f1: 82.01 	 1185 	 1368 	 1522

[32m iter_2[0m
ga 	p: 79.87 	r: 77.06 	f1: 78.44 	 4753 	 5951 	 6168
wo 	p: 93.71 	r: 84.92 	f1: 89.1 	 2877 	 3070 	 3388
ni 	p: 86.5 	r: 77.86 	f1: 81.95 	 1185 	 1370 	 1522
best_thres [[0.24, 0.36, 0.29], [0.22, 0.64, 0.31], [0.22, 0.68, 0.31]]
f [0.8199, 0.821, 0.821]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 4 	 [0.3, 0.66, 0.36] 	 lr: 0.0002 	 f: 82.54022067435123
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 6 [0m
Train...
loss: tensor(986.4642) lr: 0.0002 time: 4332.13
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.25 	r: 75.18 	f1: 78.1 	 4637 	 5707 	 6168
wo 	p: 92.2 	r: 86.16 	f1: 89.08 	 2919 	 3166 	 3388
ni 	p: 82.22 	r: 74.11 	f1: 77.95 	 1128 	 1372 	 1522

[32m iter_1[0m
ga 	p: 81.38 	r: 75.32 	f1: 78.24 	 4646 	 5709 	 6168
wo 	p: 93.06 	r: 85.89 	f1: 89.33 	 2910 	 3127 	 3388
ni 	p: 83.35 	r: 73.98 	f1: 78.38 	 1126 	 1351 	 1522

[32m iter_2[0m
ga 	p: 82.26 	r: 74.9 	f1: 78.41 	 4620 	 5616 	 6168
wo 	p: 93.22 	r: 85.63 	f1: 89.26 	 2901 	 3112 	 3388
ni 	p: 83.87 	r: 74.11 	f1: 78.69 	 1128 	 1345 	 1522
best_thres [[0.5, 0.5, 0.2], [0.47, 0.71, 0.18], [0.57, 0.79, 0.18]]
f [0.8145, 0.8155, 0.8163]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 3 	 [0.51, 0.59, 0.25] 	 lr: 0.0002 	 f: 82.46169117190892
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 7 [0m
Train...
loss: tensor(761.6190) lr: 0.0002 time: 4236.03
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.9 	r: 76.05 	f1: 78.87 	 4691 	 5728 	 6168
wo 	p: 92.92 	r: 86.39 	f1: 89.54 	 2927 	 3150 	 3388
ni 	p: 85.84 	r: 79.24 	f1: 82.41 	 1206 	 1405 	 1522

[32m iter_1[0m
ga 	p: 81.87 	r: 75.65 	f1: 78.64 	 4666 	 5699 	 6168
wo 	p: 93.19 	r: 86.04 	f1: 89.47 	 2915 	 3128 	 3388
ni 	p: 86.86 	r: 78.58 	f1: 82.51 	 1196 	 1377 	 1522

[32m iter_2[0m
ga 	p: 82.39 	r: 75.39 	f1: 78.73 	 4650 	 5644 	 6168
wo 	p: 93.28 	r: 86.1 	f1: 89.55 	 2917 	 3127 	 3388
ni 	p: 85.88 	r: 79.5 	f1: 82.57 	 1210 	 1409 	 1522
best_thres [[0.37, 0.5, 0.3], [0.37, 0.53, 0.34], [0.39, 0.54, 0.29]]
f [0.8262, 0.8255, 0.8256]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 5 	 [0.39, 0.54, 0.29] 	 lr: 0.0002 	 f: 82.5589583887576
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 6 [0m
Train...

[32m iter_0[0m
ga 	p: 81.25 	r: 75.18 	f1: 78.1 	 4637 	 5707 	 6168
wo 	p: 92.2 	r: 86.16 	f1: 89.08 	 2919 	 3166 	 3388
ni 	p: 82.22 	r: 74.11 	f1: 77.95 	 1128 	 1372 	 1522

[32m iter_1[0m
ga 	p: 81.38 	r: 75.32 	f1: 78.24 	 4646 	 5709 	 6168
wo 	p: 93.06 	r: 85.89 	f1: 89.33 	 2910 	 3127 	 3388
ni 	p: 83.35 	r: 73.98 	f1: 78.38 	 1126 	 1351 	 1522

[32m iter_2[0m
ga 	p: 82.26 	r: 74.9 	f1: 78.41 	 4620 	 5616 	 6168
wo 	p: 93.22 	r: 85.63 	f1: 89.26 	 2901 	 3112 	 3388
ni 	p: 83.87 	r: 74.11 	f1: 78.69 	 1128 	 1345 	 1522
best_thres [[0.5, 0.5, 0.2], [0.47, 0.71, 0.18], [0.57, 0.79, 0.18]]
f [0.8145, 0.8155, 0.8163]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 3 	 [0.51, 0.59, 0.25] 	 lr: 0.0002 	 f: 82.46169117190892
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 7 [0m
Train...
loss: tensor(586.1215) lr: 0.0002 time: 4564.18
pred_count_train 41644

Test...
loss: tensor(746.0405) lr: 0.0002 time: 4628.1
pred_count_train 41644

Test...
loss: tensor(597.7095) lr: 0.0002 time: 4647.19
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 78.77 	r: 75.6 	f1: 77.15 	 4663 	 5920 	 6168
wo 	p: 92.7 	r: 85.8 	f1: 89.12 	 2907 	 3136 	 3388
ni 	p: 80.0 	r: 76.48 	f1: 78.2 	 1164 	 1455 	 1522

[32m iter_1[0m
ga 	p: 79.82 	r: 75.11 	f1: 77.4 	 4633 	 5804 	 6168
wo 	p: 91.76 	r: 86.1 	f1: 88.84 	 2917 	 3179 	 3388
ni 	p: 83.79 	r: 74.7 	f1: 78.99 	 1137 	 1357 	 1522

[32m iter_2[0m
ga 	p: 81.32 	r: 74.19 	f1: 77.59 	 4576 	 5627 	 6168
wo 	p: 91.31 	r: 86.51 	f1: 88.85 	 2931 	 3210 	 3388
ni 	p: 82.28 	r: 75.95 	f1: 78.99 	 1156 	 1405 	 1522
best_thres [[0.42, 0.55, 0.19], [0.47, 0.44, 0.23], [0.65, 0.43, 0.19]]
f [0.8091, 0.8101, 0.811]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 4 	 [0.3, 0.66, 0.36] 	 lr: 0.0002 	 f: 82.54022067435123
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 8 [0m
Train...

[32m iter_0[0m
ga 	p: 81.75 	r: 74.24 	f1: 77.81 	 4579 	 5601 	 6168
wo 	p: 94.71 	r: 84.56 	f1: 89.35 	 2865 	 3025 	 3388
ni 	p: 82.76 	r: 77.6 	f1: 80.09 	 1181 	 1427 	 1522

[32m iter_1[0m
ga 	p: 81.48 	r: 74.74 	f1: 77.96 	 4610 	 5658 	 6168
wo 	p: 93.07 	r: 85.66 	f1: 89.21 	 2902 	 3118 	 3388
ni 	p: 82.67 	r: 78.65 	f1: 80.61 	 1197 	 1448 	 1522

[32m iter_2[0m
ga 	p: 81.46 	r: 74.68 	f1: 77.92 	 4606 	 5654 	 6168
wo 	p: 94.33 	r: 84.98 	f1: 89.41 	 2879 	 3052 	 3388
ni 	p: 84.93 	r: 77.0 	f1: 80.77 	 1172 	 1380 	 1522
best_thres [[0.56, 0.61, 0.21], [0.56, 0.39, 0.17], [0.61, 0.6, 0.21]]
f [0.8163, 0.817, 0.8174]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 4 	 [0.3, 0.66, 0.36] 	 lr: 0.0002 	 f: 82.54022067435123
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 7 [0m
Train...
loss: tensor(750.4536) lr: 0.0002 time: 4580.48
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.45 	r: 75.28 	f1: 77.31 	 4643 	 5844 	 6168
wo 	p: 92.83 	r: 85.18 	f1: 88.84 	 2886 	 3109 	 3388
ni 	p: 85.05 	r: 75.49 	f1: 79.99 	 1149 	 1351 	 1522

[32m iter_1[0m
ga 	p: 80.15 	r: 75.06 	f1: 77.52 	 4630 	 5777 	 6168
wo 	p: 94.04 	r: 83.83 	f1: 88.64 	 2840 	 3020 	 3388
ni 	p: 83.19 	r: 76.74 	f1: 79.84 	 1168 	 1404 	 1522

[32m iter_2[0m
ga 	p: 80.15 	r: 75.19 	f1: 77.59 	 4638 	 5787 	 6168
wo 	p: 93.49 	r: 84.74 	f1: 88.9 	 2871 	 3071 	 3388
ni 	p: 84.38 	r: 75.95 	f1: 79.94 	 1156 	 1370 	 1522
best_thres [[0.55, 0.64, 0.26], [0.58, 0.8, 0.17], [0.64, 0.78, 0.2]]
f [0.8117, 0.8118, 0.8123]
load model: epoch3
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 3 	 [0.51, 0.59, 0.25] 	 lr: 0.0002 	 f: 82.46169117190892
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 8 [0m
Train...
loss: tensor(597.7095) lr: 0.0002 time: 4373.48
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.39 	r: 72.94 	f1: 77.82 	 4499 	 5395 	 6168
wo 	p: 91.62 	r: 87.4 	f1: 89.46 	 2961 	 3232 	 3388
ni 	p: 82.04 	r: 76.22 	f1: 79.02 	 1160 	 1414 	 1522

[32m iter_1[0m
ga 	p: 81.71 	r: 74.4 	f1: 77.89 	 4589 	 5616 	 6168
wo 	p: 92.71 	r: 86.36 	f1: 89.43 	 2926 	 3156 	 3388
ni 	p: 82.26 	r: 76.48 	f1: 79.26 	 1164 	 1415 	 1522

[32m iter_2[0m
ga 	p: 81.73 	r: 74.46 	f1: 77.93 	 4593 	 5620 	 6168
wo 	p: 92.58 	r: 86.48 	f1: 89.42 	 2930 	 3165 	 3388
ni 	p: 82.73 	r: 76.15 	f1: 79.3 	 1159 	 1401 	 1522
best_thres [[0.61, 0.45, 0.14], [0.49, 0.56, 0.14], [0.49, 0.56, 0.15]]
f [0.8163, 0.8163, 0.8164]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 5 	 [0.39, 0.54, 0.29] 	 lr: 0.0002 	 f: 82.5589583887576
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 7 [0m
Train...

[32m iter_0[0m
ga 	p: 79.45 	r: 75.28 	f1: 77.31 	 4643 	 5844 	 6168
wo 	p: 92.83 	r: 85.18 	f1: 88.84 	 2886 	 3109 	 3388
ni 	p: 85.05 	r: 75.49 	f1: 79.99 	 1149 	 1351 	 1522

[32m iter_1[0m
ga 	p: 80.15 	r: 75.06 	f1: 77.52 	 4630 	 5777 	 6168
wo 	p: 94.04 	r: 83.83 	f1: 88.64 	 2840 	 3020 	 3388
ni 	p: 83.19 	r: 76.74 	f1: 79.84 	 1168 	 1404 	 1522

[32m iter_2[0m
ga 	p: 80.15 	r: 75.19 	f1: 77.59 	 4638 	 5787 	 6168
wo 	p: 93.49 	r: 84.74 	f1: 88.9 	 2871 	 3071 	 3388
ni 	p: 84.38 	r: 75.95 	f1: 79.94 	 1156 	 1370 	 1522
best_thres [[0.55, 0.64, 0.26], [0.58, 0.8, 0.17], [0.64, 0.78, 0.2]]
f [0.8117, 0.8118, 0.8123]
load model: epoch3
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 3 	 [0.51, 0.59, 0.25] 	 lr: 0.0002 	 f: 82.46169117190892
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 8 [0m
Train...
loss: tensor(488.6465) lr: 0.0002 time: 4561.12
pred_count_train 41644

Test...
loss: tensor(586.1215) lr: 0.0002 time: 4600.91
pred_count_train 41644

Test...
loss: tensor(1096.0133) lr: 0.0001 time: 4574.32
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.9 	r: 73.44 	f1: 77.44 	 4530 	 5531 	 6168
wo 	p: 93.3 	r: 85.06 	f1: 88.99 	 2882 	 3089 	 3388
ni 	p: 81.07 	r: 76.81 	f1: 78.88 	 1169 	 1442 	 1522

[32m iter_1[0m
ga 	p: 79.68 	r: 75.66 	f1: 77.62 	 4667 	 5857 	 6168
wo 	p: 92.76 	r: 85.42 	f1: 88.94 	 2894 	 3120 	 3388
ni 	p: 82.34 	r: 75.69 	f1: 78.88 	 1152 	 1399 	 1522

[32m iter_2[0m
ga 	p: 79.51 	r: 75.88 	f1: 77.65 	 4680 	 5886 	 6168
wo 	p: 92.6 	r: 86.01 	f1: 89.18 	 2914 	 3147 	 3388
ni 	p: 82.4 	r: 75.69 	f1: 78.9 	 1152 	 1398 	 1522
best_thres [[0.59, 0.8, 0.12], [0.41, 0.79, 0.12], [0.41, 0.79, 0.12]]
f [0.8118, 0.812, 0.8124]
load model: epoch4
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 4 	 [0.3, 0.66, 0.36] 	 lr: 0.0002 	 f: 82.54022067435123
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 9 [0m
Train...
loss: tensor(590.0867) lr: 0.0002 time: 4353.9
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 78.77 	r: 75.6 	f1: 77.15 	 4663 	 5920 	 6168
wo 	p: 92.7 	r: 85.8 	f1: 89.12 	 2907 	 3136 	 3388
ni 	p: 80.0 	r: 76.48 	f1: 78.2 	 1164 	 1455 	 1522

[32m iter_1[0m
ga 	p: 79.82 	r: 75.11 	f1: 77.4 	 4633 	 5804 	 6168
wo 	p: 91.76 	r: 86.1 	f1: 88.84 	 2917 	 3179 	 3388
ni 	p: 83.79 	r: 74.7 	f1: 78.99 	 1137 	 1357 	 1522

[32m iter_2[0m
ga 	p: 81.32 	r: 74.19 	f1: 77.59 	 4576 	 5627 	 6168
wo 	p: 91.31 	r: 86.51 	f1: 88.85 	 2931 	 3210 	 3388
ni 	p: 82.28 	r: 75.95 	f1: 78.99 	 1156 	 1405 	 1522
best_thres [[0.42, 0.55, 0.19], [0.47, 0.44, 0.23], [0.65, 0.43, 0.19]]
f [0.8091, 0.8101, 0.811]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 4 	 [0.3, 0.66, 0.36] 	 lr: 0.0002 	 f: 82.54022067435123
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 8 [0m
Train...

[32m iter_0[0m
ga 	p: 83.65 	r: 75.23 	f1: 79.21 	 4640 	 5547 	 6168
wo 	p: 93.32 	r: 87.43 	f1: 90.28 	 2962 	 3174 	 3388
ni 	p: 88.08 	r: 78.65 	f1: 83.1 	 1197 	 1359 	 1522

[32m iter_1[0m
ga 	p: 82.78 	r: 76.05 	f1: 79.27 	 4691 	 5667 	 6168
wo 	p: 93.08 	r: 87.37 	f1: 90.13 	 2960 	 3180 	 3388
ni 	p: 89.09 	r: 77.79 	f1: 83.06 	 1184 	 1329 	 1522

[32m iter_2[0m
ga 	p: 84.32 	r: 74.82 	f1: 79.29 	 4615 	 5473 	 6168
wo 	p: 94.74 	r: 86.07 	f1: 90.19 	 2916 	 3078 	 3388
ni 	p: 86.93 	r: 79.57 	f1: 83.09 	 1211 	 1393 	 1522
best_thres [[0.56, 0.46, 0.17], [0.52, 0.41, 0.17], [0.66, 0.68, 0.13]]
f [0.8317, 0.8316, 0.8316]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.66, 0.68, 0.13] 	 lr: 0.0001 	 f: 83.16045023173693
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 9 [0m
Train...
loss: tensor(1096.0133) lr: 0.0001 time: 4127.53
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.11 	r: 73.22 	f1: 76.51 	 4516 	 5637 	 6168
wo 	p: 90.68 	r: 86.39 	f1: 88.48 	 2927 	 3228 	 3388
ni 	p: 84.23 	r: 75.43 	f1: 79.58 	 1148 	 1363 	 1522

[32m iter_1[0m
ga 	p: 81.72 	r: 72.62 	f1: 76.9 	 4479 	 5481 	 6168
wo 	p: 91.39 	r: 85.24 	f1: 88.21 	 2888 	 3160 	 3388
ni 	p: 82.77 	r: 77.0 	f1: 79.78 	 1172 	 1416 	 1522

[32m iter_2[0m
ga 	p: 81.63 	r: 72.63 	f1: 76.87 	 4480 	 5488 	 6168
wo 	p: 91.34 	r: 85.92 	f1: 88.55 	 2911 	 3187 	 3388
ni 	p: 82.3 	r: 77.27 	f1: 79.7 	 1176 	 1429 	 1522
best_thres [[0.51, 0.53, 0.29], [0.58, 0.57, 0.24], [0.58, 0.57, 0.23]]
f [0.8064, 0.8072, 0.8078]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 5 	 [0.39, 0.54, 0.29] 	 lr: 0.0002 	 f: 82.5589583887576
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 8 [0m
Train...

[32m iter_0[0m
ga 	p: 83.65 	r: 75.23 	f1: 79.21 	 4640 	 5547 	 6168
wo 	p: 93.32 	r: 87.43 	f1: 90.28 	 2962 	 3174 	 3388
ni 	p: 88.08 	r: 78.65 	f1: 83.1 	 1197 	 1359 	 1522

[32m iter_1[0m
ga 	p: 82.78 	r: 76.05 	f1: 79.27 	 4691 	 5667 	 6168
wo 	p: 93.08 	r: 87.37 	f1: 90.13 	 2960 	 3180 	 3388
ni 	p: 89.09 	r: 77.79 	f1: 83.06 	 1184 	 1329 	 1522

[32m iter_2[0m
ga 	p: 84.32 	r: 74.82 	f1: 79.29 	 4615 	 5473 	 6168
wo 	p: 94.74 	r: 86.07 	f1: 90.19 	 2916 	 3078 	 3388
ni 	p: 86.93 	r: 79.57 	f1: 83.09 	 1211 	 1393 	 1522
best_thres [[0.56, 0.46, 0.17], [0.52, 0.41, 0.17], [0.66, 0.68, 0.13]]
f [0.8317, 0.8316, 0.8316]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.66, 0.68, 0.13] 	 lr: 0.0001 	 f: 83.16045023173693
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 9 [0m
Train...
loss: tensor(772.7319) lr: 0.0001 time: 4503.33
pred_count_train 41644

Test...
loss: tensor(488.6465) lr: 0.0002 time: 4694.26
pred_count_train 41644

Test...
loss: tensor(759.1992) lr: 0.0001 time: 4617.86
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.88 	r: 74.09 	f1: 78.68 	 4570 	 5448 	 6168
wo 	p: 91.86 	r: 87.63 	f1: 89.7 	 2969 	 3232 	 3388
ni 	p: 85.07 	r: 78.65 	f1: 81.73 	 1197 	 1407 	 1522

[32m iter_1[0m
ga 	p: 81.5 	r: 76.28 	f1: 78.8 	 4705 	 5773 	 6168
wo 	p: 93.08 	r: 86.54 	f1: 89.69 	 2932 	 3150 	 3388
ni 	p: 84.45 	r: 80.29 	f1: 82.32 	 1222 	 1447 	 1522

[32m iter_2[0m
ga 	p: 81.62 	r: 76.31 	f1: 78.88 	 4707 	 5767 	 6168
wo 	p: 92.96 	r: 86.57 	f1: 89.65 	 2933 	 3155 	 3388
ni 	p: 87.12 	r: 77.79 	f1: 82.19 	 1184 	 1359 	 1522
best_thres [[0.46, 0.42, 0.14], [0.31, 0.66, 0.1], [0.31, 0.68, 0.14]]
f [0.8255, 0.8258, 0.826]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 9 	 [0.31, 0.68, 0.14] 	 lr: 0.0001 	 f: 82.59551053585945
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 10 [0m
Train...
loss: tensor(470.3937) lr: 0.0002 time: 4548.19
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.9 	r: 73.44 	f1: 77.44 	 4530 	 5531 	 6168
wo 	p: 93.3 	r: 85.06 	f1: 88.99 	 2882 	 3089 	 3388
ni 	p: 81.07 	r: 76.81 	f1: 78.88 	 1169 	 1442 	 1522

[32m iter_1[0m
ga 	p: 79.68 	r: 75.66 	f1: 77.62 	 4667 	 5857 	 6168
wo 	p: 92.76 	r: 85.42 	f1: 88.94 	 2894 	 3120 	 3388
ni 	p: 82.34 	r: 75.69 	f1: 78.88 	 1152 	 1399 	 1522

[32m iter_2[0m
ga 	p: 79.51 	r: 75.88 	f1: 77.65 	 4680 	 5886 	 6168
wo 	p: 92.6 	r: 86.01 	f1: 89.18 	 2914 	 3147 	 3388
ni 	p: 82.4 	r: 75.69 	f1: 78.9 	 1152 	 1398 	 1522
best_thres [[0.59, 0.8, 0.12], [0.41, 0.79, 0.12], [0.41, 0.79, 0.12]]
f [0.8118, 0.812, 0.8124]
load model: epoch4
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 4 	 [0.3, 0.66, 0.36] 	 lr: 0.0002 	 f: 82.54022067435123
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 9 [0m
Train...

[32m iter_0[0m
ga 	p: 80.69 	r: 76.01 	f1: 78.28 	 4688 	 5810 	 6168
wo 	p: 93.21 	r: 86.3 	f1: 89.62 	 2924 	 3137 	 3388
ni 	p: 85.73 	r: 78.98 	f1: 82.22 	 1202 	 1402 	 1522

[32m iter_1[0m
ga 	p: 80.98 	r: 76.09 	f1: 78.46 	 4693 	 5795 	 6168
wo 	p: 93.08 	r: 86.57 	f1: 89.71 	 2933 	 3151 	 3388
ni 	p: 86.16 	r: 78.52 	f1: 82.16 	 1195 	 1387 	 1522

[32m iter_2[0m
ga 	p: 81.62 	r: 75.52 	f1: 78.45 	 4658 	 5707 	 6168
wo 	p: 93.62 	r: 86.19 	f1: 89.75 	 2920 	 3119 	 3388
ni 	p: 85.74 	r: 79.04 	f1: 82.26 	 1203 	 1403 	 1522
best_thres [[0.27, 0.56, 0.16], [0.27, 0.59, 0.13], [0.3, 0.74, 0.12]]
f [0.8227, 0.8233, 0.8236]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.66, 0.68, 0.13] 	 lr: 0.0001 	 f: 83.16045023173693
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 10 [0m
Train...
loss: tensor(759.1992) lr: 0.0001 time: 4353.66
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.43 	r: 72.42 	f1: 77.54 	 4467 	 5354 	 6168
wo 	p: 93.51 	r: 84.56 	f1: 88.81 	 2865 	 3064 	 3388
ni 	p: 82.52 	r: 74.11 	f1: 78.09 	 1128 	 1367 	 1522

[32m iter_1[0m
ga 	p: 81.02 	r: 74.66 	f1: 77.71 	 4605 	 5684 	 6168
wo 	p: 93.29 	r: 84.53 	f1: 88.7 	 2864 	 3070 	 3388
ni 	p: 83.06 	r: 73.78 	f1: 78.15 	 1123 	 1352 	 1522

[32m iter_2[0m
ga 	p: 81.01 	r: 74.82 	f1: 77.79 	 4615 	 5697 	 6168
wo 	p: 93.21 	r: 84.68 	f1: 88.74 	 2869 	 3078 	 3388
ni 	p: 82.73 	r: 73.98 	f1: 78.11 	 1126 	 1361 	 1522
best_thres [[0.64, 0.85, 0.09], [0.42, 0.8, 0.1], [0.42, 0.84, 0.09]]
f [0.811, 0.8111, 0.8113]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 5 	 [0.39, 0.54, 0.29] 	 lr: 0.0002 	 f: 82.5589583887576
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 9 [0m
Train...

[32m iter_0[0m
ga 	p: 80.69 	r: 76.01 	f1: 78.28 	 4688 	 5810 	 6168
wo 	p: 93.21 	r: 86.3 	f1: 89.62 	 2924 	 3137 	 3388
ni 	p: 85.73 	r: 78.98 	f1: 82.22 	 1202 	 1402 	 1522

[32m iter_1[0m
ga 	p: 80.98 	r: 76.09 	f1: 78.46 	 4693 	 5795 	 6168
wo 	p: 93.08 	r: 86.57 	f1: 89.71 	 2933 	 3151 	 3388
ni 	p: 86.16 	r: 78.52 	f1: 82.16 	 1195 	 1387 	 1522

[32m iter_2[0m
ga 	p: 81.62 	r: 75.52 	f1: 78.45 	 4658 	 5707 	 6168
wo 	p: 93.62 	r: 86.19 	f1: 89.75 	 2920 	 3119 	 3388
ni 	p: 85.74 	r: 79.04 	f1: 82.26 	 1203 	 1403 	 1522
best_thres [[0.27, 0.56, 0.16], [0.27, 0.59, 0.13], [0.3, 0.74, 0.12]]
f [0.8227, 0.8233, 0.8236]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.66, 0.68, 0.13] 	 lr: 0.0001 	 f: 83.16045023173693
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 10 [0m
Train...
loss: tensor(482.1248) lr: 0.0001 time: 4541.38
pred_count_train 41644

Test...
loss: tensor(455.5229) lr: 0.0001 time: 4626.98
pred_count_train 41644

Test...
loss: tensor(525.4005) lr: 0.0001 time: 4560.27
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.35 	r: 77.48 	f1: 78.89 	 4779 	 5948 	 6168
wo 	p: 94.22 	r: 86.19 	f1: 90.03 	 2920 	 3099 	 3388
ni 	p: 89.14 	r: 73.32 	f1: 80.46 	 1116 	 1252 	 1522

[32m iter_1[0m
ga 	p: 82.48 	r: 75.73 	f1: 78.96 	 4671 	 5663 	 6168
wo 	p: 93.99 	r: 86.33 	f1: 90.0 	 2925 	 3112 	 3388
ni 	p: 82.7 	r: 78.52 	f1: 80.55 	 1195 	 1445 	 1522

[32m iter_2[0m
ga 	p: 82.46 	r: 75.99 	f1: 79.09 	 4687 	 5684 	 6168
wo 	p: 93.28 	r: 86.84 	f1: 89.94 	 2942 	 3154 	 3388
ni 	p: 83.31 	r: 78.38 	f1: 80.77 	 1193 	 1432 	 1522
best_thres [[0.33, 0.66, 0.18], [0.5, 0.82, 0.06], [0.49, 0.74, 0.06]]
f [0.8247, 0.8251, 0.8256]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 9 	 [0.31, 0.68, 0.14] 	 lr: 0.0001 	 f: 82.59551053585945
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 11 [0m
Train...
loss: tensor(415.5222) lr: 0.0002 time: 4472.76
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.43 	r: 74.87 	f1: 78.01 	 4618 	 5671 	 6168
wo 	p: 92.85 	r: 85.86 	f1: 89.22 	 2909 	 3133 	 3388
ni 	p: 83.33 	r: 75.56 	f1: 79.26 	 1150 	 1380 	 1522

[32m iter_1[0m
ga 	p: 82.0 	r: 74.51 	f1: 78.08 	 4596 	 5605 	 6168
wo 	p: 92.91 	r: 85.92 	f1: 89.28 	 2911 	 3133 	 3388
ni 	p: 84.73 	r: 75.49 	f1: 79.85 	 1149 	 1356 	 1522

[32m iter_2[0m
ga 	p: 82.17 	r: 74.43 	f1: 78.11 	 4591 	 5587 	 6168
wo 	p: 92.74 	r: 85.95 	f1: 89.22 	 2912 	 3140 	 3388
ni 	p: 86.3 	r: 74.11 	f1: 79.75 	 1128 	 1307 	 1522
best_thres [[0.38, 0.37, 0.16], [0.41, 0.39, 0.14], [0.42, 0.38, 0.17]]
f [0.8162, 0.8169, 0.8172]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 4 	 [0.3, 0.66, 0.36] 	 lr: 0.0002 	 f: 82.54022067435123
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 10 [0m
Train...
loss: tensor(525.4005) lr: 0.0001 time: 4177.72
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.15 	r: 75.86 	f1: 78.88 	 4679 	 5696 	 6168
wo 	p: 92.94 	r: 86.25 	f1: 89.47 	 2922 	 3144 	 3388
ni 	p: 86.65 	r: 76.74 	f1: 81.39 	 1168 	 1348 	 1522

[32m iter_1[0m
ga 	p: 82.1 	r: 76.46 	f1: 79.18 	 4716 	 5744 	 6168
wo 	p: 93.17 	r: 86.1 	f1: 89.49 	 2917 	 3131 	 3388
ni 	p: 88.02 	r: 75.82 	f1: 81.47 	 1154 	 1311 	 1522

[32m iter_2[0m
ga 	p: 82.44 	r: 76.18 	f1: 79.19 	 4699 	 5700 	 6168
wo 	p: 92.95 	r: 86.45 	f1: 89.59 	 2929 	 3151 	 3388
ni 	p: 87.42 	r: 76.22 	f1: 81.43 	 1160 	 1327 	 1522
best_thres [[0.51, 0.6, 0.19], [0.5, 0.77, 0.18], [0.61, 0.79, 0.16]]
f [0.8247, 0.8256, 0.826]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.66, 0.68, 0.13] 	 lr: 0.0001 	 f: 83.16045023173693
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 11 [0m
Train...

[32m iter_0[0m
ga 	p: 80.32 	r: 74.43 	f1: 77.26 	 4591 	 5716 	 6168
wo 	p: 92.25 	r: 86.45 	f1: 89.26 	 2929 	 3175 	 3388
ni 	p: 85.66 	r: 76.54 	f1: 80.85 	 1165 	 1360 	 1522

[32m iter_1[0m
ga 	p: 83.19 	r: 72.13 	f1: 77.27 	 4449 	 5348 	 6168
wo 	p: 92.25 	r: 86.48 	f1: 89.27 	 2930 	 3176 	 3388
ni 	p: 84.19 	r: 77.33 	f1: 80.62 	 1177 	 1398 	 1522

[32m iter_2[0m
ga 	p: 83.24 	r: 72.0 	f1: 77.21 	 4441 	 5335 	 6168
wo 	p: 92.19 	r: 86.36 	f1: 89.18 	 2926 	 3174 	 3388
ni 	p: 83.6 	r: 78.06 	f1: 80.73 	 1188 	 1421 	 1522
best_thres [[0.38, 0.65, 0.21], [0.6, 0.64, 0.14], [0.63, 0.64, 0.12]]
f [0.8144, 0.8146, 0.8146]
load model: epoch5
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 5 	 [0.39, 0.54, 0.29] 	 lr: 0.0002 	 f: 82.5589583887576
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 10 [0m
Train...

[32m iter_0[0m
ga 	p: 82.15 	r: 75.86 	f1: 78.88 	 4679 	 5696 	 6168
wo 	p: 92.94 	r: 86.25 	f1: 89.47 	 2922 	 3144 	 3388
ni 	p: 86.65 	r: 76.74 	f1: 81.39 	 1168 	 1348 	 1522

[32m iter_1[0m
ga 	p: 82.1 	r: 76.46 	f1: 79.18 	 4716 	 5744 	 6168
wo 	p: 93.17 	r: 86.1 	f1: 89.49 	 2917 	 3131 	 3388
ni 	p: 88.02 	r: 75.82 	f1: 81.47 	 1154 	 1311 	 1522

[32m iter_2[0m
ga 	p: 82.44 	r: 76.18 	f1: 79.19 	 4699 	 5700 	 6168
wo 	p: 92.95 	r: 86.45 	f1: 89.59 	 2929 	 3151 	 3388
ni 	p: 87.42 	r: 76.22 	f1: 81.43 	 1160 	 1327 	 1522
best_thres [[0.51, 0.6, 0.19], [0.5, 0.77, 0.18], [0.61, 0.79, 0.16]]
f [0.8247, 0.8256, 0.826]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.66, 0.68, 0.13] 	 lr: 0.0001 	 f: 83.16045023173693
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 11 [0m
Train...
loss: tensor(333.2680) lr: 0.0001 time: 4332.38
pred_count_train 41644

Test...
loss: tensor(340.3774) lr: 0.0001 time: 4569.81
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.68 	r: 76.46 	f1: 78.51 	 4716 	 5845 	 6168
wo 	p: 93.75 	r: 85.92 	f1: 89.67 	 2911 	 3105 	 3388
ni 	p: 83.99 	r: 77.92 	f1: 80.85 	 1186 	 1412 	 1522

[32m iter_1[0m
ga 	p: 80.07 	r: 77.11 	f1: 78.56 	 4756 	 5940 	 6168
wo 	p: 92.65 	r: 86.66 	f1: 89.55 	 2936 	 3169 	 3388
ni 	p: 86.86 	r: 76.41 	f1: 81.3 	 1163 	 1339 	 1522

[32m iter_2[0m
ga 	p: 82.64 	r: 74.94 	f1: 78.6 	 4622 	 5593 	 6168
wo 	p: 93.63 	r: 85.86 	f1: 89.58 	 2909 	 3107 	 3388
ni 	p: 86.61 	r: 76.94 	f1: 81.49 	 1171 	 1352 	 1522
best_thres [[0.34, 0.58, 0.17], [0.27, 0.44, 0.21], [0.61, 0.79, 0.18]]
f [0.8221, 0.8224, 0.8228]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 9 	 [0.31, 0.68, 0.14] 	 lr: 0.0001 	 f: 82.59551053585945
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 12 [0m
Train...
loss: tensor(374.0410) lr: 0.0001 time: 4600.74
pred_count_train 41644

Test...
loss: tensor(547.5955) lr: 0.0001 time: 4613.3
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.89 	r: 75.36 	f1: 78.49 	 4648 	 5676 	 6168
wo 	p: 93.75 	r: 85.42 	f1: 89.39 	 2894 	 3087 	 3388
ni 	p: 84.82 	r: 77.86 	f1: 81.19 	 1185 	 1397 	 1522

[32m iter_1[0m
ga 	p: 82.83 	r: 74.71 	f1: 78.56 	 4608 	 5563 	 6168
wo 	p: 93.02 	r: 85.8 	f1: 89.27 	 2907 	 3125 	 3388
ni 	p: 84.53 	r: 78.25 	f1: 81.27 	 1191 	 1409 	 1522

[32m iter_2[0m
ga 	p: 82.85 	r: 74.79 	f1: 78.61 	 4613 	 5568 	 6168
wo 	p: 92.68 	r: 86.01 	f1: 89.22 	 2914 	 3144 	 3388
ni 	p: 83.9 	r: 79.43 	f1: 81.61 	 1209 	 1441 	 1522
best_thres [[0.34, 0.72, 0.12], [0.42, 0.84, 0.09], [0.43, 0.85, 0.07]]
f [0.8218, 0.8221, 0.8224]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 4 	 [0.3, 0.66, 0.36] 	 lr: 0.0002 	 f: 82.54022067435123
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 11 [0m
Train...
loss: tensor(374.0410) lr: 0.0001 time: 4454.42
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.0 	r: 74.17 	f1: 77.89 	 4575 	 5579 	 6168
wo 	p: 92.9 	r: 85.77 	f1: 89.2 	 2906 	 3128 	 3388
ni 	p: 86.47 	r: 76.02 	f1: 80.91 	 1157 	 1338 	 1522

[32m iter_1[0m
ga 	p: 82.72 	r: 73.74 	f1: 77.97 	 4548 	 5498 	 6168
wo 	p: 93.52 	r: 85.63 	f1: 89.4 	 2901 	 3102 	 3388
ni 	p: 85.61 	r: 75.43 	f1: 80.2 	 1148 	 1341 	 1522

[32m iter_2[0m
ga 	p: 83.55 	r: 73.1 	f1: 77.98 	 4509 	 5397 	 6168
wo 	p: 93.73 	r: 85.66 	f1: 89.51 	 2902 	 3096 	 3388
ni 	p: 87.11 	r: 74.18 	f1: 80.13 	 1129 	 1296 	 1522
best_thres [[0.36, 0.5, 0.19], [0.43, 0.67, 0.12], [0.54, 0.78, 0.14]]
f [0.8179, 0.8179, 0.8181]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.66, 0.68, 0.13] 	 lr: 0.0001 	 f: 83.16045023173693
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 12 [0m
Train...

[32m iter_0[0m
ga 	p: 82.1 	r: 76.23 	f1: 79.06 	 4702 	 5727 	 6168
wo 	p: 93.71 	r: 86.25 	f1: 89.82 	 2922 	 3118 	 3388
ni 	p: 84.49 	r: 78.38 	f1: 81.32 	 1193 	 1412 	 1522

[32m iter_1[0m
ga 	p: 82.44 	r: 75.94 	f1: 79.05 	 4684 	 5682 	 6168
wo 	p: 92.3 	r: 87.34 	f1: 89.75 	 2959 	 3206 	 3388
ni 	p: 84.68 	r: 79.17 	f1: 81.83 	 1205 	 1423 	 1522

[32m iter_2[0m
ga 	p: 82.31 	r: 76.04 	f1: 79.05 	 4690 	 5698 	 6168
wo 	p: 93.0 	r: 86.72 	f1: 89.75 	 2938 	 3159 	 3388
ni 	p: 83.45 	r: 80.16 	f1: 81.77 	 1220 	 1462 	 1522
best_thres [[0.49, 0.82, 0.13], [0.5, 0.64, 0.13], [0.49, 0.73, 0.1]]
f [0.8265, 0.8269, 0.827]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 10 	 [0.49, 0.73, 0.1] 	 lr: 0.0001 	 f: 82.6967764071053
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 11 [0m
Train...

[32m iter_0[0m
ga 	p: 82.0 	r: 74.17 	f1: 77.89 	 4575 	 5579 	 6168
wo 	p: 92.9 	r: 85.77 	f1: 89.2 	 2906 	 3128 	 3388
ni 	p: 86.47 	r: 76.02 	f1: 80.91 	 1157 	 1338 	 1522

[32m iter_1[0m
ga 	p: 82.72 	r: 73.74 	f1: 77.97 	 4548 	 5498 	 6168
wo 	p: 93.52 	r: 85.63 	f1: 89.4 	 2901 	 3102 	 3388
ni 	p: 85.61 	r: 75.43 	f1: 80.2 	 1148 	 1341 	 1522

[32m iter_2[0m
ga 	p: 83.55 	r: 73.1 	f1: 77.98 	 4509 	 5397 	 6168
wo 	p: 93.73 	r: 85.66 	f1: 89.51 	 2902 	 3096 	 3388
ni 	p: 87.11 	r: 74.18 	f1: 80.13 	 1129 	 1296 	 1522
best_thres [[0.36, 0.5, 0.19], [0.43, 0.67, 0.12], [0.54, 0.78, 0.14]]
f [0.8179, 0.8179, 0.8181]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.66, 0.68, 0.13] 	 lr: 0.0001 	 f: 83.16045023173693
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 12 [0m
Train...
loss: tensor(242.3260) lr: 0.0001 time: 4359.56
pred_count_train 41644

Test...
loss: tensor(245.0791) lr: 0.0001 time: 4547.35
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.58 	r: 74.45 	f1: 77.85 	 4592 	 5629 	 6168
wo 	p: 93.96 	r: 84.95 	f1: 89.23 	 2878 	 3063 	 3388
ni 	p: 83.52 	r: 74.57 	f1: 78.79 	 1135 	 1359 	 1522

[32m iter_1[0m
ga 	p: 81.35 	r: 75.32 	f1: 78.22 	 4646 	 5711 	 6168
wo 	p: 92.33 	r: 85.95 	f1: 89.02 	 2912 	 3154 	 3388
ni 	p: 85.58 	r: 74.11 	f1: 79.44 	 1128 	 1318 	 1522

[32m iter_2[0m
ga 	p: 81.34 	r: 75.19 	f1: 78.15 	 4638 	 5702 	 6168
wo 	p: 92.42 	r: 86.01 	f1: 89.1 	 2914 	 3153 	 3388
ni 	p: 85.74 	r: 73.46 	f1: 79.12 	 1118 	 1304 	 1522
best_thres [[0.43, 0.72, 0.07], [0.39, 0.7, 0.07], [0.39, 0.75, 0.07]]
f [0.8145, 0.8158, 0.816]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 9 	 [0.31, 0.68, 0.14] 	 lr: 0.0001 	 f: 82.59551053585945
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 13 [0m
Train...
loss: tensor(286.9432) lr: 0.0001 time: 4561.2
pred_count_train 41644

Test...
loss: tensor(309.4486) lr: 0.0001 time: 4599.05
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.3 	r: 75.06 	f1: 78.06 	 4630 	 5695 	 6168
wo 	p: 94.63 	r: 84.3 	f1: 89.17 	 2856 	 3018 	 3388
ni 	p: 86.53 	r: 74.31 	f1: 79.96 	 1131 	 1307 	 1522

[32m iter_1[0m
ga 	p: 81.17 	r: 75.05 	f1: 77.99 	 4629 	 5703 	 6168
wo 	p: 91.14 	r: 86.84 	f1: 88.94 	 2942 	 3228 	 3388
ni 	p: 85.0 	r: 76.35 	f1: 80.44 	 1162 	 1367 	 1522

[32m iter_2[0m
ga 	p: 80.52 	r: 75.6 	f1: 77.98 	 4663 	 5791 	 6168
wo 	p: 91.51 	r: 86.48 	f1: 88.92 	 2930 	 3202 	 3388
ni 	p: 85.41 	r: 76.15 	f1: 80.51 	 1159 	 1357 	 1522
best_thres [[0.58, 0.83, 0.19], [0.7, 0.47, 0.1], [0.7, 0.65, 0.1]]
f [0.8169, 0.817, 0.8169]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 4 	 [0.3, 0.66, 0.36] 	 lr: 0.0002 	 f: 82.54022067435123
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 12 [0m
Train...
loss: tensor(286.9432) lr: 0.0001 time: 4304.47
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.68 	r: 74.37 	f1: 77.85 	 4587 	 5616 	 6168
wo 	p: 92.97 	r: 85.89 	f1: 89.29 	 2910 	 3130 	 3388
ni 	p: 82.73 	r: 78.71 	f1: 80.67 	 1198 	 1448 	 1522

[32m iter_1[0m
ga 	p: 81.78 	r: 74.53 	f1: 77.99 	 4597 	 5621 	 6168
wo 	p: 92.27 	r: 86.66 	f1: 89.38 	 2936 	 3182 	 3388
ni 	p: 84.53 	r: 76.81 	f1: 80.48 	 1169 	 1383 	 1522

[32m iter_2[0m
ga 	p: 82.03 	r: 74.32 	f1: 77.99 	 4584 	 5588 	 6168
wo 	p: 92.6 	r: 86.48 	f1: 89.44 	 2930 	 3164 	 3388
ni 	p: 85.09 	r: 76.87 	f1: 80.77 	 1170 	 1375 	 1522
best_thres [[0.47, 0.71, 0.23], [0.5, 0.72, 0.23], [0.61, 0.83, 0.24]]
f [0.8175, 0.818, 0.8183]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.66, 0.68, 0.13] 	 lr: 0.0001 	 f: 83.16045023173693
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 13 [0m
Train...

[32m iter_0[0m
ga 	p: 82.28 	r: 75.15 	f1: 78.55 	 4635 	 5633 	 6168
wo 	p: 92.57 	r: 86.39 	f1: 89.37 	 2927 	 3162 	 3388
ni 	p: 85.61 	r: 78.58 	f1: 81.95 	 1196 	 1397 	 1522

[32m iter_1[0m
ga 	p: 81.92 	r: 75.83 	f1: 78.76 	 4677 	 5709 	 6168
wo 	p: 92.54 	r: 86.39 	f1: 89.36 	 2927 	 3163 	 3388
ni 	p: 85.57 	r: 78.71 	f1: 82.0 	 1198 	 1400 	 1522

[32m iter_2[0m
ga 	p: 82.36 	r: 75.32 	f1: 78.69 	 4646 	 5641 	 6168
wo 	p: 93.08 	r: 86.19 	f1: 89.5 	 2920 	 3137 	 3388
ni 	p: 85.65 	r: 78.45 	f1: 81.89 	 1194 	 1394 	 1522
best_thres [[0.47, 0.52, 0.1], [0.38, 0.54, 0.08], [0.41, 0.58, 0.08]]
f [0.8235, 0.824, 0.8242]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 10 	 [0.49, 0.73, 0.1] 	 lr: 0.0001 	 f: 82.6967764071053
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 12 [0m
Train...

[32m iter_0[0m
ga 	p: 81.68 	r: 74.37 	f1: 77.85 	 4587 	 5616 	 6168
wo 	p: 92.97 	r: 85.89 	f1: 89.29 	 2910 	 3130 	 3388
ni 	p: 82.73 	r: 78.71 	f1: 80.67 	 1198 	 1448 	 1522

[32m iter_1[0m
ga 	p: 81.78 	r: 74.53 	f1: 77.99 	 4597 	 5621 	 6168
wo 	p: 92.27 	r: 86.66 	f1: 89.38 	 2936 	 3182 	 3388
ni 	p: 84.53 	r: 76.81 	f1: 80.48 	 1169 	 1383 	 1522

[32m iter_2[0m
ga 	p: 82.03 	r: 74.32 	f1: 77.99 	 4584 	 5588 	 6168
wo 	p: 92.6 	r: 86.48 	f1: 89.44 	 2930 	 3164 	 3388
ni 	p: 85.09 	r: 76.87 	f1: 80.77 	 1170 	 1375 	 1522
best_thres [[0.47, 0.71, 0.23], [0.5, 0.72, 0.23], [0.61, 0.83, 0.24]]
f [0.8175, 0.818, 0.8183]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.66, 0.68, 0.13] 	 lr: 0.0001 	 f: 83.16045023173693
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 13 [0m
Train...
loss: tensor(199.7440) lr: 0.0001 time: 4162.91
pred_count_train 41644

Test...
loss: tensor(202.6618) lr: 0.0001 time: 4454.37
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.85 	r: 75.18 	f1: 77.91 	 4637 	 5735 	 6168
wo 	p: 94.76 	r: 85.42 	f1: 89.85 	 2894 	 3054 	 3388
ni 	p: 86.8 	r: 75.16 	f1: 80.56 	 1144 	 1318 	 1522

[32m iter_1[0m
ga 	p: 79.64 	r: 76.54 	f1: 78.06 	 4721 	 5928 	 6168
wo 	p: 93.45 	r: 86.3 	f1: 89.73 	 2924 	 3129 	 3388
ni 	p: 88.94 	r: 73.46 	f1: 80.46 	 1118 	 1257 	 1522

[32m iter_2[0m
ga 	p: 79.75 	r: 76.44 	f1: 78.06 	 4715 	 5912 	 6168
wo 	p: 93.29 	r: 86.57 	f1: 89.8 	 2933 	 3144 	 3388
ni 	p: 89.37 	r: 72.93 	f1: 80.32 	 1110 	 1242 	 1522
best_thres [[0.33, 0.74, 0.12], [0.17, 0.73, 0.14], [0.17, 0.74, 0.15]]
f [0.819, 0.8191, 0.8192]
load model: epoch9
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 9 	 [0.31, 0.68, 0.14] 	 lr: 0.0001 	 f: 82.59551053585945
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 14 [0m
Train...
loss: tensor(659.8990) lr: 5e-05 time: 4605.66
pred_count_train 41644

Test...
loss: tensor(217.2493) lr: 0.0001 time: 4580.44
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.32 	r: 74.87 	f1: 77.96 	 4618 	 5679 	 6168
wo 	p: 94.7 	r: 84.92 	f1: 89.54 	 2877 	 3038 	 3388
ni 	p: 86.46 	r: 74.7 	f1: 80.16 	 1137 	 1315 	 1522

[32m iter_1[0m
ga 	p: 80.94 	r: 75.47 	f1: 78.11 	 4655 	 5751 	 6168
wo 	p: 93.0 	r: 85.92 	f1: 89.32 	 2911 	 3130 	 3388
ni 	p: 85.87 	r: 75.49 	f1: 80.35 	 1149 	 1338 	 1522

[32m iter_2[0m
ga 	p: 80.04 	r: 76.31 	f1: 78.13 	 4707 	 5881 	 6168
wo 	p: 92.89 	r: 85.98 	f1: 89.3 	 2913 	 3136 	 3388
ni 	p: 84.63 	r: 76.68 	f1: 80.46 	 1167 	 1379 	 1522
best_thres [[0.54, 0.77, 0.19], [0.54, 0.77, 0.13], [0.41, 0.84, 0.1]]
f [0.8178, 0.8181, 0.8182]
load model: epoch4
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 4 	 [0.3, 0.66, 0.36] 	 lr: 0.0002 	 f: 82.54022067435123
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 13 [0m
Train...
loss: tensor(659.8990) lr: 5e-05 time: 4413.45
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.53 	r: 76.13 	f1: 78.74 	 4696 	 5760 	 6168
wo 	p: 93.39 	r: 86.75 	f1: 89.95 	 2939 	 3147 	 3388
ni 	p: 88.26 	r: 79.04 	f1: 83.4 	 1203 	 1363 	 1522

[32m iter_1[0m
ga 	p: 82.69 	r: 75.76 	f1: 79.08 	 4673 	 5651 	 6168
wo 	p: 93.18 	r: 86.75 	f1: 89.85 	 2939 	 3154 	 3388
ni 	p: 85.87 	r: 80.68 	f1: 83.2 	 1228 	 1430 	 1522

[32m iter_2[0m
ga 	p: 82.78 	r: 75.73 	f1: 79.1 	 4671 	 5643 	 6168
wo 	p: 92.73 	r: 87.34 	f1: 89.95 	 2959 	 3191 	 3388
ni 	p: 85.92 	r: 80.62 	f1: 83.19 	 1227 	 1428 	 1522
best_thres [[0.36, 0.39, 0.19], [0.39, 0.38, 0.11], [0.39, 0.29, 0.11]]
f [0.828, 0.8288, 0.8292]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.66, 0.68, 0.13] 	 lr: 0.0001 	 f: 83.16045023173693
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 14 [0m
Train...

[32m iter_0[0m
ga 	p: 80.57 	r: 75.62 	f1: 78.01 	 4664 	 5789 	 6168
wo 	p: 92.54 	r: 86.07 	f1: 89.19 	 2916 	 3151 	 3388
ni 	p: 85.95 	r: 74.77 	f1: 79.97 	 1138 	 1324 	 1522

[32m iter_1[0m
ga 	p: 81.34 	r: 75.05 	f1: 78.07 	 4629 	 5691 	 6168
wo 	p: 92.93 	r: 86.13 	f1: 89.4 	 2918 	 3140 	 3388
ni 	p: 85.07 	r: 74.51 	f1: 79.44 	 1134 	 1333 	 1522

[32m iter_2[0m
ga 	p: 81.14 	r: 75.15 	f1: 78.03 	 4635 	 5712 	 6168
wo 	p: 92.84 	r: 86.07 	f1: 89.32 	 2916 	 3141 	 3388
ni 	p: 85.02 	r: 74.57 	f1: 79.45 	 1135 	 1335 	 1522
best_thres [[0.35, 0.61, 0.04], [0.39, 0.62, 0.03], [0.38, 0.62, 0.03]]
f [0.817, 0.8172, 0.8171]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 10 	 [0.49, 0.73, 0.1] 	 lr: 0.0001 	 f: 82.6967764071053
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 13 [0m
Train...

[32m iter_0[0m
ga 	p: 81.53 	r: 76.13 	f1: 78.74 	 4696 	 5760 	 6168
wo 	p: 93.39 	r: 86.75 	f1: 89.95 	 2939 	 3147 	 3388
ni 	p: 88.26 	r: 79.04 	f1: 83.4 	 1203 	 1363 	 1522

[32m iter_1[0m
ga 	p: 82.69 	r: 75.76 	f1: 79.08 	 4673 	 5651 	 6168
wo 	p: 93.18 	r: 86.75 	f1: 89.85 	 2939 	 3154 	 3388
ni 	p: 85.87 	r: 80.68 	f1: 83.2 	 1228 	 1430 	 1522

[32m iter_2[0m
ga 	p: 82.78 	r: 75.73 	f1: 79.1 	 4671 	 5643 	 6168
wo 	p: 92.73 	r: 87.34 	f1: 89.95 	 2959 	 3191 	 3388
ni 	p: 85.92 	r: 80.62 	f1: 83.19 	 1227 	 1428 	 1522
best_thres [[0.36, 0.39, 0.19], [0.39, 0.38, 0.11], [0.39, 0.29, 0.11]]
f [0.828, 0.8288, 0.8292]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.66, 0.68, 0.13] 	 lr: 0.0001 	 f: 83.16045023173693
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 14 [0m
Train...
loss: tensor(394.0241) lr: 5e-05 time: 4278.95
pred_count_train 41644

Test...
loss: tensor(390.3103) lr: 5e-05 time: 4533.59
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.19 	r: 76.69 	f1: 78.87 	 4730 	 5826 	 6168
wo 	p: 93.92 	r: 86.22 	f1: 89.9 	 2921 	 3110 	 3388
ni 	p: 84.68 	r: 77.33 	f1: 80.84 	 1177 	 1390 	 1522

[32m iter_1[0m
ga 	p: 81.66 	r: 76.65 	f1: 79.08 	 4728 	 5790 	 6168
wo 	p: 93.18 	r: 86.66 	f1: 89.8 	 2936 	 3151 	 3388
ni 	p: 85.67 	r: 78.19 	f1: 81.76 	 1190 	 1389 	 1522

[32m iter_2[0m
ga 	p: 82.06 	r: 76.39 	f1: 79.13 	 4712 	 5742 	 6168
wo 	p: 93.2 	r: 86.54 	f1: 89.75 	 2932 	 3146 	 3388
ni 	p: 85.05 	r: 78.52 	f1: 81.65 	 1195 	 1405 	 1522
best_thres [[0.38, 0.63, 0.12], [0.41, 0.76, 0.09], [0.44, 0.83, 0.08]]
f [0.8249, 0.826, 0.8264]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 14 	 [0.44, 0.83, 0.08] 	 lr: 5e-05 	 f: 82.6418210429553
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 15 [0m
Train...
loss: tensor(440.9040) lr: 5e-05 time: 4570.89
pred_count_train 41644

Test...
loss: tensor(167.2511) lr: 0.0001 time: 4558.3
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.79 	r: 75.57 	f1: 78.55 	 4661 	 5699 	 6168
wo 	p: 94.3 	r: 86.39 	f1: 90.17 	 2927 	 3104 	 3388
ni 	p: 86.17 	r: 78.19 	f1: 81.98 	 1190 	 1381 	 1522

[32m iter_1[0m
ga 	p: 83.15 	r: 74.66 	f1: 78.68 	 4605 	 5538 	 6168
wo 	p: 94.28 	r: 86.07 	f1: 89.99 	 2916 	 3093 	 3388
ni 	p: 87.02 	r: 78.84 	f1: 82.73 	 1200 	 1379 	 1522

[32m iter_2[0m
ga 	p: 83.25 	r: 74.48 	f1: 78.62 	 4594 	 5518 	 6168
wo 	p: 93.93 	r: 86.36 	f1: 89.99 	 2926 	 3115 	 3388
ni 	p: 87.54 	r: 78.45 	f1: 82.74 	 1194 	 1364 	 1522
best_thres [[0.34, 0.53, 0.13], [0.42, 0.68, 0.11], [0.44, 0.62, 0.11]]
f [0.8257, 0.8264, 0.8266]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 13 	 [0.44, 0.62, 0.11] 	 lr: 5e-05 	 f: 82.65825778478518
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 14 [0m
Train...
loss: tensor(440.9040) lr: 5e-05 time: 4382.53
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.02 	r: 74.3 	f1: 77.97 	 4583 	 5588 	 6168
wo 	p: 92.72 	r: 86.51 	f1: 89.51 	 2931 	 3161 	 3388
ni 	p: 84.7 	r: 77.46 	f1: 80.92 	 1179 	 1392 	 1522

[32m iter_1[0m
ga 	p: 83.06 	r: 74.27 	f1: 78.42 	 4581 	 5515 	 6168
wo 	p: 92.71 	r: 86.66 	f1: 89.58 	 2936 	 3167 	 3388
ni 	p: 83.77 	r: 79.04 	f1: 81.34 	 1203 	 1436 	 1522

[32m iter_2[0m
ga 	p: 82.3 	r: 74.85 	f1: 78.4 	 4617 	 5610 	 6168
wo 	p: 92.93 	r: 86.48 	f1: 89.59 	 2930 	 3153 	 3388
ni 	p: 83.95 	r: 78.38 	f1: 81.07 	 1193 	 1421 	 1522
best_thres [[0.47, 0.47, 0.19], [0.55, 0.48, 0.11], [0.48, 0.61, 0.11]]
f [0.8194, 0.8211, 0.8214]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.66, 0.68, 0.13] 	 lr: 0.0001 	 f: 83.16045023173693
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 15 [0m
Train...

[32m iter_0[0m
ga 	p: 82.66 	r: 73.96 	f1: 78.07 	 4562 	 5519 	 6168
wo 	p: 92.18 	r: 86.66 	f1: 89.34 	 2936 	 3185 	 3388
ni 	p: 85.64 	r: 76.02 	f1: 80.54 	 1157 	 1351 	 1522

[32m iter_1[0m
ga 	p: 80.82 	r: 75.76 	f1: 78.21 	 4673 	 5782 	 6168
wo 	p: 93.74 	r: 85.33 	f1: 89.34 	 2891 	 3084 	 3388
ni 	p: 85.64 	r: 75.62 	f1: 80.32 	 1151 	 1344 	 1522

[32m iter_2[0m
ga 	p: 83.53 	r: 73.25 	f1: 78.05 	 4518 	 5409 	 6168
wo 	p: 93.43 	r: 85.57 	f1: 89.32 	 2899 	 3103 	 3388
ni 	p: 85.75 	r: 75.49 	f1: 80.29 	 1149 	 1340 	 1522
best_thres [[0.54, 0.34, 0.09], [0.3, 0.65, 0.09], [0.61, 0.59, 0.09]]
f [0.8191, 0.8189, 0.8188]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 10 	 [0.49, 0.73, 0.1] 	 lr: 0.0001 	 f: 82.6967764071053
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 14 [0m
Train...

[32m iter_0[0m
ga 	p: 82.02 	r: 74.3 	f1: 77.97 	 4583 	 5588 	 6168
wo 	p: 92.72 	r: 86.51 	f1: 89.51 	 2931 	 3161 	 3388
ni 	p: 84.7 	r: 77.46 	f1: 80.92 	 1179 	 1392 	 1522

[32m iter_1[0m
ga 	p: 83.06 	r: 74.27 	f1: 78.42 	 4581 	 5515 	 6168
wo 	p: 92.71 	r: 86.66 	f1: 89.58 	 2936 	 3167 	 3388
ni 	p: 83.77 	r: 79.04 	f1: 81.34 	 1203 	 1436 	 1522

[32m iter_2[0m
ga 	p: 82.3 	r: 74.85 	f1: 78.4 	 4617 	 5610 	 6168
wo 	p: 92.93 	r: 86.48 	f1: 89.59 	 2930 	 3153 	 3388
ni 	p: 83.95 	r: 78.38 	f1: 81.07 	 1193 	 1421 	 1522
best_thres [[0.47, 0.47, 0.19], [0.55, 0.48, 0.11], [0.48, 0.61, 0.11]]
f [0.8194, 0.8211, 0.8214]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.66, 0.68, 0.13] 	 lr: 0.0001 	 f: 83.16045023173693
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 15 [0m
Train...
loss: tensor(227.6638) lr: 5e-05 time: 4247.33
pred_count_train 41644

Test...
loss: tensor(229.5274) lr: 5e-05 time: 4378.26
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.39 	r: 74.72 	f1: 77.91 	 4609 	 5663 	 6168
wo 	p: 94.01 	r: 86.6 	f1: 90.15 	 2934 	 3121 	 3388
ni 	p: 86.62 	r: 75.3 	f1: 80.56 	 1146 	 1323 	 1522

[32m iter_1[0m
ga 	p: 84.5 	r: 72.58 	f1: 78.09 	 4477 	 5298 	 6168
wo 	p: 93.25 	r: 87.25 	f1: 90.15 	 2956 	 3170 	 3388
ni 	p: 89.72 	r: 74.57 	f1: 81.45 	 1135 	 1265 	 1522

[32m iter_2[0m
ga 	p: 81.11 	r: 75.47 	f1: 78.19 	 4655 	 5739 	 6168
wo 	p: 93.74 	r: 86.66 	f1: 90.06 	 2936 	 3132 	 3388
ni 	p: 88.9 	r: 75.23 	f1: 81.49 	 1145 	 1288 	 1522
best_thres [[0.28, 0.51, 0.12], [0.55, 0.44, 0.14], [0.22, 0.69, 0.11]]
f [0.8203, 0.8218, 0.8221]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 14 	 [0.44, 0.83, 0.08] 	 lr: 5e-05 	 f: 82.6418210429553
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 16 [0m
Train...
loss: tensor(297.9604) lr: 5e-05 time: 4510.33
pred_count_train 41644

Test...
loss: tensor(146.1765) lr: 0.0001 time: 4511.01
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.3 	r: 76.12 	f1: 78.62 	 4695 	 5775 	 6168
wo 	p: 94.55 	r: 85.04 	f1: 89.54 	 2881 	 3047 	 3388
ni 	p: 86.04 	r: 75.76 	f1: 80.57 	 1153 	 1340 	 1522

[32m iter_1[0m
ga 	p: 82.41 	r: 75.13 	f1: 78.6 	 4634 	 5623 	 6168
wo 	p: 92.26 	r: 87.22 	f1: 89.67 	 2955 	 3203 	 3388
ni 	p: 86.46 	r: 76.81 	f1: 81.35 	 1169 	 1352 	 1522

[32m iter_2[0m
ga 	p: 82.77 	r: 75.06 	f1: 78.73 	 4630 	 5594 	 6168
wo 	p: 92.45 	r: 87.16 	f1: 89.73 	 2953 	 3194 	 3388
ni 	p: 85.43 	r: 77.46 	f1: 81.25 	 1179 	 1380 	 1522
best_thres [[0.35, 0.77, 0.2], [0.45, 0.51, 0.16], [0.49, 0.65, 0.13]]
f [0.8219, 0.823, 0.8236]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 13 	 [0.44, 0.62, 0.11] 	 lr: 5e-05 	 f: 82.65825778478518
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 15 [0m
Train...
loss: tensor(297.9604) lr: 5e-05 time: 4363.7
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.77 	r: 75.45 	f1: 78.02 	 4654 	 5762 	 6168
wo 	p: 94.02 	r: 85.89 	f1: 89.77 	 2910 	 3095 	 3388
ni 	p: 82.52 	r: 76.94 	f1: 79.63 	 1171 	 1419 	 1522

[32m iter_1[0m
ga 	p: 82.24 	r: 74.38 	f1: 78.11 	 4588 	 5579 	 6168
wo 	p: 93.43 	r: 86.48 	f1: 89.82 	 2930 	 3136 	 3388
ni 	p: 85.66 	r: 75.36 	f1: 80.18 	 1147 	 1339 	 1522

[32m iter_2[0m
ga 	p: 80.58 	r: 75.89 	f1: 78.17 	 4681 	 5809 	 6168
wo 	p: 92.99 	r: 86.87 	f1: 89.82 	 2943 	 3165 	 3388
ni 	p: 87.62 	r: 74.38 	f1: 80.45 	 1132 	 1292 	 1522
best_thres [[0.33, 0.66, 0.11], [0.42, 0.84, 0.1], [0.28, 0.8, 0.12]]
f [0.8181, 0.8191, 0.8196]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.66, 0.68, 0.13] 	 lr: 0.0001 	 f: 83.16045023173693
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 16 [0m
Train...

[32m iter_0[0m
ga 	p: 82.58 	r: 74.25 	f1: 78.2 	 4580 	 5546 	 6168
wo 	p: 92.66 	r: 86.48 	f1: 89.47 	 2930 	 3162 	 3388
ni 	p: 86.49 	r: 72.8 	f1: 79.06 	 1108 	 1281 	 1522

[32m iter_1[0m
ga 	p: 82.56 	r: 74.22 	f1: 78.17 	 4578 	 5545 	 6168
wo 	p: 92.86 	r: 86.33 	f1: 89.48 	 2925 	 3150 	 3388
ni 	p: 81.81 	r: 77.4 	f1: 79.54 	 1178 	 1440 	 1522

[32m iter_2[0m
ga 	p: 83.32 	r: 73.64 	f1: 78.18 	 4542 	 5451 	 6168
wo 	p: 92.75 	r: 86.48 	f1: 89.51 	 2930 	 3159 	 3388
ni 	p: 84.42 	r: 75.49 	f1: 79.71 	 1149 	 1361 	 1522
best_thres [[0.53, 0.78, 0.14], [0.5, 0.84, 0.03], [0.6, 0.85, 0.05]]
f [0.8182, 0.8183, 0.8186]
load model: epoch10
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 10 	 [0.49, 0.73, 0.1] 	 lr: 0.0001 	 f: 82.6967764071053
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 15 [0m
Train...

[32m iter_0[0m
ga 	p: 80.77 	r: 75.45 	f1: 78.02 	 4654 	 5762 	 6168
wo 	p: 94.02 	r: 85.89 	f1: 89.77 	 2910 	 3095 	 3388
ni 	p: 82.52 	r: 76.94 	f1: 79.63 	 1171 	 1419 	 1522

[32m iter_1[0m
ga 	p: 82.24 	r: 74.38 	f1: 78.11 	 4588 	 5579 	 6168
wo 	p: 93.43 	r: 86.48 	f1: 89.82 	 2930 	 3136 	 3388
ni 	p: 85.66 	r: 75.36 	f1: 80.18 	 1147 	 1339 	 1522

[32m iter_2[0m
ga 	p: 80.58 	r: 75.89 	f1: 78.17 	 4681 	 5809 	 6168
wo 	p: 92.99 	r: 86.87 	f1: 89.82 	 2943 	 3165 	 3388
ni 	p: 87.62 	r: 74.38 	f1: 80.45 	 1132 	 1292 	 1522
best_thres [[0.33, 0.66, 0.11], [0.42, 0.84, 0.1], [0.28, 0.8, 0.12]]
f [0.8181, 0.8191, 0.8196]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.66, 0.68, 0.13] 	 lr: 0.0001 	 f: 83.16045023173693
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 16 [0m
Train...
loss: tensor(154.8313) lr: 5e-05 time: 4251.91
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.74 	r: 73.3 	f1: 78.17 	 4521 	 5399 	 6168
wo 	p: 94.26 	r: 85.77 	f1: 89.82 	 2906 	 3083 	 3388
ni 	p: 88.53 	r: 73.52 	f1: 80.33 	 1119 	 1264 	 1522

[32m iter_1[0m
ga 	p: 81.93 	r: 75.03 	f1: 78.33 	 4628 	 5649 	 6168
wo 	p: 94.29 	r: 85.71 	f1: 89.8 	 2904 	 3080 	 3388
ni 	p: 87.39 	r: 74.24 	f1: 80.28 	 1130 	 1293 	 1522

[32m iter_2[0m
ga 	p: 81.74 	r: 75.06 	f1: 78.26 	 4630 	 5664 	 6168
wo 	p: 94.13 	r: 86.07 	f1: 89.92 	 2916 	 3098 	 3388
ni 	p: 89.19 	r: 72.67 	f1: 80.09 	 1106 	 1240 	 1522
best_thres [[0.43, 0.58, 0.2], [0.28, 0.77, 0.12], [0.26, 0.79, 0.15]]
f [0.8208, 0.8209, 0.8209]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 14 	 [0.44, 0.83, 0.08] 	 lr: 5e-05 	 f: 82.6418210429553
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 17 [0m
Train...
loss: tensor(149.3780) lr: 5e-05 time: 4458.04
pred_count_train 41644

Test...
loss: tensor(212.4164) lr: 5e-05 time: 4596.63
pred_count_train 41644

Test...
loss: tensor(243.0389) lr: 5e-05 time: 4585.99
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.91 	r: 72.44 	f1: 77.75 	 4468 	 5325 	 6168
wo 	p: 94.48 	r: 85.83 	f1: 89.95 	 2908 	 3078 	 3388
ni 	p: 85.29 	r: 76.54 	f1: 80.68 	 1165 	 1366 	 1522

[32m iter_1[0m
ga 	p: 84.89 	r: 71.95 	f1: 77.89 	 4438 	 5228 	 6168
wo 	p: 93.38 	r: 86.63 	f1: 89.88 	 2935 	 3143 	 3388
ni 	p: 88.26 	r: 75.1 	f1: 81.15 	 1143 	 1295 	 1522

[32m iter_2[0m
ga 	p: 84.23 	r: 72.47 	f1: 77.91 	 4470 	 5307 	 6168
wo 	p: 93.75 	r: 86.28 	f1: 89.86 	 2923 	 3118 	 3388
ni 	p: 89.48 	r: 74.31 	f1: 81.19 	 1131 	 1264 	 1522
best_thres [[0.37, 0.54, 0.07], [0.46, 0.43, 0.08], [0.39, 0.63, 0.09]]
f [0.8194, 0.8202, 0.8205]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 13 	 [0.44, 0.62, 0.11] 	 lr: 5e-05 	 f: 82.65825778478518
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 16 [0m
Train...
loss: tensor(212.4164) lr: 5e-05 time: 4453.39
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.85 	r: 76.15 	f1: 77.96 	 4697 	 5882 	 6168
wo 	p: 93.58 	r: 85.63 	f1: 89.43 	 2901 	 3100 	 3388
ni 	p: 85.34 	r: 75.76 	f1: 80.26 	 1153 	 1351 	 1522

[32m iter_1[0m
ga 	p: 81.47 	r: 75.08 	f1: 78.15 	 4631 	 5684 	 6168
wo 	p: 92.85 	r: 86.28 	f1: 89.44 	 2923 	 3148 	 3388
ni 	p: 86.5 	r: 74.51 	f1: 80.06 	 1134 	 1311 	 1522

[32m iter_2[0m
ga 	p: 81.73 	r: 74.92 	f1: 78.18 	 4621 	 5654 	 6168
wo 	p: 92.47 	r: 86.63 	f1: 89.45 	 2935 	 3174 	 3388
ni 	p: 84.44 	r: 76.28 	f1: 80.15 	 1161 	 1375 	 1522
best_thres [[0.29, 0.69, 0.19], [0.4, 0.84, 0.16], [0.42, 0.82, 0.1]]
f [0.8174, 0.8181, 0.8185]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.66, 0.68, 0.13] 	 lr: 0.0001 	 f: 83.16045023173693
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 17 [0m
Train...

[32m iter_0[0m
ga 	p: 81.88 	r: 76.22 	f1: 78.95 	 4701 	 5741 	 6168
wo 	p: 93.83 	r: 86.13 	f1: 89.81 	 2918 	 3110 	 3388
ni 	p: 84.4 	r: 78.58 	f1: 81.39 	 1196 	 1417 	 1522

[32m iter_1[0m
ga 	p: 82.77 	r: 75.62 	f1: 79.03 	 4664 	 5635 	 6168
wo 	p: 92.13 	r: 87.37 	f1: 89.68 	 2960 	 3213 	 3388
ni 	p: 84.22 	r: 79.24 	f1: 81.65 	 1206 	 1432 	 1522

[32m iter_2[0m
ga 	p: 82.26 	r: 76.09 	f1: 79.05 	 4693 	 5705 	 6168
wo 	p: 92.81 	r: 86.81 	f1: 89.71 	 2941 	 3169 	 3388
ni 	p: 84.44 	r: 79.17 	f1: 81.72 	 1205 	 1427 	 1522
best_thres [[0.34, 0.67, 0.07], [0.37, 0.34, 0.06], [0.33, 0.47, 0.06]]
f [0.8259, 0.8264, 0.8266]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 10 	 [0.49, 0.73, 0.1] 	 lr: 0.0001 	 f: 82.6967764071053
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 16 [0m
Train...
loss: tensor(115.0432) lr: 5e-05 time: 4130.64
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.85 	r: 76.15 	f1: 77.96 	 4697 	 5882 	 6168
wo 	p: 93.58 	r: 85.63 	f1: 89.43 	 2901 	 3100 	 3388
ni 	p: 85.34 	r: 75.76 	f1: 80.26 	 1153 	 1351 	 1522

[32m iter_1[0m
ga 	p: 81.47 	r: 75.08 	f1: 78.15 	 4631 	 5684 	 6168
wo 	p: 92.85 	r: 86.28 	f1: 89.44 	 2923 	 3148 	 3388
ni 	p: 86.5 	r: 74.51 	f1: 80.06 	 1134 	 1311 	 1522

[32m iter_2[0m
ga 	p: 81.73 	r: 74.92 	f1: 78.18 	 4621 	 5654 	 6168
wo 	p: 92.47 	r: 86.63 	f1: 89.45 	 2935 	 3174 	 3388
ni 	p: 84.44 	r: 76.28 	f1: 80.15 	 1161 	 1375 	 1522
best_thres [[0.29, 0.69, 0.19], [0.4, 0.84, 0.16], [0.42, 0.82, 0.1]]
f [0.8174, 0.8181, 0.8185]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.66, 0.68, 0.13] 	 lr: 0.0001 	 f: 83.16045023173693
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 17 [0m
Train...

[32m iter_0[0m
ga 	p: 81.79 	r: 75.0 	f1: 78.25 	 4626 	 5656 	 6168
wo 	p: 92.84 	r: 86.89 	f1: 89.77 	 2944 	 3171 	 3388
ni 	p: 89.69 	r: 70.89 	f1: 79.19 	 1079 	 1203 	 1522

[32m iter_1[0m
ga 	p: 81.19 	r: 75.99 	f1: 78.5 	 4687 	 5773 	 6168
wo 	p: 92.71 	r: 87.1 	f1: 89.82 	 2951 	 3183 	 3388
ni 	p: 89.6 	r: 72.47 	f1: 80.13 	 1103 	 1231 	 1522

[32m iter_2[0m
ga 	p: 81.15 	r: 75.88 	f1: 78.42 	 4680 	 5767 	 6168
wo 	p: 92.69 	r: 87.16 	f1: 89.84 	 2953 	 3186 	 3388
ni 	p: 85.96 	r: 75.23 	f1: 80.24 	 1145 	 1332 	 1522
best_thres [[0.41, 0.46, 0.35], [0.34, 0.51, 0.23], [0.35, 0.5, 0.1]]
f [0.8195, 0.8208, 0.8211]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 14 	 [0.44, 0.83, 0.08] 	 lr: 5e-05 	 f: 82.6418210429553
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 18 [0m
Train...
loss: tensor(116.9215) lr: 5e-05 time: 4394.63
pred_count_train 41644

Test...
loss: tensor(624.0187) lr: 2.5e-05 time: 4580.28
pred_count_train 41644

Test...
loss: tensor(123.6196) lr: 5e-05 time: 4605.12
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.3 	r: 75.49 	f1: 78.28 	 4656 	 5727 	 6168
wo 	p: 93.14 	r: 86.22 	f1: 89.55 	 2921 	 3136 	 3388
ni 	p: 83.62 	r: 75.49 	f1: 79.35 	 1149 	 1374 	 1522

[32m iter_1[0m
ga 	p: 81.47 	r: 75.7 	f1: 78.48 	 4669 	 5731 	 6168
wo 	p: 92.45 	r: 86.69 	f1: 89.47 	 2937 	 3177 	 3388
ni 	p: 85.53 	r: 75.36 	f1: 80.13 	 1147 	 1341 	 1522

[32m iter_2[0m
ga 	p: 81.86 	r: 75.36 	f1: 78.47 	 4648 	 5678 	 6168
wo 	p: 92.38 	r: 86.63 	f1: 89.41 	 2935 	 3177 	 3388
ni 	p: 86.59 	r: 74.64 	f1: 80.17 	 1136 	 1312 	 1522
best_thres [[0.3, 0.57, 0.08], [0.26, 0.52, 0.06], [0.3, 0.55, 0.07]]
f [0.8188, 0.8198, 0.8201]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 13 	 [0.44, 0.62, 0.11] 	 lr: 5e-05 	 f: 82.65825778478518
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 17 [0m
Train...
loss: tensor(624.0187) lr: 2.5e-05 time: 4431.87
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.47 	r: 74.74 	f1: 78.86 	 4610 	 5523 	 6168
wo 	p: 93.27 	r: 87.1 	f1: 90.08 	 2951 	 3164 	 3388
ni 	p: 85.17 	r: 78.52 	f1: 81.71 	 1195 	 1403 	 1522

[32m iter_1[0m
ga 	p: 82.71 	r: 75.92 	f1: 79.17 	 4683 	 5662 	 6168
wo 	p: 93.58 	r: 86.87 	f1: 90.1 	 2943 	 3145 	 3388
ni 	p: 84.49 	r: 79.43 	f1: 81.88 	 1209 	 1431 	 1522

[32m iter_2[0m
ga 	p: 82.61 	r: 76.01 	f1: 79.17 	 4688 	 5675 	 6168
wo 	p: 93.75 	r: 86.75 	f1: 90.11 	 2939 	 3135 	 3388
ni 	p: 83.93 	r: 79.63 	f1: 81.73 	 1212 	 1444 	 1522
best_thres [[0.44, 0.48, 0.19], [0.39, 0.57, 0.14], [0.38, 0.67, 0.13]]
f [0.8273, 0.8281, 0.8283]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.66, 0.68, 0.13] 	 lr: 0.0001 	 f: 83.16045023173693
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 18 [0m
Train...

[32m iter_0[0m
ga 	p: 83.57 	r: 73.9 	f1: 78.44 	 4558 	 5454 	 6168
wo 	p: 92.14 	r: 87.16 	f1: 89.58 	 2953 	 3205 	 3388
ni 	p: 81.45 	r: 77.33 	f1: 79.34 	 1177 	 1445 	 1522

[32m iter_1[0m
ga 	p: 81.84 	r: 75.86 	f1: 78.74 	 4679 	 5717 	 6168
wo 	p: 92.93 	r: 86.51 	f1: 89.61 	 2931 	 3154 	 3388
ni 	p: 81.42 	r: 77.73 	f1: 79.53 	 1183 	 1453 	 1522

[32m iter_2[0m
ga 	p: 83.0 	r: 74.64 	f1: 78.6 	 4604 	 5547 	 6168
wo 	p: 92.81 	r: 86.45 	f1: 89.52 	 2929 	 3156 	 3388
ni 	p: 81.51 	r: 77.92 	f1: 79.68 	 1186 	 1455 	 1522
best_thres [[0.6, 0.39, 0.08], [0.39, 0.49, 0.06], [0.52, 0.49, 0.06]]
f [0.8203, 0.821, 0.8211]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 10 	 [0.49, 0.73, 0.1] 	 lr: 0.0001 	 f: 82.6967764071053
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 17 [0m
Train...
loss: tensor(84.4038) lr: 5e-05 time: 4063.79
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.47 	r: 74.74 	f1: 78.86 	 4610 	 5523 	 6168
wo 	p: 93.27 	r: 87.1 	f1: 90.08 	 2951 	 3164 	 3388
ni 	p: 85.17 	r: 78.52 	f1: 81.71 	 1195 	 1403 	 1522

[32m iter_1[0m
ga 	p: 82.71 	r: 75.92 	f1: 79.17 	 4683 	 5662 	 6168
wo 	p: 93.58 	r: 86.87 	f1: 90.1 	 2943 	 3145 	 3388
ni 	p: 84.49 	r: 79.43 	f1: 81.88 	 1209 	 1431 	 1522

[32m iter_2[0m
ga 	p: 82.61 	r: 76.01 	f1: 79.17 	 4688 	 5675 	 6168
wo 	p: 93.75 	r: 86.75 	f1: 90.11 	 2939 	 3135 	 3388
ni 	p: 83.93 	r: 79.63 	f1: 81.73 	 1212 	 1444 	 1522
best_thres [[0.44, 0.48, 0.19], [0.39, 0.57, 0.14], [0.38, 0.67, 0.13]]
f [0.8273, 0.8281, 0.8283]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.66, 0.68, 0.13] 	 lr: 0.0001 	 f: 83.16045023173693
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 18 [0m
Train...

[32m iter_0[0m
ga 	p: 83.19 	r: 73.43 	f1: 78.01 	 4529 	 5444 	 6168
wo 	p: 92.83 	r: 86.78 	f1: 89.7 	 2940 	 3167 	 3388
ni 	p: 84.38 	r: 74.9 	f1: 79.36 	 1140 	 1351 	 1522

[32m iter_1[0m
ga 	p: 81.52 	r: 75.37 	f1: 78.33 	 4649 	 5703 	 6168
wo 	p: 92.93 	r: 86.57 	f1: 89.64 	 2933 	 3156 	 3388
ni 	p: 87.34 	r: 74.31 	f1: 80.3 	 1131 	 1295 	 1522

[32m iter_2[0m
ga 	p: 80.94 	r: 75.96 	f1: 78.37 	 4685 	 5788 	 6168
wo 	p: 92.77 	r: 86.75 	f1: 89.66 	 2939 	 3168 	 3388
ni 	p: 83.02 	r: 77.73 	f1: 80.29 	 1183 	 1425 	 1522
best_thres [[0.57, 0.41, 0.12], [0.4, 0.47, 0.12], [0.32, 0.41, 0.04]]
f [0.8183, 0.8195, 0.82]
load model: epoch14
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 14 	 [0.44, 0.83, 0.08] 	 lr: 5e-05 	 f: 82.6418210429553
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 19 [0m
Train...
loss: tensor(85.1075) lr: 5e-05 time: 4374.21
pred_count_train 41644

Test...
loss: tensor(461.5408) lr: 2.5e-05 time: 4629.65
pred_count_train 41644

Test...
loss: tensor(86.1762) lr: 5e-05 time: 4568.49
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.15 	r: 74.38 	f1: 78.07 	 4588 	 5585 	 6168
wo 	p: 93.09 	r: 86.33 	f1: 89.59 	 2925 	 3142 	 3388
ni 	p: 84.95 	r: 76.02 	f1: 80.24 	 1157 	 1362 	 1522

[32m iter_1[0m
ga 	p: 81.91 	r: 75.16 	f1: 78.39 	 4636 	 5660 	 6168
wo 	p: 91.91 	r: 86.81 	f1: 89.28 	 2941 	 3200 	 3388
ni 	p: 86.03 	r: 75.23 	f1: 80.27 	 1145 	 1331 	 1522

[32m iter_2[0m
ga 	p: 81.07 	r: 75.75 	f1: 78.32 	 4672 	 5763 	 6168
wo 	p: 91.88 	r: 86.78 	f1: 89.25 	 2940 	 3200 	 3388
ni 	p: 84.09 	r: 77.07 	f1: 80.43 	 1173 	 1395 	 1522
best_thres [[0.38, 0.7, 0.08], [0.34, 0.76, 0.07], [0.25, 0.85, 0.04]]
f [0.8192, 0.8197, 0.8197]
load model: epoch13
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 13 	 [0.44, 0.62, 0.11] 	 lr: 5e-05 	 f: 82.65825778478518
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 18 [0m
Train...
loss: tensor(461.5408) lr: 2.5e-05 time: 4483.98
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.27 	r: 74.14 	f1: 78.44 	 4573 	 5492 	 6168
wo 	p: 94.23 	r: 85.83 	f1: 89.84 	 2908 	 3086 	 3388
ni 	p: 86.24 	r: 76.61 	f1: 81.14 	 1166 	 1352 	 1522

[32m iter_1[0m
ga 	p: 80.92 	r: 76.61 	f1: 78.7 	 4725 	 5839 	 6168
wo 	p: 94.37 	r: 85.54 	f1: 89.74 	 2898 	 3071 	 3388
ni 	p: 87.59 	r: 76.02 	f1: 81.39 	 1157 	 1321 	 1522

[32m iter_2[0m
ga 	p: 82.5 	r: 75.28 	f1: 78.72 	 4643 	 5628 	 6168
wo 	p: 93.87 	r: 85.95 	f1: 89.74 	 2912 	 3102 	 3388
ni 	p: 87.72 	r: 75.1 	f1: 80.92 	 1143 	 1303 	 1522
best_thres [[0.43, 0.53, 0.13], [0.28, 0.78, 0.12], [0.37, 0.67, 0.12]]
f [0.8232, 0.8236, 0.8238]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.66, 0.68, 0.13] 	 lr: 0.0001 	 f: 83.16045023173693
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 19 [0m
Train...
loss: tensor(198.8408) lr: 2.5e-05 time: 4166.48
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.26 	r: 75.41 	f1: 78.68 	 4651 	 5654 	 6168
wo 	p: 92.28 	r: 86.75 	f1: 89.43 	 2939 	 3185 	 3388
ni 	p: 86.83 	r: 75.82 	f1: 80.95 	 1154 	 1329 	 1522

[32m iter_1[0m
ga 	p: 82.58 	r: 74.94 	f1: 78.57 	 4622 	 5597 	 6168
wo 	p: 92.75 	r: 86.45 	f1: 89.49 	 2929 	 3158 	 3388
ni 	p: 85.84 	r: 77.27 	f1: 81.33 	 1176 	 1370 	 1522

[32m iter_2[0m
ga 	p: 82.15 	r: 75.23 	f1: 78.54 	 4640 	 5648 	 6168
wo 	p: 92.69 	r: 86.42 	f1: 89.45 	 2928 	 3159 	 3388
ni 	p: 85.19 	r: 77.86 	f1: 81.36 	 1185 	 1391 	 1522
best_thres [[0.48, 0.67, 0.1], [0.52, 0.74, 0.05], [0.47, 0.74, 0.04]]
f [0.8231, 0.8232, 0.823]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 10 	 [0.49, 0.73, 0.1] 	 lr: 0.0001 	 f: 82.6967764071053
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 18 [0m
Train...

[32m iter_0[0m
ga 	p: 83.27 	r: 74.14 	f1: 78.44 	 4573 	 5492 	 6168
wo 	p: 94.23 	r: 85.83 	f1: 89.84 	 2908 	 3086 	 3388
ni 	p: 86.24 	r: 76.61 	f1: 81.14 	 1166 	 1352 	 1522

[32m iter_1[0m
ga 	p: 80.92 	r: 76.61 	f1: 78.7 	 4725 	 5839 	 6168
wo 	p: 94.37 	r: 85.54 	f1: 89.74 	 2898 	 3071 	 3388
ni 	p: 87.59 	r: 76.02 	f1: 81.39 	 1157 	 1321 	 1522

[32m iter_2[0m
ga 	p: 82.5 	r: 75.28 	f1: 78.72 	 4643 	 5628 	 6168
wo 	p: 93.87 	r: 85.95 	f1: 89.74 	 2912 	 3102 	 3388
ni 	p: 87.72 	r: 75.1 	f1: 80.92 	 1143 	 1303 	 1522
best_thres [[0.43, 0.53, 0.13], [0.28, 0.78, 0.12], [0.37, 0.67, 0.12]]
f [0.8232, 0.8236, 0.8238]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.66, 0.68, 0.13] 	 lr: 0.0001 	 f: 83.16045023173693
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 19 [0m
Train...

[32m iter_0[0m
ga 	p: 80.81 	r: 76.01 	f1: 78.34 	 4688 	 5801 	 6168
wo 	p: 92.63 	r: 87.22 	f1: 89.84 	 2955 	 3190 	 3388
ni 	p: 84.9 	r: 73.52 	f1: 78.8 	 1119 	 1318 	 1522

[32m iter_1[0m
ga 	p: 82.57 	r: 74.79 	f1: 78.49 	 4613 	 5587 	 6168
wo 	p: 93.73 	r: 86.07 	f1: 89.74 	 2916 	 3111 	 3388
ni 	p: 87.57 	r: 73.59 	f1: 79.97 	 1120 	 1279 	 1522

[32m iter_2[0m
ga 	p: 82.9 	r: 74.5 	f1: 78.47 	 4595 	 5543 	 6168
wo 	p: 93.51 	r: 86.33 	f1: 89.78 	 2925 	 3128 	 3388
ni 	p: 87.97 	r: 73.0 	f1: 79.78 	 1111 	 1263 	 1522
best_thres [[0.22, 0.33, 0.13], [0.3, 0.65, 0.13], [0.32, 0.59, 0.13]]
f [0.8194, 0.8205, 0.8208]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 14 	 [0.44, 0.83, 0.08] 	 lr: 5e-05 	 f: 82.6418210429553
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 20 [0m
Train...
loss: tensor(198.0381) lr: 2.5e-05 time: 4381.04
pred_count_train 41644

Test...
loss: tensor(343.5957) lr: 2.5e-05 time: 4588.18
pred_count_train 41644

Test...
loss: tensor(63.9927) lr: 5e-05 time: 4606.71
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.71 	r: 75.47 	f1: 78.47 	 4655 	 5697 	 6168
wo 	p: 93.44 	r: 86.63 	f1: 89.91 	 2935 	 3141 	 3388
ni 	p: 86.0 	r: 75.1 	f1: 80.18 	 1143 	 1329 	 1522

[32m iter_1[0m
ga 	p: 81.55 	r: 75.89 	f1: 78.62 	 4681 	 5740 	 6168
wo 	p: 93.81 	r: 86.33 	f1: 89.92 	 2925 	 3118 	 3388
ni 	p: 88.96 	r: 74.64 	f1: 81.17 	 1136 	 1277 	 1522

[32m iter_2[0m
ga 	p: 82.03 	r: 75.5 	f1: 78.63 	 4657 	 5677 	 6168
wo 	p: 93.16 	r: 86.84 	f1: 89.89 	 2942 	 3158 	 3388
ni 	p: 88.89 	r: 74.11 	f1: 80.83 	 1128 	 1269 	 1522
best_thres [[0.34, 0.51, 0.11], [0.3, 0.78, 0.11], [0.33, 0.65, 0.11]]
f [0.8221, 0.8232, 0.8234]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 13 	 [0.44, 0.62, 0.11] 	 lr: 5e-05 	 f: 82.65825778478518
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 19 [0m
Train...
loss: tensor(343.5957) lr: 2.5e-05 time: 4570.16
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.65 	r: 75.47 	f1: 78.44 	 4655 	 5701 	 6168
wo 	p: 93.87 	r: 85.39 	f1: 89.43 	 2893 	 3082 	 3388
ni 	p: 84.83 	r: 73.13 	f1: 78.55 	 1113 	 1312 	 1522

[32m iter_1[0m
ga 	p: 81.95 	r: 75.66 	f1: 78.68 	 4667 	 5695 	 6168
wo 	p: 93.55 	r: 85.66 	f1: 89.43 	 2902 	 3102 	 3388
ni 	p: 85.29 	r: 73.92 	f1: 79.2 	 1125 	 1319 	 1522

[32m iter_2[0m
ga 	p: 83.47 	r: 74.64 	f1: 78.81 	 4604 	 5516 	 6168
wo 	p: 93.49 	r: 86.01 	f1: 89.59 	 2914 	 3117 	 3388
ni 	p: 86.31 	r: 73.32 	f1: 79.29 	 1116 	 1293 	 1522
best_thres [[0.29, 0.56, 0.12], [0.28, 0.69, 0.09], [0.35, 0.73, 0.09]]
f [0.8181, 0.8193, 0.8202]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.66, 0.68, 0.13] 	 lr: 0.0001 	 f: 83.16045023173693
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 20 [0m
Train...
loss: tensor(119.7105) lr: 2.5e-05 time: 4257.81
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.23 	r: 74.72 	f1: 78.3 	 4609 	 5605 	 6168
wo 	p: 93.4 	r: 86.04 	f1: 89.57 	 2915 	 3121 	 3388
ni 	p: 83.59 	r: 76.28 	f1: 79.77 	 1161 	 1389 	 1522

[32m iter_1[0m
ga 	p: 82.56 	r: 74.69 	f1: 78.43 	 4607 	 5580 	 6168
wo 	p: 92.93 	r: 86.51 	f1: 89.61 	 2931 	 3154 	 3388
ni 	p: 82.72 	r: 76.74 	f1: 79.62 	 1168 	 1412 	 1522

[32m iter_2[0m
ga 	p: 81.83 	r: 75.26 	f1: 78.41 	 4642 	 5673 	 6168
wo 	p: 92.94 	r: 86.63 	f1: 89.67 	 2935 	 3158 	 3388
ni 	p: 85.41 	r: 74.64 	f1: 79.66 	 1136 	 1330 	 1522
best_thres [[0.36, 0.78, 0.05], [0.34, 0.7, 0.03], [0.26, 0.7, 0.05]]
f [0.8196, 0.82, 0.8202]
load model: epoch10
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 10 	 [0.49, 0.73, 0.1] 	 lr: 0.0001 	 f: 82.6967764071053
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 19 [0m
Train...

[32m iter_0[0m
ga 	p: 81.65 	r: 75.47 	f1: 78.44 	 4655 	 5701 	 6168
wo 	p: 93.87 	r: 85.39 	f1: 89.43 	 2893 	 3082 	 3388
ni 	p: 84.83 	r: 73.13 	f1: 78.55 	 1113 	 1312 	 1522

[32m iter_1[0m
ga 	p: 81.95 	r: 75.66 	f1: 78.68 	 4667 	 5695 	 6168
wo 	p: 93.55 	r: 85.66 	f1: 89.43 	 2902 	 3102 	 3388
ni 	p: 85.29 	r: 73.92 	f1: 79.2 	 1125 	 1319 	 1522

[32m iter_2[0m
ga 	p: 83.47 	r: 74.64 	f1: 78.81 	 4604 	 5516 	 6168
wo 	p: 93.49 	r: 86.01 	f1: 89.59 	 2914 	 3117 	 3388
ni 	p: 86.31 	r: 73.32 	f1: 79.29 	 1116 	 1293 	 1522
best_thres [[0.29, 0.56, 0.12], [0.28, 0.69, 0.09], [0.35, 0.73, 0.09]]
f [0.8181, 0.8193, 0.8202]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.66, 0.68, 0.13] 	 lr: 0.0001 	 f: 83.16045023173693
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 20 [0m
Train...

[32m iter_0[0m
ga 	p: 82.1 	r: 74.58 	f1: 78.16 	 4600 	 5603 	 6168
wo 	p: 93.93 	r: 86.39 	f1: 90.01 	 2927 	 3116 	 3388
ni 	p: 83.99 	r: 73.78 	f1: 78.56 	 1123 	 1337 	 1522

[32m iter_1[0m
ga 	p: 82.68 	r: 74.76 	f1: 78.52 	 4611 	 5577 	 6168
wo 	p: 94.07 	r: 86.13 	f1: 89.92 	 2918 	 3102 	 3388
ni 	p: 85.56 	r: 73.98 	f1: 79.35 	 1126 	 1316 	 1522

[32m iter_2[0m
ga 	p: 82.65 	r: 74.71 	f1: 78.48 	 4608 	 5575 	 6168
wo 	p: 93.78 	r: 86.39 	f1: 89.94 	 2927 	 3121 	 3388
ni 	p: 83.86 	r: 75.43 	f1: 79.42 	 1148 	 1369 	 1522
best_thres [[0.33, 0.47, 0.09], [0.35, 0.64, 0.08], [0.35, 0.59, 0.05]]
f [0.8186, 0.82, 0.8205]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 14 	 [0.44, 0.83, 0.08] 	 lr: 5e-05 	 f: 82.6418210429553
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 21 [0m
Train...
loss: tensor(120.8013) lr: 2.5e-05 time: 4200.12
pred_count_train 41644

Test...
loss: tensor(250.7202) lr: 2.5e-05 time: 4457.93
pred_count_train 41644

Test...
loss: tensor(225.2299) lr: 2.5e-05 time: 4532.32
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.53 	r: 75.45 	f1: 78.38 	 4654 	 5708 	 6168
wo 	p: 92.56 	r: 87.34 	f1: 89.87 	 2959 	 3197 	 3388
ni 	p: 87.12 	r: 72.86 	f1: 79.36 	 1109 	 1273 	 1522

[32m iter_1[0m
ga 	p: 82.17 	r: 74.89 	f1: 78.36 	 4619 	 5621 	 6168
wo 	p: 93.64 	r: 86.42 	f1: 89.88 	 2928 	 3127 	 3388
ni 	p: 86.32 	r: 75.89 	f1: 80.77 	 1155 	 1338 	 1522

[32m iter_2[0m
ga 	p: 82.31 	r: 74.84 	f1: 78.4 	 4616 	 5608 	 6168
wo 	p: 93.46 	r: 86.51 	f1: 89.85 	 2931 	 3136 	 3388
ni 	p: 85.96 	r: 76.02 	f1: 80.68 	 1157 	 1346 	 1522
best_thres [[0.28, 0.3, 0.13], [0.32, 0.45, 0.06], [0.32, 0.45, 0.05]]
f [0.8207, 0.8215, 0.8218]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 13 	 [0.44, 0.62, 0.11] 	 lr: 5e-05 	 f: 82.65825778478518
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 20 [0m
Train...
loss: tensor(250.7202) lr: 2.5e-05 time: 4521.71
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.4 	r: 73.07 	f1: 77.89 	 4507 	 5404 	 6168
wo 	p: 93.69 	r: 85.51 	f1: 89.41 	 2897 	 3092 	 3388
ni 	p: 85.52 	r: 71.81 	f1: 78.07 	 1093 	 1278 	 1522

[32m iter_1[0m
ga 	p: 82.83 	r: 74.38 	f1: 78.38 	 4588 	 5539 	 6168
wo 	p: 93.9 	r: 85.39 	f1: 89.44 	 2893 	 3081 	 3388
ni 	p: 82.38 	r: 74.64 	f1: 78.32 	 1136 	 1379 	 1522

[32m iter_2[0m
ga 	p: 82.79 	r: 74.46 	f1: 78.41 	 4593 	 5548 	 6168
wo 	p: 91.93 	r: 87.1 	f1: 89.45 	 2951 	 3210 	 3388
ni 	p: 82.44 	r: 74.97 	f1: 78.53 	 1141 	 1384 	 1522
best_thres [[0.49, 0.54, 0.21], [0.45, 0.85, 0.1], [0.45, 0.32, 0.09]]
f [0.815, 0.8163, 0.8171]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.66, 0.68, 0.13] 	 lr: 0.0001 	 f: 83.16045023173693
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 21 [0m
Train...
loss: tensor(81.7854) lr: 2.5e-05 time: 4340.23
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.66 	r: 75.44 	f1: 79.34 	 4653 	 5562 	 6168
wo 	p: 92.7 	r: 86.95 	f1: 89.73 	 2946 	 3178 	 3388
ni 	p: 83.91 	r: 77.07 	f1: 80.34 	 1173 	 1398 	 1522

[32m iter_1[0m
ga 	p: 81.94 	r: 76.85 	f1: 79.31 	 4740 	 5785 	 6168
wo 	p: 93.42 	r: 86.3 	f1: 89.72 	 2924 	 3130 	 3388
ni 	p: 82.73 	r: 78.98 	f1: 80.81 	 1202 	 1453 	 1522

[32m iter_2[0m
ga 	p: 83.83 	r: 75.21 	f1: 79.29 	 4639 	 5534 	 6168
wo 	p: 93.27 	r: 86.28 	f1: 89.64 	 2923 	 3134 	 3388
ni 	p: 83.99 	r: 77.92 	f1: 80.85 	 1186 	 1412 	 1522
best_thres [[0.45, 0.57, 0.09], [0.29, 0.65, 0.06], [0.45, 0.65, 0.07]]
f [0.8269, 0.8269, 0.8269]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 10 	 [0.49, 0.73, 0.1] 	 lr: 0.0001 	 f: 82.6967764071053
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 20 [0m
Train...

[32m iter_0[0m
ga 	p: 83.4 	r: 73.07 	f1: 77.89 	 4507 	 5404 	 6168
wo 	p: 93.69 	r: 85.51 	f1: 89.41 	 2897 	 3092 	 3388
ni 	p: 85.52 	r: 71.81 	f1: 78.07 	 1093 	 1278 	 1522

[32m iter_1[0m
ga 	p: 82.83 	r: 74.38 	f1: 78.38 	 4588 	 5539 	 6168
wo 	p: 93.9 	r: 85.39 	f1: 89.44 	 2893 	 3081 	 3388
ni 	p: 82.38 	r: 74.64 	f1: 78.32 	 1136 	 1379 	 1522

[32m iter_2[0m
ga 	p: 82.79 	r: 74.46 	f1: 78.41 	 4593 	 5548 	 6168
wo 	p: 91.93 	r: 87.1 	f1: 89.45 	 2951 	 3210 	 3388
ni 	p: 82.44 	r: 74.97 	f1: 78.53 	 1141 	 1384 	 1522
best_thres [[0.49, 0.54, 0.21], [0.45, 0.85, 0.1], [0.45, 0.32, 0.09]]
f [0.815, 0.8163, 0.8171]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.66, 0.68, 0.13] 	 lr: 0.0001 	 f: 83.16045023173693
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 21 [0m
Train...

[32m iter_0[0m
ga 	p: 78.87 	r: 77.27 	f1: 78.06 	 4766 	 6043 	 6168
wo 	p: 93.45 	r: 86.33 	f1: 89.75 	 2925 	 3130 	 3388
ni 	p: 84.42 	r: 73.0 	f1: 78.29 	 1111 	 1316 	 1522

[32m iter_1[0m
ga 	p: 81.52 	r: 75.1 	f1: 78.18 	 4632 	 5682 	 6168
wo 	p: 93.53 	r: 86.16 	f1: 89.69 	 2919 	 3121 	 3388
ni 	p: 85.82 	r: 73.19 	f1: 79.01 	 1114 	 1298 	 1522

[32m iter_2[0m
ga 	p: 79.66 	r: 76.88 	f1: 78.24 	 4742 	 5953 	 6168
wo 	p: 93.65 	r: 86.13 	f1: 89.73 	 2918 	 3116 	 3388
ni 	p: 85.81 	r: 73.13 	f1: 78.96 	 1113 	 1297 	 1522
best_thres [[0.14, 0.49, 0.08], [0.28, 0.69, 0.07], [0.14, 0.84, 0.06]]
f [0.8162, 0.8172, 0.8176]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 14 	 [0.44, 0.83, 0.08] 	 lr: 5e-05 	 f: 82.6418210429553
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 22 [0m
Train...
loss: tensor(80.2697) lr: 2.5e-05 time: 4194.19
pred_count_train 41644

Test...
loss: tensor(621.3808) lr: 1.25e-05 time: 4465.55
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.65 	r: 75.32 	f1: 78.36 	 4646 	 5690 	 6168
wo 	p: 92.97 	r: 86.72 	f1: 89.74 	 2938 	 3160 	 3388
ni 	p: 84.21 	r: 72.86 	f1: 78.13 	 1109 	 1317 	 1522

[32m iter_1[0m
ga 	p: 82.72 	r: 74.48 	f1: 78.38 	 4594 	 5554 	 6168
wo 	p: 93.43 	r: 86.42 	f1: 89.79 	 2928 	 3134 	 3388
ni 	p: 88.39 	r: 71.55 	f1: 79.08 	 1089 	 1232 	 1522

[32m iter_2[0m
ga 	p: 82.7 	r: 74.55 	f1: 78.41 	 4598 	 5560 	 6168
wo 	p: 93.43 	r: 86.45 	f1: 89.81 	 2929 	 3135 	 3388
ni 	p: 88.29 	r: 71.81 	f1: 79.2 	 1093 	 1238 	 1522
best_thres [[0.26, 0.34, 0.1], [0.35, 0.41, 0.13], [0.35, 0.43, 0.11]]
f [0.8184, 0.8193, 0.8197]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 13 	 [0.44, 0.62, 0.11] 	 lr: 5e-05 	 f: 82.65825778478518
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 21 [0m
Train...
loss: tensor(122.5687) lr: 2.5e-05 time: 4578.74
pred_count_train 41644

Test...
loss: tensor(621.3808) lr: 1.25e-05 time: 4635.48
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.27 	r: 75.92 	f1: 78.97 	 4683 	 5692 	 6168
wo 	p: 92.96 	r: 87.34 	f1: 90.06 	 2959 	 3183 	 3388
ni 	p: 88.81 	r: 76.68 	f1: 82.3 	 1167 	 1314 	 1522

[32m iter_1[0m
ga 	p: 84.25 	r: 74.56 	f1: 79.11 	 4599 	 5459 	 6168
wo 	p: 93.97 	r: 86.48 	f1: 90.07 	 2930 	 3118 	 3388
ni 	p: 89.65 	r: 76.28 	f1: 82.43 	 1161 	 1295 	 1522

[32m iter_2[0m
ga 	p: 85.22 	r: 73.83 	f1: 79.12 	 4554 	 5344 	 6168
wo 	p: 94.28 	r: 86.13 	f1: 90.02 	 2918 	 3095 	 3388
ni 	p: 90.73 	r: 75.23 	f1: 82.26 	 1145 	 1262 	 1522
best_thres [[0.4, 0.45, 0.2], [0.54, 0.63, 0.19], [0.7, 0.73, 0.22]]
f [0.8284, 0.829, 0.8291]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.66, 0.68, 0.13] 	 lr: 0.0001 	 f: 83.16045023173693
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 22 [0m
Train...
loss: tensor(60.0518) lr: 2.5e-05 time: 4379.0
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 84.49 	r: 73.65 	f1: 78.7 	 4543 	 5377 	 6168
wo 	p: 93.37 	r: 86.48 	f1: 89.79 	 2930 	 3138 	 3388
ni 	p: 84.04 	r: 75.43 	f1: 79.5 	 1148 	 1366 	 1522

[32m iter_1[0m
ga 	p: 84.15 	r: 74.09 	f1: 78.8 	 4570 	 5431 	 6168
wo 	p: 92.72 	r: 87.16 	f1: 89.85 	 2953 	 3185 	 3388
ni 	p: 86.44 	r: 74.97 	f1: 80.3 	 1141 	 1320 	 1522

[32m iter_2[0m
ga 	p: 83.99 	r: 74.09 	f1: 78.73 	 4570 	 5441 	 6168
wo 	p: 92.71 	r: 87.07 	f1: 89.8 	 2950 	 3182 	 3388
ni 	p: 86.66 	r: 75.1 	f1: 80.46 	 1143 	 1319 	 1522
best_thres [[0.6, 0.67, 0.1], [0.55, 0.49, 0.11], [0.54, 0.49, 0.11]]
f [0.8227, 0.8236, 0.8238]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 10 	 [0.49, 0.73, 0.1] 	 lr: 0.0001 	 f: 82.6967764071053
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 21 [0m
Train...

[32m iter_0[0m
ga 	p: 82.27 	r: 75.92 	f1: 78.97 	 4683 	 5692 	 6168
wo 	p: 92.96 	r: 87.34 	f1: 90.06 	 2959 	 3183 	 3388
ni 	p: 88.81 	r: 76.68 	f1: 82.3 	 1167 	 1314 	 1522

[32m iter_1[0m
ga 	p: 84.25 	r: 74.56 	f1: 79.11 	 4599 	 5459 	 6168
wo 	p: 93.97 	r: 86.48 	f1: 90.07 	 2930 	 3118 	 3388
ni 	p: 89.65 	r: 76.28 	f1: 82.43 	 1161 	 1295 	 1522

[32m iter_2[0m
ga 	p: 85.22 	r: 73.83 	f1: 79.12 	 4554 	 5344 	 6168
wo 	p: 94.28 	r: 86.13 	f1: 90.02 	 2918 	 3095 	 3388
ni 	p: 90.73 	r: 75.23 	f1: 82.26 	 1145 	 1262 	 1522
best_thres [[0.4, 0.45, 0.2], [0.54, 0.63, 0.19], [0.7, 0.73, 0.22]]
f [0.8284, 0.829, 0.8291]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.66, 0.68, 0.13] 	 lr: 0.0001 	 f: 83.16045023173693
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 22 [0m
Train...

[32m iter_0[0m
ga 	p: 81.03 	r: 75.83 	f1: 78.34 	 4677 	 5772 	 6168
wo 	p: 93.61 	r: 86.01 	f1: 89.65 	 2914 	 3113 	 3388
ni 	p: 85.71 	r: 74.51 	f1: 79.72 	 1134 	 1323 	 1522

[32m iter_1[0m
ga 	p: 80.94 	r: 76.15 	f1: 78.47 	 4697 	 5803 	 6168
wo 	p: 93.19 	r: 86.07 	f1: 89.49 	 2916 	 3129 	 3388
ni 	p: 87.29 	r: 74.44 	f1: 80.35 	 1133 	 1298 	 1522

[32m iter_2[0m
ga 	p: 80.95 	r: 76.13 	f1: 78.47 	 4696 	 5801 	 6168
wo 	p: 92.81 	r: 86.16 	f1: 89.36 	 2919 	 3145 	 3388
ni 	p: 89.16 	r: 73.52 	f1: 80.59 	 1119 	 1255 	 1522
best_thres [[0.32, 0.61, 0.17], [0.29, 0.8, 0.17], [0.29, 0.83, 0.24]]
f [0.8198, 0.8204, 0.8205]
load model: epoch14
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 14 	 [0.44, 0.83, 0.08] 	 lr: 5e-05 	 f: 82.6418210429553
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 23 [0m
Train...
loss: tensor(57.5756) lr: 2.5e-05 time: 4182.23
pred_count_train 41644

Test...
loss: tensor(508.3573) lr: 1.25e-05 time: 4427.43
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.76 	r: 76.28 	f1: 77.98 	 4705 	 5899 	 6168
wo 	p: 93.4 	r: 86.51 	f1: 89.83 	 2931 	 3138 	 3388
ni 	p: 84.91 	r: 73.59 	f1: 78.85 	 1120 	 1319 	 1522

[32m iter_1[0m
ga 	p: 83.27 	r: 73.65 	f1: 78.17 	 4543 	 5456 	 6168
wo 	p: 93.58 	r: 86.07 	f1: 89.67 	 2916 	 3116 	 3388
ni 	p: 86.33 	r: 74.24 	f1: 79.83 	 1130 	 1309 	 1522

[32m iter_2[0m
ga 	p: 83.49 	r: 73.62 	f1: 78.25 	 4541 	 5439 	 6168
wo 	p: 93.11 	r: 86.19 	f1: 89.52 	 2920 	 3136 	 3388
ni 	p: 86.09 	r: 74.44 	f1: 79.84 	 1133 	 1316 	 1522
best_thres [[0.16, 0.51, 0.12], [0.46, 0.79, 0.08], [0.5, 0.77, 0.07]]
f [0.817, 0.8183, 0.8188]
load model: epoch13
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 13 	 [0.44, 0.62, 0.11] 	 lr: 5e-05 	 f: 82.65825778478518
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 22 [0m
Train...
loss: tensor(74.3150) lr: 2.5e-05 time: 4537.83
pred_count_train 41644

Test...
loss: tensor(508.3573) lr: 1.25e-05 time: 4489.82
pred_count_train 41644

Test...
loss: tensor(189.4549) lr: 1.25e-05 time: 4195.5
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.63 	r: 76.69 	f1: 78.61 	 4730 	 5866 	 6168
wo 	p: 93.48 	r: 86.75 	f1: 89.99 	 2939 	 3144 	 3388
ni 	p: 86.81 	r: 77.0 	f1: 81.62 	 1172 	 1350 	 1522

[32m iter_1[0m
ga 	p: 83.36 	r: 74.9 	f1: 78.91 	 4620 	 5542 	 6168
wo 	p: 94.81 	r: 85.68 	f1: 90.02 	 2903 	 3062 	 3388
ni 	p: 88.24 	r: 75.95 	f1: 81.64 	 1156 	 1310 	 1522

[32m iter_2[0m
ga 	p: 81.91 	r: 76.07 	f1: 78.88 	 4692 	 5728 	 6168
wo 	p: 94.63 	r: 85.89 	f1: 90.05 	 2910 	 3075 	 3388
ni 	p: 85.87 	r: 77.46 	f1: 81.45 	 1179 	 1373 	 1522
best_thres [[0.33, 0.46, 0.2], [0.47, 0.8, 0.17], [0.37, 0.78, 0.13]]
f [0.8248, 0.8258, 0.826]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.66, 0.68, 0.13] 	 lr: 0.0001 	 f: 83.16045023173693
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 23 [0m
Train...

[32m iter_0[0m
ga 	p: 81.22 	r: 76.09 	f1: 78.57 	 4693 	 5778 	 6168
wo 	p: 92.5 	r: 87.01 	f1: 89.67 	 2948 	 3187 	 3388
ni 	p: 84.99 	r: 77.0 	f1: 80.8 	 1172 	 1379 	 1522

[32m iter_1[0m
ga 	p: 81.82 	r: 76.15 	f1: 78.88 	 4697 	 5741 	 6168
wo 	p: 92.63 	r: 86.84 	f1: 89.64 	 2942 	 3176 	 3388
ni 	p: 84.98 	r: 77.33 	f1: 80.98 	 1177 	 1385 	 1522

[32m iter_2[0m
ga 	p: 81.82 	r: 76.04 	f1: 78.82 	 4690 	 5732 	 6168
wo 	p: 92.52 	r: 86.84 	f1: 89.59 	 2942 	 3180 	 3388
ni 	p: 85.99 	r: 76.61 	f1: 81.03 	 1166 	 1356 	 1522
best_thres [[0.31, 0.49, 0.09], [0.31, 0.52, 0.07], [0.31, 0.5, 0.08]]
f [0.8228, 0.8237, 0.8239]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 10 	 [0.49, 0.73, 0.1] 	 lr: 0.0001 	 f: 82.6967764071053
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 22 [0m
Train...

[32m iter_0[0m
ga 	p: 80.63 	r: 76.69 	f1: 78.61 	 4730 	 5866 	 6168
wo 	p: 93.48 	r: 86.75 	f1: 89.99 	 2939 	 3144 	 3388
ni 	p: 86.81 	r: 77.0 	f1: 81.62 	 1172 	 1350 	 1522

[32m iter_1[0m
ga 	p: 83.36 	r: 74.9 	f1: 78.91 	 4620 	 5542 	 6168
wo 	p: 94.81 	r: 85.68 	f1: 90.02 	 2903 	 3062 	 3388
ni 	p: 88.24 	r: 75.95 	f1: 81.64 	 1156 	 1310 	 1522

[32m iter_2[0m
ga 	p: 81.91 	r: 76.07 	f1: 78.88 	 4692 	 5728 	 6168
wo 	p: 94.63 	r: 85.89 	f1: 90.05 	 2910 	 3075 	 3388
ni 	p: 85.87 	r: 77.46 	f1: 81.45 	 1179 	 1373 	 1522
best_thres [[0.33, 0.46, 0.2], [0.47, 0.8, 0.17], [0.37, 0.78, 0.13]]
f [0.8248, 0.8258, 0.826]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.66, 0.68, 0.13] 	 lr: 0.0001 	 f: 83.16045023173693
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 23 [0m
Train...

[32m iter_0[0m
ga 	p: 80.87 	r: 76.62 	f1: 78.69 	 4726 	 5844 	 6168
wo 	p: 94.18 	r: 85.92 	f1: 89.86 	 2911 	 3091 	 3388
ni 	p: 85.89 	r: 73.98 	f1: 79.49 	 1126 	 1311 	 1522

[32m iter_1[0m
ga 	p: 82.67 	r: 75.1 	f1: 78.7 	 4632 	 5603 	 6168
wo 	p: 93.59 	r: 86.25 	f1: 89.77 	 2922 	 3122 	 3388
ni 	p: 89.68 	r: 73.06 	f1: 80.52 	 1112 	 1240 	 1522

[32m iter_2[0m
ga 	p: 83.19 	r: 74.64 	f1: 78.69 	 4604 	 5534 	 6168
wo 	p: 93.47 	r: 86.25 	f1: 89.71 	 2922 	 3126 	 3388
ni 	p: 88.83 	r: 73.13 	f1: 80.22 	 1113 	 1253 	 1522
best_thres [[0.28, 0.6, 0.12], [0.41, 0.67, 0.13], [0.48, 0.76, 0.11]]
f [0.8219, 0.8228, 0.8229]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 14 	 [0.44, 0.83, 0.08] 	 lr: 5e-05 	 f: 82.6418210429553
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 24 [0m
Train...
loss: tensor(187.4377) lr: 1.25e-05 time: 3496.91
pred_count_train 41644

Test...
loss: tensor(424.2790) lr: 1.25e-05 time: 3625.37
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.42 	r: 75.34 	f1: 78.72 	 4647 	 5638 	 6168
wo 	p: 93.88 	r: 86.42 	f1: 90.0 	 2928 	 3119 	 3388
ni 	p: 87.99 	r: 72.67 	f1: 79.6 	 1106 	 1257 	 1522

[32m iter_1[0m
ga 	p: 82.61 	r: 75.39 	f1: 78.83 	 4650 	 5629 	 6168
wo 	p: 93.89 	r: 86.13 	f1: 89.84 	 2918 	 3108 	 3388
ni 	p: 90.62 	r: 72.34 	f1: 80.45 	 1101 	 1215 	 1522

[32m iter_2[0m
ga 	p: 81.61 	r: 76.17 	f1: 78.79 	 4698 	 5757 	 6168
wo 	p: 93.59 	r: 86.25 	f1: 89.77 	 2922 	 3122 	 3388
ni 	p: 87.57 	r: 74.51 	f1: 80.51 	 1134 	 1295 	 1522
best_thres [[0.36, 0.45, 0.19], [0.36, 0.64, 0.2], [0.28, 0.61, 0.11]]
f [0.8232, 0.8238, 0.8238]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 13 	 [0.44, 0.62, 0.11] 	 lr: 5e-05 	 f: 82.65825778478518
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 23 [0m
Train...
loss: tensor(47.6218) lr: 2.5e-05 time: 3805.54
pred_count_train 41644

Test...
loss: tensor(424.2790) lr: 1.25e-05 time: 3815.37
pred_count_train 41644

Test...
loss: tensor(125.5761) lr: 1.25e-05 time: 3677.65
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.4 	r: 74.38 	f1: 78.19 	 4588 	 5568 	 6168
wo 	p: 93.73 	r: 86.1 	f1: 89.75 	 2917 	 3112 	 3388
ni 	p: 85.42 	r: 78.12 	f1: 81.61 	 1189 	 1392 	 1522

[32m iter_1[0m
ga 	p: 83.29 	r: 74.27 	f1: 78.52 	 4581 	 5500 	 6168
wo 	p: 94.26 	r: 85.83 	f1: 89.85 	 2908 	 3085 	 3388
ni 	p: 88.66 	r: 75.49 	f1: 81.55 	 1149 	 1296 	 1522

[32m iter_2[0m
ga 	p: 83.09 	r: 74.48 	f1: 78.55 	 4594 	 5529 	 6168
wo 	p: 94.26 	r: 85.8 	f1: 89.83 	 2907 	 3084 	 3388
ni 	p: 85.37 	r: 78.19 	f1: 81.62 	 1190 	 1394 	 1522
best_thres [[0.44, 0.54, 0.17], [0.5, 0.82, 0.18], [0.48, 0.85, 0.11]]
f [0.8221, 0.8232, 0.8236]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.66, 0.68, 0.13] 	 lr: 0.0001 	 f: 83.16045023173693
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 24 [0m
Train...

[32m iter_0[0m
ga 	p: 82.98 	r: 74.51 	f1: 78.52 	 4596 	 5539 	 6168
wo 	p: 92.88 	r: 86.98 	f1: 89.83 	 2947 	 3173 	 3388
ni 	p: 83.9 	r: 78.38 	f1: 81.05 	 1193 	 1422 	 1522

[32m iter_1[0m
ga 	p: 83.78 	r: 74.55 	f1: 78.89 	 4598 	 5488 	 6168
wo 	p: 92.38 	r: 87.25 	f1: 89.74 	 2956 	 3200 	 3388
ni 	p: 86.97 	r: 77.2 	f1: 81.8 	 1175 	 1351 	 1522

[32m iter_2[0m
ga 	p: 83.46 	r: 74.63 	f1: 78.8 	 4603 	 5515 	 6168
wo 	p: 92.57 	r: 87.13 	f1: 89.77 	 2952 	 3189 	 3388
ni 	p: 87.08 	r: 77.07 	f1: 81.77 	 1173 	 1347 	 1522
best_thres [[0.5, 0.49, 0.09], [0.54, 0.39, 0.13], [0.52, 0.42, 0.13]]
f [0.8237, 0.8252, 0.8255]
load model: epoch10
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 10 	 [0.49, 0.73, 0.1] 	 lr: 0.0001 	 f: 82.6967764071053
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 23 [0m
Train...

[32m iter_0[0m
ga 	p: 82.4 	r: 74.38 	f1: 78.19 	 4588 	 5568 	 6168
wo 	p: 93.73 	r: 86.1 	f1: 89.75 	 2917 	 3112 	 3388
ni 	p: 85.42 	r: 78.12 	f1: 81.61 	 1189 	 1392 	 1522

[32m iter_1[0m
ga 	p: 83.29 	r: 74.27 	f1: 78.52 	 4581 	 5500 	 6168
wo 	p: 94.26 	r: 85.83 	f1: 89.85 	 2908 	 3085 	 3388
ni 	p: 88.66 	r: 75.49 	f1: 81.55 	 1149 	 1296 	 1522

[32m iter_2[0m
ga 	p: 83.09 	r: 74.48 	f1: 78.55 	 4594 	 5529 	 6168
wo 	p: 94.26 	r: 85.8 	f1: 89.83 	 2907 	 3084 	 3388
ni 	p: 85.37 	r: 78.19 	f1: 81.62 	 1190 	 1394 	 1522
best_thres [[0.44, 0.54, 0.17], [0.5, 0.82, 0.18], [0.48, 0.85, 0.11]]
f [0.8221, 0.8232, 0.8236]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.66, 0.68, 0.13] 	 lr: 0.0001 	 f: 83.16045023173693
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 24 [0m
Train...

[32m iter_0[0m
ga 	p: 80.87 	r: 76.02 	f1: 78.37 	 4689 	 5798 	 6168
wo 	p: 93.25 	r: 86.81 	f1: 89.91 	 2941 	 3154 	 3388
ni 	p: 87.54 	r: 72.01 	f1: 79.02 	 1096 	 1252 	 1522

[32m iter_1[0m
ga 	p: 82.11 	r: 74.92 	f1: 78.35 	 4621 	 5628 	 6168
wo 	p: 94.23 	r: 85.74 	f1: 89.79 	 2905 	 3083 	 3388
ni 	p: 89.58 	r: 72.27 	f1: 80.0 	 1100 	 1228 	 1522

[32m iter_2[0m
ga 	p: 82.17 	r: 74.87 	f1: 78.35 	 4618 	 5620 	 6168
wo 	p: 93.45 	r: 86.39 	f1: 89.79 	 2927 	 3132 	 3388
ni 	p: 86.82 	r: 74.44 	f1: 80.16 	 1133 	 1305 	 1522
best_thres [[0.25, 0.34, 0.16], [0.32, 0.59, 0.14], [0.32, 0.39, 0.08]]
f [0.82, 0.8204, 0.8207]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 14 	 [0.44, 0.83, 0.08] 	 lr: 5e-05 	 f: 82.6418210429553
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 25 [0m
Train...
loss: tensor(127.6415) lr: 1.25e-05 time: 3433.44
pred_count_train 41644

Test...
loss: tensor(354.3726) lr: 1.25e-05 time: 3637.08
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.22 	r: 74.9 	f1: 78.39 	 4620 	 5619 	 6168
wo 	p: 94.71 	r: 85.54 	f1: 89.89 	 2898 	 3060 	 3388
ni 	p: 85.79 	r: 72.21 	f1: 78.42 	 1099 	 1281 	 1522

[32m iter_1[0m
ga 	p: 81.98 	r: 75.39 	f1: 78.55 	 4650 	 5672 	 6168
wo 	p: 93.8 	r: 86.13 	f1: 89.8 	 2918 	 3111 	 3388
ni 	p: 88.92 	r: 71.68 	f1: 79.37 	 1091 	 1227 	 1522

[32m iter_2[0m
ga 	p: 81.71 	r: 75.57 	f1: 78.52 	 4661 	 5704 	 6168
wo 	p: 93.82 	r: 86.1 	f1: 89.8 	 2917 	 3109 	 3388
ni 	p: 86.65 	r: 73.32 	f1: 79.43 	 1116 	 1288 	 1522
best_thres [[0.34, 0.62, 0.13], [0.31, 0.63, 0.14], [0.29, 0.74, 0.09]]
f [0.8192, 0.8202, 0.8205]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 13 	 [0.44, 0.62, 0.11] 	 lr: 5e-05 	 f: 82.65825778478518
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 24 [0m
Train...
loss: tensor(224.9615) lr: 1.25e-05 time: 3850.94
pred_count_train 41644

Test...
loss: tensor(354.3726) lr: 1.25e-05 time: 3856.08
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.72 	r: 74.5 	f1: 77.94 	 4595 	 5623 	 6168
wo 	p: 94.13 	r: 85.6 	f1: 89.66 	 2900 	 3081 	 3388
ni 	p: 83.68 	r: 77.14 	f1: 80.27 	 1174 	 1403 	 1522

[32m iter_1[0m
ga 	p: 82.96 	r: 73.98 	f1: 78.21 	 4563 	 5500 	 6168
wo 	p: 94.23 	r: 85.39 	f1: 89.59 	 2893 	 3070 	 3388
ni 	p: 87.2 	r: 75.23 	f1: 80.78 	 1145 	 1313 	 1522

[32m iter_2[0m
ga 	p: 82.69 	r: 74.29 	f1: 78.26 	 4582 	 5541 	 6168
wo 	p: 93.1 	r: 86.36 	f1: 89.6 	 2926 	 3143 	 3388
ni 	p: 86.36 	r: 75.3 	f1: 80.45 	 1146 	 1327 	 1522
best_thres [[0.37, 0.55, 0.13], [0.42, 0.83, 0.12], [0.4, 0.46, 0.11]]
f [0.8184, 0.8195, 0.8199]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.66, 0.68, 0.13] 	 lr: 0.0001 	 f: 83.16045023173693
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 25 [0m
Train...
loss: tensor(92.1778) lr: 1.25e-05 time: 3740.49
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.51 	r: 75.54 	f1: 79.32 	 4659 	 5579 	 6168
wo 	p: 94.29 	r: 85.8 	f1: 89.85 	 2907 	 3083 	 3388
ni 	p: 85.17 	r: 77.0 	f1: 80.88 	 1172 	 1376 	 1522

[32m iter_1[0m
ga 	p: 83.11 	r: 75.88 	f1: 79.33 	 4680 	 5631 	 6168
wo 	p: 94.32 	r: 85.74 	f1: 89.83 	 2905 	 3080 	 3388
ni 	p: 83.4 	r: 79.24 	f1: 81.27 	 1206 	 1446 	 1522

[32m iter_2[0m
ga 	p: 83.23 	r: 75.78 	f1: 79.33 	 4674 	 5616 	 6168
wo 	p: 94.23 	r: 85.77 	f1: 89.8 	 2906 	 3084 	 3388
ni 	p: 83.61 	r: 79.11 	f1: 81.3 	 1204 	 1440 	 1522
best_thres [[0.47, 0.75, 0.11], [0.43, 0.75, 0.07], [0.43, 0.75, 0.07]]
f [0.8276, 0.8278, 0.8279]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 23 	 [0.43, 0.75, 0.07] 	 lr: 1.25e-05 	 f: 82.78563450738567
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 24 [0m
Train...

[32m iter_0[0m
ga 	p: 81.72 	r: 74.5 	f1: 77.94 	 4595 	 5623 	 6168
wo 	p: 94.13 	r: 85.6 	f1: 89.66 	 2900 	 3081 	 3388
ni 	p: 83.68 	r: 77.14 	f1: 80.27 	 1174 	 1403 	 1522

[32m iter_1[0m
ga 	p: 82.96 	r: 73.98 	f1: 78.21 	 4563 	 5500 	 6168
wo 	p: 94.23 	r: 85.39 	f1: 89.59 	 2893 	 3070 	 3388
ni 	p: 87.2 	r: 75.23 	f1: 80.78 	 1145 	 1313 	 1522

[32m iter_2[0m
ga 	p: 82.69 	r: 74.29 	f1: 78.26 	 4582 	 5541 	 6168
wo 	p: 93.1 	r: 86.36 	f1: 89.6 	 2926 	 3143 	 3388
ni 	p: 86.36 	r: 75.3 	f1: 80.45 	 1146 	 1327 	 1522
best_thres [[0.37, 0.55, 0.13], [0.42, 0.83, 0.12], [0.4, 0.46, 0.11]]
f [0.8184, 0.8195, 0.8199]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.66, 0.68, 0.13] 	 lr: 0.0001 	 f: 83.16045023173693
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 25 [0m
Train...

[32m iter_0[0m
ga 	p: 83.23 	r: 73.95 	f1: 78.31 	 4561 	 5480 	 6168
wo 	p: 93.98 	r: 86.13 	f1: 89.88 	 2918 	 3105 	 3388
ni 	p: 86.18 	r: 71.68 	f1: 78.26 	 1091 	 1266 	 1522

[32m iter_1[0m
ga 	p: 83.34 	r: 74.21 	f1: 78.51 	 4577 	 5492 	 6168
wo 	p: 94.18 	r: 85.95 	f1: 89.88 	 2912 	 3092 	 3388
ni 	p: 87.52 	r: 71.88 	f1: 78.93 	 1094 	 1250 	 1522

[32m iter_2[0m
ga 	p: 82.83 	r: 74.4 	f1: 78.39 	 4589 	 5540 	 6168
wo 	p: 93.52 	r: 86.51 	f1: 89.88 	 2931 	 3134 	 3388
ni 	p: 86.18 	r: 72.93 	f1: 79.0 	 1110 	 1288 	 1522
best_thres [[0.42, 0.43, 0.14], [0.45, 0.55, 0.12], [0.4, 0.43, 0.08]]
f [0.819, 0.8199, 0.8201]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 14 	 [0.44, 0.83, 0.08] 	 lr: 5e-05 	 f: 82.6418210429553
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 26 [0m
Train...
loss: tensor(90.9476) lr: 1.25e-05 time: 3355.09
pred_count_train 41644

Test...
loss: tensor(623.7809) lr: 1e-05 time: 3449.18
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.99 	r: 75.05 	f1: 78.36 	 4629 	 5646 	 6168
wo 	p: 93.95 	r: 86.22 	f1: 89.92 	 2921 	 3109 	 3388
ni 	p: 84.96 	r: 73.13 	f1: 78.6 	 1113 	 1310 	 1522

[32m iter_1[0m
ga 	p: 82.32 	r: 74.9 	f1: 78.44 	 4620 	 5612 	 6168
wo 	p: 94.03 	r: 85.98 	f1: 89.82 	 2913 	 3098 	 3388
ni 	p: 88.47 	r: 72.6 	f1: 79.75 	 1105 	 1249 	 1522

[32m iter_2[0m
ga 	p: 82.92 	r: 74.38 	f1: 78.42 	 4588 	 5533 	 6168
wo 	p: 94.14 	r: 85.89 	f1: 89.83 	 2910 	 3091 	 3388
ni 	p: 84.81 	r: 75.23 	f1: 79.74 	 1145 	 1350 	 1522
best_thres [[0.32, 0.4, 0.1], [0.33, 0.5, 0.11], [0.39, 0.66, 0.05]]
f [0.8195, 0.8203, 0.8206]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 13 	 [0.44, 0.62, 0.11] 	 lr: 5e-05 	 f: 82.65825778478518
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 25 [0m
Train...
loss: tensor(144.4278) lr: 1.25e-05 time: 3687.8
pred_count_train 41644

Test...
loss: tensor(623.7809) lr: 1e-05 time: 3704.17
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.52 	r: 75.08 	f1: 79.07 	 4631 	 5545 	 6168
wo 	p: 93.66 	r: 86.72 	f1: 90.05 	 2938 	 3137 	 3388
ni 	p: 84.87 	r: 79.63 	f1: 82.17 	 1212 	 1428 	 1522

[32m iter_1[0m
ga 	p: 83.96 	r: 75.16 	f1: 79.32 	 4636 	 5522 	 6168
wo 	p: 93.63 	r: 86.75 	f1: 90.06 	 2939 	 3139 	 3388
ni 	p: 84.78 	r: 79.76 	f1: 82.19 	 1214 	 1432 	 1522

[32m iter_2[0m
ga 	p: 83.85 	r: 75.24 	f1: 79.31 	 4641 	 5535 	 6168
wo 	p: 93.3 	r: 87.1 	f1: 90.09 	 2951 	 3163 	 3388
ni 	p: 86.71 	r: 78.45 	f1: 82.37 	 1194 	 1377 	 1522
best_thres [[0.47, 0.45, 0.14], [0.52, 0.46, 0.11], [0.54, 0.42, 0.13]]
f [0.8289, 0.8296, 0.83]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.66, 0.68, 0.13] 	 lr: 0.0001 	 f: 83.16045023173693
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 26 [0m
Train...
loss: tensor(65.6777) lr: 1.25e-05 time: 3633.03
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.63 	r: 75.5 	f1: 78.91 	 4657 	 5636 	 6168
wo 	p: 92.23 	r: 87.25 	f1: 89.67 	 2956 	 3205 	 3388
ni 	p: 84.97 	r: 75.43 	f1: 79.92 	 1148 	 1351 	 1522

[32m iter_1[0m
ga 	p: 82.62 	r: 75.68 	f1: 79.0 	 4668 	 5650 	 6168
wo 	p: 93.1 	r: 86.45 	f1: 89.65 	 2929 	 3146 	 3388
ni 	p: 83.52 	r: 77.6 	f1: 80.45 	 1181 	 1414 	 1522

[32m iter_2[0m
ga 	p: 82.65 	r: 75.6 	f1: 78.97 	 4663 	 5642 	 6168
wo 	p: 92.96 	r: 86.51 	f1: 89.62 	 2931 	 3153 	 3388
ni 	p: 83.65 	r: 77.33 	f1: 80.37 	 1177 	 1407 	 1522
best_thres [[0.42, 0.39, 0.1], [0.39, 0.51, 0.06], [0.39, 0.49, 0.06]]
f [0.8238, 0.8242, 0.8243]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 23 	 [0.43, 0.75, 0.07] 	 lr: 1.25e-05 	 f: 82.78563450738567
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 25 [0m
Train...

[32m iter_0[0m
ga 	p: 83.52 	r: 75.08 	f1: 79.07 	 4631 	 5545 	 6168
wo 	p: 93.66 	r: 86.72 	f1: 90.05 	 2938 	 3137 	 3388
ni 	p: 84.87 	r: 79.63 	f1: 82.17 	 1212 	 1428 	 1522

[32m iter_1[0m
ga 	p: 83.96 	r: 75.16 	f1: 79.32 	 4636 	 5522 	 6168
wo 	p: 93.63 	r: 86.75 	f1: 90.06 	 2939 	 3139 	 3388
ni 	p: 84.78 	r: 79.76 	f1: 82.19 	 1214 	 1432 	 1522

[32m iter_2[0m
ga 	p: 83.85 	r: 75.24 	f1: 79.31 	 4641 	 5535 	 6168
wo 	p: 93.3 	r: 87.1 	f1: 90.09 	 2951 	 3163 	 3388
ni 	p: 86.71 	r: 78.45 	f1: 82.37 	 1194 	 1377 	 1522
best_thres [[0.47, 0.45, 0.14], [0.52, 0.46, 0.11], [0.54, 0.42, 0.13]]
f [0.8289, 0.8296, 0.83]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.66, 0.68, 0.13] 	 lr: 0.0001 	 f: 83.16045023173693
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 26 [0m
Train...

[32m iter_0[0m
ga 	p: 82.88 	r: 73.88 	f1: 78.12 	 4557 	 5498 	 6168
wo 	p: 93.44 	r: 86.54 	f1: 89.86 	 2932 	 3138 	 3388
ni 	p: 86.69 	r: 71.48 	f1: 78.36 	 1088 	 1255 	 1522

[32m iter_1[0m
ga 	p: 83.78 	r: 73.44 	f1: 78.27 	 4530 	 5407 	 6168
wo 	p: 93.64 	r: 86.42 	f1: 89.88 	 2928 	 3127 	 3388
ni 	p: 85.47 	r: 74.18 	f1: 79.42 	 1129 	 1321 	 1522

[32m iter_2[0m
ga 	p: 83.62 	r: 73.49 	f1: 78.23 	 4533 	 5421 	 6168
wo 	p: 93.55 	r: 86.51 	f1: 89.89 	 2931 	 3133 	 3388
ni 	p: 86.01 	r: 73.92 	f1: 79.51 	 1125 	 1308 	 1522
best_thres [[0.34, 0.36, 0.11], [0.42, 0.4, 0.05], [0.41, 0.4, 0.05]]
f [0.8181, 0.8192, 0.8196]
load model: epoch14
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 14 	 [0.44, 0.83, 0.08] 	 lr: 5e-05 	 f: 82.6418210429553
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 27 [0m
Train...
loss: tensor(64.4625) lr: 1.25e-05 time: 3378.7
pred_count_train 41644

Test...
loss: tensor(523.4798) lr: 1e-05 time: 3493.61
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.15 	r: 75.71 	f1: 78.34 	 4670 	 5755 	 6168
wo 	p: 94.3 	r: 85.92 	f1: 89.92 	 2911 	 3087 	 3388
ni 	p: 85.49 	r: 71.62 	f1: 77.94 	 1090 	 1275 	 1522

[32m iter_1[0m
ga 	p: 83.54 	r: 74.04 	f1: 78.5 	 4567 	 5467 	 6168
wo 	p: 94.0 	r: 86.07 	f1: 89.86 	 2916 	 3102 	 3388
ni 	p: 86.07 	r: 72.67 	f1: 78.8 	 1106 	 1285 	 1522

[32m iter_2[0m
ga 	p: 83.47 	r: 73.93 	f1: 78.41 	 4560 	 5463 	 6168
wo 	p: 93.32 	r: 86.63 	f1: 89.85 	 2935 	 3145 	 3388
ni 	p: 87.3 	r: 72.27 	f1: 79.08 	 1100 	 1260 	 1522
best_thres [[0.23, 0.47, 0.11], [0.4, 0.47, 0.08], [0.4, 0.35, 0.08]]
f [0.8182, 0.8194, 0.8198]
load model: epoch13
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 13 	 [0.44, 0.62, 0.11] 	 lr: 5e-05 	 f: 82.65825778478518
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 26 [0m
Train...
loss: tensor(100.0956) lr: 1.25e-05 time: 3798.18
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.25 	r: 75.42 	f1: 78.69 	 4652 	 5656 	 6168
wo 	p: 93.6 	r: 86.78 	f1: 90.06 	 2940 	 3141 	 3388
ni 	p: 84.96 	r: 78.32 	f1: 81.5 	 1192 	 1403 	 1522

[32m iter_1[0m
ga 	p: 83.96 	r: 74.53 	f1: 78.97 	 4597 	 5475 	 6168
wo 	p: 94.5 	r: 86.19 	f1: 90.15 	 2920 	 3090 	 3388
ni 	p: 86.21 	r: 77.2 	f1: 81.46 	 1175 	 1363 	 1522

[32m iter_2[0m
ga 	p: 84.22 	r: 74.35 	f1: 78.98 	 4586 	 5445 	 6168
wo 	p: 93.95 	r: 86.57 	f1: 90.11 	 2933 	 3122 	 3388
ni 	p: 83.92 	r: 78.84 	f1: 81.3 	 1200 	 1430 	 1522
best_thres [[0.39, 0.47, 0.13], [0.48, 0.71, 0.12], [0.52, 0.61, 0.09]]
f [0.8256, 0.8266, 0.8269]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.66, 0.68, 0.13] 	 lr: 0.0001 	 f: 83.16045023173693
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 27 [0m
Train...
loss: tensor(523.4798) lr: 1e-05 time: 3803.7
pred_count_train 41644

Test...
loss: tensor(188.4673) lr: 1e-05 time: 3769.0
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.76 	r: 76.33 	f1: 78.95 	 4708 	 5758 	 6168
wo 	p: 93.12 	r: 86.72 	f1: 89.81 	 2938 	 3155 	 3388
ni 	p: 86.48 	r: 74.38 	f1: 79.97 	 1132 	 1309 	 1522

[32m iter_1[0m
ga 	p: 82.65 	r: 75.78 	f1: 79.07 	 4674 	 5655 	 6168
wo 	p: 93.8 	r: 85.8 	f1: 89.63 	 2907 	 3099 	 3388
ni 	p: 84.83 	r: 76.41 	f1: 80.4 	 1163 	 1371 	 1522

[32m iter_2[0m
ga 	p: 81.92 	r: 76.35 	f1: 79.04 	 4709 	 5748 	 6168
wo 	p: 93.04 	r: 86.39 	f1: 89.59 	 2927 	 3146 	 3388
ni 	p: 85.04 	r: 76.22 	f1: 80.39 	 1160 	 1364 	 1522
best_thres [[0.32, 0.54, 0.13], [0.37, 0.67, 0.08], [0.3, 0.56, 0.08]]
f [0.8242, 0.8245, 0.8245]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 23 	 [0.43, 0.75, 0.07] 	 lr: 1.25e-05 	 f: 82.78563450738567
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 26 [0m
Train...

[32m iter_0[0m
ga 	p: 82.25 	r: 75.42 	f1: 78.69 	 4652 	 5656 	 6168
wo 	p: 93.6 	r: 86.78 	f1: 90.06 	 2940 	 3141 	 3388
ni 	p: 84.96 	r: 78.32 	f1: 81.5 	 1192 	 1403 	 1522

[32m iter_1[0m
ga 	p: 83.96 	r: 74.53 	f1: 78.97 	 4597 	 5475 	 6168
wo 	p: 94.5 	r: 86.19 	f1: 90.15 	 2920 	 3090 	 3388
ni 	p: 86.21 	r: 77.2 	f1: 81.46 	 1175 	 1363 	 1522

[32m iter_2[0m
ga 	p: 84.22 	r: 74.35 	f1: 78.98 	 4586 	 5445 	 6168
wo 	p: 93.95 	r: 86.57 	f1: 90.11 	 2933 	 3122 	 3388
ni 	p: 83.92 	r: 78.84 	f1: 81.3 	 1200 	 1430 	 1522
best_thres [[0.39, 0.47, 0.13], [0.48, 0.71, 0.12], [0.52, 0.61, 0.09]]
f [0.8256, 0.8266, 0.8269]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.66, 0.68, 0.13] 	 lr: 0.0001 	 f: 83.16045023173693
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 27 [0m
Train...

[32m iter_0[0m
ga 	p: 82.68 	r: 74.53 	f1: 78.39 	 4597 	 5560 	 6168
wo 	p: 93.08 	r: 86.89 	f1: 89.88 	 2944 	 3163 	 3388
ni 	p: 89.43 	r: 70.63 	f1: 78.93 	 1075 	 1202 	 1522

[32m iter_1[0m
ga 	p: 83.51 	r: 74.64 	f1: 78.83 	 4604 	 5513 	 6168
wo 	p: 93.55 	r: 86.42 	f1: 89.84 	 2928 	 3130 	 3388
ni 	p: 89.23 	r: 72.4 	f1: 79.94 	 1102 	 1235 	 1522

[32m iter_2[0m
ga 	p: 83.83 	r: 74.45 	f1: 78.86 	 4592 	 5478 	 6168
wo 	p: 93.57 	r: 86.39 	f1: 89.84 	 2927 	 3128 	 3388
ni 	p: 87.84 	r: 73.06 	f1: 79.77 	 1112 	 1266 	 1522
best_thres [[0.38, 0.4, 0.21], [0.42, 0.56, 0.14], [0.45, 0.65, 0.11]]
f [0.8205, 0.8222, 0.8228]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 14 	 [0.44, 0.83, 0.08] 	 lr: 5e-05 	 f: 82.6418210429553
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 28 [0m
Train...
loss: tensor(187.3884) lr: 1e-05 time: 3417.08
pred_count_train 41644

Test...
loss: tensor(449.3054) lr: 1e-05 time: 3467.11
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.6 	r: 74.81 	f1: 78.51 	 4614 	 5586 	 6168
wo 	p: 94.07 	r: 86.16 	f1: 89.94 	 2919 	 3103 	 3388
ni 	p: 89.54 	r: 71.42 	f1: 79.46 	 1087 	 1214 	 1522

[32m iter_1[0m
ga 	p: 83.05 	r: 74.69 	f1: 78.65 	 4607 	 5547 	 6168
wo 	p: 93.86 	r: 86.25 	f1: 89.89 	 2922 	 3113 	 3388
ni 	p: 89.76 	r: 73.13 	f1: 80.59 	 1113 	 1240 	 1522

[32m iter_2[0m
ga 	p: 82.74 	r: 74.98 	f1: 78.67 	 4625 	 5590 	 6168
wo 	p: 93.92 	r: 86.16 	f1: 89.87 	 2919 	 3108 	 3388
ni 	p: 90.1 	r: 72.93 	f1: 80.61 	 1110 	 1232 	 1522
best_thres [[0.38, 0.47, 0.22], [0.41, 0.52, 0.16], [0.38, 0.66, 0.15]]
f [0.8217, 0.8228, 0.8232]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 13 	 [0.44, 0.62, 0.11] 	 lr: 5e-05 	 f: 82.65825778478518
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 27 [0m
Train...
loss: tensor(68.7624) lr: 1.25e-05 time: 3787.86
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.71 	r: 73.8 	f1: 78.44 	 4552 	 5438 	 6168
wo 	p: 93.44 	r: 86.66 	f1: 89.92 	 2936 	 3142 	 3388
ni 	p: 87.58 	r: 75.95 	f1: 81.35 	 1156 	 1320 	 1522

[32m iter_1[0m
ga 	p: 81.88 	r: 75.83 	f1: 78.74 	 4677 	 5712 	 6168
wo 	p: 94.15 	r: 86.01 	f1: 89.9 	 2914 	 3095 	 3388
ni 	p: 83.43 	r: 79.04 	f1: 81.17 	 1203 	 1442 	 1522

[32m iter_2[0m
ga 	p: 81.87 	r: 75.86 	f1: 78.75 	 4679 	 5715 	 6168
wo 	p: 94.19 	r: 86.07 	f1: 89.94 	 2916 	 3096 	 3388
ni 	p: 84.84 	r: 77.92 	f1: 81.23 	 1186 	 1398 	 1522
best_thres [[0.46, 0.43, 0.2], [0.34, 0.66, 0.09], [0.34, 0.71, 0.1]]
f [0.8241, 0.8244, 0.8246]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.66, 0.68, 0.13] 	 lr: 0.0001 	 f: 83.16045023173693
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 28 [0m
Train...
loss: tensor(449.3054) lr: 1e-05 time: 3824.22
pred_count_train 41644

Test...
loss: tensor(132.8685) lr: 1e-05 time: 3795.31
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.17 	r: 76.09 	f1: 78.54 	 4693 	 5782 	 6168
wo 	p: 92.48 	r: 87.13 	f1: 89.73 	 2952 	 3192 	 3388
ni 	p: 87.15 	r: 73.06 	f1: 79.49 	 1112 	 1276 	 1522

[32m iter_1[0m
ga 	p: 82.07 	r: 75.62 	f1: 78.71 	 4664 	 5683 	 6168
wo 	p: 92.5 	r: 86.95 	f1: 89.64 	 2946 	 3185 	 3388
ni 	p: 82.83 	r: 77.0 	f1: 79.81 	 1172 	 1415 	 1522

[32m iter_2[0m
ga 	p: 82.19 	r: 75.32 	f1: 78.61 	 4646 	 5653 	 6168
wo 	p: 93.12 	r: 86.3 	f1: 89.58 	 2924 	 3140 	 3388
ni 	p: 87.03 	r: 73.65 	f1: 79.79 	 1121 	 1288 	 1522
best_thres [[0.28, 0.41, 0.13], [0.32, 0.41, 0.05], [0.34, 0.55, 0.1]]
f [0.8212, 0.8217, 0.8216]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 23 	 [0.43, 0.75, 0.07] 	 lr: 1.25e-05 	 f: 82.78563450738567
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 27 [0m
Train...

[32m iter_0[0m
ga 	p: 83.71 	r: 73.8 	f1: 78.44 	 4552 	 5438 	 6168
wo 	p: 93.44 	r: 86.66 	f1: 89.92 	 2936 	 3142 	 3388
ni 	p: 87.58 	r: 75.95 	f1: 81.35 	 1156 	 1320 	 1522

[32m iter_1[0m
ga 	p: 81.88 	r: 75.83 	f1: 78.74 	 4677 	 5712 	 6168
wo 	p: 94.15 	r: 86.01 	f1: 89.9 	 2914 	 3095 	 3388
ni 	p: 83.43 	r: 79.04 	f1: 81.17 	 1203 	 1442 	 1522

[32m iter_2[0m
ga 	p: 81.87 	r: 75.86 	f1: 78.75 	 4679 	 5715 	 6168
wo 	p: 94.19 	r: 86.07 	f1: 89.94 	 2916 	 3096 	 3388
ni 	p: 84.84 	r: 77.92 	f1: 81.23 	 1186 	 1398 	 1522
best_thres [[0.46, 0.43, 0.2], [0.34, 0.66, 0.09], [0.34, 0.71, 0.1]]
f [0.8241, 0.8244, 0.8246]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.66, 0.68, 0.13] 	 lr: 0.0001 	 f: 83.16045023173693
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 28 [0m
Train...
loss: tensor(131.8102) lr: 1e-05 time: 3326.83
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.02 	r: 75.16 	f1: 78.44 	 4636 	 5652 	 6168
wo 	p: 94.35 	r: 85.68 	f1: 89.81 	 2903 	 3077 	 3388
ni 	p: 86.98 	r: 72.4 	f1: 79.02 	 1102 	 1267 	 1522

[32m iter_1[0m
ga 	p: 83.95 	r: 73.85 	f1: 78.58 	 4555 	 5426 	 6168
wo 	p: 94.14 	r: 85.77 	f1: 89.76 	 2906 	 3087 	 3388
ni 	p: 88.61 	r: 73.13 	f1: 80.13 	 1113 	 1256 	 1522

[32m iter_2[0m
ga 	p: 83.99 	r: 73.9 	f1: 78.62 	 4558 	 5427 	 6168
wo 	p: 93.94 	r: 86.01 	f1: 89.8 	 2914 	 3102 	 3388
ni 	p: 85.81 	r: 74.7 	f1: 79.87 	 1137 	 1325 	 1522
best_thres [[0.3, 0.52, 0.16], [0.43, 0.63, 0.13], [0.43, 0.65, 0.08]]
f [0.8201, 0.8213, 0.8217]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 14 	 [0.44, 0.83, 0.08] 	 lr: 5e-05 	 f: 82.6418210429553
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 29 [0m
Train...
loss: tensor(387.7455) lr: 1e-05 time: 3273.7
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.5 	r: 75.23 	f1: 78.24 	 4640 	 5693 	 6168
wo 	p: 94.38 	r: 85.8 	f1: 89.89 	 2907 	 3080 	 3388
ni 	p: 89.51 	r: 69.51 	f1: 78.25 	 1058 	 1182 	 1522

[32m iter_1[0m
ga 	p: 83.53 	r: 74.03 	f1: 78.49 	 4566 	 5466 	 6168
wo 	p: 93.96 	r: 85.89 	f1: 89.75 	 2910 	 3097 	 3388
ni 	p: 88.02 	r: 71.94 	f1: 79.18 	 1095 	 1244 	 1522

[32m iter_2[0m
ga 	p: 83.41 	r: 73.95 	f1: 78.39 	 4561 	 5468 	 6168
wo 	p: 93.47 	r: 86.13 	f1: 89.65 	 2918 	 3122 	 3388
ni 	p: 86.46 	r: 73.0 	f1: 79.16 	 1111 	 1285 	 1522
best_thres [[0.29, 0.52, 0.2], [0.43, 0.7, 0.11], [0.43, 0.55, 0.08]]
f [0.8182, 0.8195, 0.8196]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 13 	 [0.44, 0.62, 0.11] 	 lr: 5e-05 	 f: 82.65825778478518
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 28 [0m
Train...

[32m iter_0[0m
ga 	p: 81.55 	r: 75.29 	f1: 78.29 	 4644 	 5695 	 6168
wo 	p: 94.02 	r: 85.92 	f1: 89.79 	 2911 	 3096 	 3388
ni 	p: 86.71 	r: 75.89 	f1: 80.94 	 1155 	 1332 	 1522

[32m iter_1[0m
ga 	p: 82.07 	r: 75.47 	f1: 78.63 	 4655 	 5672 	 6168
wo 	p: 94.14 	r: 85.86 	f1: 89.81 	 2909 	 3090 	 3388
ni 	p: 85.65 	r: 76.48 	f1: 80.81 	 1164 	 1359 	 1522

[32m iter_2[0m
ga 	p: 81.78 	r: 75.52 	f1: 78.52 	 4658 	 5696 	 6168
wo 	p: 93.51 	r: 86.3 	f1: 89.76 	 2924 	 3127 	 3388
ni 	p: 87.09 	r: 75.36 	f1: 80.8 	 1147 	 1317 	 1522
best_thres [[0.37, 0.51, 0.18], [0.37, 0.68, 0.12], [0.36, 0.49, 0.13]]
f [0.8217, 0.8225, 0.8226]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	best in epoch 8 	 [0.66, 0.68, 0.13] 	 lr: 0.0001 	 f: 83.16045023173693
2018 {'GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse': {'best_epoch': 8, 'best_thres': [0.43, 0.54, 0.19], 'best_lr': 0.0001, 'best_performance': 83.14960139511709}}
2020 {'GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse': {'best_epoch': 8, 'best_thres': [0.66, 0.68, 0.13], 'best_lr': 0.0001, 'best_performance': 83.16045023173693}}
loss: tensor(47.9151) lr: 1.25e-05 time: 3618.9
pred_count_train 41644

Test...
loss: tensor(387.7455) lr: 1e-05 time: 3604.64
pred_count_train 41644

Test...
loss: tensor(99.2309) lr: 1e-05 time: 3567.76
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.38 	r: 74.89 	f1: 78.45 	 4619 	 5607 	 6168
wo 	p: 92.96 	r: 86.48 	f1: 89.6 	 2930 	 3152 	 3388
ni 	p: 85.23 	r: 74.31 	f1: 79.4 	 1131 	 1327 	 1522

[32m iter_1[0m
ga 	p: 81.1 	r: 76.17 	f1: 78.56 	 4698 	 5793 	 6168
wo 	p: 93.32 	r: 86.16 	f1: 89.59 	 2919 	 3128 	 3388
ni 	p: 84.53 	r: 75.76 	f1: 79.9 	 1153 	 1364 	 1522

[32m iter_2[0m
ga 	p: 81.32 	r: 76.22 	f1: 78.68 	 4701 	 5781 	 6168
wo 	p: 92.98 	r: 86.36 	f1: 89.55 	 2926 	 3147 	 3388
ni 	p: 84.66 	r: 75.43 	f1: 79.78 	 1148 	 1356 	 1522
best_thres [[0.36, 0.56, 0.08], [0.21, 0.6, 0.05], [0.21, 0.55, 0.05]]
f [0.8203, 0.8207, 0.821]
load model: epoch23
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 23 	 [0.43, 0.75, 0.07] 	 lr: 1.25e-05 	 f: 82.78563450738567
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 28 [0m
Train...
loss: tensor(98.3686) lr: 1e-05 time: 2955.67
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.55 	r: 75.29 	f1: 78.29 	 4644 	 5695 	 6168
wo 	p: 94.02 	r: 85.92 	f1: 89.79 	 2911 	 3096 	 3388
ni 	p: 86.71 	r: 75.89 	f1: 80.94 	 1155 	 1332 	 1522

[32m iter_1[0m
ga 	p: 82.07 	r: 75.47 	f1: 78.63 	 4655 	 5672 	 6168
wo 	p: 94.14 	r: 85.86 	f1: 89.81 	 2909 	 3090 	 3388
ni 	p: 85.65 	r: 76.48 	f1: 80.81 	 1164 	 1359 	 1522

[32m iter_2[0m
ga 	p: 81.78 	r: 75.52 	f1: 78.52 	 4658 	 5696 	 6168
wo 	p: 93.51 	r: 86.3 	f1: 89.76 	 2924 	 3127 	 3388
ni 	p: 87.09 	r: 75.36 	f1: 80.8 	 1147 	 1317 	 1522
best_thres [[0.37, 0.51, 0.18], [0.37, 0.68, 0.12], [0.36, 0.49, 0.13]]
f [0.8217, 0.8225, 0.8226]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	best in epoch 8 	 [0.66, 0.68, 0.13] 	 lr: 0.0001 	 f: 83.16045023173693
2018 {'GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse': {'best_epoch': 8, 'best_thres': [0.43, 0.54, 0.19], 'best_lr': 0.0001, 'best_performance': 83.14960139511709}}
2020 {'GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse': {'best_epoch': 8, 'best_thres': [0.66, 0.68, 0.13], 'best_lr': 0.0001, 'best_performance': 83.16045023173693}}

[32m iter_0[0m
ga 	p: 80.7 	r: 76.31 	f1: 78.44 	 4707 	 5833 	 6168
wo 	p: 94.68 	r: 85.63 	f1: 89.93 	 2901 	 3064 	 3388
ni 	p: 83.49 	r: 74.44 	f1: 78.71 	 1133 	 1357 	 1522

[32m iter_1[0m
ga 	p: 84.15 	r: 73.25 	f1: 78.32 	 4518 	 5369 	 6168
wo 	p: 94.14 	r: 85.83 	f1: 89.79 	 2908 	 3089 	 3388
ni 	p: 86.38 	r: 73.78 	f1: 79.59 	 1123 	 1300 	 1522

[32m iter_2[0m
ga 	p: 81.19 	r: 75.65 	f1: 78.32 	 4666 	 5747 	 6168
wo 	p: 93.91 	r: 86.01 	f1: 89.79 	 2914 	 3103 	 3388
ni 	p: 87.11 	r: 73.72 	f1: 79.86 	 1122 	 1288 	 1522
best_thres [[0.19, 0.58, 0.09], [0.46, 0.72, 0.09], [0.2, 0.76, 0.09]]
f [0.8195, 0.8201, 0.8201]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 14 	 [0.44, 0.83, 0.08] 	 lr: 5e-05 	 f: 82.6418210429553
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 30 [0m
Train...

[32m iter_0[0m
ga 	p: 81.43 	r: 75.71 	f1: 78.47 	 4670 	 5735 	 6168
wo 	p: 95.3 	r: 85.04 	f1: 89.88 	 2881 	 3023 	 3388
ni 	p: 87.81 	r: 70.5 	f1: 78.21 	 1073 	 1222 	 1522

[32m iter_1[0m
ga 	p: 81.96 	r: 75.21 	f1: 78.44 	 4639 	 5660 	 6168
wo 	p: 93.43 	r: 86.45 	f1: 89.81 	 2929 	 3135 	 3388
ni 	p: 87.23 	r: 72.73 	f1: 79.33 	 1107 	 1269 	 1522

[32m iter_2[0m
ga 	p: 81.88 	r: 75.29 	f1: 78.45 	 4644 	 5672 	 6168
wo 	p: 94.08 	r: 85.86 	f1: 89.78 	 2909 	 3092 	 3388
ni 	p: 87.27 	r: 72.54 	f1: 79.22 	 1104 	 1265 	 1522
best_thres [[0.25, 0.67, 0.17], [0.27, 0.43, 0.1], [0.25, 0.77, 0.09]]
f [0.8191, 0.8199, 0.82]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	current best epoch 13 	 [0.44, 0.62, 0.11] 	 lr: 5e-05 	 f: 82.65825778478518
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse[0m [33m epoch 29 [0m
Train...
loss: tensor(144.1368) lr: 1e-05 time: 2556.79
pred_count_train 41644

Test...
loss: tensor(72.1317) lr: 1e-05 time: 2539.7
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 84.31 	r: 74.21 	f1: 78.93 	 4577 	 5429 	 6168
wo 	p: 92.24 	r: 87.66 	f1: 89.89 	 2970 	 3220 	 3388
ni 	p: 86.2 	r: 76.35 	f1: 80.98 	 1162 	 1348 	 1522

[32m iter_1[0m
ga 	p: 84.22 	r: 74.25 	f1: 78.92 	 4580 	 5438 	 6168
wo 	p: 92.54 	r: 87.16 	f1: 89.77 	 2953 	 3191 	 3388
ni 	p: 84.65 	r: 78.25 	f1: 81.32 	 1191 	 1407 	 1522

[32m iter_2[0m
ga 	p: 83.92 	r: 74.45 	f1: 78.9 	 4592 	 5472 	 6168
wo 	p: 93.9 	r: 85.92 	f1: 89.73 	 2911 	 3100 	 3388
ni 	p: 86.68 	r: 76.94 	f1: 81.52 	 1171 	 1351 	 1522
best_thres [[0.58, 0.46, 0.14], [0.57, 0.52, 0.09], [0.54, 0.75, 0.12]]
f [0.8265, 0.8264, 0.8263]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 23 	 [0.43, 0.75, 0.07] 	 lr: 1.25e-05 	 f: 82.78563450738567
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 29 [0m
Train...
loss: tensor(73.2258) lr: 1e-05 time: 2410.21
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.38 	r: 76.07 	f1: 78.17 	 4692 	 5837 	 6168
wo 	p: 92.81 	r: 86.92 	f1: 89.77 	 2945 	 3173 	 3388
ni 	p: 84.11 	r: 72.67 	f1: 77.97 	 1106 	 1315 	 1522

[32m iter_1[0m
ga 	p: 83.71 	r: 73.62 	f1: 78.34 	 4541 	 5425 	 6168
wo 	p: 93.99 	r: 85.89 	f1: 89.76 	 2910 	 3096 	 3388
ni 	p: 86.3 	r: 72.86 	f1: 79.02 	 1109 	 1285 	 1522

[32m iter_2[0m
ga 	p: 83.76 	r: 73.61 	f1: 78.36 	 4540 	 5420 	 6168
wo 	p: 93.81 	r: 85.95 	f1: 89.71 	 2912 	 3104 	 3388
ni 	p: 86.99 	r: 72.47 	f1: 79.07 	 1103 	 1268 	 1522
best_thres [[0.2, 0.31, 0.1], [0.42, 0.53, 0.09], [0.43, 0.58, 0.09]]
f [0.817, 0.8184, 0.8188]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	best in epoch 14 	 [0.44, 0.83, 0.08] 	 lr: 5e-05 	 f: 82.6418210429553
2018 {'GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse': {'best_epoch': 8, 'best_thres': [0.45, 0.56, 0.13], 'best_lr': 0.0001, 'best_performance': 82.97808936777554}}
2020 {'GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse': {'best_epoch': 14, 'best_thres': [0.44, 0.83, 0.08], 'best_lr': 5e-05, 'best_performance': 82.6418210429553}}

[32m iter_0[0m
ga 	p: 80.44 	r: 76.35 	f1: 78.34 	 4709 	 5854 	 6168
wo 	p: 94.04 	r: 86.19 	f1: 89.94 	 2920 	 3105 	 3388
ni 	p: 85.26 	r: 72.6 	f1: 78.42 	 1105 	 1296 	 1522

[32m iter_1[0m
ga 	p: 81.73 	r: 74.92 	f1: 78.18 	 4621 	 5654 	 6168
wo 	p: 94.39 	r: 85.98 	f1: 89.99 	 2913 	 3086 	 3388
ni 	p: 88.57 	r: 71.81 	f1: 79.32 	 1093 	 1234 	 1522

[32m iter_2[0m
ga 	p: 81.71 	r: 74.95 	f1: 78.18 	 4623 	 5658 	 6168
wo 	p: 94.19 	r: 86.07 	f1: 89.94 	 2916 	 3096 	 3388
ni 	p: 86.49 	r: 73.59 	f1: 79.52 	 1120 	 1295 	 1522
best_thres [[0.17, 0.48, 0.1], [0.23, 0.76, 0.11], [0.22, 0.82, 0.07]]
f [0.8188, 0.8192, 0.8194]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse 	best in epoch 13 	 [0.44, 0.62, 0.11] 	 lr: 5e-05 	 f: 82.65825778478518
2018 {'GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse': {'best_epoch': 8, 'best_thres': [0.45, 0.56, 0.13], 'best_lr': 0.0001, 'best_performance': 82.97808936777554}}
2020 {'GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse': {'best_epoch': 13, 'best_thres': [0.44, 0.62, 0.11], 'best_lr': 5e-05, 'best_performance': 82.65825778478518}}
loss: tensor(103.3896) lr: 1e-05 time: 1856.75
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.08 	r: 75.23 	f1: 78.96 	 4640 	 5585 	 6168
wo 	p: 93.12 	r: 86.63 	f1: 89.76 	 2935 	 3152 	 3388
ni 	p: 85.13 	r: 75.62 	f1: 80.1 	 1151 	 1352 	 1522

[32m iter_1[0m
ga 	p: 81.64 	r: 76.62 	f1: 79.05 	 4726 	 5789 	 6168
wo 	p: 93.78 	r: 85.89 	f1: 89.66 	 2910 	 3103 	 3388
ni 	p: 83.35 	r: 77.92 	f1: 80.54 	 1186 	 1423 	 1522

[32m iter_2[0m
ga 	p: 81.88 	r: 76.43 	f1: 79.06 	 4714 	 5757 	 6168
wo 	p: 92.72 	r: 86.78 	f1: 89.65 	 2940 	 3171 	 3388
ni 	p: 84.72 	r: 76.48 	f1: 80.39 	 1164 	 1374 	 1522
best_thres [[0.45, 0.54, 0.11], [0.3, 0.66, 0.06], [0.31, 0.47, 0.08]]
f [0.8245, 0.8246, 0.8247]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 23 	 [0.43, 0.75, 0.07] 	 lr: 1.25e-05 	 f: 82.78563450738567
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 30 [0m
Train...
loss: tensor(75.6688) lr: 1e-05 time: 1601.46
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.49 	r: 76.17 	f1: 78.74 	 4698 	 5765 	 6168
wo 	p: 93.31 	r: 86.51 	f1: 89.78 	 2931 	 3141 	 3388
ni 	p: 85.03 	r: 75.36 	f1: 79.9 	 1147 	 1349 	 1522

[32m iter_1[0m
ga 	p: 82.69 	r: 75.42 	f1: 78.89 	 4652 	 5626 	 6168
wo 	p: 92.45 	r: 87.13 	f1: 89.71 	 2952 	 3193 	 3388
ni 	p: 86.26 	r: 75.1 	f1: 80.3 	 1143 	 1325 	 1522

[32m iter_2[0m
ga 	p: 82.88 	r: 75.18 	f1: 78.84 	 4637 	 5595 	 6168
wo 	p: 92.66 	r: 86.87 	f1: 89.67 	 2943 	 3176 	 3388
ni 	p: 86.02 	r: 75.16 	f1: 80.22 	 1144 	 1330 	 1522
best_thres [[0.31, 0.61, 0.11], [0.39, 0.44, 0.11], [0.4, 0.5, 0.1]]
f [0.8228, 0.8235, 0.8236]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 23 	 [0.43, 0.75, 0.07] 	 lr: 1.25e-05 	 f: 82.78563450738567
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 31 [0m
Train...
loss: tensor(54.3555) lr: 1e-05 time: 1645.58
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.96 	r: 74.82 	f1: 78.68 	 4615 	 5563 	 6168
wo 	p: 92.64 	r: 86.98 	f1: 89.72 	 2947 	 3181 	 3388
ni 	p: 84.23 	r: 76.15 	f1: 79.99 	 1159 	 1376 	 1522

[32m iter_1[0m
ga 	p: 83.06 	r: 74.98 	f1: 78.82 	 4625 	 5568 	 6168
wo 	p: 92.66 	r: 86.84 	f1: 89.65 	 2942 	 3175 	 3388
ni 	p: 84.54 	r: 76.87 	f1: 80.52 	 1170 	 1384 	 1522

[32m iter_2[0m
ga 	p: 83.7 	r: 74.58 	f1: 78.88 	 4600 	 5496 	 6168
wo 	p: 93.1 	r: 86.48 	f1: 89.67 	 2930 	 3147 	 3388
ni 	p: 85.56 	r: 75.89 	f1: 80.43 	 1155 	 1350 	 1522
best_thres [[0.42, 0.4, 0.07], [0.41, 0.4, 0.05], [0.47, 0.51, 0.06]]
f [0.8228, 0.8234, 0.8237]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	best in epoch 23 	 [0.43, 0.75, 0.07] 	 lr: 1.25e-05 	 f: 82.78563450738567
2018 {'GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse': {'best_epoch': 25, 'best_thres': [0.46, 0.45, 0.11], 'best_lr': 1e-05, 'best_performance': 83.1601738344834}}
2020 {'GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse': {'best_epoch': 23, 'best_thres': [0.43, 0.75, 0.07], 'best_lr': 1.25e-05, 'best_performance': 82.78563450738567}}
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/test.json
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/test.json
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 1 [0m
Train...
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 1 [0m
Train...
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(3663.6003) lr: 0.0001 time: 4357.59
pred_count_train 41644

Test...
loss: tensor(3663.6003) lr: 0.0001 time: 4379.69
pred_count_train 41644

Test...
loss: tensor(3657.8479) lr: 0.0001 time: 4370.67
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 74.69 	r: 72.83 	f1: 73.75 	 4492 	 6014 	 6168
wo 	p: 89.83 	r: 83.38 	f1: 86.48 	 2825 	 3145 	 3388
ni 	p: 81.26 	r: 76.94 	f1: 79.04 	 1171 	 1441 	 1522

[32m iter_1[0m
ga 	p: 75.18 	r: 72.32 	f1: 73.72 	 4461 	 5934 	 6168
wo 	p: 91.68 	r: 81.35 	f1: 86.21 	 2756 	 3006 	 3388
ni 	p: 80.89 	r: 77.33 	f1: 79.07 	 1177 	 1455 	 1522

[32m iter_2[0m
ga 	p: 75.16 	r: 72.24 	f1: 73.67 	 4456 	 5929 	 6168
wo 	p: 89.72 	r: 82.91 	f1: 86.18 	 2809 	 3131 	 3388
ni 	p: 81.02 	r: 77.4 	f1: 79.17 	 1178 	 1454 	 1522
best_thres [[0.35, 0.34, 0.27], [0.36, 0.43, 0.23], [0.36, 0.32, 0.23]]
f [0.7831, 0.7825, 0.7823]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 1 	 [0.36, 0.32, 0.23] 	 lr: 0.0001 	 f: 78.23239578023879
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 2 [0m
Train...

[32m iter_0[0m
ga 	p: 76.3 	r: 71.58 	f1: 73.87 	 4415 	 5786 	 6168
wo 	p: 90.14 	r: 82.85 	f1: 86.34 	 2807 	 3114 	 3388
ni 	p: 79.73 	r: 77.79 	f1: 78.75 	 1184 	 1485 	 1522

[32m iter_1[0m
ga 	p: 76.38 	r: 71.61 	f1: 73.92 	 4417 	 5783 	 6168
wo 	p: 91.53 	r: 81.67 	f1: 86.32 	 2767 	 3023 	 3388
ni 	p: 79.83 	r: 77.99 	f1: 78.9 	 1187 	 1487 	 1522

[32m iter_2[0m
ga 	p: 76.3 	r: 71.6 	f1: 73.87 	 4416 	 5788 	 6168
wo 	p: 91.05 	r: 81.94 	f1: 86.25 	 2776 	 3049 	 3388
ni 	p: 80.42 	r: 77.46 	f1: 78.92 	 1179 	 1466 	 1522
best_thres [[0.42, 0.35, 0.24], [0.42, 0.42, 0.22], [0.42, 0.4, 0.23]]
f [0.7833, 0.7833, 0.7832]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 1 	 [0.42, 0.4, 0.23] 	 lr: 0.0001 	 f: 78.32437903916531
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 2 [0m
Train...

[32m iter_0[0m
ga 	p: 74.69 	r: 72.83 	f1: 73.75 	 4492 	 6014 	 6168
wo 	p: 89.83 	r: 83.38 	f1: 86.48 	 2825 	 3145 	 3388
ni 	p: 81.26 	r: 76.94 	f1: 79.04 	 1171 	 1441 	 1522

[32m iter_1[0m
ga 	p: 75.18 	r: 72.32 	f1: 73.72 	 4461 	 5934 	 6168
wo 	p: 91.68 	r: 81.35 	f1: 86.21 	 2756 	 3006 	 3388
ni 	p: 80.89 	r: 77.33 	f1: 79.07 	 1177 	 1455 	 1522

[32m iter_2[0m
ga 	p: 75.16 	r: 72.24 	f1: 73.67 	 4456 	 5929 	 6168
wo 	p: 89.72 	r: 82.91 	f1: 86.18 	 2809 	 3131 	 3388
ni 	p: 81.02 	r: 77.4 	f1: 79.17 	 1178 	 1454 	 1522
best_thres [[0.35, 0.34, 0.27], [0.36, 0.43, 0.23], [0.36, 0.32, 0.23]]
f [0.7831, 0.7825, 0.7823]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 1 	 [0.36, 0.32, 0.23] 	 lr: 0.0001 	 f: 78.23239578023879
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 2 [0m
Train...
loss: tensor(2252.9207) lr: 0.0001 time: 4331.65
pred_count_train 41644

Test...
loss: tensor(2247.5291) lr: 0.0001 time: 4311.75
pred_count_train 41644

Test...
loss: tensor(2252.9207) lr: 0.0001 time: 4323.67
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.38 	r: 74.56 	f1: 76.89 	 4599 	 5794 	 6168
wo 	p: 92.13 	r: 85.71 	f1: 88.81 	 2904 	 3152 	 3388
ni 	p: 85.57 	r: 79.11 	f1: 82.21 	 1204 	 1407 	 1522

[32m iter_1[0m
ga 	p: 79.23 	r: 74.66 	f1: 76.88 	 4605 	 5812 	 6168
wo 	p: 91.4 	r: 86.3 	f1: 88.78 	 2924 	 3199 	 3388
ni 	p: 84.61 	r: 79.11 	f1: 81.77 	 1204 	 1423 	 1522

[32m iter_2[0m
ga 	p: 79.26 	r: 74.64 	f1: 76.88 	 4604 	 5809 	 6168
wo 	p: 91.4 	r: 86.28 	f1: 88.76 	 2923 	 3198 	 3388
ni 	p: 85.37 	r: 78.58 	f1: 81.83 	 1196 	 1401 	 1522
best_thres [[0.27, 0.39, 0.15], [0.26, 0.33, 0.13], [0.26, 0.33, 0.14]]
f [0.8126, 0.8122, 0.8121]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 2 	 [0.26, 0.33, 0.14] 	 lr: 0.0001 	 f: 81.21498083161309
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 3 [0m
Train...

[32m iter_0[0m
ga 	p: 79.27 	r: 74.66 	f1: 76.9 	 4605 	 5809 	 6168
wo 	p: 92.89 	r: 84.83 	f1: 88.68 	 2874 	 3094 	 3388
ni 	p: 83.57 	r: 79.89 	f1: 81.69 	 1216 	 1455 	 1522

[32m iter_1[0m
ga 	p: 79.86 	r: 74.3 	f1: 76.98 	 4583 	 5739 	 6168
wo 	p: 93.52 	r: 84.3 	f1: 88.67 	 2856 	 3054 	 3388
ni 	p: 84.34 	r: 79.24 	f1: 81.71 	 1206 	 1430 	 1522

[32m iter_2[0m
ga 	p: 79.75 	r: 74.32 	f1: 76.94 	 4584 	 5748 	 6168
wo 	p: 93.52 	r: 84.33 	f1: 88.69 	 2857 	 3055 	 3388
ni 	p: 84.42 	r: 79.37 	f1: 81.82 	 1208 	 1431 	 1522
best_thres [[0.25, 0.41, 0.14], [0.26, 0.44, 0.15], [0.26, 0.44, 0.15]]
f [0.8113, 0.8115, 0.8115]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 2 	 [0.26, 0.44, 0.15] 	 lr: 0.0001 	 f: 81.15349185779635
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 3 [0m
Train...

[32m iter_0[0m
ga 	p: 79.38 	r: 74.56 	f1: 76.89 	 4599 	 5794 	 6168
wo 	p: 92.13 	r: 85.71 	f1: 88.81 	 2904 	 3152 	 3388
ni 	p: 85.57 	r: 79.11 	f1: 82.21 	 1204 	 1407 	 1522

[32m iter_1[0m
ga 	p: 79.23 	r: 74.66 	f1: 76.88 	 4605 	 5812 	 6168
wo 	p: 91.4 	r: 86.3 	f1: 88.78 	 2924 	 3199 	 3388
ni 	p: 84.61 	r: 79.11 	f1: 81.77 	 1204 	 1423 	 1522

[32m iter_2[0m
ga 	p: 79.26 	r: 74.64 	f1: 76.88 	 4604 	 5809 	 6168
wo 	p: 91.4 	r: 86.28 	f1: 88.76 	 2923 	 3198 	 3388
ni 	p: 85.37 	r: 78.58 	f1: 81.83 	 1196 	 1401 	 1522
best_thres [[0.27, 0.39, 0.15], [0.26, 0.33, 0.13], [0.26, 0.33, 0.14]]
f [0.8126, 0.8122, 0.8121]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 2 	 [0.26, 0.33, 0.14] 	 lr: 0.0001 	 f: 81.21498083161309
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 3 [0m
Train...
loss: tensor(1857.1644) lr: 0.0001 time: 4309.54
pred_count_train 41644

Test...
loss: tensor(1850.4734) lr: 0.0001 time: 4295.59
pred_count_train 41644

Test...
loss: tensor(1857.1644) lr: 0.0001 time: 4300.84
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.86 	r: 75.29 	f1: 77.51 	 4644 	 5815 	 6168
wo 	p: 93.0 	r: 86.28 	f1: 89.51 	 2923 	 3143 	 3388
ni 	p: 85.26 	r: 80.16 	f1: 82.63 	 1220 	 1431 	 1522

[32m iter_1[0m
ga 	p: 80.5 	r: 75.34 	f1: 77.83 	 4647 	 5773 	 6168
wo 	p: 92.51 	r: 86.39 	f1: 89.35 	 2927 	 3164 	 3388
ni 	p: 84.33 	r: 81.34 	f1: 82.81 	 1238 	 1468 	 1522

[32m iter_2[0m
ga 	p: 80.48 	r: 75.24 	f1: 77.77 	 4641 	 5767 	 6168
wo 	p: 92.4 	r: 86.54 	f1: 89.38 	 2932 	 3173 	 3388
ni 	p: 85.79 	r: 80.16 	f1: 82.88 	 1220 	 1422 	 1522
best_thres [[0.34, 0.48, 0.14], [0.35, 0.44, 0.12], [0.35, 0.43, 0.14]]
f [0.8187, 0.8195, 0.8198]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 3 	 [0.35, 0.43, 0.14] 	 lr: 0.0001 	 f: 81.97546202826526
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 4 [0m
Train...

[32m iter_0[0m
ga 	p: 79.49 	r: 76.2 	f1: 77.81 	 4700 	 5913 	 6168
wo 	p: 92.62 	r: 86.36 	f1: 89.38 	 2926 	 3159 	 3388
ni 	p: 87.59 	r: 78.84 	f1: 82.99 	 1200 	 1370 	 1522

[32m iter_1[0m
ga 	p: 79.65 	r: 75.89 	f1: 77.73 	 4681 	 5877 	 6168
wo 	p: 92.87 	r: 86.13 	f1: 89.37 	 2918 	 3142 	 3388
ni 	p: 88.13 	r: 79.04 	f1: 83.34 	 1203 	 1365 	 1522

[32m iter_2[0m
ga 	p: 79.56 	r: 75.97 	f1: 77.72 	 4686 	 5890 	 6168
wo 	p: 92.88 	r: 86.19 	f1: 89.41 	 2920 	 3144 	 3388
ni 	p: 85.24 	r: 81.6 	f1: 83.38 	 1242 	 1457 	 1522
best_thres [[0.3, 0.49, 0.19], [0.32, 0.5, 0.21], [0.32, 0.5, 0.14]]
f [0.8203, 0.8203, 0.8203]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 3 	 [0.32, 0.5, 0.14] 	 lr: 0.0001 	 f: 82.03126210283342
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 4 [0m
Train...

[32m iter_0[0m
ga 	p: 79.86 	r: 75.29 	f1: 77.51 	 4644 	 5815 	 6168
wo 	p: 93.0 	r: 86.28 	f1: 89.51 	 2923 	 3143 	 3388
ni 	p: 85.26 	r: 80.16 	f1: 82.63 	 1220 	 1431 	 1522

[32m iter_1[0m
ga 	p: 80.5 	r: 75.34 	f1: 77.83 	 4647 	 5773 	 6168
wo 	p: 92.51 	r: 86.39 	f1: 89.35 	 2927 	 3164 	 3388
ni 	p: 84.33 	r: 81.34 	f1: 82.81 	 1238 	 1468 	 1522

[32m iter_2[0m
ga 	p: 80.48 	r: 75.24 	f1: 77.77 	 4641 	 5767 	 6168
wo 	p: 92.4 	r: 86.54 	f1: 89.38 	 2932 	 3173 	 3388
ni 	p: 85.79 	r: 80.16 	f1: 82.88 	 1220 	 1422 	 1522
best_thres [[0.34, 0.48, 0.14], [0.35, 0.44, 0.12], [0.35, 0.43, 0.14]]
f [0.8187, 0.8195, 0.8198]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 3 	 [0.35, 0.43, 0.14] 	 lr: 0.0001 	 f: 81.97546202826526
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 4 [0m
Train...
loss: tensor(1557.3167) lr: 0.0001 time: 4292.25
pred_count_train 41644

Test...
loss: tensor(1558.2358) lr: 0.0001 time: 4280.3
pred_count_train 41644

Test...
loss: tensor(1557.3167) lr: 0.0001 time: 4270.71
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.58 	r: 74.97 	f1: 78.13 	 4624 	 5668 	 6168
wo 	p: 93.26 	r: 85.8 	f1: 89.38 	 2907 	 3117 	 3388
ni 	p: 86.67 	r: 78.19 	f1: 82.21 	 1190 	 1373 	 1522

[32m iter_1[0m
ga 	p: 82.0 	r: 75.11 	f1: 78.41 	 4633 	 5650 	 6168
wo 	p: 92.92 	r: 86.01 	f1: 89.33 	 2914 	 3136 	 3388
ni 	p: 85.98 	r: 79.37 	f1: 82.54 	 1208 	 1405 	 1522

[32m iter_2[0m
ga 	p: 81.78 	r: 75.39 	f1: 78.45 	 4650 	 5686 	 6168
wo 	p: 91.97 	r: 86.92 	f1: 89.38 	 2945 	 3202 	 3388
ni 	p: 86.13 	r: 79.17 	f1: 82.51 	 1205 	 1399 	 1522
best_thres [[0.44, 0.3, 0.17], [0.44, 0.27, 0.14], [0.43, 0.21, 0.14]]
f [0.8213, 0.8223, 0.8228]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 4 	 [0.43, 0.21, 0.14] 	 lr: 0.0001 	 f: 82.27963049945201
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 5 [0m
Train...

[32m iter_0[0m
ga 	p: 80.63 	r: 76.82 	f1: 78.68 	 4738 	 5876 	 6168
wo 	p: 94.49 	r: 85.01 	f1: 89.5 	 2880 	 3048 	 3388
ni 	p: 85.62 	r: 80.22 	f1: 82.84 	 1221 	 1426 	 1522

[32m iter_1[0m
ga 	p: 81.23 	r: 76.25 	f1: 78.66 	 4703 	 5790 	 6168
wo 	p: 93.44 	r: 85.8 	f1: 89.46 	 2907 	 3111 	 3388
ni 	p: 86.72 	r: 79.83 	f1: 83.13 	 1215 	 1401 	 1522

[32m iter_2[0m
ga 	p: 81.83 	r: 75.84 	f1: 78.72 	 4678 	 5717 	 6168
wo 	p: 92.99 	r: 86.19 	f1: 89.46 	 2920 	 3140 	 3388
ni 	p: 86.73 	r: 79.89 	f1: 83.17 	 1216 	 1402 	 1522
best_thres [[0.39, 0.4, 0.18], [0.43, 0.31, 0.19], [0.45, 0.28, 0.19]]
f [0.825, 0.8253, 0.8256]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 4 	 [0.45, 0.28, 0.19] 	 lr: 0.0001 	 f: 82.55670745966171
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 5 [0m
Train...

[32m iter_0[0m
ga 	p: 81.58 	r: 74.97 	f1: 78.13 	 4624 	 5668 	 6168
wo 	p: 93.26 	r: 85.8 	f1: 89.38 	 2907 	 3117 	 3388
ni 	p: 86.67 	r: 78.19 	f1: 82.21 	 1190 	 1373 	 1522

[32m iter_1[0m
ga 	p: 82.0 	r: 75.11 	f1: 78.41 	 4633 	 5650 	 6168
wo 	p: 92.92 	r: 86.01 	f1: 89.33 	 2914 	 3136 	 3388
ni 	p: 85.98 	r: 79.37 	f1: 82.54 	 1208 	 1405 	 1522

[32m iter_2[0m
ga 	p: 81.78 	r: 75.39 	f1: 78.45 	 4650 	 5686 	 6168
wo 	p: 91.97 	r: 86.92 	f1: 89.38 	 2945 	 3202 	 3388
ni 	p: 86.13 	r: 79.17 	f1: 82.51 	 1205 	 1399 	 1522
best_thres [[0.44, 0.3, 0.17], [0.44, 0.27, 0.14], [0.43, 0.21, 0.14]]
f [0.8213, 0.8223, 0.8228]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 4 	 [0.43, 0.21, 0.14] 	 lr: 0.0001 	 f: 82.27963049945201
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 5 [0m
Train...
loss: tensor(1289.2684) lr: 0.0001 time: 4326.19
pred_count_train 41644

Test...
loss: tensor(1278.1925) lr: 0.0001 time: 4327.92
pred_count_train 41644

Test...
loss: tensor(1289.2684) lr: 0.0001 time: 4369.15
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.94 	r: 76.04 	f1: 77.94 	 4690 	 5867 	 6168
wo 	p: 93.37 	r: 85.68 	f1: 89.36 	 2903 	 3109 	 3388
ni 	p: 83.55 	r: 80.42 	f1: 81.96 	 1224 	 1465 	 1522

[32m iter_1[0m
ga 	p: 82.27 	r: 74.08 	f1: 77.96 	 4569 	 5554 	 6168
wo 	p: 93.71 	r: 85.3 	f1: 89.31 	 2890 	 3084 	 3388
ni 	p: 86.89 	r: 78.84 	f1: 82.67 	 1200 	 1381 	 1522

[32m iter_2[0m
ga 	p: 81.0 	r: 75.08 	f1: 77.93 	 4631 	 5717 	 6168
wo 	p: 91.86 	r: 86.95 	f1: 89.34 	 2946 	 3207 	 3388
ni 	p: 88.53 	r: 77.6 	f1: 82.7 	 1181 	 1334 	 1522
best_thres [[0.31, 0.38, 0.32], [0.41, 0.41, 0.43], [0.35, 0.25, 0.55]]
f [0.8195, 0.8202, 0.8204]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 4 	 [0.43, 0.21, 0.14] 	 lr: 0.0001 	 f: 82.27963049945201
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 6 [0m
Train...

[32m iter_0[0m
ga 	p: 81.21 	r: 75.41 	f1: 78.2 	 4651 	 5727 	 6168
wo 	p: 92.53 	r: 86.33 	f1: 89.33 	 2925 	 3161 	 3388
ni 	p: 85.53 	r: 79.24 	f1: 82.26 	 1206 	 1410 	 1522

[32m iter_1[0m
ga 	p: 79.36 	r: 77.06 	f1: 78.19 	 4753 	 5989 	 6168
wo 	p: 92.28 	r: 86.48 	f1: 89.29 	 2930 	 3175 	 3388
ni 	p: 86.58 	r: 79.24 	f1: 82.74 	 1206 	 1393 	 1522

[32m iter_2[0m
ga 	p: 80.56 	r: 75.86 	f1: 78.14 	 4679 	 5808 	 6168
wo 	p: 93.02 	r: 85.8 	f1: 89.27 	 2907 	 3125 	 3388
ni 	p: 86.65 	r: 79.3 	f1: 82.81 	 1207 	 1393 	 1522
best_thres [[0.39, 0.28, 0.32], [0.29, 0.25, 0.33], [0.35, 0.33, 0.33]]
f [0.8217, 0.8217, 0.8217]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 4 	 [0.45, 0.28, 0.19] 	 lr: 0.0001 	 f: 82.55670745966171
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 6 [0m
Train...

[32m iter_0[0m
ga 	p: 79.94 	r: 76.04 	f1: 77.94 	 4690 	 5867 	 6168
wo 	p: 93.37 	r: 85.68 	f1: 89.36 	 2903 	 3109 	 3388
ni 	p: 83.55 	r: 80.42 	f1: 81.96 	 1224 	 1465 	 1522

[32m iter_1[0m
ga 	p: 82.27 	r: 74.08 	f1: 77.96 	 4569 	 5554 	 6168
wo 	p: 93.71 	r: 85.3 	f1: 89.31 	 2890 	 3084 	 3388
ni 	p: 86.89 	r: 78.84 	f1: 82.67 	 1200 	 1381 	 1522

[32m iter_2[0m
ga 	p: 81.0 	r: 75.08 	f1: 77.93 	 4631 	 5717 	 6168
wo 	p: 91.86 	r: 86.95 	f1: 89.34 	 2946 	 3207 	 3388
ni 	p: 88.53 	r: 77.6 	f1: 82.7 	 1181 	 1334 	 1522
best_thres [[0.31, 0.38, 0.32], [0.41, 0.41, 0.43], [0.35, 0.25, 0.55]]
f [0.8195, 0.8202, 0.8204]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 4 	 [0.43, 0.21, 0.14] 	 lr: 0.0001 	 f: 82.27963049945201
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 6 [0m
Train...
loss: tensor(1024.7300) lr: 0.0001 time: 4424.54
pred_count_train 41644

Test...
loss: tensor(1016.2843) lr: 0.0001 time: 4410.22
pred_count_train 41644

Test...
loss: tensor(1024.7300) lr: 0.0001 time: 4305.04
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.13 	r: 75.05 	f1: 77.51 	 4629 	 5777 	 6168
wo 	p: 92.99 	r: 85.74 	f1: 89.22 	 2905 	 3124 	 3388
ni 	p: 87.88 	r: 72.86 	f1: 79.67 	 1109 	 1262 	 1522

[32m iter_1[0m
ga 	p: 80.55 	r: 75.36 	f1: 77.87 	 4648 	 5770 	 6168
wo 	p: 93.6 	r: 85.54 	f1: 89.39 	 2898 	 3096 	 3388
ni 	p: 82.72 	r: 78.32 	f1: 80.46 	 1192 	 1441 	 1522

[32m iter_2[0m
ga 	p: 80.71 	r: 75.37 	f1: 77.95 	 4649 	 5760 	 6168
wo 	p: 93.55 	r: 85.6 	f1: 89.4 	 2900 	 3100 	 3388
ni 	p: 85.01 	r: 76.74 	f1: 80.66 	 1168 	 1374 	 1522
best_thres [[0.4, 0.5, 0.24], [0.4, 0.63, 0.1], [0.41, 0.67, 0.13]]
f [0.8138, 0.8155, 0.8164]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 4 	 [0.43, 0.21, 0.14] 	 lr: 0.0001 	 f: 82.27963049945201
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 7 [0m
Train...

[32m iter_0[0m
ga 	p: 80.47 	r: 75.68 	f1: 78.0 	 4668 	 5801 	 6168
wo 	p: 93.08 	r: 85.71 	f1: 89.24 	 2904 	 3120 	 3388
ni 	p: 84.38 	r: 75.62 	f1: 79.76 	 1151 	 1364 	 1522

[32m iter_1[0m
ga 	p: 81.92 	r: 74.32 	f1: 77.93 	 4584 	 5596 	 6168
wo 	p: 93.74 	r: 85.33 	f1: 89.34 	 2891 	 3084 	 3388
ni 	p: 87.03 	r: 74.05 	f1: 80.01 	 1127 	 1295 	 1522

[32m iter_2[0m
ga 	p: 81.77 	r: 74.45 	f1: 77.94 	 4592 	 5616 	 6168
wo 	p: 93.39 	r: 85.54 	f1: 89.29 	 2898 	 3103 	 3388
ni 	p: 87.12 	r: 73.78 	f1: 79.9 	 1123 	 1289 	 1522
best_thres [[0.39, 0.64, 0.12], [0.45, 0.68, 0.14], [0.44, 0.66, 0.14]]
f [0.8166, 0.8169, 0.8169]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 4 	 [0.45, 0.28, 0.19] 	 lr: 0.0001 	 f: 82.55670745966171
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 7 [0m
Train...

[32m iter_0[0m
ga 	p: 80.13 	r: 75.05 	f1: 77.51 	 4629 	 5777 	 6168
wo 	p: 92.99 	r: 85.74 	f1: 89.22 	 2905 	 3124 	 3388
ni 	p: 87.88 	r: 72.86 	f1: 79.67 	 1109 	 1262 	 1522

[32m iter_1[0m
ga 	p: 80.55 	r: 75.36 	f1: 77.87 	 4648 	 5770 	 6168
wo 	p: 93.6 	r: 85.54 	f1: 89.39 	 2898 	 3096 	 3388
ni 	p: 82.72 	r: 78.32 	f1: 80.46 	 1192 	 1441 	 1522

[32m iter_2[0m
ga 	p: 80.71 	r: 75.37 	f1: 77.95 	 4649 	 5760 	 6168
wo 	p: 93.55 	r: 85.6 	f1: 89.4 	 2900 	 3100 	 3388
ni 	p: 85.01 	r: 76.74 	f1: 80.66 	 1168 	 1374 	 1522
best_thres [[0.4, 0.5, 0.24], [0.4, 0.63, 0.1], [0.41, 0.67, 0.13]]
f [0.8138, 0.8155, 0.8164]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 4 	 [0.43, 0.21, 0.14] 	 lr: 0.0001 	 f: 82.27963049945201
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 7 [0m
Train...
loss: tensor(795.2424) lr: 0.0001 time: 4282.79
pred_count_train 41644

Test...
loss: tensor(788.4985) lr: 0.0001 time: 4275.12
pred_count_train 41644

Test...
loss: tensor(795.2424) lr: 0.0001 time: 4247.61
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.39 	r: 73.05 	f1: 76.55 	 4506 	 5605 	 6168
wo 	p: 92.49 	r: 85.48 	f1: 88.85 	 2896 	 3131 	 3388
ni 	p: 85.71 	r: 70.96 	f1: 77.64 	 1080 	 1260 	 1522

[32m iter_1[0m
ga 	p: 80.08 	r: 73.83 	f1: 76.83 	 4554 	 5687 	 6168
wo 	p: 92.18 	r: 85.92 	f1: 88.94 	 2911 	 3158 	 3388
ni 	p: 81.77 	r: 75.76 	f1: 78.65 	 1153 	 1410 	 1522

[32m iter_2[0m
ga 	p: 80.86 	r: 73.27 	f1: 76.87 	 4519 	 5589 	 6168
wo 	p: 92.96 	r: 85.39 	f1: 89.02 	 2893 	 3112 	 3388
ni 	p: 81.79 	r: 75.56 	f1: 78.55 	 1150 	 1406 	 1522
best_thres [[0.52, 0.31, 0.24], [0.49, 0.24, 0.11], [0.64, 0.32, 0.11]]
f [0.805, 0.8065, 0.8071]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 4 	 [0.43, 0.21, 0.14] 	 lr: 0.0001 	 f: 82.27963049945201
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 8 [0m
Train...

[32m iter_0[0m
ga 	p: 80.89 	r: 74.34 	f1: 77.48 	 4585 	 5668 	 6168
wo 	p: 93.36 	r: 85.48 	f1: 89.24 	 2896 	 3102 	 3388
ni 	p: 83.63 	r: 72.86 	f1: 77.88 	 1109 	 1326 	 1522

[32m iter_1[0m
ga 	p: 80.36 	r: 74.45 	f1: 77.29 	 4592 	 5714 	 6168
wo 	p: 93.57 	r: 85.09 	f1: 89.13 	 2883 	 3081 	 3388
ni 	p: 86.68 	r: 71.42 	f1: 78.31 	 1087 	 1254 	 1522

[32m iter_2[0m
ga 	p: 80.12 	r: 74.74 	f1: 77.34 	 4610 	 5754 	 6168
wo 	p: 93.54 	r: 85.06 	f1: 89.1 	 2882 	 3081 	 3388
ni 	p: 85.88 	r: 71.94 	f1: 78.3 	 1095 	 1275 	 1522
best_thres [[0.48, 0.48, 0.15], [0.43, 0.5, 0.17], [0.41, 0.5, 0.15]]
f [0.8114, 0.811, 0.8108]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 4 	 [0.45, 0.28, 0.19] 	 lr: 0.0001 	 f: 82.55670745966171
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 8 [0m
Train...

[32m iter_0[0m
ga 	p: 80.39 	r: 73.05 	f1: 76.55 	 4506 	 5605 	 6168
wo 	p: 92.49 	r: 85.48 	f1: 88.85 	 2896 	 3131 	 3388
ni 	p: 85.71 	r: 70.96 	f1: 77.64 	 1080 	 1260 	 1522

[32m iter_1[0m
ga 	p: 80.08 	r: 73.83 	f1: 76.83 	 4554 	 5687 	 6168
wo 	p: 92.18 	r: 85.92 	f1: 88.94 	 2911 	 3158 	 3388
ni 	p: 81.77 	r: 75.76 	f1: 78.65 	 1153 	 1410 	 1522

[32m iter_2[0m
ga 	p: 80.86 	r: 73.27 	f1: 76.87 	 4519 	 5589 	 6168
wo 	p: 92.96 	r: 85.39 	f1: 89.02 	 2893 	 3112 	 3388
ni 	p: 81.79 	r: 75.56 	f1: 78.55 	 1150 	 1406 	 1522
best_thres [[0.52, 0.31, 0.24], [0.49, 0.24, 0.11], [0.64, 0.32, 0.11]]
f [0.805, 0.8065, 0.8071]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 4 	 [0.43, 0.21, 0.14] 	 lr: 0.0001 	 f: 82.27963049945201
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 8 [0m
Train...
loss: tensor(617.5202) lr: 0.0001 time: 4242.05
pred_count_train 41644

Test...
loss: tensor(608.1269) lr: 0.0001 time: 4237.24
pred_count_train 41644

Test...
loss: tensor(617.5202) lr: 0.0001 time: 4202.09
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.49 	r: 72.86 	f1: 76.93 	 4494 	 5515 	 6168
wo 	p: 92.32 	r: 85.48 	f1: 88.77 	 2896 	 3137 	 3388
ni 	p: 81.22 	r: 74.44 	f1: 77.68 	 1133 	 1395 	 1522

[32m iter_1[0m
ga 	p: 81.52 	r: 73.95 	f1: 77.55 	 4561 	 5595 	 6168
wo 	p: 90.55 	r: 87.07 	f1: 88.78 	 2950 	 3258 	 3388
ni 	p: 85.19 	r: 72.21 	f1: 78.17 	 1099 	 1290 	 1522

[32m iter_2[0m
ga 	p: 81.08 	r: 74.21 	f1: 77.49 	 4577 	 5645 	 6168
wo 	p: 91.0 	r: 86.81 	f1: 88.85 	 2941 	 3232 	 3388
ni 	p: 83.37 	r: 73.78 	f1: 78.29 	 1123 	 1347 	 1522
best_thres [[0.53, 0.49, 0.16], [0.53, 0.27, 0.17], [0.49, 0.32, 0.12]]
f [0.8069, 0.8092, 0.8099]
load model: epoch4
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 4 	 [0.43, 0.21, 0.14] 	 lr: 0.0001 	 f: 82.27963049945201
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 9 [0m
Train...

[32m iter_0[0m
ga 	p: 79.87 	r: 74.69 	f1: 77.2 	 4607 	 5768 	 6168
wo 	p: 92.22 	r: 86.07 	f1: 89.04 	 2916 	 3162 	 3388
ni 	p: 85.52 	r: 74.51 	f1: 79.63 	 1134 	 1326 	 1522

[32m iter_1[0m
ga 	p: 80.23 	r: 74.55 	f1: 77.28 	 4598 	 5731 	 6168
wo 	p: 93.01 	r: 85.24 	f1: 88.96 	 2888 	 3105 	 3388
ni 	p: 81.55 	r: 78.12 	f1: 79.8 	 1189 	 1458 	 1522

[32m iter_2[0m
ga 	p: 79.62 	r: 74.98 	f1: 77.23 	 4625 	 5809 	 6168
wo 	p: 93.07 	r: 85.27 	f1: 89.0 	 2889 	 3104 	 3388
ni 	p: 82.37 	r: 77.33 	f1: 79.77 	 1177 	 1429 	 1522
best_thres [[0.31, 0.34, 0.21], [0.32, 0.45, 0.09], [0.28, 0.45, 0.1]]
f [0.8116, 0.8117, 0.8116]
load model: epoch4
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 4 	 [0.45, 0.28, 0.19] 	 lr: 0.0001 	 f: 82.55670745966171
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 9 [0m
Train...

[32m iter_0[0m
ga 	p: 81.49 	r: 72.86 	f1: 76.93 	 4494 	 5515 	 6168
wo 	p: 92.32 	r: 85.48 	f1: 88.77 	 2896 	 3137 	 3388
ni 	p: 81.22 	r: 74.44 	f1: 77.68 	 1133 	 1395 	 1522

[32m iter_1[0m
ga 	p: 81.52 	r: 73.95 	f1: 77.55 	 4561 	 5595 	 6168
wo 	p: 90.55 	r: 87.07 	f1: 88.78 	 2950 	 3258 	 3388
ni 	p: 85.19 	r: 72.21 	f1: 78.17 	 1099 	 1290 	 1522

[32m iter_2[0m
ga 	p: 81.08 	r: 74.21 	f1: 77.49 	 4577 	 5645 	 6168
wo 	p: 91.0 	r: 86.81 	f1: 88.85 	 2941 	 3232 	 3388
ni 	p: 83.37 	r: 73.78 	f1: 78.29 	 1123 	 1347 	 1522
best_thres [[0.53, 0.49, 0.16], [0.53, 0.27, 0.17], [0.49, 0.32, 0.12]]
f [0.8069, 0.8092, 0.8099]
load model: epoch4
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 4 	 [0.43, 0.21, 0.14] 	 lr: 0.0001 	 f: 82.27963049945201
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 9 [0m
Train...
loss: tensor(1121.0336) lr: 5e-05 time: 4276.38
pred_count_train 41644

Test...
loss: tensor(1116.5455) lr: 5e-05 time: 4266.53
pred_count_train 41644

Test...
loss: tensor(1121.0336) lr: 5e-05 time: 4250.59
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.29 	r: 76.49 	f1: 77.87 	 4718 	 5950 	 6168
wo 	p: 93.69 	r: 86.39 	f1: 89.9 	 2927 	 3124 	 3388
ni 	p: 87.36 	r: 74.44 	f1: 80.38 	 1133 	 1297 	 1522

[32m iter_1[0m
ga 	p: 82.44 	r: 74.42 	f1: 78.22 	 4590 	 5568 	 6168
wo 	p: 94.06 	r: 85.98 	f1: 89.84 	 2913 	 3097 	 3388
ni 	p: 85.71 	r: 76.08 	f1: 80.61 	 1158 	 1351 	 1522

[32m iter_2[0m
ga 	p: 82.84 	r: 74.19 	f1: 78.28 	 4576 	 5524 	 6168
wo 	p: 93.35 	r: 86.6 	f1: 89.85 	 2934 	 3143 	 3388
ni 	p: 81.95 	r: 79.37 	f1: 80.64 	 1208 	 1474 	 1522
best_thres [[0.37, 0.47, 0.28], [0.5, 0.55, 0.2], [0.55, 0.45, 0.12]]
f [0.8185, 0.8198, 0.8205]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 4 	 [0.43, 0.21, 0.14] 	 lr: 0.0001 	 f: 82.27963049945201
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 10 [0m
Train...

[32m iter_0[0m
ga 	p: 80.71 	r: 76.18 	f1: 78.38 	 4699 	 5822 	 6168
wo 	p: 94.04 	r: 86.22 	f1: 89.96 	 2921 	 3106 	 3388
ni 	p: 87.58 	r: 78.32 	f1: 82.69 	 1192 	 1361 	 1522

[32m iter_1[0m
ga 	p: 82.84 	r: 74.42 	f1: 78.4 	 4590 	 5541 	 6168
wo 	p: 92.31 	r: 87.84 	f1: 90.02 	 2976 	 3224 	 3388
ni 	p: 85.36 	r: 80.42 	f1: 82.81 	 1224 	 1434 	 1522

[32m iter_2[0m
ga 	p: 82.84 	r: 74.45 	f1: 78.42 	 4592 	 5543 	 6168
wo 	p: 92.2 	r: 87.87 	f1: 89.98 	 2977 	 3229 	 3388
ni 	p: 84.72 	r: 80.88 	f1: 82.76 	 1231 	 1453 	 1522
best_thres [[0.45, 0.6, 0.26], [0.54, 0.4, 0.18], [0.54, 0.39, 0.17]]
f [0.8248, 0.8255, 0.8257]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 9 	 [0.54, 0.39, 0.17] 	 lr: 5e-05 	 f: 82.57463211722207
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 10 [0m
Train...

[32m iter_0[0m
ga 	p: 79.29 	r: 76.49 	f1: 77.87 	 4718 	 5950 	 6168
wo 	p: 93.69 	r: 86.39 	f1: 89.9 	 2927 	 3124 	 3388
ni 	p: 87.36 	r: 74.44 	f1: 80.38 	 1133 	 1297 	 1522

[32m iter_1[0m
ga 	p: 82.44 	r: 74.42 	f1: 78.22 	 4590 	 5568 	 6168
wo 	p: 94.06 	r: 85.98 	f1: 89.84 	 2913 	 3097 	 3388
ni 	p: 85.71 	r: 76.08 	f1: 80.61 	 1158 	 1351 	 1522

[32m iter_2[0m
ga 	p: 82.84 	r: 74.19 	f1: 78.28 	 4576 	 5524 	 6168
wo 	p: 93.35 	r: 86.6 	f1: 89.85 	 2934 	 3143 	 3388
ni 	p: 81.95 	r: 79.37 	f1: 80.64 	 1208 	 1474 	 1522
best_thres [[0.37, 0.47, 0.28], [0.5, 0.55, 0.2], [0.55, 0.45, 0.12]]
f [0.8185, 0.8198, 0.8205]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 4 	 [0.43, 0.21, 0.14] 	 lr: 0.0001 	 f: 82.27963049945201
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 10 [0m
Train...
loss: tensor(886.9482) lr: 5e-05 time: 4375.47
pred_count_train 41644

Test...
loss: tensor(885.4316) lr: 5e-05 time: 4377.85
pred_count_train 41644

Test...
loss: tensor(886.9482) lr: 5e-05 time: 4346.44
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.78 	r: 73.64 	f1: 77.04 	 4542 	 5623 	 6168
wo 	p: 92.47 	r: 85.92 	f1: 89.08 	 2911 	 3148 	 3388
ni 	p: 82.68 	r: 77.14 	f1: 79.81 	 1174 	 1420 	 1522

[32m iter_1[0m
ga 	p: 80.91 	r: 74.3 	f1: 77.47 	 4583 	 5664 	 6168
wo 	p: 93.76 	r: 84.74 	f1: 89.02 	 2871 	 3062 	 3388
ni 	p: 85.78 	r: 76.08 	f1: 80.64 	 1158 	 1350 	 1522

[32m iter_2[0m
ga 	p: 80.62 	r: 74.48 	f1: 77.43 	 4594 	 5698 	 6168
wo 	p: 93.79 	r: 84.74 	f1: 89.04 	 2871 	 3061 	 3388
ni 	p: 86.1 	r: 76.08 	f1: 80.78 	 1158 	 1345 	 1522
best_thres [[0.31, 0.26, 0.15], [0.28, 0.34, 0.14], [0.27, 0.34, 0.14]]
f [0.8112, 0.8127, 0.8132]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 4 	 [0.43, 0.21, 0.14] 	 lr: 0.0001 	 f: 82.27963049945201
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 11 [0m
Train...

[32m iter_0[0m
ga 	p: 80.86 	r: 76.12 	f1: 78.42 	 4695 	 5806 	 6168
wo 	p: 93.36 	r: 86.33 	f1: 89.71 	 2925 	 3133 	 3388
ni 	p: 85.96 	r: 78.06 	f1: 81.82 	 1188 	 1382 	 1522

[32m iter_1[0m
ga 	p: 81.67 	r: 75.55 	f1: 78.49 	 4660 	 5706 	 6168
wo 	p: 92.46 	r: 86.81 	f1: 89.54 	 2941 	 3181 	 3388
ni 	p: 86.12 	r: 78.25 	f1: 82.0 	 1191 	 1383 	 1522

[32m iter_2[0m
ga 	p: 81.88 	r: 75.41 	f1: 78.51 	 4651 	 5680 	 6168
wo 	p: 92.02 	r: 87.13 	f1: 89.51 	 2952 	 3208 	 3388
ni 	p: 86.16 	r: 78.12 	f1: 81.94 	 1189 	 1380 	 1522
best_thres [[0.37, 0.35, 0.21], [0.4, 0.26, 0.17], [0.42, 0.23, 0.17]]
f [0.8232, 0.8234, 0.8236]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 9 	 [0.54, 0.39, 0.17] 	 lr: 5e-05 	 f: 82.57463211722207
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 11 [0m
Train...

[32m iter_0[0m
ga 	p: 80.78 	r: 73.64 	f1: 77.04 	 4542 	 5623 	 6168
wo 	p: 92.47 	r: 85.92 	f1: 89.08 	 2911 	 3148 	 3388
ni 	p: 82.68 	r: 77.14 	f1: 79.81 	 1174 	 1420 	 1522

[32m iter_1[0m
ga 	p: 80.91 	r: 74.3 	f1: 77.47 	 4583 	 5664 	 6168
wo 	p: 93.76 	r: 84.74 	f1: 89.02 	 2871 	 3062 	 3388
ni 	p: 85.78 	r: 76.08 	f1: 80.64 	 1158 	 1350 	 1522

[32m iter_2[0m
ga 	p: 80.62 	r: 74.48 	f1: 77.43 	 4594 	 5698 	 6168
wo 	p: 93.79 	r: 84.74 	f1: 89.04 	 2871 	 3061 	 3388
ni 	p: 86.1 	r: 76.08 	f1: 80.78 	 1158 	 1345 	 1522
best_thres [[0.31, 0.26, 0.15], [0.28, 0.34, 0.14], [0.27, 0.34, 0.14]]
f [0.8112, 0.8127, 0.8132]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 4 	 [0.43, 0.21, 0.14] 	 lr: 0.0001 	 f: 82.27963049945201
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 11 [0m
Train...
loss: tensor(697.6166) lr: 5e-05 time: 4311.25
pred_count_train 41644

Test...
loss: tensor(700.0416) lr: 5e-05 time: 4300.55
pred_count_train 41644

Test...
loss: tensor(697.6166) lr: 5e-05 time: 4162.58
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.98 	r: 74.61 	f1: 77.2 	 4602 	 5754 	 6168
wo 	p: 92.31 	r: 86.45 	f1: 89.29 	 2929 	 3173 	 3388
ni 	p: 84.05 	r: 74.77 	f1: 79.14 	 1138 	 1354 	 1522

[32m iter_1[0m
ga 	p: 79.74 	r: 75.28 	f1: 77.44 	 4643 	 5823 	 6168
wo 	p: 91.86 	r: 86.95 	f1: 89.34 	 2946 	 3207 	 3388
ni 	p: 86.92 	r: 73.78 	f1: 79.82 	 1123 	 1292 	 1522

[32m iter_2[0m
ga 	p: 80.15 	r: 75.26 	f1: 77.63 	 4642 	 5792 	 6168
wo 	p: 92.26 	r: 86.6 	f1: 89.34 	 2934 	 3180 	 3388
ni 	p: 88.83 	r: 73.13 	f1: 80.22 	 1113 	 1253 	 1522
best_thres [[0.45, 0.43, 0.21], [0.44, 0.38, 0.21], [0.45, 0.42, 0.24]]
f [0.8117, 0.813, 0.8139]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 4 	 [0.43, 0.21, 0.14] 	 lr: 0.0001 	 f: 82.27963049945201
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 12 [0m
Train...

[32m iter_0[0m
ga 	p: 81.83 	r: 73.67 	f1: 77.54 	 4544 	 5553 	 6168
wo 	p: 93.47 	r: 85.3 	f1: 89.2 	 2890 	 3092 	 3388
ni 	p: 86.83 	r: 74.51 	f1: 80.2 	 1134 	 1306 	 1522

[32m iter_1[0m
ga 	p: 79.99 	r: 75.42 	f1: 77.64 	 4652 	 5816 	 6168
wo 	p: 93.4 	r: 85.6 	f1: 89.33 	 2900 	 3105 	 3388
ni 	p: 84.61 	r: 77.66 	f1: 80.99 	 1182 	 1397 	 1522

[32m iter_2[0m
ga 	p: 80.0 	r: 75.34 	f1: 77.6 	 4647 	 5809 	 6168
wo 	p: 93.37 	r: 85.6 	f1: 89.31 	 2900 	 3106 	 3388
ni 	p: 84.93 	r: 77.4 	f1: 80.99 	 1178 	 1387 	 1522
best_thres [[0.57, 0.74, 0.37], [0.44, 0.72, 0.24], [0.44, 0.72, 0.24]]
f [0.8149, 0.8157, 0.8158]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 9 	 [0.54, 0.39, 0.17] 	 lr: 5e-05 	 f: 82.57463211722207
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 12 [0m
Train...

[32m iter_0[0m
ga 	p: 79.98 	r: 74.61 	f1: 77.2 	 4602 	 5754 	 6168
wo 	p: 92.31 	r: 86.45 	f1: 89.29 	 2929 	 3173 	 3388
ni 	p: 84.05 	r: 74.77 	f1: 79.14 	 1138 	 1354 	 1522

[32m iter_1[0m
ga 	p: 79.74 	r: 75.28 	f1: 77.44 	 4643 	 5823 	 6168
wo 	p: 91.86 	r: 86.95 	f1: 89.34 	 2946 	 3207 	 3388
ni 	p: 86.92 	r: 73.78 	f1: 79.82 	 1123 	 1292 	 1522

[32m iter_2[0m
ga 	p: 80.15 	r: 75.26 	f1: 77.63 	 4642 	 5792 	 6168
wo 	p: 92.26 	r: 86.6 	f1: 89.34 	 2934 	 3180 	 3388
ni 	p: 88.83 	r: 73.13 	f1: 80.22 	 1113 	 1253 	 1522
best_thres [[0.45, 0.43, 0.21], [0.44, 0.38, 0.21], [0.45, 0.42, 0.24]]
f [0.8117, 0.813, 0.8139]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 4 	 [0.43, 0.21, 0.14] 	 lr: 0.0001 	 f: 82.27963049945201
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 12 [0m
Train...
loss: tensor(555.6152) lr: 5e-05 time: 4242.06
pred_count_train 41644

Test...
loss: tensor(547.1395) lr: 5e-05 time: 4234.84
pred_count_train 41644

Test...
loss: tensor(555.6152) lr: 5e-05 time: 4186.35
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.02 	r: 73.3 	f1: 76.51 	 4521 	 5650 	 6168
wo 	p: 91.72 	r: 86.95 	f1: 89.27 	 2946 	 3212 	 3388
ni 	p: 88.69 	r: 71.09 	f1: 78.92 	 1082 	 1220 	 1522

[32m iter_1[0m
ga 	p: 80.55 	r: 74.17 	f1: 77.23 	 4575 	 5680 	 6168
wo 	p: 92.18 	r: 86.98 	f1: 89.51 	 2947 	 3197 	 3388
ni 	p: 88.74 	r: 73.0 	f1: 80.1 	 1111 	 1252 	 1522

[32m iter_2[0m
ga 	p: 79.46 	r: 75.0 	f1: 77.16 	 4626 	 5822 	 6168
wo 	p: 92.38 	r: 86.63 	f1: 89.41 	 2935 	 3177 	 3388
ni 	p: 88.63 	r: 73.26 	f1: 80.22 	 1115 	 1258 	 1522
best_thres [[0.49, 0.26, 0.53], [0.56, 0.26, 0.47], [0.45, 0.27, 0.45]]
f [0.808, 0.8111, 0.8118]
load model: epoch4
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 4 	 [0.43, 0.21, 0.14] 	 lr: 0.0001 	 f: 82.27963049945201
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 13 [0m
Train...

[32m iter_0[0m
ga 	p: 81.02 	r: 73.9 	f1: 77.29 	 4558 	 5626 	 6168
wo 	p: 93.26 	r: 86.19 	f1: 89.58 	 2920 	 3131 	 3388
ni 	p: 84.51 	r: 75.3 	f1: 79.64 	 1146 	 1356 	 1522

[32m iter_1[0m
ga 	p: 80.94 	r: 74.08 	f1: 77.36 	 4569 	 5645 	 6168
wo 	p: 93.93 	r: 85.48 	f1: 89.51 	 2896 	 3083 	 3388
ni 	p: 85.08 	r: 75.69 	f1: 80.11 	 1152 	 1354 	 1522

[32m iter_2[0m
ga 	p: 81.83 	r: 73.3 	f1: 77.33 	 4521 	 5525 	 6168
wo 	p: 93.93 	r: 85.45 	f1: 89.49 	 2895 	 3082 	 3388
ni 	p: 85.28 	r: 75.76 	f1: 80.24 	 1153 	 1352 	 1522
best_thres [[0.5, 0.51, 0.28], [0.48, 0.57, 0.22], [0.53, 0.58, 0.21]]
f [0.8139, 0.8142, 0.8143]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 9 	 [0.54, 0.39, 0.17] 	 lr: 5e-05 	 f: 82.57463211722207
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 13 [0m
Train...

[32m iter_0[0m
ga 	p: 80.02 	r: 73.3 	f1: 76.51 	 4521 	 5650 	 6168
wo 	p: 91.72 	r: 86.95 	f1: 89.27 	 2946 	 3212 	 3388
ni 	p: 88.69 	r: 71.09 	f1: 78.92 	 1082 	 1220 	 1522

[32m iter_1[0m
ga 	p: 80.55 	r: 74.17 	f1: 77.23 	 4575 	 5680 	 6168
wo 	p: 92.18 	r: 86.98 	f1: 89.51 	 2947 	 3197 	 3388
ni 	p: 88.74 	r: 73.0 	f1: 80.1 	 1111 	 1252 	 1522

[32m iter_2[0m
ga 	p: 79.46 	r: 75.0 	f1: 77.16 	 4626 	 5822 	 6168
wo 	p: 92.38 	r: 86.63 	f1: 89.41 	 2935 	 3177 	 3388
ni 	p: 88.63 	r: 73.26 	f1: 80.22 	 1115 	 1258 	 1522
best_thres [[0.49, 0.26, 0.53], [0.56, 0.26, 0.47], [0.45, 0.27, 0.45]]
f [0.808, 0.8111, 0.8118]
load model: epoch4
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 4 	 [0.43, 0.21, 0.14] 	 lr: 0.0001 	 f: 82.27963049945201
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 13 [0m
Train...
loss: tensor(1052.4047) lr: 2.5e-05 time: 4257.58
pred_count_train 41644

Test...
loss: tensor(421.7863) lr: 5e-05 time: 4253.39
pred_count_train 41644

Test...
loss: tensor(1052.4047) lr: 2.5e-05 time: 4192.61
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.11 	r: 75.23 	f1: 78.52 	 4640 	 5651 	 6168
wo 	p: 93.45 	r: 86.75 	f1: 89.97 	 2939 	 3145 	 3388
ni 	p: 84.96 	r: 78.71 	f1: 81.72 	 1198 	 1410 	 1522

[32m iter_1[0m
ga 	p: 82.25 	r: 75.6 	f1: 78.79 	 4663 	 5669 	 6168
wo 	p: 94.24 	r: 85.98 	f1: 89.92 	 2913 	 3091 	 3388
ni 	p: 86.3 	r: 77.4 	f1: 81.61 	 1178 	 1365 	 1522

[32m iter_2[0m
ga 	p: 82.09 	r: 75.94 	f1: 78.9 	 4684 	 5706 	 6168
wo 	p: 94.65 	r: 85.66 	f1: 89.93 	 2902 	 3066 	 3388
ni 	p: 86.71 	r: 77.14 	f1: 81.64 	 1174 	 1354 	 1522
best_thres [[0.52, 0.37, 0.14], [0.56, 0.42, 0.13], [0.57, 0.47, 0.13]]
f [0.8248, 0.8252, 0.8256]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 13 	 [0.57, 0.47, 0.13] 	 lr: 2.5e-05 	 f: 82.557975224129
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 14 [0m
Train...

[32m iter_0[0m
ga 	p: 79.54 	r: 74.81 	f1: 77.1 	 4614 	 5801 	 6168
wo 	p: 92.83 	r: 85.6 	f1: 89.07 	 2900 	 3124 	 3388
ni 	p: 82.92 	r: 72.08 	f1: 77.12 	 1097 	 1323 	 1522

[32m iter_1[0m
ga 	p: 81.38 	r: 73.98 	f1: 77.5 	 4563 	 5607 	 6168
wo 	p: 93.22 	r: 85.24 	f1: 89.05 	 2888 	 3098 	 3388
ni 	p: 82.46 	r: 73.52 	f1: 77.74 	 1119 	 1357 	 1522

[32m iter_2[0m
ga 	p: 81.48 	r: 74.06 	f1: 77.59 	 4568 	 5606 	 6168
wo 	p: 93.16 	r: 85.24 	f1: 89.03 	 2888 	 3100 	 3388
ni 	p: 82.01 	r: 73.98 	f1: 77.79 	 1126 	 1373 	 1522
best_thres [[0.6, 0.53, 0.23], [0.7, 0.58, 0.14], [0.7, 0.57, 0.12]]
f [0.8076, 0.8092, 0.8099]
load model: epoch9
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 9 	 [0.54, 0.39, 0.17] 	 lr: 5e-05 	 f: 82.57463211722207
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 14 [0m
Train...

[32m iter_0[0m
ga 	p: 82.11 	r: 75.23 	f1: 78.52 	 4640 	 5651 	 6168
wo 	p: 93.45 	r: 86.75 	f1: 89.97 	 2939 	 3145 	 3388
ni 	p: 84.96 	r: 78.71 	f1: 81.72 	 1198 	 1410 	 1522

[32m iter_1[0m
ga 	p: 82.25 	r: 75.6 	f1: 78.79 	 4663 	 5669 	 6168
wo 	p: 94.24 	r: 85.98 	f1: 89.92 	 2913 	 3091 	 3388
ni 	p: 86.3 	r: 77.4 	f1: 81.61 	 1178 	 1365 	 1522

[32m iter_2[0m
ga 	p: 82.09 	r: 75.94 	f1: 78.9 	 4684 	 5706 	 6168
wo 	p: 94.65 	r: 85.66 	f1: 89.93 	 2902 	 3066 	 3388
ni 	p: 86.71 	r: 77.14 	f1: 81.64 	 1174 	 1354 	 1522
best_thres [[0.52, 0.37, 0.14], [0.56, 0.42, 0.13], [0.57, 0.47, 0.13]]
f [0.8248, 0.8252, 0.8256]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 13 	 [0.57, 0.47, 0.13] 	 lr: 2.5e-05 	 f: 82.557975224129
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 14 [0m
Train...
loss: tensor(880.4377) lr: 2.5e-05 time: 4249.88
pred_count_train 41644

Test...
loss: tensor(798.7921) lr: 2.5e-05 time: 4249.06
pred_count_train 41644

Test...
loss: tensor(880.4377) lr: 2.5e-05 time: 4285.78
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.77 	r: 74.63 	f1: 78.04 	 4603 	 5629 	 6168
wo 	p: 93.79 	r: 86.1 	f1: 89.78 	 2917 	 3110 	 3388
ni 	p: 84.29 	r: 73.0 	f1: 78.24 	 1111 	 1318 	 1522

[32m iter_1[0m
ga 	p: 81.6 	r: 75.23 	f1: 78.29 	 4640 	 5686 	 6168
wo 	p: 93.53 	r: 86.57 	f1: 89.91 	 2933 	 3136 	 3388
ni 	p: 84.94 	r: 73.39 	f1: 78.75 	 1117 	 1315 	 1522

[32m iter_2[0m
ga 	p: 83.46 	r: 73.85 	f1: 78.36 	 4555 	 5458 	 6168
wo 	p: 93.66 	r: 86.36 	f1: 89.86 	 2926 	 3124 	 3388
ni 	p: 82.32 	r: 75.56 	f1: 78.79 	 1150 	 1397 	 1522
best_thres [[0.41, 0.47, 0.11], [0.38, 0.44, 0.08], [0.48, 0.47, 0.06]]
f [0.8167, 0.818, 0.8186]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 13 	 [0.57, 0.47, 0.13] 	 lr: 2.5e-05 	 f: 82.557975224129
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 15 [0m
Train...

[32m iter_0[0m
ga 	p: 83.07 	r: 74.32 	f1: 78.45 	 4584 	 5518 	 6168
wo 	p: 93.32 	r: 87.4 	f1: 90.26 	 2961 	 3173 	 3388
ni 	p: 83.74 	r: 76.48 	f1: 79.95 	 1164 	 1390 	 1522

[32m iter_1[0m
ga 	p: 83.07 	r: 74.69 	f1: 78.66 	 4607 	 5546 	 6168
wo 	p: 93.72 	r: 86.84 	f1: 90.15 	 2942 	 3139 	 3388
ni 	p: 84.1 	r: 76.08 	f1: 79.89 	 1158 	 1377 	 1522

[32m iter_2[0m
ga 	p: 83.05 	r: 74.72 	f1: 78.67 	 4609 	 5550 	 6168
wo 	p: 93.69 	r: 86.84 	f1: 90.13 	 2942 	 3140 	 3388
ni 	p: 80.48 	r: 79.11 	f1: 79.79 	 1204 	 1496 	 1522
best_thres [[0.46, 0.42, 0.09], [0.44, 0.47, 0.08], [0.44, 0.47, 0.05]]
f [0.8232, 0.8235, 0.8235]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 9 	 [0.54, 0.39, 0.17] 	 lr: 5e-05 	 f: 82.57463211722207
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 15 [0m
Train...

[32m iter_0[0m
ga 	p: 81.77 	r: 74.63 	f1: 78.04 	 4603 	 5629 	 6168
wo 	p: 93.79 	r: 86.1 	f1: 89.78 	 2917 	 3110 	 3388
ni 	p: 84.29 	r: 73.0 	f1: 78.24 	 1111 	 1318 	 1522

[32m iter_1[0m
ga 	p: 81.6 	r: 75.23 	f1: 78.29 	 4640 	 5686 	 6168
wo 	p: 93.53 	r: 86.57 	f1: 89.91 	 2933 	 3136 	 3388
ni 	p: 84.94 	r: 73.39 	f1: 78.75 	 1117 	 1315 	 1522

[32m iter_2[0m
ga 	p: 83.46 	r: 73.85 	f1: 78.36 	 4555 	 5458 	 6168
wo 	p: 93.66 	r: 86.36 	f1: 89.86 	 2926 	 3124 	 3388
ni 	p: 82.32 	r: 75.56 	f1: 78.79 	 1150 	 1397 	 1522
best_thres [[0.41, 0.47, 0.11], [0.38, 0.44, 0.08], [0.48, 0.47, 0.06]]
f [0.8167, 0.818, 0.8186]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 13 	 [0.57, 0.47, 0.13] 	 lr: 2.5e-05 	 f: 82.557975224129
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 15 [0m
Train...
loss: tensor(738.0099) lr: 2.5e-05 time: 4346.42
pred_count_train 41644

Test...
loss: tensor(658.6924) lr: 2.5e-05 time: 4349.66
pred_count_train 41644

Test...
loss: tensor(738.0099) lr: 2.5e-05 time: 4156.54
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.16 	r: 74.38 	f1: 78.08 	 4588 	 5584 	 6168
wo 	p: 93.18 	r: 86.72 	f1: 89.83 	 2938 	 3153 	 3388
ni 	p: 85.78 	r: 74.51 	f1: 79.75 	 1134 	 1322 	 1522

[32m iter_1[0m
ga 	p: 81.23 	r: 76.04 	f1: 78.55 	 4690 	 5774 	 6168
wo 	p: 92.53 	r: 87.04 	f1: 89.7 	 2949 	 3187 	 3388
ni 	p: 87.2 	r: 75.16 	f1: 80.73 	 1144 	 1312 	 1522

[32m iter_2[0m
ga 	p: 81.88 	r: 75.54 	f1: 78.58 	 4659 	 5690 	 6168
wo 	p: 93.07 	r: 86.48 	f1: 89.66 	 2930 	 3148 	 3388
ni 	p: 85.67 	r: 76.61 	f1: 80.89 	 1166 	 1361 	 1522
best_thres [[0.49, 0.34, 0.16], [0.42, 0.26, 0.13], [0.47, 0.3, 0.1]]
f [0.8194, 0.8211, 0.8217]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 13 	 [0.57, 0.47, 0.13] 	 lr: 2.5e-05 	 f: 82.557975224129
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 16 [0m
Train...

[32m iter_0[0m
ga 	p: 81.97 	r: 75.34 	f1: 78.52 	 4647 	 5669 	 6168
wo 	p: 94.24 	r: 86.51 	f1: 90.21 	 2931 	 3110 	 3388
ni 	p: 86.0 	r: 77.07 	f1: 81.29 	 1173 	 1364 	 1522

[32m iter_1[0m
ga 	p: 82.53 	r: 75.19 	f1: 78.69 	 4638 	 5620 	 6168
wo 	p: 94.0 	r: 86.51 	f1: 90.1 	 2931 	 3118 	 3388
ni 	p: 86.56 	r: 77.0 	f1: 81.5 	 1172 	 1354 	 1522

[32m iter_2[0m
ga 	p: 82.51 	r: 75.28 	f1: 78.73 	 4643 	 5627 	 6168
wo 	p: 94.06 	r: 86.51 	f1: 90.13 	 2931 	 3116 	 3388
ni 	p: 86.16 	r: 77.33 	f1: 81.51 	 1177 	 1366 	 1522
best_thres [[0.5, 0.55, 0.17], [0.51, 0.51, 0.14], [0.51, 0.51, 0.13]]
f [0.8247, 0.8253, 0.8255]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 9 	 [0.54, 0.39, 0.17] 	 lr: 5e-05 	 f: 82.57463211722207
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 16 [0m
Train...

[32m iter_0[0m
ga 	p: 82.16 	r: 74.38 	f1: 78.08 	 4588 	 5584 	 6168
wo 	p: 93.18 	r: 86.72 	f1: 89.83 	 2938 	 3153 	 3388
ni 	p: 85.78 	r: 74.51 	f1: 79.75 	 1134 	 1322 	 1522

[32m iter_1[0m
ga 	p: 81.23 	r: 76.04 	f1: 78.55 	 4690 	 5774 	 6168
wo 	p: 92.53 	r: 87.04 	f1: 89.7 	 2949 	 3187 	 3388
ni 	p: 87.2 	r: 75.16 	f1: 80.73 	 1144 	 1312 	 1522

[32m iter_2[0m
ga 	p: 81.88 	r: 75.54 	f1: 78.58 	 4659 	 5690 	 6168
wo 	p: 93.07 	r: 86.48 	f1: 89.66 	 2930 	 3148 	 3388
ni 	p: 85.67 	r: 76.61 	f1: 80.89 	 1166 	 1361 	 1522
best_thres [[0.49, 0.34, 0.16], [0.42, 0.26, 0.13], [0.47, 0.3, 0.1]]
f [0.8194, 0.8211, 0.8217]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 13 	 [0.57, 0.47, 0.13] 	 lr: 2.5e-05 	 f: 82.557975224129
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 16 [0m
Train...
loss: tensor(621.4756) lr: 2.5e-05 time: 4263.41
pred_count_train 41644

Test...
loss: tensor(544.0150) lr: 2.5e-05 time: 4252.56
pred_count_train 41644

Test...
loss: tensor(621.4756) lr: 2.5e-05 time: 4168.49
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.7 	r: 74.08 	f1: 77.24 	 4569 	 5662 	 6168
wo 	p: 93.24 	r: 86.3 	f1: 89.64 	 2924 	 3136 	 3388
ni 	p: 84.66 	r: 72.54 	f1: 78.13 	 1104 	 1304 	 1522

[32m iter_1[0m
ga 	p: 81.21 	r: 74.84 	f1: 77.89 	 4616 	 5684 	 6168
wo 	p: 93.23 	r: 86.13 	f1: 89.54 	 2918 	 3130 	 3388
ni 	p: 84.07 	r: 74.9 	f1: 79.22 	 1140 	 1356 	 1522

[32m iter_2[0m
ga 	p: 82.52 	r: 73.91 	f1: 77.98 	 4559 	 5525 	 6168
wo 	p: 92.95 	r: 86.39 	f1: 89.55 	 2927 	 3149 	 3388
ni 	p: 84.63 	r: 74.18 	f1: 79.06 	 1129 	 1334 	 1522
best_thres [[0.43, 0.34, 0.19], [0.44, 0.33, 0.1], [0.67, 0.31, 0.1]]
f [0.8118, 0.8141, 0.8151]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 13 	 [0.57, 0.47, 0.13] 	 lr: 2.5e-05 	 f: 82.557975224129
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 17 [0m
Train...

[32m iter_0[0m
ga 	p: 79.81 	r: 75.83 	f1: 77.77 	 4677 	 5860 	 6168
wo 	p: 93.97 	r: 86.07 	f1: 89.85 	 2916 	 3103 	 3388
ni 	p: 86.49 	r: 72.34 	f1: 78.78 	 1101 	 1273 	 1522

[32m iter_1[0m
ga 	p: 79.91 	r: 76.41 	f1: 78.12 	 4713 	 5898 	 6168
wo 	p: 93.74 	r: 86.13 	f1: 89.77 	 2918 	 3113 	 3388
ni 	p: 87.2 	r: 72.93 	f1: 79.43 	 1110 	 1273 	 1522

[32m iter_2[0m
ga 	p: 80.24 	r: 76.04 	f1: 78.08 	 4690 	 5845 	 6168
wo 	p: 93.67 	r: 85.98 	f1: 89.66 	 2913 	 3110 	 3388
ni 	p: 87.05 	r: 72.86 	f1: 79.33 	 1109 	 1274 	 1522
best_thres [[0.36, 0.54, 0.27], [0.33, 0.5, 0.22], [0.35, 0.5, 0.21]]
f [0.8158, 0.8171, 0.8173]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 9 	 [0.54, 0.39, 0.17] 	 lr: 5e-05 	 f: 82.57463211722207
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 17 [0m
Train...

[32m iter_0[0m
ga 	p: 80.7 	r: 74.08 	f1: 77.24 	 4569 	 5662 	 6168
wo 	p: 93.24 	r: 86.3 	f1: 89.64 	 2924 	 3136 	 3388
ni 	p: 84.66 	r: 72.54 	f1: 78.13 	 1104 	 1304 	 1522

[32m iter_1[0m
ga 	p: 81.21 	r: 74.84 	f1: 77.89 	 4616 	 5684 	 6168
wo 	p: 93.23 	r: 86.13 	f1: 89.54 	 2918 	 3130 	 3388
ni 	p: 84.07 	r: 74.9 	f1: 79.22 	 1140 	 1356 	 1522

[32m iter_2[0m
ga 	p: 82.52 	r: 73.91 	f1: 77.98 	 4559 	 5525 	 6168
wo 	p: 92.95 	r: 86.39 	f1: 89.55 	 2927 	 3149 	 3388
ni 	p: 84.63 	r: 74.18 	f1: 79.06 	 1129 	 1334 	 1522
best_thres [[0.43, 0.34, 0.19], [0.44, 0.33, 0.1], [0.67, 0.31, 0.1]]
f [0.8118, 0.8141, 0.8151]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 13 	 [0.57, 0.47, 0.13] 	 lr: 2.5e-05 	 f: 82.557975224129
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 17 [0m
Train...
loss: tensor(514.1273) lr: 2.5e-05 time: 4250.85
pred_count_train 41644

Test...
loss: tensor(441.3683) lr: 2.5e-05 time: 4249.23
pred_count_train 41644

Test...
loss: tensor(514.1273) lr: 2.5e-05 time: 4179.19
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.59 	r: 73.23 	f1: 76.73 	 4517 	 5605 	 6168
wo 	p: 92.0 	r: 86.25 	f1: 89.03 	 2922 	 3176 	 3388
ni 	p: 82.94 	r: 71.55 	f1: 76.83 	 1089 	 1313 	 1522

[32m iter_1[0m
ga 	p: 80.33 	r: 74.34 	f1: 77.21 	 4585 	 5708 	 6168
wo 	p: 92.09 	r: 86.3 	f1: 89.11 	 2924 	 3175 	 3388
ni 	p: 81.47 	r: 75.1 	f1: 78.15 	 1143 	 1403 	 1522

[32m iter_2[0m
ga 	p: 80.27 	r: 74.61 	f1: 77.34 	 4602 	 5733 	 6168
wo 	p: 92.04 	r: 86.36 	f1: 89.11 	 2926 	 3179 	 3388
ni 	p: 84.04 	r: 73.72 	f1: 78.54 	 1122 	 1335 	 1522
best_thres [[0.5, 0.45, 0.23], [0.49, 0.46, 0.11], [0.48, 0.45, 0.14]]
f [0.8056, 0.8078, 0.8089]
load model: epoch13
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 13 	 [0.57, 0.47, 0.13] 	 lr: 2.5e-05 	 f: 82.557975224129
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 18 [0m
Train...

[32m iter_0[0m
ga 	p: 79.61 	r: 75.57 	f1: 77.53 	 4661 	 5855 	 6168
wo 	p: 93.58 	r: 86.01 	f1: 89.63 	 2914 	 3114 	 3388
ni 	p: 81.54 	r: 74.31 	f1: 77.76 	 1131 	 1387 	 1522

[32m iter_1[0m
ga 	p: 79.99 	r: 75.63 	f1: 77.75 	 4665 	 5832 	 6168
wo 	p: 93.25 	r: 86.04 	f1: 89.5 	 2915 	 3126 	 3388
ni 	p: 82.98 	r: 74.97 	f1: 78.77 	 1141 	 1375 	 1522

[32m iter_2[0m
ga 	p: 79.78 	r: 75.92 	f1: 77.8 	 4683 	 5870 	 6168
wo 	p: 94.33 	r: 84.98 	f1: 89.41 	 2879 	 3052 	 3388
ni 	p: 83.94 	r: 74.18 	f1: 78.76 	 1129 	 1345 	 1522
best_thres [[0.41, 0.63, 0.18], [0.38, 0.55, 0.13], [0.35, 0.71, 0.14]]
f [0.8124, 0.8135, 0.8138]
load model: epoch9
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 9 	 [0.54, 0.39, 0.17] 	 lr: 5e-05 	 f: 82.57463211722207
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 18 [0m
Train...

[32m iter_0[0m
ga 	p: 80.59 	r: 73.23 	f1: 76.73 	 4517 	 5605 	 6168
wo 	p: 92.0 	r: 86.25 	f1: 89.03 	 2922 	 3176 	 3388
ni 	p: 82.94 	r: 71.55 	f1: 76.83 	 1089 	 1313 	 1522

[32m iter_1[0m
ga 	p: 80.33 	r: 74.34 	f1: 77.21 	 4585 	 5708 	 6168
wo 	p: 92.09 	r: 86.3 	f1: 89.11 	 2924 	 3175 	 3388
ni 	p: 81.47 	r: 75.1 	f1: 78.15 	 1143 	 1403 	 1522

[32m iter_2[0m
ga 	p: 80.27 	r: 74.61 	f1: 77.34 	 4602 	 5733 	 6168
wo 	p: 92.04 	r: 86.36 	f1: 89.11 	 2926 	 3179 	 3388
ni 	p: 84.04 	r: 73.72 	f1: 78.54 	 1122 	 1335 	 1522
best_thres [[0.5, 0.45, 0.23], [0.49, 0.46, 0.11], [0.48, 0.45, 0.14]]
f [0.8056, 0.8078, 0.8089]
load model: epoch13
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 13 	 [0.57, 0.47, 0.13] 	 lr: 2.5e-05 	 f: 82.557975224129
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 18 [0m
Train...
loss: tensor(837.6509) lr: 1.25e-05 time: 4228.25
pred_count_train 41644

Test...
loss: tensor(770.3761) lr: 1.25e-05 time: 4226.96
pred_count_train 41644

Test...
loss: tensor(837.6509) lr: 1.25e-05 time: 4203.24
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.82 	r: 75.24 	f1: 78.4 	 4641 	 5672 	 6168
wo 	p: 93.49 	r: 86.42 	f1: 89.82 	 2928 	 3132 	 3388
ni 	p: 85.05 	r: 77.4 	f1: 81.05 	 1178 	 1385 	 1522

[32m iter_1[0m
ga 	p: 80.68 	r: 76.72 	f1: 78.65 	 4732 	 5865 	 6168
wo 	p: 94.27 	r: 85.89 	f1: 89.88 	 2910 	 3087 	 3388
ni 	p: 87.37 	r: 76.35 	f1: 81.49 	 1162 	 1330 	 1522

[32m iter_2[0m
ga 	p: 80.77 	r: 76.75 	f1: 78.71 	 4734 	 5861 	 6168
wo 	p: 93.81 	r: 86.36 	f1: 89.93 	 2926 	 3119 	 3388
ni 	p: 86.34 	r: 76.81 	f1: 81.29 	 1169 	 1354 	 1522
best_thres [[0.39, 0.41, 0.15], [0.31, 0.46, 0.14], [0.31, 0.41, 0.12]]
f [0.8226, 0.8235, 0.8239]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 13 	 [0.57, 0.47, 0.13] 	 lr: 2.5e-05 	 f: 82.557975224129
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 19 [0m
Train...

[32m iter_0[0m
ga 	p: 82.6 	r: 75.81 	f1: 79.06 	 4676 	 5661 	 6168
wo 	p: 92.81 	r: 88.08 	f1: 90.38 	 2984 	 3215 	 3388
ni 	p: 87.62 	r: 77.66 	f1: 82.34 	 1182 	 1349 	 1522

[32m iter_1[0m
ga 	p: 82.77 	r: 75.68 	f1: 79.07 	 4668 	 5640 	 6168
wo 	p: 93.19 	r: 88.02 	f1: 90.53 	 2982 	 3200 	 3388
ni 	p: 87.31 	r: 77.73 	f1: 82.24 	 1183 	 1355 	 1522

[32m iter_2[0m
ga 	p: 82.84 	r: 75.78 	f1: 79.15 	 4674 	 5642 	 6168
wo 	p: 93.02 	r: 88.13 	f1: 90.51 	 2986 	 3210 	 3388
ni 	p: 87.45 	r: 77.86 	f1: 82.38 	 1185 	 1355 	 1522
best_thres [[0.44, 0.36, 0.19], [0.44, 0.37, 0.16], [0.44, 0.36, 0.16]]
f [0.8301, 0.8303, 0.8306]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 18 	 [0.44, 0.36, 0.16] 	 lr: 1.25e-05 	 f: 83.05538591628694
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 19 [0m
Train...

[32m iter_0[0m
ga 	p: 81.82 	r: 75.24 	f1: 78.4 	 4641 	 5672 	 6168
wo 	p: 93.49 	r: 86.42 	f1: 89.82 	 2928 	 3132 	 3388
ni 	p: 85.05 	r: 77.4 	f1: 81.05 	 1178 	 1385 	 1522

[32m iter_1[0m
ga 	p: 80.68 	r: 76.72 	f1: 78.65 	 4732 	 5865 	 6168
wo 	p: 94.27 	r: 85.89 	f1: 89.88 	 2910 	 3087 	 3388
ni 	p: 87.37 	r: 76.35 	f1: 81.49 	 1162 	 1330 	 1522

[32m iter_2[0m
ga 	p: 80.77 	r: 76.75 	f1: 78.71 	 4734 	 5861 	 6168
wo 	p: 93.81 	r: 86.36 	f1: 89.93 	 2926 	 3119 	 3388
ni 	p: 86.34 	r: 76.81 	f1: 81.29 	 1169 	 1354 	 1522
best_thres [[0.39, 0.41, 0.15], [0.31, 0.46, 0.14], [0.31, 0.41, 0.12]]
f [0.8226, 0.8235, 0.8239]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 13 	 [0.57, 0.47, 0.13] 	 lr: 2.5e-05 	 f: 82.557975224129
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 19 [0m
Train...
loss: tensor(751.2003) lr: 1.25e-05 time: 4213.66
pred_count_train 41644

Test...
loss: tensor(675.6660) lr: 1.25e-05 time: 4244.55
pred_count_train 41644

Test...
loss: tensor(751.2003) lr: 1.25e-05 time: 4220.08
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.27 	r: 74.85 	f1: 77.93 	 4617 	 5681 	 6168
wo 	p: 93.31 	r: 86.51 	f1: 89.78 	 2931 	 3141 	 3388
ni 	p: 82.95 	r: 77.99 	f1: 80.39 	 1187 	 1431 	 1522

[32m iter_1[0m
ga 	p: 82.12 	r: 74.92 	f1: 78.36 	 4621 	 5627 	 6168
wo 	p: 93.16 	r: 86.84 	f1: 89.89 	 2942 	 3158 	 3388
ni 	p: 86.24 	r: 75.76 	f1: 80.66 	 1153 	 1337 	 1522

[32m iter_2[0m
ga 	p: 81.67 	r: 75.36 	f1: 78.39 	 4648 	 5691 	 6168
wo 	p: 92.88 	r: 87.07 	f1: 89.88 	 2950 	 3176 	 3388
ni 	p: 85.36 	r: 76.61 	f1: 80.75 	 1166 	 1366 	 1522
best_thres [[0.4, 0.45, 0.16], [0.42, 0.41, 0.16], [0.39, 0.39, 0.14]]
f [0.819, 0.8206, 0.8212]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 13 	 [0.57, 0.47, 0.13] 	 lr: 2.5e-05 	 f: 82.557975224129
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 20 [0m
Train...

[32m iter_0[0m
ga 	p: 81.48 	r: 75.62 	f1: 78.44 	 4664 	 5724 	 6168
wo 	p: 93.92 	r: 86.57 	f1: 90.09 	 2933 	 3123 	 3388
ni 	p: 87.97 	r: 76.41 	f1: 81.79 	 1163 	 1322 	 1522

[32m iter_1[0m
ga 	p: 83.65 	r: 74.01 	f1: 78.54 	 4565 	 5457 	 6168
wo 	p: 93.56 	r: 87.01 	f1: 90.17 	 2948 	 3151 	 3388
ni 	p: 89.28 	r: 74.97 	f1: 81.5 	 1141 	 1278 	 1522

[32m iter_2[0m
ga 	p: 81.58 	r: 75.76 	f1: 78.56 	 4673 	 5728 	 6168
wo 	p: 93.56 	r: 87.07 	f1: 90.2 	 2950 	 3153 	 3388
ni 	p: 85.15 	r: 78.38 	f1: 81.63 	 1193 	 1401 	 1522
best_thres [[0.43, 0.58, 0.23], [0.53, 0.52, 0.24], [0.41, 0.51, 0.15]]
f [0.8246, 0.8251, 0.8252]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 18 	 [0.44, 0.36, 0.16] 	 lr: 1.25e-05 	 f: 83.05538591628694
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 20 [0m
Train...

[32m iter_0[0m
ga 	p: 81.27 	r: 74.85 	f1: 77.93 	 4617 	 5681 	 6168
wo 	p: 93.31 	r: 86.51 	f1: 89.78 	 2931 	 3141 	 3388
ni 	p: 82.95 	r: 77.99 	f1: 80.39 	 1187 	 1431 	 1522

[32m iter_1[0m
ga 	p: 82.12 	r: 74.92 	f1: 78.36 	 4621 	 5627 	 6168
wo 	p: 93.16 	r: 86.84 	f1: 89.89 	 2942 	 3158 	 3388
ni 	p: 86.24 	r: 75.76 	f1: 80.66 	 1153 	 1337 	 1522

[32m iter_2[0m
ga 	p: 81.67 	r: 75.36 	f1: 78.39 	 4648 	 5691 	 6168
wo 	p: 92.88 	r: 87.07 	f1: 89.88 	 2950 	 3176 	 3388
ni 	p: 85.36 	r: 76.61 	f1: 80.75 	 1166 	 1366 	 1522
best_thres [[0.4, 0.45, 0.16], [0.42, 0.41, 0.16], [0.39, 0.39, 0.14]]
f [0.819, 0.8206, 0.8212]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 13 	 [0.57, 0.47, 0.13] 	 lr: 2.5e-05 	 f: 82.557975224129
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 20 [0m
Train...
loss: tensor(670.2593) lr: 1.25e-05 time: 4323.5
pred_count_train 41644

Test...
loss: tensor(595.8660) lr: 1.25e-05 time: 4322.0
pred_count_train 41644

Test...
loss: tensor(670.2593) lr: 1.25e-05 time: 4115.45
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.7 	r: 73.9 	f1: 77.6 	 4558 	 5579 	 6168
wo 	p: 94.37 	r: 85.57 	f1: 89.75 	 2899 	 3072 	 3388
ni 	p: 86.13 	r: 75.89 	f1: 80.68 	 1155 	 1341 	 1522

[32m iter_1[0m
ga 	p: 81.8 	r: 74.74 	f1: 78.11 	 4610 	 5636 	 6168
wo 	p: 94.31 	r: 85.54 	f1: 89.71 	 2898 	 3073 	 3388
ni 	p: 85.92 	r: 77.0 	f1: 81.22 	 1172 	 1364 	 1522

[32m iter_2[0m
ga 	p: 81.6 	r: 74.87 	f1: 78.09 	 4618 	 5659 	 6168
wo 	p: 93.87 	r: 85.89 	f1: 89.7 	 2910 	 3100 	 3388
ni 	p: 86.22 	r: 76.87 	f1: 81.28 	 1170 	 1357 	 1522
best_thres [[0.39, 0.53, 0.17], [0.35, 0.7, 0.11], [0.34, 0.63, 0.11]]
f [0.8175, 0.8191, 0.8197]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 13 	 [0.57, 0.47, 0.13] 	 lr: 2.5e-05 	 f: 82.557975224129
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 21 [0m
Train...

[32m iter_0[0m
ga 	p: 80.94 	r: 75.79 	f1: 78.28 	 4675 	 5776 	 6168
wo 	p: 93.89 	r: 86.57 	f1: 90.08 	 2933 	 3124 	 3388
ni 	p: 85.22 	r: 78.45 	f1: 81.7 	 1194 	 1401 	 1522

[32m iter_1[0m
ga 	p: 80.49 	r: 76.36 	f1: 78.37 	 4710 	 5852 	 6168
wo 	p: 92.98 	r: 87.13 	f1: 89.96 	 2952 	 3175 	 3388
ni 	p: 85.09 	r: 78.38 	f1: 81.6 	 1193 	 1402 	 1522

[32m iter_2[0m
ga 	p: 80.43 	r: 76.31 	f1: 78.32 	 4707 	 5852 	 6168
wo 	p: 92.67 	r: 87.37 	f1: 89.94 	 2960 	 3194 	 3388
ni 	p: 85.36 	r: 78.12 	f1: 81.58 	 1189 	 1393 	 1522
best_thres [[0.36, 0.58, 0.16], [0.3, 0.47, 0.12], [0.3, 0.43, 0.12]]
f [0.8234, 0.8234, 0.8233]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 18 	 [0.44, 0.36, 0.16] 	 lr: 1.25e-05 	 f: 83.05538591628694
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 21 [0m
Train...

[32m iter_0[0m
ga 	p: 81.7 	r: 73.9 	f1: 77.6 	 4558 	 5579 	 6168
wo 	p: 94.37 	r: 85.57 	f1: 89.75 	 2899 	 3072 	 3388
ni 	p: 86.13 	r: 75.89 	f1: 80.68 	 1155 	 1341 	 1522

[32m iter_1[0m
ga 	p: 81.8 	r: 74.74 	f1: 78.11 	 4610 	 5636 	 6168
wo 	p: 94.31 	r: 85.54 	f1: 89.71 	 2898 	 3073 	 3388
ni 	p: 85.92 	r: 77.0 	f1: 81.22 	 1172 	 1364 	 1522

[32m iter_2[0m
ga 	p: 81.6 	r: 74.87 	f1: 78.09 	 4618 	 5659 	 6168
wo 	p: 93.87 	r: 85.89 	f1: 89.7 	 2910 	 3100 	 3388
ni 	p: 86.22 	r: 76.87 	f1: 81.28 	 1170 	 1357 	 1522
best_thres [[0.39, 0.53, 0.17], [0.35, 0.7, 0.11], [0.34, 0.63, 0.11]]
f [0.8175, 0.8191, 0.8197]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 13 	 [0.57, 0.47, 0.13] 	 lr: 2.5e-05 	 f: 82.557975224129
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 21 [0m
Train...
loss: tensor(598.9451) lr: 1.25e-05 time: 4220.21
pred_count_train 41644

Test...
loss: tensor(527.1609) lr: 1.25e-05 time: 4227.77
pred_count_train 41644

Test...
loss: tensor(598.9451) lr: 1.25e-05 time: 4153.29
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.67 	r: 74.98 	f1: 77.26 	 4625 	 5805 	 6168
wo 	p: 92.86 	r: 86.81 	f1: 89.73 	 2941 	 3167 	 3388
ni 	p: 86.5 	r: 72.4 	f1: 78.83 	 1102 	 1274 	 1522

[32m iter_1[0m
ga 	p: 81.36 	r: 74.79 	f1: 77.94 	 4613 	 5670 	 6168
wo 	p: 93.32 	r: 86.57 	f1: 89.82 	 2933 	 3143 	 3388
ni 	p: 83.03 	r: 75.56 	f1: 79.12 	 1150 	 1385 	 1522

[32m iter_2[0m
ga 	p: 80.87 	r: 75.31 	f1: 77.99 	 4645 	 5744 	 6168
wo 	p: 93.32 	r: 86.66 	f1: 89.87 	 2936 	 3146 	 3388
ni 	p: 82.6 	r: 76.74 	f1: 79.56 	 1168 	 1414 	 1522
best_thres [[0.31, 0.38, 0.23], [0.35, 0.38, 0.11], [0.32, 0.37, 0.09]]
f [0.813, 0.8152, 0.8163]
load model: epoch13
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 13 	 [0.57, 0.47, 0.13] 	 lr: 2.5e-05 	 f: 82.557975224129
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 22 [0m
Train...

[32m iter_0[0m
ga 	p: 80.53 	r: 75.58 	f1: 77.98 	 4662 	 5789 	 6168
wo 	p: 93.51 	r: 86.81 	f1: 90.04 	 2941 	 3145 	 3388
ni 	p: 86.19 	r: 75.03 	f1: 80.22 	 1142 	 1325 	 1522

[32m iter_1[0m
ga 	p: 82.28 	r: 74.76 	f1: 78.34 	 4611 	 5604 	 6168
wo 	p: 93.4 	r: 86.51 	f1: 89.83 	 2931 	 3138 	 3388
ni 	p: 85.45 	r: 76.41 	f1: 80.68 	 1163 	 1361 	 1522

[32m iter_2[0m
ga 	p: 81.86 	r: 75.0 	f1: 78.28 	 4626 	 5651 	 6168
wo 	p: 93.55 	r: 86.42 	f1: 89.84 	 2928 	 3130 	 3388
ni 	p: 84.64 	r: 77.14 	f1: 80.72 	 1174 	 1387 	 1522
best_thres [[0.39, 0.53, 0.27], [0.46, 0.53, 0.18], [0.44, 0.55, 0.16]]
f [0.8197, 0.8208, 0.8211]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 18 	 [0.44, 0.36, 0.16] 	 lr: 1.25e-05 	 f: 83.05538591628694
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 22 [0m
Train...

[32m iter_0[0m
ga 	p: 79.67 	r: 74.98 	f1: 77.26 	 4625 	 5805 	 6168
wo 	p: 92.86 	r: 86.81 	f1: 89.73 	 2941 	 3167 	 3388
ni 	p: 86.5 	r: 72.4 	f1: 78.83 	 1102 	 1274 	 1522

[32m iter_1[0m
ga 	p: 81.36 	r: 74.79 	f1: 77.94 	 4613 	 5670 	 6168
wo 	p: 93.32 	r: 86.57 	f1: 89.82 	 2933 	 3143 	 3388
ni 	p: 83.03 	r: 75.56 	f1: 79.12 	 1150 	 1385 	 1522

[32m iter_2[0m
ga 	p: 80.87 	r: 75.31 	f1: 77.99 	 4645 	 5744 	 6168
wo 	p: 93.32 	r: 86.66 	f1: 89.87 	 2936 	 3146 	 3388
ni 	p: 82.6 	r: 76.74 	f1: 79.56 	 1168 	 1414 	 1522
best_thres [[0.31, 0.38, 0.23], [0.35, 0.38, 0.11], [0.32, 0.37, 0.09]]
f [0.813, 0.8152, 0.8163]
load model: epoch13
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 13 	 [0.57, 0.47, 0.13] 	 lr: 2.5e-05 	 f: 82.557975224129
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 22 [0m
Train...
loss: tensor(820.4857) lr: 6.25e-06 time: 4212.6
pred_count_train 41644

Test...
loss: tensor(457.9648) lr: 1.25e-05 time: 4222.28
pred_count_train 41644

Test...
loss: tensor(820.4857) lr: 6.25e-06 time: 4151.84
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.17 	r: 75.89 	f1: 78.44 	 4681 	 5767 	 6168
wo 	p: 92.32 	r: 87.34 	f1: 89.76 	 2959 	 3205 	 3388
ni 	p: 87.13 	r: 76.48 	f1: 81.46 	 1164 	 1336 	 1522

[32m iter_1[0m
ga 	p: 81.82 	r: 76.17 	f1: 78.89 	 4698 	 5742 	 6168
wo 	p: 93.2 	r: 86.95 	f1: 89.97 	 2946 	 3161 	 3388
ni 	p: 88.03 	r: 75.89 	f1: 81.51 	 1155 	 1312 	 1522

[32m iter_2[0m
ga 	p: 81.94 	r: 75.99 	f1: 78.85 	 4687 	 5720 	 6168
wo 	p: 93.26 	r: 87.01 	f1: 90.03 	 2948 	 3161 	 3388
ni 	p: 86.54 	r: 77.33 	f1: 81.68 	 1177 	 1360 	 1522
best_thres [[0.37, 0.35, 0.18], [0.36, 0.38, 0.15], [0.37, 0.38, 0.12]]
f [0.8233, 0.8249, 0.8255]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 13 	 [0.57, 0.47, 0.13] 	 lr: 2.5e-05 	 f: 82.557975224129
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 23 [0m
Train...

[32m iter_0[0m
ga 	p: 81.27 	r: 74.5 	f1: 77.74 	 4595 	 5654 	 6168
wo 	p: 94.18 	r: 85.92 	f1: 89.86 	 2911 	 3091 	 3388
ni 	p: 86.61 	r: 73.13 	f1: 79.3 	 1113 	 1285 	 1522

[32m iter_1[0m
ga 	p: 81.47 	r: 74.64 	f1: 77.91 	 4604 	 5651 	 6168
wo 	p: 94.33 	r: 85.48 	f1: 89.69 	 2896 	 3070 	 3388
ni 	p: 85.37 	r: 74.77 	f1: 79.72 	 1138 	 1333 	 1522

[32m iter_2[0m
ga 	p: 81.57 	r: 74.61 	f1: 77.93 	 4602 	 5642 	 6168
wo 	p: 94.33 	r: 85.51 	f1: 89.7 	 2897 	 3071 	 3388
ni 	p: 85.64 	r: 74.44 	f1: 79.65 	 1133 	 1323 	 1522
best_thres [[0.48, 0.56, 0.26], [0.46, 0.57, 0.15], [0.46, 0.57, 0.15]]
f [0.8167, 0.8171, 0.8173]
load model: epoch18
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 18 	 [0.44, 0.36, 0.16] 	 lr: 1.25e-05 	 f: 83.05538591628694
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 23 [0m
Train...

[32m iter_0[0m
ga 	p: 81.17 	r: 75.89 	f1: 78.44 	 4681 	 5767 	 6168
wo 	p: 92.32 	r: 87.34 	f1: 89.76 	 2959 	 3205 	 3388
ni 	p: 87.13 	r: 76.48 	f1: 81.46 	 1164 	 1336 	 1522

[32m iter_1[0m
ga 	p: 81.82 	r: 76.17 	f1: 78.89 	 4698 	 5742 	 6168
wo 	p: 93.2 	r: 86.95 	f1: 89.97 	 2946 	 3161 	 3388
ni 	p: 88.03 	r: 75.89 	f1: 81.51 	 1155 	 1312 	 1522

[32m iter_2[0m
ga 	p: 81.94 	r: 75.99 	f1: 78.85 	 4687 	 5720 	 6168
wo 	p: 93.26 	r: 87.01 	f1: 90.03 	 2948 	 3161 	 3388
ni 	p: 86.54 	r: 77.33 	f1: 81.68 	 1177 	 1360 	 1522
best_thres [[0.37, 0.35, 0.18], [0.36, 0.38, 0.15], [0.37, 0.38, 0.12]]
f [0.8233, 0.8249, 0.8255]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 13 	 [0.57, 0.47, 0.13] 	 lr: 2.5e-05 	 f: 82.557975224129
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 23 [0m
Train...
loss: tensor(769.0200) lr: 6.25e-06 time: 4188.7
pred_count_train 41644

Test...
loss: tensor(655.9869) lr: 6.25e-06 time: 4214.3
pred_count_train 41644

Test...
loss: tensor(769.0200) lr: 6.25e-06 time: 4172.43
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.59 	r: 74.58 	f1: 77.93 	 4600 	 5638 	 6168
wo 	p: 93.63 	r: 86.36 	f1: 89.85 	 2926 	 3125 	 3388
ni 	p: 86.37 	r: 75.76 	f1: 80.71 	 1153 	 1335 	 1522

[32m iter_1[0m
ga 	p: 82.09 	r: 75.63 	f1: 78.73 	 4665 	 5683 	 6168
wo 	p: 93.62 	r: 86.69 	f1: 90.02 	 2937 	 3137 	 3388
ni 	p: 86.73 	r: 75.56 	f1: 80.76 	 1150 	 1326 	 1522

[32m iter_2[0m
ga 	p: 82.36 	r: 75.39 	f1: 78.72 	 4650 	 5646 	 6168
wo 	p: 93.42 	r: 86.81 	f1: 89.99 	 2941 	 3148 	 3388
ni 	p: 86.42 	r: 76.08 	f1: 80.92 	 1158 	 1340 	 1522
best_thres [[0.4, 0.42, 0.17], [0.38, 0.37, 0.13], [0.39, 0.35, 0.12]]
f [0.8197, 0.8222, 0.8231]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 13 	 [0.57, 0.47, 0.13] 	 lr: 2.5e-05 	 f: 82.557975224129
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 24 [0m
Train...

[32m iter_0[0m
ga 	p: 81.1 	r: 76.23 	f1: 78.59 	 4702 	 5798 	 6168
wo 	p: 93.81 	r: 86.81 	f1: 90.17 	 2941 	 3135 	 3388
ni 	p: 89.17 	r: 75.76 	f1: 81.92 	 1153 	 1293 	 1522

[32m iter_1[0m
ga 	p: 82.73 	r: 74.94 	f1: 78.64 	 4622 	 5587 	 6168
wo 	p: 93.18 	r: 87.49 	f1: 90.24 	 2964 	 3181 	 3388
ni 	p: 86.18 	r: 78.25 	f1: 82.02 	 1191 	 1382 	 1522

[32m iter_2[0m
ga 	p: 82.58 	r: 75.15 	f1: 78.69 	 4635 	 5613 	 6168
wo 	p: 93.29 	r: 87.4 	f1: 90.25 	 2961 	 3174 	 3388
ni 	p: 86.29 	r: 78.19 	f1: 82.04 	 1190 	 1379 	 1522
best_thres [[0.37, 0.52, 0.22], [0.43, 0.42, 0.14], [0.42, 0.43, 0.14]]
f [0.8258, 0.8263, 0.8266]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 18 	 [0.44, 0.36, 0.16] 	 lr: 1.25e-05 	 f: 83.05538591628694
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 24 [0m
Train...

[32m iter_0[0m
ga 	p: 81.59 	r: 74.58 	f1: 77.93 	 4600 	 5638 	 6168
wo 	p: 93.63 	r: 86.36 	f1: 89.85 	 2926 	 3125 	 3388
ni 	p: 86.37 	r: 75.76 	f1: 80.71 	 1153 	 1335 	 1522

[32m iter_1[0m
ga 	p: 82.09 	r: 75.63 	f1: 78.73 	 4665 	 5683 	 6168
wo 	p: 93.62 	r: 86.69 	f1: 90.02 	 2937 	 3137 	 3388
ni 	p: 86.73 	r: 75.56 	f1: 80.76 	 1150 	 1326 	 1522

[32m iter_2[0m
ga 	p: 82.36 	r: 75.39 	f1: 78.72 	 4650 	 5646 	 6168
wo 	p: 93.42 	r: 86.81 	f1: 89.99 	 2941 	 3148 	 3388
ni 	p: 86.42 	r: 76.08 	f1: 80.92 	 1158 	 1340 	 1522
best_thres [[0.4, 0.42, 0.17], [0.38, 0.37, 0.13], [0.39, 0.35, 0.12]]
f [0.8197, 0.8222, 0.8231]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 13 	 [0.57, 0.47, 0.13] 	 lr: 2.5e-05 	 f: 82.557975224129
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 24 [0m
Train...
loss: tensor(721.4249) lr: 6.25e-06 time: 4239.05
pred_count_train 41644

Test...
loss: tensor(610.4877) lr: 6.25e-06 time: 4275.3
pred_count_train 41644

Test...
loss: tensor(721.4249) lr: 6.25e-06 time: 4185.44
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.09 	r: 74.68 	f1: 77.75 	 4606 	 5680 	 6168
wo 	p: 94.53 	r: 85.21 	f1: 89.63 	 2887 	 3054 	 3388
ni 	p: 82.94 	r: 78.25 	f1: 80.53 	 1191 	 1436 	 1522

[32m iter_1[0m
ga 	p: 82.11 	r: 74.85 	f1: 78.31 	 4617 	 5623 	 6168
wo 	p: 93.41 	r: 86.54 	f1: 89.84 	 2932 	 3139 	 3388
ni 	p: 87.14 	r: 74.77 	f1: 80.48 	 1138 	 1306 	 1522

[32m iter_2[0m
ga 	p: 82.24 	r: 74.84 	f1: 78.36 	 4616 	 5613 	 6168
wo 	p: 93.92 	r: 86.16 	f1: 89.87 	 2919 	 3108 	 3388
ni 	p: 83.15 	r: 78.12 	f1: 80.56 	 1189 	 1430 	 1522
best_thres [[0.38, 0.53, 0.14], [0.39, 0.39, 0.16], [0.39, 0.46, 0.1]]
f [0.8174, 0.8195, 0.8203]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 13 	 [0.57, 0.47, 0.13] 	 lr: 2.5e-05 	 f: 82.557975224129
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 25 [0m
Train...

[32m iter_0[0m
ga 	p: 80.25 	r: 76.57 	f1: 78.37 	 4723 	 5885 	 6168
wo 	p: 92.78 	r: 87.66 	f1: 90.15 	 2970 	 3201 	 3388
ni 	p: 86.0 	r: 77.92 	f1: 81.76 	 1186 	 1379 	 1522

[32m iter_1[0m
ga 	p: 81.8 	r: 75.32 	f1: 78.43 	 4646 	 5680 	 6168
wo 	p: 92.94 	r: 87.46 	f1: 90.12 	 2963 	 3188 	 3388
ni 	p: 87.77 	r: 76.41 	f1: 81.7 	 1163 	 1325 	 1522

[32m iter_2[0m
ga 	p: 81.85 	r: 75.37 	f1: 78.48 	 4649 	 5680 	 6168
wo 	p: 92.97 	r: 87.46 	f1: 90.13 	 2963 	 3187 	 3388
ni 	p: 87.58 	r: 76.48 	f1: 81.66 	 1164 	 1329 	 1522
best_thres [[0.34, 0.38, 0.19], [0.4, 0.38, 0.19], [0.4, 0.38, 0.18]]
f [0.8243, 0.8245, 0.8247]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 18 	 [0.44, 0.36, 0.16] 	 lr: 1.25e-05 	 f: 83.05538591628694
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 25 [0m
Train...

[32m iter_0[0m
ga 	p: 81.09 	r: 74.68 	f1: 77.75 	 4606 	 5680 	 6168
wo 	p: 94.53 	r: 85.21 	f1: 89.63 	 2887 	 3054 	 3388
ni 	p: 82.94 	r: 78.25 	f1: 80.53 	 1191 	 1436 	 1522

[32m iter_1[0m
ga 	p: 82.11 	r: 74.85 	f1: 78.31 	 4617 	 5623 	 6168
wo 	p: 93.41 	r: 86.54 	f1: 89.84 	 2932 	 3139 	 3388
ni 	p: 87.14 	r: 74.77 	f1: 80.48 	 1138 	 1306 	 1522

[32m iter_2[0m
ga 	p: 82.24 	r: 74.84 	f1: 78.36 	 4616 	 5613 	 6168
wo 	p: 93.92 	r: 86.16 	f1: 89.87 	 2919 	 3108 	 3388
ni 	p: 83.15 	r: 78.12 	f1: 80.56 	 1189 	 1430 	 1522
best_thres [[0.38, 0.53, 0.14], [0.39, 0.39, 0.16], [0.39, 0.46, 0.1]]
f [0.8174, 0.8195, 0.8203]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 13 	 [0.57, 0.47, 0.13] 	 lr: 2.5e-05 	 f: 82.557975224129
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 25 [0m
Train...
loss: tensor(677.5366) lr: 6.25e-06 time: 4285.57
pred_count_train 41644

Test...
loss: tensor(569.1270) lr: 6.25e-06 time: 4268.1
pred_count_train 41644

Test...
loss: tensor(677.5366) lr: 6.25e-06 time: 4085.33
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.97 	r: 74.71 	f1: 77.71 	 4608 	 5691 	 6168
wo 	p: 91.94 	r: 87.22 	f1: 89.52 	 2955 	 3214 	 3388
ni 	p: 83.19 	r: 76.41 	f1: 79.66 	 1163 	 1398 	 1522

[32m iter_1[0m
ga 	p: 81.0 	r: 75.76 	f1: 78.29 	 4673 	 5769 	 6168
wo 	p: 93.3 	r: 86.25 	f1: 89.63 	 2922 	 3132 	 3388
ni 	p: 84.8 	r: 75.89 	f1: 80.1 	 1155 	 1362 	 1522

[32m iter_2[0m
ga 	p: 81.41 	r: 75.47 	f1: 78.33 	 4655 	 5718 	 6168
wo 	p: 93.33 	r: 86.3 	f1: 89.68 	 2924 	 3133 	 3388
ni 	p: 81.76 	r: 78.32 	f1: 80.0 	 1192 	 1458 	 1522
best_thres [[0.4, 0.35, 0.16], [0.37, 0.42, 0.13], [0.39, 0.42, 0.09]]
f [0.8162, 0.8181, 0.8188]
load model: epoch13
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 13 	 [0.57, 0.47, 0.13] 	 lr: 2.5e-05 	 f: 82.557975224129
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 26 [0m
Train...

[32m iter_0[0m
ga 	p: 78.72 	r: 77.71 	f1: 78.21 	 4793 	 6089 	 6168
wo 	p: 94.29 	r: 86.33 	f1: 90.14 	 2925 	 3102 	 3388
ni 	p: 87.25 	r: 75.1 	f1: 80.72 	 1143 	 1310 	 1522

[32m iter_1[0m
ga 	p: 84.3 	r: 73.41 	f1: 78.48 	 4528 	 5371 	 6168
wo 	p: 92.76 	r: 87.37 	f1: 89.98 	 2960 	 3191 	 3388
ni 	p: 87.58 	r: 75.49 	f1: 81.09 	 1149 	 1312 	 1522

[32m iter_2[0m
ga 	p: 83.54 	r: 73.91 	f1: 78.43 	 4559 	 5457 	 6168
wo 	p: 92.95 	r: 87.13 	f1: 89.95 	 2952 	 3176 	 3388
ni 	p: 87.1 	r: 75.43 	f1: 80.85 	 1148 	 1318 	 1522
best_thres [[0.3, 0.6, 0.24], [0.6, 0.42, 0.2], [0.56, 0.44, 0.19]]
f [0.8213, 0.8228, 0.8231]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 18 	 [0.44, 0.36, 0.16] 	 lr: 1.25e-05 	 f: 83.05538591628694
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 26 [0m
Train...

[32m iter_0[0m
ga 	p: 80.97 	r: 74.71 	f1: 77.71 	 4608 	 5691 	 6168
wo 	p: 91.94 	r: 87.22 	f1: 89.52 	 2955 	 3214 	 3388
ni 	p: 83.19 	r: 76.41 	f1: 79.66 	 1163 	 1398 	 1522

[32m iter_1[0m
ga 	p: 81.0 	r: 75.76 	f1: 78.29 	 4673 	 5769 	 6168
wo 	p: 93.3 	r: 86.25 	f1: 89.63 	 2922 	 3132 	 3388
ni 	p: 84.8 	r: 75.89 	f1: 80.1 	 1155 	 1362 	 1522

[32m iter_2[0m
ga 	p: 81.41 	r: 75.47 	f1: 78.33 	 4655 	 5718 	 6168
wo 	p: 93.33 	r: 86.3 	f1: 89.68 	 2924 	 3133 	 3388
ni 	p: 81.76 	r: 78.32 	f1: 80.0 	 1192 	 1458 	 1522
best_thres [[0.4, 0.35, 0.16], [0.37, 0.42, 0.13], [0.39, 0.42, 0.09]]
f [0.8162, 0.8181, 0.8188]
load model: epoch13
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 13 	 [0.57, 0.47, 0.13] 	 lr: 2.5e-05 	 f: 82.557975224129
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 26 [0m
Train...
loss: tensor(821.7766) lr: 5e-06 time: 4167.54
pred_count_train 41644

Test...
loss: tensor(530.2861) lr: 6.25e-06 time: 4185.56
pred_count_train 41644

Test...
loss: tensor(821.7766) lr: 5e-06 time: 4096.93
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.63 	r: 75.92 	f1: 78.21 	 4683 	 5808 	 6168
wo 	p: 91.52 	r: 87.96 	f1: 89.7 	 2980 	 3256 	 3388
ni 	p: 87.69 	r: 76.28 	f1: 81.59 	 1161 	 1324 	 1522

[32m iter_1[0m
ga 	p: 81.66 	r: 76.02 	f1: 78.74 	 4689 	 5742 	 6168
wo 	p: 93.38 	r: 86.63 	f1: 89.88 	 2935 	 3143 	 3388
ni 	p: 84.51 	r: 78.52 	f1: 81.4 	 1195 	 1414 	 1522

[32m iter_2[0m
ga 	p: 81.56 	r: 76.09 	f1: 78.73 	 4693 	 5754 	 6168
wo 	p: 93.24 	r: 86.75 	f1: 89.88 	 2939 	 3152 	 3388
ni 	p: 84.57 	r: 78.52 	f1: 81.43 	 1195 	 1413 	 1522
best_thres [[0.36, 0.26, 0.18], [0.37, 0.36, 0.1], [0.36, 0.35, 0.1]]
f [0.8221, 0.8236, 0.8241]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 13 	 [0.57, 0.47, 0.13] 	 lr: 2.5e-05 	 f: 82.557975224129
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 27 [0m
Train...

[32m iter_0[0m
ga 	p: 79.56 	r: 76.49 	f1: 78.0 	 4718 	 5930 	 6168
wo 	p: 94.15 	r: 86.45 	f1: 90.14 	 2929 	 3111 	 3388
ni 	p: 84.81 	r: 76.28 	f1: 80.32 	 1161 	 1369 	 1522

[32m iter_1[0m
ga 	p: 81.71 	r: 74.87 	f1: 78.14 	 4618 	 5652 	 6168
wo 	p: 93.87 	r: 86.28 	f1: 89.91 	 2923 	 3114 	 3388
ni 	p: 85.3 	r: 76.28 	f1: 80.54 	 1161 	 1361 	 1522

[32m iter_2[0m
ga 	p: 80.64 	r: 75.89 	f1: 78.19 	 4681 	 5805 	 6168
wo 	p: 93.89 	r: 86.19 	f1: 89.87 	 2920 	 3110 	 3388
ni 	p: 84.78 	r: 76.48 	f1: 80.41 	 1164 	 1373 	 1522
best_thres [[0.35, 0.52, 0.19], [0.44, 0.49, 0.15], [0.37, 0.49, 0.14]]
f [0.8198, 0.8203, 0.8203]
load model: epoch18
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 18 	 [0.44, 0.36, 0.16] 	 lr: 1.25e-05 	 f: 83.05538591628694
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 27 [0m
Train...

[32m iter_0[0m
ga 	p: 80.63 	r: 75.92 	f1: 78.21 	 4683 	 5808 	 6168
wo 	p: 91.52 	r: 87.96 	f1: 89.7 	 2980 	 3256 	 3388
ni 	p: 87.69 	r: 76.28 	f1: 81.59 	 1161 	 1324 	 1522

[32m iter_1[0m
ga 	p: 81.66 	r: 76.02 	f1: 78.74 	 4689 	 5742 	 6168
wo 	p: 93.38 	r: 86.63 	f1: 89.88 	 2935 	 3143 	 3388
ni 	p: 84.51 	r: 78.52 	f1: 81.4 	 1195 	 1414 	 1522

[32m iter_2[0m
ga 	p: 81.56 	r: 76.09 	f1: 78.73 	 4693 	 5754 	 6168
wo 	p: 93.24 	r: 86.75 	f1: 89.88 	 2939 	 3152 	 3388
ni 	p: 84.57 	r: 78.52 	f1: 81.43 	 1195 	 1413 	 1522
best_thres [[0.36, 0.26, 0.18], [0.37, 0.36, 0.1], [0.36, 0.35, 0.1]]
f [0.8221, 0.8236, 0.8241]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 13 	 [0.57, 0.47, 0.13] 	 lr: 2.5e-05 	 f: 82.557975224129
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 27 [0m
Train...
loss: tensor(773.8079) lr: 5e-06 time: 4124.53
pred_count_train 41644

Test...
loss: tensor(653.4038) lr: 5e-06 time: 4161.53
pred_count_train 41644

Test...
loss: tensor(773.8079) lr: 5e-06 time: 4142.26
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.46 	r: 74.14 	f1: 78.08 	 4573 	 5546 	 6168
wo 	p: 94.07 	r: 85.74 	f1: 89.72 	 2905 	 3088 	 3388
ni 	p: 85.11 	r: 77.0 	f1: 80.86 	 1172 	 1377 	 1522

[32m iter_1[0m
ga 	p: 82.7 	r: 74.92 	f1: 78.62 	 4621 	 5588 	 6168
wo 	p: 93.52 	r: 86.54 	f1: 89.9 	 2932 	 3135 	 3388
ni 	p: 85.47 	r: 76.54 	f1: 80.76 	 1165 	 1363 	 1522

[32m iter_2[0m
ga 	p: 81.92 	r: 75.65 	f1: 78.66 	 4666 	 5696 	 6168
wo 	p: 93.92 	r: 86.25 	f1: 89.92 	 2922 	 3111 	 3388
ni 	p: 87.59 	r: 75.1 	f1: 80.86 	 1143 	 1305 	 1522
best_thres [[0.45, 0.47, 0.16], [0.45, 0.38, 0.12], [0.38, 0.41, 0.14]]
f [0.8203, 0.8221, 0.8228]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 13 	 [0.57, 0.47, 0.13] 	 lr: 2.5e-05 	 f: 82.557975224129
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 28 [0m
Train...

[32m iter_0[0m
ga 	p: 79.9 	r: 77.14 	f1: 78.5 	 4758 	 5955 	 6168
wo 	p: 93.78 	r: 86.84 	f1: 90.18 	 2942 	 3137 	 3388
ni 	p: 86.46 	r: 77.6 	f1: 81.79 	 1181 	 1366 	 1522

[32m iter_1[0m
ga 	p: 82.06 	r: 75.55 	f1: 78.67 	 4660 	 5679 	 6168
wo 	p: 93.49 	r: 86.95 	f1: 90.11 	 2946 	 3151 	 3388
ni 	p: 87.4 	r: 77.0 	f1: 81.87 	 1172 	 1341 	 1522

[32m iter_2[0m
ga 	p: 81.94 	r: 75.68 	f1: 78.69 	 4668 	 5697 	 6168
wo 	p: 94.55 	r: 85.98 	f1: 90.06 	 2913 	 3081 	 3388
ni 	p: 87.82 	r: 76.74 	f1: 81.91 	 1168 	 1330 	 1522
best_thres [[0.35, 0.49, 0.18], [0.45, 0.44, 0.17], [0.44, 0.57, 0.18]]
f [0.8248, 0.8255, 0.8256]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 18 	 [0.44, 0.36, 0.16] 	 lr: 1.25e-05 	 f: 83.05538591628694
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 28 [0m
Train...

[32m iter_0[0m
ga 	p: 82.46 	r: 74.14 	f1: 78.08 	 4573 	 5546 	 6168
wo 	p: 94.07 	r: 85.74 	f1: 89.72 	 2905 	 3088 	 3388
ni 	p: 85.11 	r: 77.0 	f1: 80.86 	 1172 	 1377 	 1522

[32m iter_1[0m
ga 	p: 82.7 	r: 74.92 	f1: 78.62 	 4621 	 5588 	 6168
wo 	p: 93.52 	r: 86.54 	f1: 89.9 	 2932 	 3135 	 3388
ni 	p: 85.47 	r: 76.54 	f1: 80.76 	 1165 	 1363 	 1522

[32m iter_2[0m
ga 	p: 81.92 	r: 75.65 	f1: 78.66 	 4666 	 5696 	 6168
wo 	p: 93.92 	r: 86.25 	f1: 89.92 	 2922 	 3111 	 3388
ni 	p: 87.59 	r: 75.1 	f1: 80.86 	 1143 	 1305 	 1522
best_thres [[0.45, 0.47, 0.16], [0.45, 0.38, 0.12], [0.38, 0.41, 0.14]]
f [0.8203, 0.8221, 0.8228]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 13 	 [0.57, 0.47, 0.13] 	 lr: 2.5e-05 	 f: 82.557975224129
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 28 [0m
Train...
loss: tensor(734.9661) lr: 5e-06 time: 4171.55
pred_count_train 41644

Test...
loss: tensor(613.8554) lr: 5e-06 time: 4216.24
pred_count_train 41644

Test...
loss: tensor(734.9661) lr: 5e-06 time: 4120.73
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.58 	r: 75.39 	f1: 77.9 	 4650 	 5771 	 6168
wo 	p: 94.42 	r: 85.39 	f1: 89.68 	 2893 	 3064 	 3388
ni 	p: 85.38 	r: 76.74 	f1: 80.83 	 1168 	 1368 	 1522

[32m iter_1[0m
ga 	p: 81.79 	r: 75.36 	f1: 78.44 	 4648 	 5683 	 6168
wo 	p: 93.38 	r: 86.63 	f1: 89.88 	 2935 	 3143 	 3388
ni 	p: 84.41 	r: 78.25 	f1: 81.21 	 1191 	 1411 	 1522

[32m iter_2[0m
ga 	p: 81.97 	r: 75.45 	f1: 78.58 	 4654 	 5678 	 6168
wo 	p: 93.38 	r: 86.57 	f1: 89.85 	 2933 	 3141 	 3388
ni 	p: 84.86 	r: 78.06 	f1: 81.31 	 1188 	 1400 	 1522
best_thres [[0.36, 0.53, 0.17], [0.38, 0.4, 0.11], [0.38, 0.4, 0.11]]
f [0.8187, 0.821, 0.822]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 13 	 [0.57, 0.47, 0.13] 	 lr: 2.5e-05 	 f: 82.557975224129
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 29 [0m
Train...

[32m iter_0[0m
ga 	p: 80.26 	r: 76.62 	f1: 78.4 	 4726 	 5888 	 6168
wo 	p: 94.0 	r: 86.54 	f1: 90.12 	 2932 	 3119 	 3388
ni 	p: 88.84 	r: 75.3 	f1: 81.51 	 1146 	 1290 	 1522

[32m iter_1[0m
ga 	p: 85.11 	r: 72.81 	f1: 78.48 	 4491 	 5277 	 6168
wo 	p: 94.69 	r: 85.74 	f1: 89.99 	 2905 	 3068 	 3388
ni 	p: 87.31 	r: 76.87 	f1: 81.76 	 1170 	 1340 	 1522

[32m iter_2[0m
ga 	p: 80.11 	r: 76.99 	f1: 78.52 	 4749 	 5928 	 6168
wo 	p: 94.81 	r: 85.66 	f1: 90.0 	 2902 	 3061 	 3388
ni 	p: 87.37 	r: 76.81 	f1: 81.75 	 1169 	 1338 	 1522
best_thres [[0.36, 0.53, 0.22], [0.62, 0.61, 0.16], [0.32, 0.62, 0.16]]
f [0.8238, 0.8244, 0.8243]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 18 	 [0.44, 0.36, 0.16] 	 lr: 1.25e-05 	 f: 83.05538591628694
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 29 [0m
Train...

[32m iter_0[0m
ga 	p: 80.58 	r: 75.39 	f1: 77.9 	 4650 	 5771 	 6168
wo 	p: 94.42 	r: 85.39 	f1: 89.68 	 2893 	 3064 	 3388
ni 	p: 85.38 	r: 76.74 	f1: 80.83 	 1168 	 1368 	 1522

[32m iter_1[0m
ga 	p: 81.79 	r: 75.36 	f1: 78.44 	 4648 	 5683 	 6168
wo 	p: 93.38 	r: 86.63 	f1: 89.88 	 2935 	 3143 	 3388
ni 	p: 84.41 	r: 78.25 	f1: 81.21 	 1191 	 1411 	 1522

[32m iter_2[0m
ga 	p: 81.97 	r: 75.45 	f1: 78.58 	 4654 	 5678 	 6168
wo 	p: 93.38 	r: 86.57 	f1: 89.85 	 2933 	 3141 	 3388
ni 	p: 84.86 	r: 78.06 	f1: 81.31 	 1188 	 1400 	 1522
best_thres [[0.36, 0.53, 0.17], [0.38, 0.4, 0.11], [0.38, 0.4, 0.11]]
f [0.8187, 0.821, 0.822]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	current best epoch 13 	 [0.57, 0.47, 0.13] 	 lr: 2.5e-05 	 f: 82.557975224129
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse[0m [33m epoch 29 [0m
Train...
loss: tensor(698.4570) lr: 5e-06 time: 4135.76
pred_count_train 41644

Test...
loss: tensor(579.7032) lr: 5e-06 time: 4211.12
pred_count_train 41644

Test...
loss: tensor(698.4570) lr: 5e-06 time: 4154.55
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.43 	r: 75.57 	f1: 77.92 	 4661 	 5795 	 6168
wo 	p: 92.83 	r: 86.75 	f1: 89.69 	 2939 	 3166 	 3388
ni 	p: 83.23 	r: 76.28 	f1: 79.6 	 1161 	 1395 	 1522

[32m iter_1[0m
ga 	p: 81.16 	r: 75.73 	f1: 78.35 	 4671 	 5755 	 6168
wo 	p: 93.31 	r: 86.42 	f1: 89.73 	 2928 	 3138 	 3388
ni 	p: 81.37 	r: 78.06 	f1: 79.68 	 1188 	 1460 	 1522

[32m iter_2[0m
ga 	p: 81.01 	r: 75.94 	f1: 78.39 	 4684 	 5782 	 6168
wo 	p: 93.56 	r: 86.25 	f1: 89.76 	 2922 	 3123 	 3388
ni 	p: 84.54 	r: 75.43 	f1: 79.72 	 1148 	 1358 	 1522
best_thres [[0.37, 0.36, 0.14], [0.37, 0.37, 0.08], [0.36, 0.41, 0.11]]
f [0.8175, 0.8188, 0.8193]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	best in epoch 13 	 [0.57, 0.47, 0.13] 	 lr: 2.5e-05 	 f: 82.557975224129
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 1 [0m
Train...

[32m iter_0[0m
ga 	p: 80.37 	r: 76.25 	f1: 78.25 	 4703 	 5852 	 6168
wo 	p: 93.89 	r: 86.57 	f1: 90.08 	 2933 	 3124 	 3388
ni 	p: 87.08 	r: 74.84 	f1: 80.49 	 1139 	 1308 	 1522

[32m iter_1[0m
ga 	p: 82.17 	r: 74.92 	f1: 78.38 	 4621 	 5624 	 6168
wo 	p: 92.8 	r: 87.49 	f1: 90.06 	 2964 	 3194 	 3388
ni 	p: 87.51 	r: 74.57 	f1: 80.53 	 1135 	 1297 	 1522

[32m iter_2[0m
ga 	p: 80.74 	r: 76.33 	f1: 78.47 	 4708 	 5831 	 6168
wo 	p: 92.79 	r: 87.4 	f1: 90.01 	 2961 	 3191 	 3388
ni 	p: 82.71 	r: 78.25 	f1: 80.42 	 1191 	 1440 	 1522
best_thres [[0.38, 0.51, 0.21], [0.47, 0.37, 0.19], [0.37, 0.37, 0.1]]
f [0.8216, 0.8222, 0.8224]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	current best epoch 18 	 [0.44, 0.36, 0.16] 	 lr: 1.25e-05 	 f: 83.05538591628694
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 30 [0m
Train...

[32m iter_0[0m
ga 	p: 80.43 	r: 75.57 	f1: 77.92 	 4661 	 5795 	 6168
wo 	p: 92.83 	r: 86.75 	f1: 89.69 	 2939 	 3166 	 3388
ni 	p: 83.23 	r: 76.28 	f1: 79.6 	 1161 	 1395 	 1522

[32m iter_1[0m
ga 	p: 81.16 	r: 75.73 	f1: 78.35 	 4671 	 5755 	 6168
wo 	p: 93.31 	r: 86.42 	f1: 89.73 	 2928 	 3138 	 3388
ni 	p: 81.37 	r: 78.06 	f1: 79.68 	 1188 	 1460 	 1522

[32m iter_2[0m
ga 	p: 81.01 	r: 75.94 	f1: 78.39 	 4684 	 5782 	 6168
wo 	p: 93.56 	r: 86.25 	f1: 89.76 	 2922 	 3123 	 3388
ni 	p: 84.54 	r: 75.43 	f1: 79.72 	 1148 	 1358 	 1522
best_thres [[0.37, 0.36, 0.14], [0.37, 0.37, 0.08], [0.36, 0.41, 0.11]]
f [0.8175, 0.8188, 0.8193]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse 	best in epoch 13 	 [0.57, 0.47, 0.13] 	 lr: 2.5e-05 	 f: 82.557975224129
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(3687.7969) lr: 0.0001 time: 4274.25
pred_count_train 41644

Test...
loss: tensor(547.8282) lr: 5e-06 time: 4225.18
pred_count_train 41644

Test...
loss: tensor(3687.7969) lr: 0.0001 time: 4055.74
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 72.64 	r: 72.65 	f1: 72.64 	 4481 	 6169 	 6168
wo 	p: 89.62 	r: 83.56 	f1: 86.48 	 2831 	 3159 	 3388
ni 	p: 80.63 	r: 79.3 	f1: 79.96 	 1207 	 1497 	 1522

[32m iter_1[0m
ga 	p: 74.62 	r: 70.7 	f1: 72.61 	 4361 	 5844 	 6168
wo 	p: 90.03 	r: 82.85 	f1: 86.29 	 2807 	 3118 	 3388
ni 	p: 81.1 	r: 78.65 	f1: 79.85 	 1197 	 1476 	 1522

[32m iter_2[0m
ga 	p: 74.65 	r: 70.75 	f1: 72.65 	 4364 	 5846 	 6168
wo 	p: 90.13 	r: 82.79 	f1: 86.31 	 2805 	 3112 	 3388
ni 	p: 81.0 	r: 78.71 	f1: 79.84 	 1198 	 1479 	 1522
best_thres [[0.27, 0.24, 0.17], [0.33, 0.24, 0.18], [0.33, 0.24, 0.18]]
f [0.7779, 0.7777, 0.7777]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 1 	 [0.33, 0.24, 0.18] 	 lr: 0.0001 	 f: 77.77435549943019
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 2 [0m
Train...

[32m iter_0[0m
ga 	p: 78.54 	r: 77.32 	f1: 77.92 	 4769 	 6072 	 6168
wo 	p: 93.83 	r: 86.57 	f1: 90.05 	 2933 	 3126 	 3388
ni 	p: 86.94 	r: 75.23 	f1: 80.66 	 1145 	 1317 	 1522

[32m iter_1[0m
ga 	p: 79.49 	r: 76.73 	f1: 78.09 	 4733 	 5954 	 6168
wo 	p: 93.63 	r: 86.78 	f1: 90.07 	 2940 	 3140 	 3388
ni 	p: 82.93 	r: 78.84 	f1: 80.84 	 1200 	 1447 	 1522

[32m iter_2[0m
ga 	p: 83.23 	r: 73.56 	f1: 78.1 	 4537 	 5451 	 6168
wo 	p: 93.6 	r: 86.75 	f1: 90.04 	 2939 	 3140 	 3388
ni 	p: 83.32 	r: 78.45 	f1: 80.81 	 1194 	 1433 	 1522
best_thres [[0.26, 0.45, 0.19], [0.28, 0.41, 0.09], [0.5, 0.41, 0.09]]
f [0.8194, 0.8201, 0.8207]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse 	best in epoch 18 	 [0.44, 0.36, 0.16] 	 lr: 1.25e-05 	 f: 83.05538591628694
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 1 [0m
Train...

[32m iter_0[0m
ga 	p: 72.64 	r: 72.65 	f1: 72.64 	 4481 	 6169 	 6168
wo 	p: 89.62 	r: 83.56 	f1: 86.48 	 2831 	 3159 	 3388
ni 	p: 80.63 	r: 79.3 	f1: 79.96 	 1207 	 1497 	 1522

[32m iter_1[0m
ga 	p: 74.62 	r: 70.7 	f1: 72.61 	 4361 	 5844 	 6168
wo 	p: 90.03 	r: 82.85 	f1: 86.29 	 2807 	 3118 	 3388
ni 	p: 81.1 	r: 78.65 	f1: 79.85 	 1197 	 1476 	 1522

[32m iter_2[0m
ga 	p: 74.65 	r: 70.75 	f1: 72.65 	 4364 	 5846 	 6168
wo 	p: 90.13 	r: 82.79 	f1: 86.31 	 2805 	 3112 	 3388
ni 	p: 81.0 	r: 78.71 	f1: 79.84 	 1198 	 1479 	 1522
best_thres [[0.27, 0.24, 0.17], [0.33, 0.24, 0.18], [0.33, 0.24, 0.18]]
f [0.7779, 0.7777, 0.7777]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 1 	 [0.33, 0.24, 0.18] 	 lr: 0.0001 	 f: 77.77435549943019
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 2 [0m
Train...
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse[0m [33m epoch 1 [0m
Train...
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/test.json
6 3 2
vocab size:  29396
prediction mode: model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse test [0.5, 0.5, 0.5]
{'ga': {'nn': 6306, 'np': 2605, 'pn': 4758, 'pp': 14185},
 'ni': {'nn': 22068, 'np': 1569, 'pn': 915, 'pp': 1833},
 'o': {'nn': 15544, 'np': 746, 'pn': 1294, 'pp': 9004}}
ga:	prec: 84.48, recall: 74.88, f1: 79.39
o:	prec: 92.35, recall: 87.43, f1: 89.82
ni:	prec: 53.88, recall: 66.7, f1: 59.61
all:	prec: 83.57, recall: 78.22, f1: 80.81
pred_count_test 26367
loss: tensor(2249.6646) lr: 0.0001 time: 4168.72
pred_count_train 41644

Test...
loss: tensor(3681.0312) lr: 0.0001 time: 4157.09
pred_count_train 41644

Test...
loss: tensor(2249.6646) lr: 0.0001 time: 4107.47
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 76.79 	r: 73.96 	f1: 75.35 	 4562 	 5941 	 6168
wo 	p: 90.39 	r: 85.83 	f1: 88.05 	 2908 	 3217 	 3388
ni 	p: 82.77 	r: 80.81 	f1: 81.78 	 1230 	 1486 	 1522

[32m iter_1[0m
ga 	p: 75.04 	r: 75.97 	f1: 75.5 	 4686 	 6245 	 6168
wo 	p: 90.78 	r: 85.15 	f1: 87.88 	 2885 	 3178 	 3388
ni 	p: 82.33 	r: 80.81 	f1: 81.56 	 1230 	 1494 	 1522

[32m iter_2[0m
ga 	p: 75.07 	r: 76.01 	f1: 75.53 	 4688 	 6245 	 6168
wo 	p: 91.31 	r: 84.68 	f1: 87.87 	 2869 	 3142 	 3388
ni 	p: 84.96 	r: 78.32 	f1: 81.5 	 1192 	 1403 	 1522
best_thres [[0.27, 0.24, 0.2], [0.2, 0.24, 0.19], [0.2, 0.27, 0.27]]
f [0.801, 0.8006, 0.8005]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 2 	 [0.2, 0.27, 0.27] 	 lr: 0.0001 	 f: 80.04879164443089
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 3 [0m
Train...

[32m iter_0[0m
ga 	p: 74.75 	r: 71.24 	f1: 72.95 	 4394 	 5878 	 6168
wo 	p: 91.74 	r: 81.67 	f1: 86.41 	 2767 	 3016 	 3388
ni 	p: 81.26 	r: 77.79 	f1: 79.49 	 1184 	 1457 	 1522

[32m iter_1[0m
ga 	p: 73.98 	r: 71.43 	f1: 72.68 	 4406 	 5956 	 6168
wo 	p: 91.67 	r: 81.85 	f1: 86.48 	 2773 	 3025 	 3388
ni 	p: 79.18 	r: 79.96 	f1: 79.57 	 1217 	 1537 	 1522

[32m iter_2[0m
ga 	p: 74.05 	r: 71.48 	f1: 72.74 	 4409 	 5954 	 6168
wo 	p: 91.7 	r: 81.82 	f1: 86.48 	 2772 	 3023 	 3388
ni 	p: 79.08 	r: 79.96 	f1: 79.52 	 1217 	 1539 	 1522
best_thres [[0.31, 0.28, 0.17], [0.3, 0.27, 0.13], [0.3, 0.27, 0.13]]
f [0.7789, 0.7782, 0.7781]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 1 	 [0.3, 0.27, 0.13] 	 lr: 0.0001 	 f: 77.80683699840606
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 2 [0m
Train...

[32m iter_0[0m
ga 	p: 76.79 	r: 73.96 	f1: 75.35 	 4562 	 5941 	 6168
wo 	p: 90.39 	r: 85.83 	f1: 88.05 	 2908 	 3217 	 3388
ni 	p: 82.77 	r: 80.81 	f1: 81.78 	 1230 	 1486 	 1522

[32m iter_1[0m
ga 	p: 75.04 	r: 75.97 	f1: 75.5 	 4686 	 6245 	 6168
wo 	p: 90.78 	r: 85.15 	f1: 87.88 	 2885 	 3178 	 3388
ni 	p: 82.33 	r: 80.81 	f1: 81.56 	 1230 	 1494 	 1522

[32m iter_2[0m
ga 	p: 75.07 	r: 76.01 	f1: 75.53 	 4688 	 6245 	 6168
wo 	p: 91.31 	r: 84.68 	f1: 87.87 	 2869 	 3142 	 3388
ni 	p: 84.96 	r: 78.32 	f1: 81.5 	 1192 	 1403 	 1522
best_thres [[0.27, 0.24, 0.2], [0.2, 0.24, 0.19], [0.2, 0.27, 0.27]]
f [0.801, 0.8006, 0.8005]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 2 	 [0.2, 0.27, 0.27] 	 lr: 0.0001 	 f: 80.04879164443089
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 3 [0m
Train...
loss: tensor(1861.2449) lr: 0.0001 time: 4164.87
pred_count_train 41644

Test...
loss: tensor(2241.1819) lr: 0.0001 time: 4194.44
pred_count_train 41644

Test...
loss: tensor(1861.2449) lr: 0.0001 time: 4131.53
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.58 	r: 75.29 	f1: 77.37 	 4644 	 5836 	 6168
wo 	p: 93.32 	r: 85.42 	f1: 89.2 	 2894 	 3101 	 3388
ni 	p: 86.67 	r: 79.89 	f1: 83.15 	 1216 	 1403 	 1522

[32m iter_1[0m
ga 	p: 80.45 	r: 74.46 	f1: 77.34 	 4593 	 5709 	 6168
wo 	p: 93.46 	r: 85.15 	f1: 89.11 	 2885 	 3087 	 3388
ni 	p: 84.72 	r: 81.21 	f1: 82.93 	 1236 	 1459 	 1522

[32m iter_2[0m
ga 	p: 79.23 	r: 75.68 	f1: 77.41 	 4668 	 5892 	 6168
wo 	p: 92.92 	r: 85.63 	f1: 89.12 	 2901 	 3122 	 3388
ni 	p: 85.48 	r: 80.49 	f1: 82.91 	 1225 	 1433 	 1522
best_thres [[0.47, 0.4, 0.26], [0.51, 0.4, 0.2], [0.45, 0.35, 0.22]]
f [0.8174, 0.8172, 0.8172]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 3 	 [0.45, 0.35, 0.22] 	 lr: 0.0001 	 f: 81.71634824817973
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 4 [0m
Train...

[32m iter_0[0m
ga 	p: 75.47 	r: 75.11 	f1: 75.29 	 4633 	 6139 	 6168
wo 	p: 89.86 	r: 86.28 	f1: 88.03 	 2923 	 3253 	 3388
ni 	p: 84.27 	r: 79.89 	f1: 82.02 	 1216 	 1443 	 1522

[32m iter_1[0m
ga 	p: 77.5 	r: 73.25 	f1: 75.31 	 4518 	 5830 	 6168
wo 	p: 90.13 	r: 85.95 	f1: 87.99 	 2912 	 3231 	 3388
ni 	p: 84.12 	r: 79.37 	f1: 81.68 	 1208 	 1436 	 1522

[32m iter_2[0m
ga 	p: 76.86 	r: 73.82 	f1: 75.31 	 4553 	 5924 	 6168
wo 	p: 90.1 	r: 85.98 	f1: 87.99 	 2913 	 3233 	 3388
ni 	p: 84.13 	r: 79.43 	f1: 81.72 	 1209 	 1437 	 1522
best_thres [[0.22, 0.2, 0.23], [0.28, 0.2, 0.24], [0.26, 0.2, 0.24]]
f [0.8006, 0.8007, 0.8006]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 2 	 [0.26, 0.2, 0.24] 	 lr: 0.0001 	 f: 80.06445672191529
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 3 [0m
Train...

[32m iter_0[0m
ga 	p: 79.58 	r: 75.29 	f1: 77.37 	 4644 	 5836 	 6168
wo 	p: 93.32 	r: 85.42 	f1: 89.2 	 2894 	 3101 	 3388
ni 	p: 86.67 	r: 79.89 	f1: 83.15 	 1216 	 1403 	 1522

[32m iter_1[0m
ga 	p: 80.45 	r: 74.46 	f1: 77.34 	 4593 	 5709 	 6168
wo 	p: 93.46 	r: 85.15 	f1: 89.11 	 2885 	 3087 	 3388
ni 	p: 84.72 	r: 81.21 	f1: 82.93 	 1236 	 1459 	 1522

[32m iter_2[0m
ga 	p: 79.23 	r: 75.68 	f1: 77.41 	 4668 	 5892 	 6168
wo 	p: 92.92 	r: 85.63 	f1: 89.12 	 2901 	 3122 	 3388
ni 	p: 85.48 	r: 80.49 	f1: 82.91 	 1225 	 1433 	 1522
best_thres [[0.47, 0.4, 0.26], [0.51, 0.4, 0.2], [0.45, 0.35, 0.22]]
f [0.8174, 0.8172, 0.8172]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 3 	 [0.45, 0.35, 0.22] 	 lr: 0.0001 	 f: 81.71634824817973
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 4 [0m
Train...
loss: tensor(1565.5698) lr: 0.0001 time: 4205.79
pred_count_train 41644

Test...
loss: tensor(1851.6012) lr: 0.0001 time: 4200.88
pred_count_train 41644

Test...
loss: tensor(1565.5698) lr: 0.0001 time: 4125.91
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.42 	r: 75.86 	f1: 78.07 	 4679 	 5818 	 6168
wo 	p: 92.64 	r: 85.83 	f1: 89.11 	 2908 	 3139 	 3388
ni 	p: 86.78 	r: 80.68 	f1: 83.62 	 1228 	 1415 	 1522

[32m iter_1[0m
ga 	p: 79.3 	r: 77.09 	f1: 78.18 	 4755 	 5996 	 6168
wo 	p: 90.65 	r: 87.54 	f1: 89.07 	 2966 	 3272 	 3388
ni 	p: 86.97 	r: 81.14 	f1: 83.96 	 1235 	 1420 	 1522

[32m iter_2[0m
ga 	p: 79.29 	r: 77.03 	f1: 78.14 	 4751 	 5992 	 6168
wo 	p: 90.75 	r: 87.49 	f1: 89.09 	 2964 	 3266 	 3388
ni 	p: 87.03 	r: 81.14 	f1: 83.99 	 1235 	 1419 	 1522
best_thres [[0.34, 0.49, 0.25], [0.28, 0.31, 0.22], [0.28, 0.31, 0.22]]
f [0.8219, 0.8224, 0.8226]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 4 	 [0.28, 0.31, 0.22] 	 lr: 0.0001 	 f: 82.25515999445906
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 5 [0m
Train...

[32m iter_0[0m
ga 	p: 80.52 	r: 74.37 	f1: 77.32 	 4587 	 5697 	 6168
wo 	p: 93.18 	r: 85.48 	f1: 89.16 	 2896 	 3108 	 3388
ni 	p: 86.47 	r: 80.22 	f1: 83.23 	 1221 	 1412 	 1522

[32m iter_1[0m
ga 	p: 79.89 	r: 75.11 	f1: 77.43 	 4633 	 5799 	 6168
wo 	p: 94.24 	r: 84.45 	f1: 89.07 	 2861 	 3036 	 3388
ni 	p: 83.83 	r: 82.46 	f1: 83.14 	 1255 	 1497 	 1522

[32m iter_2[0m
ga 	p: 80.32 	r: 74.63 	f1: 77.37 	 4603 	 5731 	 6168
wo 	p: 94.27 	r: 84.47 	f1: 89.1 	 2862 	 3036 	 3388
ni 	p: 83.96 	r: 82.52 	f1: 83.23 	 1256 	 1496 	 1522
best_thres [[0.49, 0.38, 0.21], [0.46, 0.47, 0.15], [0.48, 0.47, 0.15]]
f [0.8175, 0.8174, 0.8174]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 3 	 [0.48, 0.47, 0.15] 	 lr: 0.0001 	 f: 81.73500296661774
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 4 [0m
Train...
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse[0m [33m epoch 1 [0m
Train...
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/test.json

[32m iter_0[0m
ga 	p: 80.42 	r: 75.86 	f1: 78.07 	 4679 	 5818 	 6168
wo 	p: 92.64 	r: 85.83 	f1: 89.11 	 2908 	 3139 	 3388
ni 	p: 86.78 	r: 80.68 	f1: 83.62 	 1228 	 1415 	 1522

[32m iter_1[0m
ga 	p: 79.3 	r: 77.09 	f1: 78.18 	 4755 	 5996 	 6168
wo 	p: 90.65 	r: 87.54 	f1: 89.07 	 2966 	 3272 	 3388
ni 	p: 86.97 	r: 81.14 	f1: 83.96 	 1235 	 1420 	 1522

[32m iter_2[0m
ga 	p: 79.29 	r: 77.03 	f1: 78.14 	 4751 	 5992 	 6168
wo 	p: 90.75 	r: 87.49 	f1: 89.09 	 2964 	 3266 	 3388
ni 	p: 87.03 	r: 81.14 	f1: 83.99 	 1235 	 1419 	 1522
best_thres [[0.34, 0.49, 0.25], [0.28, 0.31, 0.22], [0.28, 0.31, 0.22]]
f [0.8219, 0.8224, 0.8226]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 4 	 [0.28, 0.31, 0.22] 	 lr: 0.0001 	 f: 82.25515999445906
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 5 [0m
Train...
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
6 3 2
vocab size:  29396
prediction mode: model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse dev [0.5, 0.5, 0.5]
{'ga': {'nn': 3269, 'np': 1413, 'pn': 2628, 'pp': 7409},
 'ni': {'nn': 11614, 'np': 528, 'pn': 584, 'pp': 1165},
 'o': {'nn': 8135, 'np': 403, 'pn': 709, 'pp': 4776}}
ga:	prec: 83.98, recall: 73.82, f1: 78.57
o:	prec: 92.22, recall: 87.07, f1: 89.57
ni:	prec: 68.81, recall: 66.61, f1: 67.69
all:	prec: 85.06, recall: 77.3, f1: 80.99
pred_count_test 13880
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
6 3 2
vocab size:  29396
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
6 3 2
vocab size:  29396
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2016_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
6 3 2
vocab size:  29396
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
6 3 2
vocab size:  29396
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
6 3 2
vocab size:  29396
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
6 3 2
vocab size:  29396
prediction mode: model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse dev [0.5, 0.5, 0.5]
{'ga': {'nn': 3289, 'np': 1368, 'pn': 2619, 'pp': 7418},
 'ni': {'nn': 11584, 'np': 559, 'pn': 544, 'pp': 1205},
 'o': {'nn': 8140, 'np': 387, 'pn': 693, 'pp': 4792}}
ga:	prec: 84.43, recall: 73.91, f1: 78.82
o:	prec: 92.53, recall: 87.37, f1: 89.87
ni:	prec: 68.31, recall: 68.9, f1: 68.6
all:	prec: 85.29, recall: 77.67, f1: 81.3
pred_count_test 13880
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
6 3 2
vocab size:  29396
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
6 3 2
vocab size:  29396
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
6 3 2
vocab size:  29396
prediction mode: model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse dev [0.5, 0.5, 0.5]
{'ga': {'nn': 3172, 'np': 1661, 'pn': 2248, 'pp': 7789},
 'ni': {'nn': 11517, 'np': 624, 'pn': 502, 'pp': 1247},
 'o': {'nn': 8121, 'np': 432, 'pn': 641, 'pp': 4844}}
ga:	prec: 82.42, recall: 77.6, f1: 79.94
o:	prec: 91.81, recall: 88.31, f1: 90.03
ni:	prec: 66.65, recall: 71.3, f1: 68.9
all:	prec: 83.63, recall: 80.37, f1: 81.97
pred_count_test 13880
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
6 3 2
vocab size:  29396
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
6 3 2
vocab size:  29396
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2016_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
6 3 2
vocab size:  29396
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
6 3 2
vocab size:  29396
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
6 3 2
vocab size:  29396
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
6 3 2
vocab size:  29396
prediction mode: model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse dev [0.5, 0.5, 0.5]
{'ga': {'nn': 3290, 'np': 1385, 'pn': 2408, 'pp': 7629},
 'ni': {'nn': 11554, 'np': 584, 'pn': 513, 'pp': 1236},
 'o': {'nn': 8145, 'np': 401, 'pn': 664, 'pp': 4821}}
ga:	prec: 84.64, recall: 76.01, f1: 80.09
o:	prec: 92.32, recall: 87.89, f1: 90.05
ni:	prec: 67.91, recall: 70.67, f1: 69.26
all:	prec: 85.24, recall: 79.24, f1: 82.13
pred_count_test 13880
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
6 3 2
vocab size:  29396
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
6 3 2
vocab size:  29396
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
6 3 2
vocab size:  29396
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
6 3 2
vocab size:  29396
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
6 3 2
vocab size:  29396
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2016_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
6 3 2
vocab size:  29396
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
6 3 2
vocab size:  29396
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
6 3 2
vocab size:  29396
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
6 3 2
vocab size:  29396
prediction mode: model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse dev [0.5, 0.5, 0.5]
{'ga': {'nn': 3269, 'np': 1413, 'pn': 2628, 'pp': 7409},
 'ni': {'nn': 11614, 'np': 528, 'pn': 584, 'pp': 1165},
 'o': {'nn': 8135, 'np': 403, 'pn': 709, 'pp': 4776}}
ga:	prec: 83.98, recall: 73.82, f1: 78.57
o:	prec: 92.22, recall: 87.07, f1: 89.57
ni:	prec: 68.81, recall: 66.61, f1: 67.69
all:	prec: 85.06, recall: 77.3, f1: 80.99
pred_count_test 13880
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1032_th0.2_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1032_th0.2_it3_rs2016_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
loss: tensor(1285.1829) lr: 0.0001 time: 4153.15
pred_count_train 41644

Test...
6 3 2
vocab size:  29396
prediction mode: model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1032_th0.2_it3_rs2016_preFalse dev [0.5, 0.5, 0.5]
{'ga': {'nn': 3285, 'np': 1353, 'pn': 2612, 'pp': 7425},
 'ni': {'nn': 11653, 'np': 488, 'pn': 664, 'pp': 1085},
 'o': {'nn': 8179, 'np': 349, 'pn': 747, 'pp': 4738}}
ga:	prec: 84.59, recall: 73.98, f1: 78.93
o:	prec: 93.14, recall: 86.38, f1: 89.63
ni:	prec: 68.98, recall: 62.04, f1: 65.32
all:	prec: 85.81, recall: 76.71, f1: 81.01
pred_count_test 13880
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
loss: tensor(1553.7142) lr: 0.0001 time: 4223.45
pred_count_train 41644

Test...
loss: tensor(1285.1829) lr: 0.0001 time: 4077.83
pred_count_train 41644

Test...
_lr0.0001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse dev [0.5, 0.5, 0.5]
{'ga': {'nn': 3285, 'np': 1385, 'pn': 2581, 'pp': 7456},
 'ni': {'nn': 11680, 'np': 461, 'pn': 654, 'pp': 1095},
 'o': {'nn': 8192, 'np': 347, 'pn': 783, 'pp': 4702}}
ga:	prec: 84.33, recall: 74.29, f1: 78.99
o:	prec: 93.13, recall: 85.72, f1: 89.27
ni:	prec: 70.37, recall: 62.61, f1: 66.26
all:	prec: 85.8, recall: 76.74, f1: 81.02
pred_count_test 13880
model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse.h5
e2e-stack model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
6 3 2
vocab size:  29396
prediction mode: model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse dev [0.5, 0.5, 0.5]
{'ga': {'nn': 3158, 'np': 1659, 'pn': 2328, 'pp': 7709},
 'ni': {'nn': 11686, 'np': 452, 'pn': 640, 'pp': 1109},
 'o': {'nn': 8221, 'np': 284, 'pn': 763, 'pp': 4722}}
ga:	prec: 82.29, recall: 76.81, f1: 79.45
o:	prec: 94.33, recall: 86.09, f1: 90.02
ni:	prec: 71.04, recall: 63.41, f1: 67.01
all:	prec: 84.97, recall: 78.4, f1: 81.55
pred_count_test 13880
model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse.h5
e2e-stack model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json

[32m iter_0[0m
ga 	p: 79.15 	r: 76.07 	f1: 77.58 	 4692 	 5928 	 6168
wo 	p: 92.7 	r: 86.25 	f1: 89.36 	 2922 	 3152 	 3388
ni 	p: 85.78 	r: 76.87 	f1: 81.08 	 1170 	 1364 	 1522

[32m iter_1[0m
ga 	p: 79.41 	r: 76.23 	f1: 77.79 	 4702 	 5921 	 6168
wo 	p: 94.29 	r: 84.8 	f1: 89.29 	 2873 	 3047 	 3388
ni 	p: 84.09 	r: 79.5 	f1: 81.73 	 1210 	 1439 	 1522

[32m iter_2[0m
ga 	p: 79.78 	r: 76.01 	f1: 77.85 	 4688 	 5876 	 6168
wo 	p: 94.0 	r: 85.15 	f1: 89.36 	 2885 	 3069 	 3388
ni 	p: 85.7 	r: 78.78 	f1: 82.1 	 1199 	 1399 	 1522
best_thres [[0.18, 0.55, 0.25], [0.16, 0.82, 0.17], [0.17, 0.83, 0.2]]
f [0.8163, 0.817, 0.8177]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 4 	 [0.28, 0.31, 0.22] 	 lr: 0.0001 	 f: 82.25515999445906
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 6 [0m
Train...

[32m iter_0[0m
ga 	p: 78.84 	r: 77.12 	f1: 77.97 	 4757 	 6034 	 6168
wo 	p: 93.16 	r: 86.01 	f1: 89.44 	 2914 	 3128 	 3388
ni 	p: 88.97 	r: 78.98 	f1: 83.68 	 1202 	 1351 	 1522

[32m iter_1[0m
ga 	p: 79.86 	r: 76.33 	f1: 78.06 	 4708 	 5895 	 6168
wo 	p: 92.31 	r: 86.84 	f1: 89.49 	 2942 	 3187 	 3388
ni 	p: 86.64 	r: 81.41 	f1: 83.94 	 1239 	 1430 	 1522

[32m iter_2[0m
ga 	p: 80.11 	r: 76.05 	f1: 78.03 	 4691 	 5856 	 6168
wo 	p: 92.2 	r: 86.89 	f1: 89.47 	 2944 	 3193 	 3388
ni 	p: 86.57 	r: 81.34 	f1: 83.88 	 1238 	 1430 	 1522
best_thres [[0.31, 0.5, 0.28], [0.33, 0.4, 0.2], [0.34, 0.39, 0.2]]
f [0.8219, 0.8227, 0.8229]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 4 	 [0.34, 0.39, 0.2] 	 lr: 0.0001 	 f: 82.28552009638852
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 5 [0m
Train...
6 3 2
vocab size:  29396
prediction mode: model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse dev [0.5, 0.5, 0.5]
{'ga': {'nn': 3339, 'np': 1289, 'pn': 2716, 'pp': 7321},
 'ni': {'nn': 11470, 'np': 674, 'pn': 445, 'pp': 1304},
 'o': {'nn': 8165, 'np': 365, 'pn': 743, 'pp': 4742}}
ga:	prec: 85.03, recall: 72.94, f1: 78.52
o:	prec: 92.85, recall: 86.45, f1: 89.54
ni:	prec: 65.93, recall: 74.56, f1: 69.98
all:	prec: 85.17, recall: 77.4, f1: 81.1
pred_count_test 13880
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
6 3 2
vocab size:  29396
prediction mode: model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse dev [0.5, 0.5, 0.5]
{'ga': {'nn': 3289, 'np': 1368, 'pn': 2619, 'pp': 7418},
 'ni': {'nn': 11584, 'np': 559, 'pn': 544, 'pp': 1205},
 'o': {'nn': 8140, 'np': 387, 'pn': 693, 'pp': 4792}}
ga:	prec: 84.43, recall: 73.91, f1: 78.82
o:	prec: 92.53, recall: 87.37, f1: 89.87
ni:	prec: 68.31, recall: 68.9, f1: 68.6
all:	prec: 85.29, recall: 77.67, f1: 81.3
pred_count_test 13880
model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse.h5
e2e-stack model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json

[32m iter_0[0m
ga 	p: 79.15 	r: 76.07 	f1: 77.58 	 4692 	 5928 	 6168
wo 	p: 92.7 	r: 86.25 	f1: 89.36 	 2922 	 3152 	 3388
ni 	p: 85.78 	r: 76.87 	f1: 81.08 	 1170 	 1364 	 1522

[32m iter_1[0m
ga 	p: 79.41 	r: 76.23 	f1: 77.79 	 4702 	 5921 	 6168
wo 	p: 94.29 	r: 84.8 	f1: 89.29 	 2873 	 3047 	 3388
ni 	p: 84.09 	r: 79.5 	f1: 81.73 	 1210 	 1439 	 1522

[32m iter_2[0m
ga 	p: 79.78 	r: 76.01 	f1: 77.85 	 4688 	 5876 	 6168
wo 	p: 94.0 	r: 85.15 	f1: 89.36 	 2885 	 3069 	 3388
ni 	p: 85.7 	r: 78.78 	f1: 82.1 	 1199 	 1399 	 1522
best_thres [[0.18, 0.55, 0.25], [0.16, 0.82, 0.17], [0.17, 0.83, 0.2]]
f [0.8163, 0.817, 0.8177]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 4 	 [0.28, 0.31, 0.22] 	 lr: 0.0001 	 f: 82.25515999445906
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 6 [0m
Train...
6 3 2
vocab size:  29396
prediction mode: model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse dev [0.5, 0.5, 0.5]
{'ga': {'nn': 3297, 'np': 1374, 'pn': 2484, 'pp': 7553},
 'ni': {'nn': 11649, 'np': 490, 'pn': 597, 'pp': 1152},
 'o': {'nn': 8203, 'np': 315, 'pn': 706, 'pp': 4779}}
ga:	prec: 84.61, recall: 75.25, f1: 79.66
o:	prec: 93.82, recall: 87.13, f1: 90.35
ni:	prec: 70.16, recall: 65.87, f1: 67.94
all:	prec: 86.09, recall: 78.07, f1: 81.88
pred_count_test 13880
model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse.h5
e2e-stack model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
6 3 2
vocab size:  29396
prediction mode: model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse dev [0.5, 0.5, 0.5]
{'ga': {'nn': 3285, 'np': 1378, 'pn': 2665, 'pp': 7372},
 'ni': {'nn': 11492, 'np': 650, 'pn': 484, 'pp': 1265},
 'o': {'nn': 8193, 'np': 328, 'pn': 757, 'pp': 4728}}
ga:	prec: 84.25, recall: 73.45, f1: 78.48
o:	prec: 93.51, recall: 86.2, f1: 89.71
ni:	prec: 66.06, recall: 72.33, f1: 69.05
all:	prec: 85.01, recall: 77.38, f1: 81.02
pred_count_test 13880
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
6 3 2
vocab size:  29396
prediction mode: model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse dev [0.5, 0.5, 0.5]
{'ga': {'nn': 3172, 'np': 1661, 'pn': 2248, 'pp': 7789},
 'ni': {'nn': 11517, 'np': 624, 'pn': 502, 'pp': 1247},
 'o': {'nn': 8121, 'np': 432, 'pn': 641, 'pp': 4844}}
ga:	prec: 82.42, recall: 77.6, f1: 79.94
o:	prec: 91.81, recall: 88.31, f1: 90.03
ni:	prec: 66.65, recall: 71.3, f1: 68.9
all:	prec: 83.63, recall: 80.37, f1: 81.97
pred_count_test 13880
model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse.h5
e2e-stack model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
6 3 2
vocab size:  29396
prediction mode: model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2018_preFalse dev [0.5, 0.5, 0.5]
{'ga': {'nn': 3239, 'np': 1499, 'pn': 2323, 'pp': 7714},
 'ni': {'nn': 11616, 'np': 522, 'pn': 546, 'pp': 1203},
 'o': {'nn': 8190, 'np': 353, 'pn': 731, 'pp': 4754}}
ga:	prec: 83.73, recall: 76.86, f1: 80.15
o:	prec: 93.09, recall: 86.67, f1: 89.77
ni:	prec: 69.74, recall: 68.78, f1: 69.26
all:	prec: 85.2, recall: 79.16, f1: 82.07
pred_count_test 13880
model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse.h5
e2e-stack model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
6 3 2
vocab size:  29396
prediction mode: model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2020_preFalse dev [0.5, 0.5, 0.5]
{'ga': {'nn': 3307, 'np': 1370, 'pn': 2570, 'pp': 7467},
 'ni': {'nn': 11623, 'np': 515, 'pn': 522, 'pp': 1227},
 'o': {'nn': 8189, 'np': 348, 'pn': 715, 'pp': 4770}}
ga:	prec: 84.5, recall: 74.39, f1: 79.12
o:	prec: 93.2, recall: 86.96, f1: 89.97
ni:	prec: 70.44, recall: 70.15, f1: 70.3
all:	prec: 85.77, recall: 77.96, f1: 81.68
pred_count_test 13880
model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse.h5
e2e-stack model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
6 3 2
vocab size:  29396
prediction mode: model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse dev [0.5, 0.5, 0.5]
{'ga': {'nn': 3226, 'np': 1508, 'pn': 2347, 'pp': 7690},
 'ni': {'nn': 11569, 'np': 573, 'pn': 485, 'pp': 1264},
 'o': {'nn': 8137, 'np': 406, 'pn': 686, 'pp': 4799}}
ga:	prec: 83.61, recall: 76.62, f1: 79.96
o:	prec: 92.2, recall: 87.49, f1: 89.78
ni:	prec: 68.81, recall: 72.27, f1: 70.5
all:	prec: 84.69, recall: 79.63, f1: 82.08
pred_count_test 13880
model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse.h5
e2e-stack model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
loss: tensor(1025.3339) lr: 0.0001 time: 4227.91
pred_count_train 41644

Test...
6 3 2
vocab size:  29396
prediction mode: model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse dev [0.5, 0.5, 0.5]
{'ga': {'nn': 3163, 'np': 1606, 'pn': 2298, 'pp': 7739},
 'ni': {'nn': 11653, 'np': 486, 'pn': 582, 'pp': 1167},
 'o': {'nn': 8185, 'np': 351, 'pn': 700, 'pp': 4785}}
ga:	prec: 82.81, recall: 77.1, f1: 79.86
o:	prec: 93.17, recall: 87.24, f1: 90.1
ni:	prec: 70.6, recall: 66.72, f1: 68.61
all:	prec: 84.86, recall: 79.27, f1: 81.97
pred_count_test 13880
model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse.h5
e2e-stack model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
loss: tensor(1279.2933) lr: 0.0001 time: 4207.95
pred_count_train 41644

Test...
6 3 2
vocab size:  29396
prediction mode: model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse dev [0.5, 0.5, 0.5]
{'ga': {'nn': 3341, 'np': 1216, 'pn': 2544, 'pp': 7493},
 'ni': {'nn': 11638, 'np': 503, 'pn': 587, 'pp': 1162},
 'o': {'nn': 8231, 'np': 281, 'pn': 746, 'pp': 4739}}
ga:	prec: 86.04, recall: 74.65, f1: 79.94
o:	prec: 94.4, recall: 86.4, f1: 90.22
ni:	prec: 69.79, recall: 66.44, f1: 68.07
all:	prec: 87.01, recall: 77.55, f1: 82.01
pred_count_test 13880
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
6 3 2
vocab size:  29396
prediction mode: model-e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse dev [0.5, 0.5, 0.5]
{'ga': {'nn': 3290, 'np': 1385, 'pn': 2408, 'pp': 7629},
 'ni': {'nn': 11554, 'np': 584, 'pn': 513, 'pp': 1236},
 'o': {'nn': 8145, 'np': 401, 'pn': 664, 'pp': 4821}}
ga:	prec: 84.64, recall: 76.01, f1: 80.09
o:	prec: 92.32, recall: 87.89, f1: 90.05
ni:	prec: 67.91, recall: 70.67, f1: 69.26
all:	prec: 85.24, recall: 79.24, f1: 82.13
pred_count_test 13880
model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse.h5
e2e-stack model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
loss: tensor(1025.3339) lr: 0.0001 time: 4042.02
pred_count_train 41644

Test...
6 3 2
vocab size:  29396
prediction mode: model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse dev [0.5, 0.5, 0.5]
{'ga': {'nn': 3247, 'np': 1434, 'pn': 2374, 'pp': 7663},
 'ni': {'nn': 11652, 'np': 487, 'pn': 598, 'pp': 1151},
 'o': {'nn': 8184, 'np': 348, 'pn': 664, 'pp': 4821}}
ga:	prec: 84.24, recall: 76.35, f1: 80.1
o:	prec: 93.27, recall: 87.89, f1: 90.5
ni:	prec: 70.27, recall: 65.81, f1: 67.97
all:	prec: 85.73, recall: 78.95, f1: 82.2
pred_count_test 13880
model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse.h5
e2e-stack model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json

[32m iter_0[0m
ga 	p: 81.56 	r: 73.85 	f1: 77.51 	 4555 	 5585 	 6168
wo 	p: 92.13 	r: 86.04 	f1: 88.98 	 2915 	 3164 	 3388
ni 	p: 85.69 	r: 70.83 	f1: 77.55 	 1078 	 1258 	 1522

[32m iter_1[0m
ga 	p: 82.55 	r: 73.1 	f1: 77.54 	 4509 	 5462 	 6168
wo 	p: 92.71 	r: 85.95 	f1: 89.2 	 2912 	 3141 	 3388
ni 	p: 84.17 	r: 73.0 	f1: 78.18 	 1111 	 1320 	 1522

[32m iter_2[0m
ga 	p: 81.56 	r: 74.12 	f1: 77.66 	 4572 	 5606 	 6168
wo 	p: 92.73 	r: 85.8 	f1: 89.13 	 2907 	 3135 	 3388
ni 	p: 85.64 	r: 72.47 	f1: 78.51 	 1103 	 1288 	 1522
best_thres [[0.52, 0.34, 0.24], [0.58, 0.35, 0.15], [0.53, 0.35, 0.16]]
f [0.8108, 0.8117, 0.8122]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 4 	 [0.28, 0.31, 0.22] 	 lr: 0.0001 	 f: 82.25515999445906
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 7 [0m
Train...

[32m iter_0[0m
ga 	p: 78.79 	r: 76.78 	f1: 77.77 	 4736 	 6011 	 6168
wo 	p: 91.93 	r: 86.75 	f1: 89.26 	 2939 	 3197 	 3388
ni 	p: 89.76 	r: 74.84 	f1: 81.62 	 1139 	 1269 	 1522

[32m iter_1[0m
ga 	p: 79.08 	r: 77.14 	f1: 78.1 	 4758 	 6017 	 6168
wo 	p: 91.84 	r: 86.72 	f1: 89.21 	 2938 	 3199 	 3388
ni 	p: 89.83 	r: 74.84 	f1: 81.65 	 1139 	 1268 	 1522

[32m iter_2[0m
ga 	p: 79.33 	r: 76.9 	f1: 78.09 	 4743 	 5979 	 6168
wo 	p: 92.0 	r: 86.51 	f1: 89.17 	 2931 	 3186 	 3388
ni 	p: 89.81 	r: 74.7 	f1: 81.56 	 1137 	 1266 	 1522
best_thres [[0.18, 0.57, 0.29], [0.16, 0.56, 0.29], [0.17, 0.57, 0.29]]
f [0.8178, 0.8187, 0.8189]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 4 	 [0.34, 0.39, 0.2] 	 lr: 0.0001 	 f: 82.28552009638852
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 6 [0m
Train...
6 3 2
vocab size:  29396
prediction mode: model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse dev [0.5, 0.5, 0.5]
{'ga': {'nn': 3262, 'np': 1394, 'pn': 2450, 'pp': 7587},
 'ni': {'nn': 11679, 'np': 461, 'pn': 597, 'pp': 1152},
 'o': {'nn': 8156, 'np': 402, 'pn': 682, 'pp': 4803}}
ga:	prec: 84.48, recall: 75.59, f1: 79.79
o:	prec: 92.28, recall: 87.57, f1: 89.86
ni:	prec: 71.42, recall: 65.87, f1: 68.53
all:	prec: 85.71, recall: 78.41, f1: 81.9
pred_count_test 13880
model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse.h5
e2e-stack model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
6 3 2
vocab size:  29396
prediction mode: model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse dev [0.5, 0.5, 0.5]
{'ga': {'nn': 3199, 'np': 1509, 'pn': 2427, 'pp': 7610},
 'ni': {'nn': 11627, 'np': 514, 'pn': 536, 'pp': 1213},
 'o': {'nn': 8136, 'np': 406, 'pn': 720, 'pp': 4765}}
ga:	prec: 83.45, recall: 75.82, f1: 79.45
o:	prec: 92.15, recall: 86.87, f1: 89.43
ni:	prec: 70.24, recall: 69.35, f1: 69.79
all:	prec: 84.83, recall: 78.68, f1: 81.64
pred_count_test 13880
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json

[32m iter_0[0m
ga 	p: 81.56 	r: 73.85 	f1: 77.51 	 4555 	 5585 	 6168
wo 	p: 92.13 	r: 86.04 	f1: 88.98 	 2915 	 3164 	 3388
ni 	p: 85.69 	r: 70.83 	f1: 77.55 	 1078 	 1258 	 1522

[32m iter_1[0m
ga 	p: 82.55 	r: 73.1 	f1: 77.54 	 4509 	 5462 	 6168
wo 	p: 92.71 	r: 85.95 	f1: 89.2 	 2912 	 3141 	 3388
ni 	p: 84.17 	r: 73.0 	f1: 78.18 	 1111 	 1320 	 1522

[32m iter_2[0m
ga 	p: 81.56 	r: 74.12 	f1: 77.66 	 4572 	 5606 	 6168
wo 	p: 92.73 	r: 85.8 	f1: 89.13 	 2907 	 3135 	 3388
ni 	p: 85.64 	r: 72.47 	f1: 78.51 	 1103 	 1288 	 1522
best_thres [[0.52, 0.34, 0.24], [0.58, 0.35, 0.15], [0.53, 0.35, 0.16]]
f [0.8108, 0.8117, 0.8122]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 4 	 [0.28, 0.31, 0.22] 	 lr: 0.0001 	 f: 82.25515999445906
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 7 [0m
Train...
6 3 2
vocab size:  29396
prediction mode: model-e2e-stack_ve256_vu256_depth6_adam_lr0.0005_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse dev [0.5, 0.5, 0.5]
{'ga': {'nn': 3159, 'np': 1583, 'pn': 2353, 'pp': 7684},
 'ni': {'nn': 11640, 'np': 504, 'pn': 539, 'pp': 1210},
 'o': {'nn': 8148, 'np': 403, 'pn': 710, 'pp': 4775}}
ga:	prec: 82.92, recall: 76.56, f1: 79.61
o:	prec: 92.22, recall: 87.06, f1: 89.56
ni:	prec: 70.6, recall: 69.18, f1: 69.88
all:	prec: 84.59, recall: 79.14, f1: 81.78
pred_count_test 13880
model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse.h5
e2e-stack model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
6 3 2
vocab size:  29396
prediction mode: model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.2_it3_rs2016_preFalse dev [0.5, 0.5, 0.5]
{'ga': {'nn': 3172, 'np': 1590, 'pn': 2716, 'pp': 7321},
 'ni': {'nn': 11652, 'np': 486, 'pn': 624, 'pp': 1125},
 'o': {'nn': 8181, 'np': 350, 'pn': 847, 'pp': 4638}}
ga:	prec: 82.16, recall: 72.94, f1: 77.27
o:	prec: 92.98, recall: 84.56, f1: 88.57
ni:	prec: 69.83, recall: 64.32, f1: 66.96
all:	prec: 84.36, recall: 75.76, f1: 79.83
pred_count_test 13880
model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse.h5
e2e-stack model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
6 3 2
vocab size:  29396
prediction mode: model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1031_th0.5_it3_rs2016_preFalse dev [0.5, 0.5, 0.5]
{'ga': {'nn': 3153, 'np': 1627, 'pn': 2662, 'pp': 7375},
 'ni': {'nn': 11617, 'np': 520, 'pn': 555, 'pp': 1194},
 'o': {'nn': 8127, 'np': 434, 'pn': 829, 'pp': 4656}}
ga:	prec: 81.93, recall: 73.48, f1: 77.47
o:	prec: 91.47, recall: 84.89, f1: 88.06
ni:	prec: 69.66, recall: 68.27, f1: 68.96
all:	prec: 83.67, recall: 76.57, f1: 79.96
pred_count_test 13880
model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse.h5
e2e-stack model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/dev.json
6 3 2
vocab size:  29396
prediction mode: model-GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse dev [0.5, 0.5, 0.5]
{'ga': {'nn': 3114, 'np': 1746, 'pn': 2589, 'pp': 7448},
 'ni': {'nn': 11618, 'np': 522, 'pn': 614, 'pp': 1135},
 'o': {'nn': 8126, 'np': 452, 'pn': 811, 'pp': 4674}}
ga:	prec: 81.01, recall: 74.21, f1: 77.46
o:	prec: 91.18, recall: 85.21, f1: 88.1
ni:	prec: 68.5, recall: 64.89, f1: 66.65
all:	prec: 82.98, recall: 76.76, f1: 79.75
pred_count_test 13880
loss: tensor(790.1855) lr: 0.0001 time: 4130.02
pred_count_train 41644

Test...
loss: tensor(1018.2416) lr: 0.0001 time: 4159.63
pred_count_train 41644

Test...
loss: tensor(790.1855) lr: 0.0001 time: 4124.85
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.21 	r: 75.0 	f1: 77.05 	 4626 	 5840 	 6168
wo 	p: 90.82 	r: 86.39 	f1: 88.55 	 2927 	 3223 	 3388
ni 	p: 85.81 	r: 73.13 	f1: 78.96 	 1113 	 1297 	 1522

[32m iter_1[0m
ga 	p: 79.14 	r: 75.6 	f1: 77.33 	 4663 	 5892 	 6168
wo 	p: 92.44 	r: 85.12 	f1: 88.63 	 2884 	 3120 	 3388
ni 	p: 85.38 	r: 73.65 	f1: 79.08 	 1121 	 1313 	 1522

[32m iter_2[0m
ga 	p: 80.26 	r: 74.76 	f1: 77.41 	 4611 	 5745 	 6168
wo 	p: 90.76 	r: 86.66 	f1: 88.66 	 2936 	 3235 	 3388
ni 	p: 85.7 	r: 73.65 	f1: 79.22 	 1121 	 1308 	 1522
best_thres [[0.45, 0.28, 0.28], [0.43, 0.38, 0.21], [0.54, 0.23, 0.21]]
f [0.8085, 0.8092, 0.8099]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 4 	 [0.28, 0.31, 0.22] 	 lr: 0.0001 	 f: 82.25515999445906
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 8 [0m
Train...

[32m iter_0[0m
ga 	p: 81.36 	r: 74.74 	f1: 77.91 	 4610 	 5666 	 6168
wo 	p: 92.92 	r: 84.5 	f1: 88.51 	 2863 	 3081 	 3388
ni 	p: 83.08 	r: 72.93 	f1: 77.68 	 1110 	 1336 	 1522

[32m iter_1[0m
ga 	p: 80.94 	r: 75.34 	f1: 78.04 	 4647 	 5741 	 6168
wo 	p: 93.47 	r: 84.06 	f1: 88.52 	 2848 	 3047 	 3388
ni 	p: 84.46 	r: 72.14 	f1: 77.82 	 1098 	 1300 	 1522

[32m iter_2[0m
ga 	p: 80.85 	r: 75.37 	f1: 78.02 	 4649 	 5750 	 6168
wo 	p: 93.59 	r: 84.03 	f1: 88.55 	 2847 	 3042 	 3388
ni 	p: 84.51 	r: 72.08 	f1: 77.8 	 1097 	 1298 	 1522
best_thres [[0.48, 0.56, 0.19], [0.44, 0.59, 0.2], [0.44, 0.59, 0.2]]
f [0.8112, 0.8116, 0.8117]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 4 	 [0.34, 0.39, 0.2] 	 lr: 0.0001 	 f: 82.28552009638852
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 7 [0m
Train...

[32m iter_0[0m
ga 	p: 79.21 	r: 75.0 	f1: 77.05 	 4626 	 5840 	 6168
wo 	p: 90.82 	r: 86.39 	f1: 88.55 	 2927 	 3223 	 3388
ni 	p: 85.81 	r: 73.13 	f1: 78.96 	 1113 	 1297 	 1522

[32m iter_1[0m
ga 	p: 79.14 	r: 75.6 	f1: 77.33 	 4663 	 5892 	 6168
wo 	p: 92.44 	r: 85.12 	f1: 88.63 	 2884 	 3120 	 3388
ni 	p: 85.38 	r: 73.65 	f1: 79.08 	 1121 	 1313 	 1522

[32m iter_2[0m
ga 	p: 80.26 	r: 74.76 	f1: 77.41 	 4611 	 5745 	 6168
wo 	p: 90.76 	r: 86.66 	f1: 88.66 	 2936 	 3235 	 3388
ni 	p: 85.7 	r: 73.65 	f1: 79.22 	 1121 	 1308 	 1522
best_thres [[0.45, 0.28, 0.28], [0.43, 0.38, 0.21], [0.54, 0.23, 0.21]]
f [0.8085, 0.8092, 0.8099]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 4 	 [0.28, 0.31, 0.22] 	 lr: 0.0001 	 f: 82.25515999445906
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 8 [0m
Train...
loss: tensor(615.7471) lr: 0.0001 time: 4154.19
pred_count_train 41644

Test...
loss: tensor(791.3877) lr: 0.0001 time: 4216.71
pred_count_train 41644

Test...
loss: tensor(615.7471) lr: 0.0001 time: 4041.86
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.66 	r: 73.01 	f1: 76.64 	 4503 	 5583 	 6168
wo 	p: 92.65 	r: 84.83 	f1: 88.57 	 2874 	 3102 	 3388
ni 	p: 82.67 	r: 75.56 	f1: 78.96 	 1150 	 1391 	 1522

[32m iter_1[0m
ga 	p: 79.3 	r: 74.16 	f1: 76.64 	 4574 	 5768 	 6168
wo 	p: 91.78 	r: 86.04 	f1: 88.82 	 2915 	 3176 	 3388
ni 	p: 85.5 	r: 74.38 	f1: 79.55 	 1132 	 1324 	 1522

[32m iter_2[0m
ga 	p: 78.75 	r: 74.43 	f1: 76.53 	 4591 	 5830 	 6168
wo 	p: 92.43 	r: 85.45 	f1: 88.8 	 2895 	 3132 	 3388
ni 	p: 83.19 	r: 75.76 	f1: 79.3 	 1153 	 1386 	 1522
best_thres [[0.64, 0.6, 0.14], [0.68, 0.44, 0.12], [0.69, 0.66, 0.09]]
f [0.8062, 0.807, 0.8068]
load model: epoch4
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 4 	 [0.28, 0.31, 0.22] 	 lr: 0.0001 	 f: 82.25515999445906
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 9 [0m
Train...

[32m iter_0[0m
ga 	p: 80.24 	r: 75.65 	f1: 77.88 	 4666 	 5815 	 6168
wo 	p: 91.51 	r: 85.92 	f1: 88.63 	 2911 	 3181 	 3388
ni 	p: 86.38 	r: 76.28 	f1: 81.02 	 1161 	 1344 	 1522

[32m iter_1[0m
ga 	p: 81.0 	r: 75.42 	f1: 78.11 	 4652 	 5743 	 6168
wo 	p: 91.93 	r: 85.39 	f1: 88.54 	 2893 	 3147 	 3388
ni 	p: 87.6 	r: 75.62 	f1: 81.17 	 1151 	 1314 	 1522

[32m iter_2[0m
ga 	p: 80.98 	r: 75.45 	f1: 78.12 	 4654 	 5747 	 6168
wo 	p: 91.92 	r: 85.33 	f1: 88.5 	 2891 	 3145 	 3388
ni 	p: 84.99 	r: 77.4 	f1: 81.02 	 1178 	 1386 	 1522
best_thres [[0.53, 0.34, 0.34], [0.56, 0.38, 0.4], [0.56, 0.38, 0.29]]
f [0.8159, 0.8166, 0.8167]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 4 	 [0.34, 0.39, 0.2] 	 lr: 0.0001 	 f: 82.28552009638852
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 8 [0m
Train...

[32m iter_0[0m
ga 	p: 80.66 	r: 73.01 	f1: 76.64 	 4503 	 5583 	 6168
wo 	p: 92.65 	r: 84.83 	f1: 88.57 	 2874 	 3102 	 3388
ni 	p: 82.67 	r: 75.56 	f1: 78.96 	 1150 	 1391 	 1522

[32m iter_1[0m
ga 	p: 79.3 	r: 74.16 	f1: 76.64 	 4574 	 5768 	 6168
wo 	p: 91.78 	r: 86.04 	f1: 88.82 	 2915 	 3176 	 3388
ni 	p: 85.5 	r: 74.38 	f1: 79.55 	 1132 	 1324 	 1522

[32m iter_2[0m
ga 	p: 78.75 	r: 74.43 	f1: 76.53 	 4591 	 5830 	 6168
wo 	p: 92.43 	r: 85.45 	f1: 88.8 	 2895 	 3132 	 3388
ni 	p: 83.19 	r: 75.76 	f1: 79.3 	 1153 	 1386 	 1522
best_thres [[0.64, 0.6, 0.14], [0.68, 0.44, 0.12], [0.69, 0.66, 0.09]]
f [0.8062, 0.807, 0.8068]
load model: epoch4
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 4 	 [0.28, 0.31, 0.22] 	 lr: 0.0001 	 f: 82.25515999445906
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 9 [0m
Train...
loss: tensor(1118.7863) lr: 5e-05 time: 4094.35
pred_count_train 41644

Test...
loss: tensor(603.7929) lr: 0.0001 time: 4117.6
pred_count_train 41644

Test...
loss: tensor(1118.7863) lr: 5e-05 time: 4137.53
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.33 	r: 74.85 	f1: 78.41 	 4617 	 5608 	 6168
wo 	p: 91.57 	r: 87.87 	f1: 89.68 	 2977 	 3251 	 3388
ni 	p: 81.11 	r: 78.71 	f1: 79.89 	 1198 	 1477 	 1522

[32m iter_1[0m
ga 	p: 82.45 	r: 74.95 	f1: 78.52 	 4623 	 5607 	 6168
wo 	p: 92.83 	r: 86.81 	f1: 89.72 	 2941 	 3168 	 3388
ni 	p: 81.28 	r: 78.19 	f1: 79.71 	 1190 	 1464 	 1522

[32m iter_2[0m
ga 	p: 80.98 	r: 76.3 	f1: 78.57 	 4706 	 5811 	 6168
wo 	p: 92.8 	r: 86.72 	f1: 89.66 	 2938 	 3166 	 3388
ni 	p: 81.69 	r: 78.25 	f1: 79.93 	 1191 	 1458 	 1522
best_thres [[0.41, 0.36, 0.12], [0.4, 0.46, 0.1], [0.32, 0.46, 0.1]]
f [0.8211, 0.8212, 0.8213]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 4 	 [0.28, 0.31, 0.22] 	 lr: 0.0001 	 f: 82.25515999445906
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 10 [0m
Train...

[32m iter_0[0m
ga 	p: 80.24 	r: 74.08 	f1: 77.04 	 4569 	 5694 	 6168
wo 	p: 93.34 	r: 84.45 	f1: 88.67 	 2861 	 3065 	 3388
ni 	p: 83.43 	r: 76.74 	f1: 79.95 	 1168 	 1400 	 1522

[32m iter_1[0m
ga 	p: 80.0 	r: 74.82 	f1: 77.32 	 4615 	 5769 	 6168
wo 	p: 92.66 	r: 85.01 	f1: 88.67 	 2880 	 3108 	 3388
ni 	p: 88.38 	r: 72.93 	f1: 79.91 	 1110 	 1256 	 1522

[32m iter_2[0m
ga 	p: 80.1 	r: 75.0 	f1: 77.47 	 4626 	 5775 	 6168
wo 	p: 93.29 	r: 84.56 	f1: 88.71 	 2865 	 3071 	 3388
ni 	p: 84.68 	r: 75.89 	f1: 80.04 	 1155 	 1364 	 1522
best_thres [[0.65, 0.78, 0.09], [0.59, 0.73, 0.2], [0.59, 0.81, 0.1]]
f [0.8097, 0.8105, 0.8111]
load model: epoch4
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 4 	 [0.34, 0.39, 0.2] 	 lr: 0.0001 	 f: 82.28552009638852
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 9 [0m
Train...

[32m iter_0[0m
ga 	p: 82.33 	r: 74.85 	f1: 78.41 	 4617 	 5608 	 6168
wo 	p: 91.57 	r: 87.87 	f1: 89.68 	 2977 	 3251 	 3388
ni 	p: 81.11 	r: 78.71 	f1: 79.89 	 1198 	 1477 	 1522

[32m iter_1[0m
ga 	p: 82.45 	r: 74.95 	f1: 78.52 	 4623 	 5607 	 6168
wo 	p: 92.83 	r: 86.81 	f1: 89.72 	 2941 	 3168 	 3388
ni 	p: 81.28 	r: 78.19 	f1: 79.71 	 1190 	 1464 	 1522

[32m iter_2[0m
ga 	p: 80.98 	r: 76.3 	f1: 78.57 	 4706 	 5811 	 6168
wo 	p: 92.8 	r: 86.72 	f1: 89.66 	 2938 	 3166 	 3388
ni 	p: 81.69 	r: 78.25 	f1: 79.93 	 1191 	 1458 	 1522
best_thres [[0.41, 0.36, 0.12], [0.4, 0.46, 0.1], [0.32, 0.46, 0.1]]
f [0.8211, 0.8212, 0.8213]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 4 	 [0.28, 0.31, 0.22] 	 lr: 0.0001 	 f: 82.25515999445906
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 10 [0m
Train...
loss: tensor(889.9937) lr: 5e-05 time: 4135.12
pred_count_train 41644

Test...
loss: tensor(1118.6991) lr: 5e-05 time: 4256.95
pred_count_train 41644

Test...
loss: tensor(889.9937) lr: 5e-05 time: 4014.68
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.23 	r: 75.55 	f1: 78.29 	 4660 	 5737 	 6168
wo 	p: 92.23 	r: 86.57 	f1: 89.31 	 2933 	 3180 	 3388
ni 	p: 84.01 	r: 77.99 	f1: 80.89 	 1187 	 1413 	 1522

[32m iter_1[0m
ga 	p: 81.25 	r: 75.71 	f1: 78.38 	 4670 	 5748 	 6168
wo 	p: 93.06 	r: 85.92 	f1: 89.35 	 2911 	 3128 	 3388
ni 	p: 86.62 	r: 77.0 	f1: 81.53 	 1172 	 1353 	 1522

[32m iter_2[0m
ga 	p: 80.97 	r: 75.94 	f1: 78.37 	 4684 	 5785 	 6168
wo 	p: 92.85 	r: 86.28 	f1: 89.44 	 2923 	 3148 	 3388
ni 	p: 86.73 	r: 76.87 	f1: 81.5 	 1170 	 1349 	 1522
best_thres [[0.46, 0.51, 0.14], [0.47, 0.75, 0.13], [0.45, 0.76, 0.13]]
f [0.8203, 0.8209, 0.8212]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 4 	 [0.28, 0.31, 0.22] 	 lr: 0.0001 	 f: 82.25515999445906
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 11 [0m
Train...

[32m iter_0[0m
ga 	p: 81.14 	r: 76.67 	f1: 78.84 	 4729 	 5828 	 6168
wo 	p: 93.16 	r: 86.42 	f1: 89.66 	 2928 	 3143 	 3388
ni 	p: 82.31 	r: 81.01 	f1: 81.66 	 1233 	 1498 	 1522

[32m iter_1[0m
ga 	p: 83.13 	r: 75.41 	f1: 79.08 	 4651 	 5595 	 6168
wo 	p: 92.34 	r: 86.87 	f1: 89.52 	 2943 	 3187 	 3388
ni 	p: 83.7 	r: 79.96 	f1: 81.79 	 1217 	 1454 	 1522

[32m iter_2[0m
ga 	p: 83.47 	r: 75.06 	f1: 79.04 	 4630 	 5547 	 6168
wo 	p: 92.1 	r: 87.04 	f1: 89.5 	 2949 	 3202 	 3388
ni 	p: 82.1 	r: 81.67 	f1: 81.88 	 1243 	 1514 	 1522
best_thres [[0.34, 0.47, 0.14], [0.41, 0.39, 0.14], [0.43, 0.36, 0.11]]
f [0.8252, 0.826, 0.8262]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 9 	 [0.43, 0.36, 0.11] 	 lr: 5e-05 	 f: 82.623594280552
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 10 [0m
Train...

[32m iter_0[0m
ga 	p: 81.23 	r: 75.55 	f1: 78.29 	 4660 	 5737 	 6168
wo 	p: 92.23 	r: 86.57 	f1: 89.31 	 2933 	 3180 	 3388
ni 	p: 84.01 	r: 77.99 	f1: 80.89 	 1187 	 1413 	 1522

[32m iter_1[0m
ga 	p: 81.25 	r: 75.71 	f1: 78.38 	 4670 	 5748 	 6168
wo 	p: 93.06 	r: 85.92 	f1: 89.35 	 2911 	 3128 	 3388
ni 	p: 86.62 	r: 77.0 	f1: 81.53 	 1172 	 1353 	 1522

[32m iter_2[0m
ga 	p: 80.97 	r: 75.94 	f1: 78.37 	 4684 	 5785 	 6168
wo 	p: 92.85 	r: 86.28 	f1: 89.44 	 2923 	 3148 	 3388
ni 	p: 86.73 	r: 76.87 	f1: 81.5 	 1170 	 1349 	 1522
best_thres [[0.46, 0.51, 0.14], [0.47, 0.75, 0.13], [0.45, 0.76, 0.13]]
f [0.8203, 0.8209, 0.8212]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 4 	 [0.28, 0.31, 0.22] 	 lr: 0.0001 	 f: 82.25515999445906
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 11 [0m
Train...
loss: tensor(707.7722) lr: 5e-05 time: 4142.67
pred_count_train 41644

Test...
loss: tensor(889.4365) lr: 5e-05 time: 4133.94
pred_count_train 41644

Test...
loss: tensor(707.7722) lr: 5e-05 time: 4067.17
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.03 	r: 73.74 	f1: 77.66 	 4548 	 5544 	 6168
wo 	p: 92.71 	r: 86.75 	f1: 89.63 	 2939 	 3170 	 3388
ni 	p: 83.86 	r: 79.57 	f1: 81.66 	 1211 	 1444 	 1522

[32m iter_1[0m
ga 	p: 82.31 	r: 73.87 	f1: 77.86 	 4556 	 5535 	 6168
wo 	p: 92.77 	r: 86.78 	f1: 89.68 	 2940 	 3169 	 3388
ni 	p: 83.58 	r: 80.29 	f1: 81.9 	 1222 	 1462 	 1522

[32m iter_2[0m
ga 	p: 81.07 	r: 75.05 	f1: 77.94 	 4629 	 5710 	 6168
wo 	p: 92.58 	r: 86.92 	f1: 89.66 	 2945 	 3181 	 3388
ni 	p: 84.53 	r: 78.98 	f1: 81.66 	 1202 	 1422 	 1522
best_thres [[0.41, 0.51, 0.16], [0.41, 0.54, 0.1], [0.33, 0.54, 0.12]]
f [0.8192, 0.82, 0.8202]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 4 	 [0.28, 0.31, 0.22] 	 lr: 0.0001 	 f: 82.25515999445906
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 12 [0m
Train...

[32m iter_0[0m
ga 	p: 80.74 	r: 76.91 	f1: 78.78 	 4744 	 5876 	 6168
wo 	p: 92.4 	r: 86.51 	f1: 89.36 	 2931 	 3172 	 3388
ni 	p: 84.97 	r: 77.66 	f1: 81.15 	 1182 	 1391 	 1522

[32m iter_1[0m
ga 	p: 80.77 	r: 76.9 	f1: 78.79 	 4743 	 5872 	 6168
wo 	p: 92.88 	r: 86.22 	f1: 89.42 	 2921 	 3145 	 3388
ni 	p: 84.19 	r: 78.71 	f1: 81.36 	 1198 	 1423 	 1522

[32m iter_2[0m
ga 	p: 81.12 	r: 76.69 	f1: 78.84 	 4730 	 5831 	 6168
wo 	p: 92.8 	r: 86.04 	f1: 89.29 	 2915 	 3141 	 3388
ni 	p: 86.25 	r: 77.07 	f1: 81.4 	 1173 	 1360 	 1522
best_thres [[0.46, 0.52, 0.08], [0.44, 0.54, 0.06], [0.46, 0.54, 0.08]]
f [0.8233, 0.8235, 0.8236]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 9 	 [0.43, 0.36, 0.11] 	 lr: 5e-05 	 f: 82.623594280552
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 11 [0m
Train...

[32m iter_0[0m
ga 	p: 82.03 	r: 73.74 	f1: 77.66 	 4548 	 5544 	 6168
wo 	p: 92.71 	r: 86.75 	f1: 89.63 	 2939 	 3170 	 3388
ni 	p: 83.86 	r: 79.57 	f1: 81.66 	 1211 	 1444 	 1522

[32m iter_1[0m
ga 	p: 82.31 	r: 73.87 	f1: 77.86 	 4556 	 5535 	 6168
wo 	p: 92.77 	r: 86.78 	f1: 89.68 	 2940 	 3169 	 3388
ni 	p: 83.58 	r: 80.29 	f1: 81.9 	 1222 	 1462 	 1522

[32m iter_2[0m
ga 	p: 81.07 	r: 75.05 	f1: 77.94 	 4629 	 5710 	 6168
wo 	p: 92.58 	r: 86.92 	f1: 89.66 	 2945 	 3181 	 3388
ni 	p: 84.53 	r: 78.98 	f1: 81.66 	 1202 	 1422 	 1522
best_thres [[0.41, 0.51, 0.16], [0.41, 0.54, 0.1], [0.33, 0.54, 0.12]]
f [0.8192, 0.82, 0.8202]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 4 	 [0.28, 0.31, 0.22] 	 lr: 0.0001 	 f: 82.25515999445906
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 12 [0m
Train...
loss: tensor(561.8298) lr: 5e-05 time: 4067.83
pred_count_train 41644

Test...
loss: tensor(700.5112) lr: 5e-05 time: 4183.27
pred_count_train 41644

Test...
loss: tensor(561.8298) lr: 5e-05 time: 4058.31
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.73 	r: 72.88 	f1: 77.05 	 4495 	 5500 	 6168
wo 	p: 92.54 	r: 86.1 	f1: 89.2 	 2917 	 3152 	 3388
ni 	p: 81.32 	r: 76.35 	f1: 78.75 	 1162 	 1429 	 1522

[32m iter_1[0m
ga 	p: 80.97 	r: 73.93 	f1: 77.29 	 4560 	 5632 	 6168
wo 	p: 92.39 	r: 85.98 	f1: 89.07 	 2913 	 3153 	 3388
ni 	p: 85.24 	r: 74.38 	f1: 79.44 	 1132 	 1328 	 1522

[32m iter_2[0m
ga 	p: 81.46 	r: 73.61 	f1: 77.34 	 4540 	 5573 	 6168
wo 	p: 91.05 	r: 87.07 	f1: 89.02 	 2950 	 3240 	 3388
ni 	p: 81.74 	r: 77.66 	f1: 79.65 	 1182 	 1446 	 1522
best_thres [[0.52, 0.58, 0.14], [0.47, 0.75, 0.14], [0.6, 0.4, 0.08]]
f [0.8104, 0.8113, 0.8118]
load model: epoch4
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 4 	 [0.28, 0.31, 0.22] 	 lr: 0.0001 	 f: 82.25515999445906
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 13 [0m
Train...

[32m iter_0[0m
ga 	p: 80.62 	r: 74.81 	f1: 77.6 	 4614 	 5723 	 6168
wo 	p: 92.84 	r: 86.51 	f1: 89.56 	 2931 	 3157 	 3388
ni 	p: 85.53 	r: 75.36 	f1: 80.13 	 1147 	 1341 	 1522

[32m iter_1[0m
ga 	p: 80.13 	r: 75.92 	f1: 77.97 	 4683 	 5844 	 6168
wo 	p: 93.75 	r: 85.48 	f1: 89.42 	 2896 	 3089 	 3388
ni 	p: 84.26 	r: 77.0 	f1: 80.47 	 1172 	 1391 	 1522

[32m iter_2[0m
ga 	p: 80.18 	r: 75.83 	f1: 77.94 	 4677 	 5833 	 6168
wo 	p: 93.38 	r: 85.74 	f1: 89.4 	 2905 	 3111 	 3388
ni 	p: 84.01 	r: 77.33 	f1: 80.53 	 1177 	 1401 	 1522
best_thres [[0.37, 0.45, 0.18], [0.29, 0.57, 0.12], [0.29, 0.52, 0.11]]
f [0.8162, 0.817, 0.8172]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 9 	 [0.43, 0.36, 0.11] 	 lr: 5e-05 	 f: 82.623594280552
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 12 [0m
Train...

[32m iter_0[0m
ga 	p: 81.73 	r: 72.88 	f1: 77.05 	 4495 	 5500 	 6168
wo 	p: 92.54 	r: 86.1 	f1: 89.2 	 2917 	 3152 	 3388
ni 	p: 81.32 	r: 76.35 	f1: 78.75 	 1162 	 1429 	 1522

[32m iter_1[0m
ga 	p: 80.97 	r: 73.93 	f1: 77.29 	 4560 	 5632 	 6168
wo 	p: 92.39 	r: 85.98 	f1: 89.07 	 2913 	 3153 	 3388
ni 	p: 85.24 	r: 74.38 	f1: 79.44 	 1132 	 1328 	 1522

[32m iter_2[0m
ga 	p: 81.46 	r: 73.61 	f1: 77.34 	 4540 	 5573 	 6168
wo 	p: 91.05 	r: 87.07 	f1: 89.02 	 2950 	 3240 	 3388
ni 	p: 81.74 	r: 77.66 	f1: 79.65 	 1182 	 1446 	 1522
best_thres [[0.52, 0.58, 0.14], [0.47, 0.75, 0.14], [0.6, 0.4, 0.08]]
f [0.8104, 0.8113, 0.8118]
load model: epoch4
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 4 	 [0.28, 0.31, 0.22] 	 lr: 0.0001 	 f: 82.25515999445906
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 13 [0m
Train...
loss: tensor(1063.2059) lr: 2.5e-05 time: 4101.97
pred_count_train 41644

Test...
loss: tensor(555.5684) lr: 5e-05 time: 4127.38
pred_count_train 41644

Test...
loss: tensor(1063.2059) lr: 2.5e-05 time: 4030.28
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.19 	r: 75.71 	f1: 78.82 	 4670 	 5682 	 6168
wo 	p: 91.82 	r: 87.51 	f1: 89.62 	 2965 	 3229 	 3388
ni 	p: 87.94 	r: 79.57 	f1: 83.55 	 1211 	 1377 	 1522

[32m iter_1[0m
ga 	p: 82.33 	r: 75.39 	f1: 78.71 	 4650 	 5648 	 6168
wo 	p: 94.06 	r: 85.48 	f1: 89.56 	 2896 	 3079 	 3388
ni 	p: 88.33 	r: 79.57 	f1: 83.72 	 1211 	 1371 	 1522

[32m iter_2[0m
ga 	p: 82.4 	r: 75.37 	f1: 78.73 	 4649 	 5642 	 6168
wo 	p: 94.13 	r: 85.6 	f1: 89.66 	 2900 	 3081 	 3388
ni 	p: 88.44 	r: 79.43 	f1: 83.7 	 1209 	 1367 	 1522
best_thres [[0.38, 0.29, 0.16], [0.39, 0.49, 0.14], [0.39, 0.49, 0.14]]
f [0.828, 0.8276, 0.8275]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 13 	 [0.39, 0.49, 0.14] 	 lr: 2.5e-05 	 f: 82.75309998430387
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 14 [0m
Train...

[32m iter_0[0m
ga 	p: 80.33 	r: 75.36 	f1: 77.76 	 4648 	 5786 	 6168
wo 	p: 93.95 	r: 85.18 	f1: 89.35 	 2886 	 3072 	 3388
ni 	p: 85.55 	r: 73.92 	f1: 79.31 	 1125 	 1315 	 1522

[32m iter_1[0m
ga 	p: 81.45 	r: 74.87 	f1: 78.02 	 4618 	 5670 	 6168
wo 	p: 93.52 	r: 85.57 	f1: 89.36 	 2899 	 3100 	 3388
ni 	p: 82.46 	r: 76.94 	f1: 79.61 	 1171 	 1420 	 1522

[32m iter_2[0m
ga 	p: 81.47 	r: 74.87 	f1: 78.03 	 4618 	 5668 	 6168
wo 	p: 93.69 	r: 85.51 	f1: 89.41 	 2897 	 3092 	 3388
ni 	p: 82.99 	r: 76.94 	f1: 79.85 	 1171 	 1411 	 1522
best_thres [[0.47, 0.62, 0.17], [0.5, 0.57, 0.1], [0.5, 0.59, 0.1]]
f [0.8149, 0.816, 0.8165]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 9 	 [0.43, 0.36, 0.11] 	 lr: 5e-05 	 f: 82.623594280552
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 13 [0m
Train...

[32m iter_0[0m
ga 	p: 82.19 	r: 75.71 	f1: 78.82 	 4670 	 5682 	 6168
wo 	p: 91.82 	r: 87.51 	f1: 89.62 	 2965 	 3229 	 3388
ni 	p: 87.94 	r: 79.57 	f1: 83.55 	 1211 	 1377 	 1522

[32m iter_1[0m
ga 	p: 82.33 	r: 75.39 	f1: 78.71 	 4650 	 5648 	 6168
wo 	p: 94.06 	r: 85.48 	f1: 89.56 	 2896 	 3079 	 3388
ni 	p: 88.33 	r: 79.57 	f1: 83.72 	 1211 	 1371 	 1522

[32m iter_2[0m
ga 	p: 82.4 	r: 75.37 	f1: 78.73 	 4649 	 5642 	 6168
wo 	p: 94.13 	r: 85.6 	f1: 89.66 	 2900 	 3081 	 3388
ni 	p: 88.44 	r: 79.43 	f1: 83.7 	 1209 	 1367 	 1522
best_thres [[0.38, 0.29, 0.16], [0.39, 0.49, 0.14], [0.39, 0.49, 0.14]]
f [0.828, 0.8276, 0.8275]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 13 	 [0.39, 0.49, 0.14] 	 lr: 2.5e-05 	 f: 82.75309998430387
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 14 [0m
Train...
loss: tensor(876.8588) lr: 2.5e-05 time: 3986.33
pred_count_train 41644

Test...
loss: tensor(429.5372) lr: 5e-05 time: 4154.36
pred_count_train 41644

Test...
loss: tensor(876.8588) lr: 2.5e-05 time: 4097.01
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.58 	r: 75.63 	f1: 78.03 	 4665 	 5789 	 6168
wo 	p: 93.31 	r: 86.04 	f1: 89.53 	 2915 	 3124 	 3388
ni 	p: 84.65 	r: 79.7 	f1: 82.1 	 1213 	 1433 	 1522

[32m iter_1[0m
ga 	p: 82.52 	r: 74.42 	f1: 78.26 	 4590 	 5562 	 6168
wo 	p: 93.05 	r: 86.13 	f1: 89.45 	 2918 	 3136 	 3388
ni 	p: 84.83 	r: 79.7 	f1: 82.18 	 1213 	 1430 	 1522

[32m iter_2[0m
ga 	p: 82.89 	r: 74.16 	f1: 78.28 	 4574 	 5518 	 6168
wo 	p: 92.91 	r: 86.3 	f1: 89.49 	 2924 	 3147 	 3388
ni 	p: 84.13 	r: 80.81 	f1: 82.44 	 1230 	 1462 	 1522
best_thres [[0.39, 0.53, 0.16], [0.48, 0.59, 0.13], [0.56, 0.54, 0.11]]
f [0.8209, 0.8217, 0.8222]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 13 	 [0.39, 0.49, 0.14] 	 lr: 2.5e-05 	 f: 82.75309998430387
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 15 [0m
Train...

[32m iter_0[0m
ga 	p: 81.64 	r: 73.02 	f1: 77.09 	 4504 	 5517 	 6168
wo 	p: 92.16 	r: 86.1 	f1: 89.03 	 2917 	 3165 	 3388
ni 	p: 83.33 	r: 76.22 	f1: 79.62 	 1160 	 1392 	 1522

[32m iter_1[0m
ga 	p: 80.29 	r: 74.19 	f1: 77.12 	 4576 	 5699 	 6168
wo 	p: 91.62 	r: 86.84 	f1: 89.17 	 2942 	 3211 	 3388
ni 	p: 85.73 	r: 74.97 	f1: 79.99 	 1141 	 1331 	 1522

[32m iter_2[0m
ga 	p: 81.51 	r: 73.27 	f1: 77.17 	 4519 	 5544 	 6168
wo 	p: 91.65 	r: 86.84 	f1: 89.18 	 2942 	 3210 	 3388
ni 	p: 85.2 	r: 75.3 	f1: 79.94 	 1146 	 1345 	 1522
best_thres [[0.51, 0.56, 0.09], [0.34, 0.46, 0.1], [0.44, 0.45, 0.09]]
f [0.8114, 0.8118, 0.8122]
load model: epoch9
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 9 	 [0.43, 0.36, 0.11] 	 lr: 5e-05 	 f: 82.623594280552
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 14 [0m
Train...

[32m iter_0[0m
ga 	p: 80.58 	r: 75.63 	f1: 78.03 	 4665 	 5789 	 6168
wo 	p: 93.31 	r: 86.04 	f1: 89.53 	 2915 	 3124 	 3388
ni 	p: 84.65 	r: 79.7 	f1: 82.1 	 1213 	 1433 	 1522

[32m iter_1[0m
ga 	p: 82.52 	r: 74.42 	f1: 78.26 	 4590 	 5562 	 6168
wo 	p: 93.05 	r: 86.13 	f1: 89.45 	 2918 	 3136 	 3388
ni 	p: 84.83 	r: 79.7 	f1: 82.18 	 1213 	 1430 	 1522

[32m iter_2[0m
ga 	p: 82.89 	r: 74.16 	f1: 78.28 	 4574 	 5518 	 6168
wo 	p: 92.91 	r: 86.3 	f1: 89.49 	 2924 	 3147 	 3388
ni 	p: 84.13 	r: 80.81 	f1: 82.44 	 1230 	 1462 	 1522
best_thres [[0.39, 0.53, 0.16], [0.48, 0.59, 0.13], [0.56, 0.54, 0.11]]
f [0.8209, 0.8217, 0.8222]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 13 	 [0.39, 0.49, 0.14] 	 lr: 2.5e-05 	 f: 82.75309998430387
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 15 [0m
Train...
loss: tensor(738.8793) lr: 2.5e-05 time: 4077.4
pred_count_train 41644

Test...
loss: tensor(806.0151) lr: 2.5e-05 time: 4182.5
pred_count_train 41644

Test...
loss: tensor(738.8793) lr: 2.5e-05 time: 3972.2
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.09 	r: 73.14 	f1: 77.36 	 4511 	 5495 	 6168
wo 	p: 91.38 	r: 87.66 	f1: 89.48 	 2970 	 3250 	 3388
ni 	p: 87.37 	r: 74.57 	f1: 80.47 	 1135 	 1299 	 1522

[32m iter_1[0m
ga 	p: 79.65 	r: 75.5 	f1: 77.52 	 4657 	 5847 	 6168
wo 	p: 92.4 	r: 86.81 	f1: 89.51 	 2941 	 3183 	 3388
ni 	p: 84.86 	r: 78.06 	f1: 81.31 	 1188 	 1400 	 1522

[32m iter_2[0m
ga 	p: 82.47 	r: 73.15 	f1: 77.53 	 4512 	 5471 	 6168
wo 	p: 92.33 	r: 86.72 	f1: 89.44 	 2938 	 3182 	 3388
ni 	p: 85.48 	r: 78.12 	f1: 81.63 	 1189 	 1391 	 1522
best_thres [[0.43, 0.37, 0.2], [0.3, 0.49, 0.1], [0.44, 0.49, 0.1]]
f [0.8158, 0.8164, 0.8169]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 13 	 [0.39, 0.49, 0.14] 	 lr: 2.5e-05 	 f: 82.75309998430387
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 16 [0m
Train...

[32m iter_0[0m
ga 	p: 79.69 	r: 77.68 	f1: 78.67 	 4791 	 6012 	 6168
wo 	p: 93.0 	r: 86.28 	f1: 89.51 	 2923 	 3143 	 3388
ni 	p: 84.24 	r: 79.7 	f1: 81.9 	 1213 	 1440 	 1522

[32m iter_1[0m
ga 	p: 82.49 	r: 75.62 	f1: 78.9 	 4664 	 5654 	 6168
wo 	p: 92.94 	r: 86.28 	f1: 89.48 	 2923 	 3145 	 3388
ni 	p: 84.35 	r: 79.3 	f1: 81.75 	 1207 	 1431 	 1522

[32m iter_2[0m
ga 	p: 82.76 	r: 75.34 	f1: 78.88 	 4647 	 5615 	 6168
wo 	p: 92.91 	r: 86.25 	f1: 89.45 	 2922 	 3145 	 3388
ni 	p: 83.85 	r: 79.83 	f1: 81.79 	 1215 	 1449 	 1522
best_thres [[0.37, 0.52, 0.15], [0.5, 0.51, 0.14], [0.52, 0.51, 0.13]]
f [0.8238, 0.8246, 0.8248]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 9 	 [0.43, 0.36, 0.11] 	 lr: 5e-05 	 f: 82.623594280552
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 15 [0m
Train...

[32m iter_0[0m
ga 	p: 82.09 	r: 73.14 	f1: 77.36 	 4511 	 5495 	 6168
wo 	p: 91.38 	r: 87.66 	f1: 89.48 	 2970 	 3250 	 3388
ni 	p: 87.37 	r: 74.57 	f1: 80.47 	 1135 	 1299 	 1522

[32m iter_1[0m
ga 	p: 79.65 	r: 75.5 	f1: 77.52 	 4657 	 5847 	 6168
wo 	p: 92.4 	r: 86.81 	f1: 89.51 	 2941 	 3183 	 3388
ni 	p: 84.86 	r: 78.06 	f1: 81.31 	 1188 	 1400 	 1522

[32m iter_2[0m
ga 	p: 82.47 	r: 73.15 	f1: 77.53 	 4512 	 5471 	 6168
wo 	p: 92.33 	r: 86.72 	f1: 89.44 	 2938 	 3182 	 3388
ni 	p: 85.48 	r: 78.12 	f1: 81.63 	 1189 	 1391 	 1522
best_thres [[0.43, 0.37, 0.2], [0.3, 0.49, 0.1], [0.44, 0.49, 0.1]]
f [0.8158, 0.8164, 0.8169]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 13 	 [0.39, 0.49, 0.14] 	 lr: 2.5e-05 	 f: 82.75309998430387
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 16 [0m
Train...
loss: tensor(616.0328) lr: 2.5e-05 time: 4066.32
pred_count_train 41644

Test...
loss: tensor(665.1323) lr: 2.5e-05 time: 4111.39
pred_count_train 41644

Test...
loss: tensor(616.0328) lr: 2.5e-05 time: 4074.89
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.24 	r: 73.59 	f1: 77.23 	 4539 	 5587 	 6168
wo 	p: 92.63 	r: 85.71 	f1: 89.04 	 2904 	 3135 	 3388
ni 	p: 81.46 	r: 76.48 	f1: 78.89 	 1164 	 1429 	 1522

[32m iter_1[0m
ga 	p: 82.15 	r: 73.62 	f1: 77.65 	 4541 	 5528 	 6168
wo 	p: 93.0 	r: 85.51 	f1: 89.1 	 2897 	 3115 	 3388
ni 	p: 80.63 	r: 77.92 	f1: 79.25 	 1186 	 1471 	 1522

[32m iter_2[0m
ga 	p: 82.02 	r: 73.59 	f1: 77.58 	 4539 	 5534 	 6168
wo 	p: 93.09 	r: 85.45 	f1: 89.1 	 2895 	 3110 	 3388
ni 	p: 82.6 	r: 76.41 	f1: 79.39 	 1163 	 1408 	 1522
best_thres [[0.43, 0.51, 0.12], [0.47, 0.74, 0.07], [0.47, 0.79, 0.08]]
f [0.8109, 0.8124, 0.8128]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 13 	 [0.39, 0.49, 0.14] 	 lr: 2.5e-05 	 f: 82.75309998430387
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 17 [0m
Train...

[32m iter_0[0m
ga 	p: 81.26 	r: 74.71 	f1: 77.84 	 4608 	 5671 	 6168
wo 	p: 92.97 	r: 86.25 	f1: 89.48 	 2922 	 3143 	 3388
ni 	p: 85.27 	r: 76.08 	f1: 80.42 	 1158 	 1358 	 1522

[32m iter_1[0m
ga 	p: 82.34 	r: 74.29 	f1: 78.1 	 4582 	 5565 	 6168
wo 	p: 93.04 	r: 86.39 	f1: 89.59 	 2927 	 3146 	 3388
ni 	p: 83.79 	r: 77.79 	f1: 80.68 	 1184 	 1413 	 1522

[32m iter_2[0m
ga 	p: 82.41 	r: 74.27 	f1: 78.13 	 4581 	 5559 	 6168
wo 	p: 92.62 	r: 86.69 	f1: 89.56 	 2937 	 3171 	 3388
ni 	p: 84.18 	r: 77.6 	f1: 80.75 	 1181 	 1403 	 1522
best_thres [[0.44, 0.57, 0.18], [0.47, 0.54, 0.12], [0.48, 0.49, 0.12]]
f [0.8177, 0.8189, 0.8193]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 9 	 [0.43, 0.36, 0.11] 	 lr: 5e-05 	 f: 82.623594280552
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 16 [0m
Train...

[32m iter_0[0m
ga 	p: 81.24 	r: 73.59 	f1: 77.23 	 4539 	 5587 	 6168
wo 	p: 92.63 	r: 85.71 	f1: 89.04 	 2904 	 3135 	 3388
ni 	p: 81.46 	r: 76.48 	f1: 78.89 	 1164 	 1429 	 1522

[32m iter_1[0m
ga 	p: 82.15 	r: 73.62 	f1: 77.65 	 4541 	 5528 	 6168
wo 	p: 93.0 	r: 85.51 	f1: 89.1 	 2897 	 3115 	 3388
ni 	p: 80.63 	r: 77.92 	f1: 79.25 	 1186 	 1471 	 1522

[32m iter_2[0m
ga 	p: 82.02 	r: 73.59 	f1: 77.58 	 4539 	 5534 	 6168
wo 	p: 93.09 	r: 85.45 	f1: 89.1 	 2895 	 3110 	 3388
ni 	p: 82.6 	r: 76.41 	f1: 79.39 	 1163 	 1408 	 1522
best_thres [[0.43, 0.51, 0.12], [0.47, 0.74, 0.07], [0.47, 0.79, 0.08]]
f [0.8109, 0.8124, 0.8128]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 13 	 [0.39, 0.49, 0.14] 	 lr: 2.5e-05 	 f: 82.75309998430387
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 17 [0m
Train...
loss: tensor(513.6449) lr: 2.5e-05 time: 3967.65
pred_count_train 41644

Test...
loss: tensor(546.5847) lr: 2.5e-05 time: 4138.55
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.5 	r: 74.3 	f1: 76.81 	 4583 	 5765 	 6168
wo 	p: 92.7 	r: 85.83 	f1: 89.13 	 2908 	 3137 	 3388
ni 	p: 83.54 	r: 74.05 	f1: 78.51 	 1127 	 1349 	 1522

[32m iter_1[0m
ga 	p: 81.26 	r: 73.27 	f1: 77.06 	 4519 	 5561 	 6168
wo 	p: 92.65 	r: 85.92 	f1: 89.16 	 2911 	 3142 	 3388
ni 	p: 83.04 	r: 76.22 	f1: 79.48 	 1160 	 1397 	 1522

[32m iter_2[0m
ga 	p: 80.59 	r: 73.98 	f1: 77.14 	 4563 	 5662 	 6168
wo 	p: 92.94 	r: 85.45 	f1: 89.04 	 2895 	 3115 	 3388
ni 	p: 83.45 	r: 76.54 	f1: 79.85 	 1165 	 1396 	 1522
best_thres [[0.38, 0.54, 0.23], [0.49, 0.66, 0.13], [0.43, 0.83, 0.12]]
f [0.8081, 0.8097, 0.8103]
load model: epoch13
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 13 	 [0.39, 0.49, 0.14] 	 lr: 2.5e-05 	 f: 82.75309998430387
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 18 [0m
Train...
loss: tensor(513.6449) lr: 2.5e-05 time: 3974.93
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.46 	r: 76.01 	f1: 77.69 	 4688 	 5900 	 6168
wo 	p: 92.91 	r: 86.28 	f1: 89.47 	 2923 	 3146 	 3388
ni 	p: 85.44 	r: 74.44 	f1: 79.56 	 1133 	 1326 	 1522

[32m iter_1[0m
ga 	p: 80.5 	r: 75.84 	f1: 78.1 	 4678 	 5811 	 6168
wo 	p: 92.31 	r: 86.78 	f1: 89.46 	 2940 	 3185 	 3388
ni 	p: 81.07 	r: 77.4 	f1: 79.19 	 1178 	 1453 	 1522

[32m iter_2[0m
ga 	p: 80.53 	r: 75.79 	f1: 78.09 	 4675 	 5805 	 6168
wo 	p: 93.78 	r: 85.45 	f1: 89.42 	 2895 	 3087 	 3388
ni 	p: 80.27 	r: 78.06 	f1: 79.15 	 1188 	 1480 	 1522
best_thres [[0.34, 0.56, 0.16], [0.36, 0.48, 0.08], [0.36, 0.65, 0.07]]
f [0.8153, 0.8163, 0.8164]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 9 	 [0.43, 0.36, 0.11] 	 lr: 5e-05 	 f: 82.623594280552
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 17 [0m
Train...

[32m iter_0[0m
ga 	p: 79.5 	r: 74.3 	f1: 76.81 	 4583 	 5765 	 6168
wo 	p: 92.7 	r: 85.83 	f1: 89.13 	 2908 	 3137 	 3388
ni 	p: 83.54 	r: 74.05 	f1: 78.51 	 1127 	 1349 	 1522

[32m iter_1[0m
ga 	p: 81.26 	r: 73.27 	f1: 77.06 	 4519 	 5561 	 6168
wo 	p: 92.65 	r: 85.92 	f1: 89.16 	 2911 	 3142 	 3388
ni 	p: 83.04 	r: 76.22 	f1: 79.48 	 1160 	 1397 	 1522

[32m iter_2[0m
ga 	p: 80.59 	r: 73.98 	f1: 77.14 	 4563 	 5662 	 6168
wo 	p: 92.94 	r: 85.45 	f1: 89.04 	 2895 	 3115 	 3388
ni 	p: 83.45 	r: 76.54 	f1: 79.85 	 1165 	 1396 	 1522
best_thres [[0.38, 0.54, 0.23], [0.49, 0.66, 0.13], [0.43, 0.83, 0.12]]
f [0.8081, 0.8097, 0.8103]
load model: epoch13
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 13 	 [0.39, 0.49, 0.14] 	 lr: 2.5e-05 	 f: 82.75309998430387
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 18 [0m
Train...
loss: tensor(838.4487) lr: 1.25e-05 time: 3977.91
pred_count_train 41644

Test...
loss: tensor(452.6873) lr: 2.5e-05 time: 4053.46
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.65 	r: 75.29 	f1: 78.34 	 4644 	 5688 	 6168
wo 	p: 92.2 	r: 87.19 	f1: 89.62 	 2954 	 3204 	 3388
ni 	p: 84.65 	r: 78.65 	f1: 81.54 	 1197 	 1414 	 1522

[32m iter_1[0m
ga 	p: 85.03 	r: 72.65 	f1: 78.35 	 4481 	 5270 	 6168
wo 	p: 92.76 	r: 86.66 	f1: 89.61 	 2936 	 3165 	 3388
ni 	p: 84.94 	r: 78.91 	f1: 81.81 	 1201 	 1414 	 1522

[32m iter_2[0m
ga 	p: 84.73 	r: 72.96 	f1: 78.4 	 4500 	 5311 	 6168
wo 	p: 92.67 	r: 86.57 	f1: 89.52 	 2933 	 3165 	 3388
ni 	p: 85.73 	r: 78.58 	f1: 82.0 	 1196 	 1395 	 1522
best_thres [[0.32, 0.38, 0.09], [0.46, 0.41, 0.07], [0.44, 0.41, 0.07]]
f [0.8226, 0.8231, 0.8233]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 13 	 [0.39, 0.49, 0.14] 	 lr: 2.5e-05 	 f: 82.75309998430387
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 19 [0m
Train...
loss: tensor(838.4487) lr: 1.25e-05 time: 4060.75
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.28 	r: 75.66 	f1: 77.43 	 4667 	 5887 	 6168
wo 	p: 92.44 	r: 86.25 	f1: 89.23 	 2922 	 3161 	 3388
ni 	p: 81.81 	r: 75.03 	f1: 78.27 	 1142 	 1396 	 1522

[32m iter_1[0m
ga 	p: 79.6 	r: 75.99 	f1: 77.75 	 4687 	 5888 	 6168
wo 	p: 92.52 	r: 86.19 	f1: 89.24 	 2920 	 3156 	 3388
ni 	p: 81.25 	r: 76.61 	f1: 78.86 	 1166 	 1435 	 1522

[32m iter_2[0m
ga 	p: 80.33 	r: 75.34 	f1: 77.75 	 4647 	 5785 	 6168
wo 	p: 92.55 	r: 86.22 	f1: 89.27 	 2921 	 3156 	 3388
ni 	p: 81.44 	r: 76.41 	f1: 78.85 	 1163 	 1428 	 1522
best_thres [[0.38, 0.62, 0.2], [0.36, 0.64, 0.15], [0.41, 0.64, 0.15]]
f [0.8114, 0.8126, 0.8132]
load model: epoch9
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 9 	 [0.43, 0.36, 0.11] 	 lr: 5e-05 	 f: 82.623594280552
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 18 [0m
Train...

[32m iter_0[0m
ga 	p: 81.65 	r: 75.29 	f1: 78.34 	 4644 	 5688 	 6168
wo 	p: 92.2 	r: 87.19 	f1: 89.62 	 2954 	 3204 	 3388
ni 	p: 84.65 	r: 78.65 	f1: 81.54 	 1197 	 1414 	 1522

[32m iter_1[0m
ga 	p: 85.03 	r: 72.65 	f1: 78.35 	 4481 	 5270 	 6168
wo 	p: 92.76 	r: 86.66 	f1: 89.61 	 2936 	 3165 	 3388
ni 	p: 84.94 	r: 78.91 	f1: 81.81 	 1201 	 1414 	 1522

[32m iter_2[0m
ga 	p: 84.73 	r: 72.96 	f1: 78.4 	 4500 	 5311 	 6168
wo 	p: 92.67 	r: 86.57 	f1: 89.52 	 2933 	 3165 	 3388
ni 	p: 85.73 	r: 78.58 	f1: 82.0 	 1196 	 1395 	 1522
best_thres [[0.32, 0.38, 0.09], [0.46, 0.41, 0.07], [0.44, 0.41, 0.07]]
f [0.8226, 0.8231, 0.8233]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 13 	 [0.39, 0.49, 0.14] 	 lr: 2.5e-05 	 f: 82.75309998430387
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 19 [0m
Train...
loss: tensor(749.3634) lr: 1.25e-05 time: 3895.97
pred_count_train 41644

Test...
loss: tensor(775.7238) lr: 1.25e-05 time: 4140.48
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.78 	r: 76.31 	f1: 78.48 	 4707 	 5827 	 6168
wo 	p: 91.17 	r: 87.75 	f1: 89.43 	 2973 	 3261 	 3388
ni 	p: 87.62 	r: 75.82 	f1: 81.3 	 1154 	 1317 	 1522

[32m iter_1[0m
ga 	p: 81.89 	r: 75.37 	f1: 78.5 	 4649 	 5677 	 6168
wo 	p: 91.65 	r: 87.1 	f1: 89.32 	 2951 	 3220 	 3388
ni 	p: 84.9 	r: 78.71 	f1: 81.69 	 1198 	 1411 	 1522

[32m iter_2[0m
ga 	p: 81.91 	r: 75.37 	f1: 78.5 	 4649 	 5676 	 6168
wo 	p: 93.72 	r: 85.48 	f1: 89.41 	 2896 	 3090 	 3388
ni 	p: 86.97 	r: 77.2 	f1: 81.8 	 1175 	 1351 	 1522
best_thres [[0.32, 0.26, 0.17], [0.36, 0.29, 0.09], [0.36, 0.59, 0.11]]
f [0.8224, 0.8226, 0.8227]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 13 	 [0.39, 0.49, 0.14] 	 lr: 2.5e-05 	 f: 82.75309998430387
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 20 [0m
Train...
loss: tensor(749.3634) lr: 1.25e-05 time: 4063.37
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.61 	r: 76.28 	f1: 78.86 	 4705 	 5765 	 6168
wo 	p: 91.93 	r: 87.81 	f1: 89.82 	 2975 	 3236 	 3388
ni 	p: 86.5 	r: 79.17 	f1: 82.68 	 1205 	 1393 	 1522

[32m iter_1[0m
ga 	p: 82.71 	r: 75.99 	f1: 79.21 	 4687 	 5667 	 6168
wo 	p: 92.37 	r: 87.6 	f1: 89.93 	 2968 	 3213 	 3388
ni 	p: 82.82 	r: 81.41 	f1: 82.11 	 1239 	 1496 	 1522

[32m iter_2[0m
ga 	p: 82.5 	r: 76.07 	f1: 79.16 	 4692 	 5687 	 6168
wo 	p: 92.37 	r: 87.6 	f1: 89.93 	 2968 	 3213 	 3388
ni 	p: 83.02 	r: 81.6 	f1: 82.31 	 1242 	 1496 	 1522
best_thres [[0.37, 0.38, 0.12], [0.4, 0.4, 0.07], [0.39, 0.4, 0.07]]
f [0.8276, 0.8284, 0.8286]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 18 	 [0.39, 0.4, 0.07] 	 lr: 1.25e-05 	 f: 82.86024844720497
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 19 [0m
Train...

[32m iter_0[0m
ga 	p: 80.78 	r: 76.31 	f1: 78.48 	 4707 	 5827 	 6168
wo 	p: 91.17 	r: 87.75 	f1: 89.43 	 2973 	 3261 	 3388
ni 	p: 87.62 	r: 75.82 	f1: 81.3 	 1154 	 1317 	 1522

[32m iter_1[0m
ga 	p: 81.89 	r: 75.37 	f1: 78.5 	 4649 	 5677 	 6168
wo 	p: 91.65 	r: 87.1 	f1: 89.32 	 2951 	 3220 	 3388
ni 	p: 84.9 	r: 78.71 	f1: 81.69 	 1198 	 1411 	 1522

[32m iter_2[0m
ga 	p: 81.91 	r: 75.37 	f1: 78.5 	 4649 	 5676 	 6168
wo 	p: 93.72 	r: 85.48 	f1: 89.41 	 2896 	 3090 	 3388
ni 	p: 86.97 	r: 77.2 	f1: 81.8 	 1175 	 1351 	 1522
best_thres [[0.32, 0.26, 0.17], [0.36, 0.29, 0.09], [0.36, 0.59, 0.11]]
f [0.8224, 0.8226, 0.8227]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 13 	 [0.39, 0.49, 0.14] 	 lr: 2.5e-05 	 f: 82.75309998430387
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 20 [0m
Train...
loss: tensor(669.2514) lr: 1.25e-05 time: 3906.18
pred_count_train 41644

Test...
loss: tensor(676.7909) lr: 1.25e-05 time: 4080.53
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.26 	r: 74.68 	f1: 77.83 	 4606 	 5668 	 6168
wo 	p: 92.43 	r: 86.48 	f1: 89.36 	 2930 	 3170 	 3388
ni 	p: 82.91 	r: 76.48 	f1: 79.56 	 1164 	 1404 	 1522

[32m iter_1[0m
ga 	p: 82.51 	r: 74.11 	f1: 78.08 	 4571 	 5540 	 6168
wo 	p: 92.71 	r: 86.28 	f1: 89.37 	 2923 	 3153 	 3388
ni 	p: 86.47 	r: 74.77 	f1: 80.2 	 1138 	 1316 	 1522

[32m iter_2[0m
ga 	p: 82.54 	r: 74.09 	f1: 78.09 	 4570 	 5537 	 6168
wo 	p: 92.82 	r: 86.19 	f1: 89.38 	 2920 	 3146 	 3388
ni 	p: 87.15 	r: 74.44 	f1: 80.3 	 1133 	 1300 	 1522
best_thres [[0.35, 0.42, 0.15], [0.4, 0.44, 0.14], [0.4, 0.45, 0.15]]
f [0.8161, 0.8174, 0.8179]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 13 	 [0.39, 0.49, 0.14] 	 lr: 2.5e-05 	 f: 82.75309998430387
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 21 [0m
Train...
loss: tensor(669.2514) lr: 1.25e-05 time: 3878.13
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.37 	r: 76.95 	f1: 78.62 	 4746 	 5905 	 6168
wo 	p: 92.31 	r: 87.19 	f1: 89.68 	 2954 	 3200 	 3388
ni 	p: 85.53 	r: 78.06 	f1: 81.62 	 1188 	 1389 	 1522

[32m iter_1[0m
ga 	p: 83.68 	r: 74.5 	f1: 78.82 	 4595 	 5491 	 6168
wo 	p: 92.9 	r: 86.57 	f1: 89.63 	 2933 	 3157 	 3388
ni 	p: 87.45 	r: 75.56 	f1: 81.07 	 1150 	 1315 	 1522

[32m iter_2[0m
ga 	p: 83.7 	r: 74.51 	f1: 78.84 	 4596 	 5491 	 6168
wo 	p: 92.87 	r: 86.54 	f1: 89.6 	 2932 	 3157 	 3388
ni 	p: 82.8 	r: 79.7 	f1: 81.22 	 1213 	 1465 	 1522
best_thres [[0.34, 0.4, 0.14], [0.5, 0.46, 0.17], [0.5, 0.46, 0.09]]
f [0.824, 0.8244, 0.8246]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 18 	 [0.39, 0.4, 0.07] 	 lr: 1.25e-05 	 f: 82.86024844720497
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 20 [0m
Train...

[32m iter_0[0m
ga 	p: 81.26 	r: 74.68 	f1: 77.83 	 4606 	 5668 	 6168
wo 	p: 92.43 	r: 86.48 	f1: 89.36 	 2930 	 3170 	 3388
ni 	p: 82.91 	r: 76.48 	f1: 79.56 	 1164 	 1404 	 1522

[32m iter_1[0m
ga 	p: 82.51 	r: 74.11 	f1: 78.08 	 4571 	 5540 	 6168
wo 	p: 92.71 	r: 86.28 	f1: 89.37 	 2923 	 3153 	 3388
ni 	p: 86.47 	r: 74.77 	f1: 80.2 	 1138 	 1316 	 1522

[32m iter_2[0m
ga 	p: 82.54 	r: 74.09 	f1: 78.09 	 4570 	 5537 	 6168
wo 	p: 92.82 	r: 86.19 	f1: 89.38 	 2920 	 3146 	 3388
ni 	p: 87.15 	r: 74.44 	f1: 80.3 	 1133 	 1300 	 1522
best_thres [[0.35, 0.42, 0.15], [0.4, 0.44, 0.14], [0.4, 0.45, 0.15]]
f [0.8161, 0.8174, 0.8179]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 13 	 [0.39, 0.49, 0.14] 	 lr: 2.5e-05 	 f: 82.75309998430387
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 21 [0m
Train...
loss: tensor(598.4114) lr: 1.25e-05 time: 3265.89
pred_count_train 41644

Test...
loss: tensor(600.3229) lr: 1.25e-05 time: 3311.91
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.07 	r: 74.97 	f1: 77.43 	 4624 	 5775 	 6168
wo 	p: 92.22 	r: 86.75 	f1: 89.4 	 2939 	 3187 	 3388
ni 	p: 83.95 	r: 76.28 	f1: 79.93 	 1161 	 1383 	 1522

[32m iter_1[0m
ga 	p: 80.29 	r: 75.28 	f1: 77.7 	 4643 	 5783 	 6168
wo 	p: 92.48 	r: 86.42 	f1: 89.35 	 2928 	 3166 	 3388
ni 	p: 85.48 	r: 75.82 	f1: 80.36 	 1154 	 1350 	 1522

[32m iter_2[0m
ga 	p: 82.15 	r: 73.64 	f1: 77.66 	 4542 	 5529 	 6168
wo 	p: 92.25 	r: 86.75 	f1: 89.41 	 2939 	 3186 	 3388
ni 	p: 85.5 	r: 75.95 	f1: 80.45 	 1156 	 1352 	 1522
best_thres [[0.33, 0.45, 0.18], [0.31, 0.48, 0.14], [0.43, 0.43, 0.13]]
f [0.8145, 0.8154, 0.8159]
load model: epoch13
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 13 	 [0.39, 0.49, 0.14] 	 lr: 2.5e-05 	 f: 82.75309998430387
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 22 [0m
Train...
loss: tensor(598.4114) lr: 1.25e-05 time: 3119.21
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.77 	r: 75.24 	f1: 78.37 	 4641 	 5676 	 6168
wo 	p: 91.33 	r: 88.28 	f1: 89.78 	 2991 	 3275 	 3388
ni 	p: 87.21 	r: 74.84 	f1: 80.55 	 1139 	 1306 	 1522

[32m iter_1[0m
ga 	p: 79.96 	r: 77.38 	f1: 78.65 	 4773 	 5969 	 6168
wo 	p: 92.9 	r: 86.87 	f1: 89.78 	 2943 	 3168 	 3388
ni 	p: 85.14 	r: 76.81 	f1: 80.76 	 1169 	 1373 	 1522

[32m iter_2[0m
ga 	p: 81.05 	r: 76.33 	f1: 78.62 	 4708 	 5809 	 6168
wo 	p: 93.58 	r: 86.1 	f1: 89.68 	 2917 	 3117 	 3388
ni 	p: 85.63 	r: 76.74 	f1: 80.94 	 1168 	 1364 	 1522
best_thres [[0.4, 0.32, 0.19], [0.27, 0.49, 0.13], [0.32, 0.57, 0.13]]
f [0.8222, 0.8227, 0.8228]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 18 	 [0.39, 0.4, 0.07] 	 lr: 1.25e-05 	 f: 82.86024844720497
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 21 [0m
Train...

[32m iter_0[0m
ga 	p: 80.07 	r: 74.97 	f1: 77.43 	 4624 	 5775 	 6168
wo 	p: 92.22 	r: 86.75 	f1: 89.4 	 2939 	 3187 	 3388
ni 	p: 83.95 	r: 76.28 	f1: 79.93 	 1161 	 1383 	 1522

[32m iter_1[0m
ga 	p: 80.29 	r: 75.28 	f1: 77.7 	 4643 	 5783 	 6168
wo 	p: 92.48 	r: 86.42 	f1: 89.35 	 2928 	 3166 	 3388
ni 	p: 85.48 	r: 75.82 	f1: 80.36 	 1154 	 1350 	 1522

[32m iter_2[0m
ga 	p: 82.15 	r: 73.64 	f1: 77.66 	 4542 	 5529 	 6168
wo 	p: 92.25 	r: 86.75 	f1: 89.41 	 2939 	 3186 	 3388
ni 	p: 85.5 	r: 75.95 	f1: 80.45 	 1156 	 1352 	 1522
best_thres [[0.33, 0.45, 0.18], [0.31, 0.48, 0.14], [0.43, 0.43, 0.13]]
f [0.8145, 0.8154, 0.8159]
load model: epoch13
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 13 	 [0.39, 0.49, 0.14] 	 lr: 2.5e-05 	 f: 82.75309998430387
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 22 [0m
Train...
loss: tensor(821.1422) lr: 6.25e-06 time: 2149.62
pred_count_train 41644

Test...
loss: tensor(535.0440) lr: 1.25e-05 time: 2153.51
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.79 	r: 75.68 	f1: 78.62 	 4668 	 5707 	 6168
wo 	p: 92.98 	r: 86.36 	f1: 89.55 	 2926 	 3147 	 3388
ni 	p: 87.52 	r: 76.48 	f1: 81.63 	 1164 	 1330 	 1522

[32m iter_1[0m
ga 	p: 82.81 	r: 75.19 	f1: 78.82 	 4638 	 5601 	 6168
wo 	p: 92.85 	r: 86.6 	f1: 89.62 	 2934 	 3160 	 3388
ni 	p: 83.32 	r: 80.09 	f1: 81.68 	 1219 	 1463 	 1522

[32m iter_2[0m
ga 	p: 82.72 	r: 75.45 	f1: 78.92 	 4654 	 5626 	 6168
wo 	p: 93.15 	r: 86.25 	f1: 89.56 	 2922 	 3137 	 3388
ni 	p: 83.71 	r: 80.03 	f1: 81.83 	 1218 	 1455 	 1522
best_thres [[0.38, 0.45, 0.18], [0.41, 0.42, 0.1], [0.4, 0.46, 0.1]]
f [0.8238, 0.8246, 0.825]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 13 	 [0.39, 0.49, 0.14] 	 lr: 2.5e-05 	 f: 82.75309998430387
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 23 [0m
Train...
loss: tensor(821.1422) lr: 6.25e-06 time: 2141.94
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.09 	r: 75.92 	f1: 77.95 	 4683 	 5847 	 6168
wo 	p: 92.64 	r: 86.6 	f1: 89.52 	 2934 	 3167 	 3388
ni 	p: 87.69 	r: 74.44 	f1: 80.53 	 1133 	 1292 	 1522

[32m iter_1[0m
ga 	p: 81.61 	r: 75.78 	f1: 78.59 	 4674 	 5727 	 6168
wo 	p: 93.32 	r: 85.83 	f1: 89.42 	 2908 	 3116 	 3388
ni 	p: 85.29 	r: 76.54 	f1: 80.68 	 1165 	 1366 	 1522

[32m iter_2[0m
ga 	p: 81.68 	r: 75.7 	f1: 78.58 	 4669 	 5716 	 6168
wo 	p: 92.63 	r: 86.45 	f1: 89.44 	 2929 	 3162 	 3388
ni 	p: 85.29 	r: 76.22 	f1: 80.5 	 1160 	 1360 	 1522
best_thres [[0.39, 0.5, 0.24], [0.41, 0.58, 0.16], [0.41, 0.5, 0.16]]
f [0.8184, 0.8201, 0.8206]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 18 	 [0.39, 0.4, 0.07] 	 lr: 1.25e-05 	 f: 82.86024844720497
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 22 [0m
Train...

[32m iter_0[0m
ga 	p: 81.79 	r: 75.68 	f1: 78.62 	 4668 	 5707 	 6168
wo 	p: 92.98 	r: 86.36 	f1: 89.55 	 2926 	 3147 	 3388
ni 	p: 87.52 	r: 76.48 	f1: 81.63 	 1164 	 1330 	 1522

[32m iter_1[0m
ga 	p: 82.81 	r: 75.19 	f1: 78.82 	 4638 	 5601 	 6168
wo 	p: 92.85 	r: 86.6 	f1: 89.62 	 2934 	 3160 	 3388
ni 	p: 83.32 	r: 80.09 	f1: 81.68 	 1219 	 1463 	 1522

[32m iter_2[0m
ga 	p: 82.72 	r: 75.45 	f1: 78.92 	 4654 	 5626 	 6168
wo 	p: 93.15 	r: 86.25 	f1: 89.56 	 2922 	 3137 	 3388
ni 	p: 83.71 	r: 80.03 	f1: 81.83 	 1218 	 1455 	 1522
best_thres [[0.38, 0.45, 0.18], [0.41, 0.42, 0.1], [0.4, 0.46, 0.1]]
f [0.8238, 0.8246, 0.825]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 13 	 [0.39, 0.49, 0.14] 	 lr: 2.5e-05 	 f: 82.75309998430387
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 23 [0m
Train...
loss: tensor(768.4893) lr: 6.25e-06 time: 2058.11
pred_count_train 41644

Test...
loss: tensor(469.3145) lr: 1.25e-05 time: 2150.98
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.69 	r: 75.08 	f1: 78.25 	 4631 	 5669 	 6168
wo 	p: 91.4 	r: 87.78 	f1: 89.55 	 2974 	 3254 	 3388
ni 	p: 87.21 	r: 77.07 	f1: 81.83 	 1173 	 1345 	 1522

[32m iter_1[0m
ga 	p: 81.49 	r: 75.57 	f1: 78.42 	 4661 	 5720 	 6168
wo 	p: 92.59 	r: 86.3 	f1: 89.34 	 2924 	 3158 	 3388
ni 	p: 85.58 	r: 78.78 	f1: 82.04 	 1199 	 1401 	 1522

[32m iter_2[0m
ga 	p: 81.65 	r: 75.55 	f1: 78.48 	 4660 	 5707 	 6168
wo 	p: 92.35 	r: 86.6 	f1: 89.38 	 2934 	 3177 	 3388
ni 	p: 85.08 	r: 79.04 	f1: 81.95 	 1203 	 1414 	 1522
best_thres [[0.36, 0.3, 0.17], [0.33, 0.39, 0.11], [0.33, 0.36, 0.1]]
f [0.8224, 0.8225, 0.8227]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 13 	 [0.39, 0.49, 0.14] 	 lr: 2.5e-05 	 f: 82.75309998430387
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 24 [0m
Train...
loss: tensor(768.4893) lr: 6.25e-06 time: 2144.64
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.86 	r: 74.24 	f1: 77.86 	 4579 	 5594 	 6168
wo 	p: 93.04 	r: 86.07 	f1: 89.42 	 2916 	 3134 	 3388
ni 	p: 85.81 	r: 73.92 	f1: 79.42 	 1125 	 1311 	 1522

[32m iter_1[0m
ga 	p: 80.36 	r: 76.36 	f1: 78.31 	 4710 	 5861 	 6168
wo 	p: 92.64 	r: 86.54 	f1: 89.49 	 2932 	 3165 	 3388
ni 	p: 82.32 	r: 76.81 	f1: 79.47 	 1169 	 1420 	 1522

[32m iter_2[0m
ga 	p: 80.5 	r: 76.31 	f1: 78.35 	 4707 	 5847 	 6168
wo 	p: 91.83 	r: 87.25 	f1: 89.48 	 2956 	 3219 	 3388
ni 	p: 84.05 	r: 75.49 	f1: 79.54 	 1149 	 1367 	 1522
best_thres [[0.54, 0.6, 0.17], [0.38, 0.53, 0.09], [0.39, 0.44, 0.11]]
f [0.8164, 0.8176, 0.8182]
load model: epoch18
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 18 	 [0.39, 0.4, 0.07] 	 lr: 1.25e-05 	 f: 82.86024844720497
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 23 [0m
Train...

[32m iter_0[0m
ga 	p: 81.69 	r: 75.08 	f1: 78.25 	 4631 	 5669 	 6168
wo 	p: 91.4 	r: 87.78 	f1: 89.55 	 2974 	 3254 	 3388
ni 	p: 87.21 	r: 77.07 	f1: 81.83 	 1173 	 1345 	 1522

[32m iter_1[0m
ga 	p: 81.49 	r: 75.57 	f1: 78.42 	 4661 	 5720 	 6168
wo 	p: 92.59 	r: 86.3 	f1: 89.34 	 2924 	 3158 	 3388
ni 	p: 85.58 	r: 78.78 	f1: 82.04 	 1199 	 1401 	 1522

[32m iter_2[0m
ga 	p: 81.65 	r: 75.55 	f1: 78.48 	 4660 	 5707 	 6168
wo 	p: 92.35 	r: 86.6 	f1: 89.38 	 2934 	 3177 	 3388
ni 	p: 85.08 	r: 79.04 	f1: 81.95 	 1203 	 1414 	 1522
best_thres [[0.36, 0.3, 0.17], [0.33, 0.39, 0.11], [0.33, 0.36, 0.1]]
f [0.8224, 0.8225, 0.8227]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 13 	 [0.39, 0.49, 0.14] 	 lr: 2.5e-05 	 f: 82.75309998430387
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 24 [0m
Train...
loss: tensor(722.2554) lr: 6.25e-06 time: 2046.65
pred_count_train 41644

Test...
loss: tensor(663.2387) lr: 6.25e-06 time: 2130.06
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.17 	r: 74.12 	f1: 77.94 	 4572 	 5564 	 6168
wo 	p: 91.38 	r: 87.57 	f1: 89.43 	 2967 	 3247 	 3388
ni 	p: 85.25 	r: 77.46 	f1: 81.17 	 1179 	 1383 	 1522

[32m iter_1[0m
ga 	p: 82.1 	r: 74.59 	f1: 78.17 	 4601 	 5604 	 6168
wo 	p: 92.96 	r: 86.07 	f1: 89.38 	 2916 	 3137 	 3388
ni 	p: 85.76 	r: 77.53 	f1: 81.44 	 1180 	 1376 	 1522

[32m iter_2[0m
ga 	p: 82.59 	r: 74.38 	f1: 78.27 	 4588 	 5555 	 6168
wo 	p: 92.14 	r: 86.84 	f1: 89.41 	 2942 	 3193 	 3388
ni 	p: 85.62 	r: 78.25 	f1: 81.77 	 1191 	 1391 	 1522
best_thres [[0.38, 0.28, 0.12], [0.35, 0.38, 0.09], [0.37, 0.31, 0.08]]
f [0.8197, 0.8202, 0.8208]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 13 	 [0.39, 0.49, 0.14] 	 lr: 2.5e-05 	 f: 82.75309998430387
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 25 [0m
Train...
loss: tensor(722.2554) lr: 6.25e-06 time: 2133.27
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.11 	r: 76.23 	f1: 78.6 	 4702 	 5797 	 6168
wo 	p: 92.55 	r: 87.28 	f1: 89.84 	 2957 	 3195 	 3388
ni 	p: 86.96 	r: 77.99 	f1: 82.23 	 1187 	 1365 	 1522

[32m iter_1[0m
ga 	p: 82.2 	r: 76.12 	f1: 79.04 	 4695 	 5712 	 6168
wo 	p: 93.39 	r: 86.36 	f1: 89.74 	 2926 	 3133 	 3388
ni 	p: 84.57 	r: 79.24 	f1: 81.82 	 1206 	 1426 	 1522

[32m iter_2[0m
ga 	p: 82.2 	r: 76.07 	f1: 79.02 	 4692 	 5708 	 6168
wo 	p: 91.53 	r: 88.05 	f1: 89.75 	 2983 	 3259 	 3388
ni 	p: 85.4 	r: 78.78 	f1: 81.95 	 1199 	 1404 	 1522
best_thres [[0.4, 0.43, 0.16], [0.41, 0.51, 0.12], [0.41, 0.31, 0.13]]
f [0.8254, 0.8261, 0.8266]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 18 	 [0.39, 0.4, 0.07] 	 lr: 1.25e-05 	 f: 82.86024844720497
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 24 [0m
Train...

[32m iter_0[0m
ga 	p: 82.17 	r: 74.12 	f1: 77.94 	 4572 	 5564 	 6168
wo 	p: 91.38 	r: 87.57 	f1: 89.43 	 2967 	 3247 	 3388
ni 	p: 85.25 	r: 77.46 	f1: 81.17 	 1179 	 1383 	 1522

[32m iter_1[0m
ga 	p: 82.1 	r: 74.59 	f1: 78.17 	 4601 	 5604 	 6168
wo 	p: 92.96 	r: 86.07 	f1: 89.38 	 2916 	 3137 	 3388
ni 	p: 85.76 	r: 77.53 	f1: 81.44 	 1180 	 1376 	 1522

[32m iter_2[0m
ga 	p: 82.59 	r: 74.38 	f1: 78.27 	 4588 	 5555 	 6168
wo 	p: 92.14 	r: 86.84 	f1: 89.41 	 2942 	 3193 	 3388
ni 	p: 85.62 	r: 78.25 	f1: 81.77 	 1191 	 1391 	 1522
best_thres [[0.38, 0.28, 0.12], [0.35, 0.38, 0.09], [0.37, 0.31, 0.08]]
f [0.8197, 0.8202, 0.8208]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 13 	 [0.39, 0.49, 0.14] 	 lr: 2.5e-05 	 f: 82.75309998430387
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 25 [0m
Train...
loss: tensor(676.8455) lr: 6.25e-06 time: 2037.66
pred_count_train 41644

Test...
loss: tensor(616.2632) lr: 6.25e-06 time: 2107.47
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.24 	r: 75.05 	f1: 78.02 	 4629 	 5698 	 6168
wo 	p: 92.25 	r: 86.78 	f1: 89.43 	 2940 	 3187 	 3388
ni 	p: 85.48 	r: 76.22 	f1: 80.58 	 1160 	 1357 	 1522

[32m iter_1[0m
ga 	p: 82.01 	r: 74.87 	f1: 78.28 	 4618 	 5631 	 6168
wo 	p: 92.4 	r: 86.42 	f1: 89.31 	 2928 	 3169 	 3388
ni 	p: 85.47 	r: 76.94 	f1: 80.98 	 1171 	 1370 	 1522

[32m iter_2[0m
ga 	p: 81.88 	r: 75.02 	f1: 78.3 	 4627 	 5651 	 6168
wo 	p: 92.64 	r: 86.19 	f1: 89.3 	 2920 	 3152 	 3388
ni 	p: 85.55 	r: 77.0 	f1: 81.05 	 1172 	 1370 	 1522
best_thres [[0.36, 0.38, 0.18], [0.37, 0.38, 0.13], [0.36, 0.4, 0.12]]
f [0.8189, 0.8197, 0.82]
load model: epoch13
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 13 	 [0.39, 0.49, 0.14] 	 lr: 2.5e-05 	 f: 82.75309998430387
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 26 [0m
Train...
loss: tensor(676.8455) lr: 6.25e-06 time: 2119.0
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.97 	r: 74.95 	f1: 78.3 	 4623 	 5640 	 6168
wo 	p: 91.92 	r: 87.69 	f1: 89.76 	 2971 	 3232 	 3388
ni 	p: 86.67 	r: 76.48 	f1: 81.26 	 1164 	 1343 	 1522

[32m iter_1[0m
ga 	p: 82.35 	r: 75.11 	f1: 78.57 	 4633 	 5626 	 6168
wo 	p: 92.31 	r: 87.13 	f1: 89.64 	 2952 	 3198 	 3388
ni 	p: 83.87 	r: 78.58 	f1: 81.14 	 1196 	 1426 	 1522

[32m iter_2[0m
ga 	p: 82.19 	r: 75.26 	f1: 78.57 	 4642 	 5648 	 6168
wo 	p: 91.81 	r: 87.69 	f1: 89.7 	 2971 	 3236 	 3388
ni 	p: 83.97 	r: 78.45 	f1: 81.11 	 1194 	 1422 	 1522
best_thres [[0.43, 0.35, 0.13], [0.41, 0.4, 0.08], [0.4, 0.33, 0.08]]
f [0.8226, 0.823, 0.8232]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 18 	 [0.39, 0.4, 0.07] 	 lr: 1.25e-05 	 f: 82.86024844720497
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 25 [0m
Train...

[32m iter_0[0m
ga 	p: 81.24 	r: 75.05 	f1: 78.02 	 4629 	 5698 	 6168
wo 	p: 92.25 	r: 86.78 	f1: 89.43 	 2940 	 3187 	 3388
ni 	p: 85.48 	r: 76.22 	f1: 80.58 	 1160 	 1357 	 1522

[32m iter_1[0m
ga 	p: 82.01 	r: 74.87 	f1: 78.28 	 4618 	 5631 	 6168
wo 	p: 92.4 	r: 86.42 	f1: 89.31 	 2928 	 3169 	 3388
ni 	p: 85.47 	r: 76.94 	f1: 80.98 	 1171 	 1370 	 1522

[32m iter_2[0m
ga 	p: 81.88 	r: 75.02 	f1: 78.3 	 4627 	 5651 	 6168
wo 	p: 92.64 	r: 86.19 	f1: 89.3 	 2920 	 3152 	 3388
ni 	p: 85.55 	r: 77.0 	f1: 81.05 	 1172 	 1370 	 1522
best_thres [[0.36, 0.38, 0.18], [0.37, 0.38, 0.13], [0.36, 0.4, 0.12]]
f [0.8189, 0.8197, 0.82]
load model: epoch13
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 13 	 [0.39, 0.49, 0.14] 	 lr: 2.5e-05 	 f: 82.75309998430387
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 26 [0m
Train...
loss: tensor(818.5778) lr: 5e-06 time: 2035.29
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.73 	r: 74.71 	f1: 78.51 	 4608 	 5570 	 6168
wo 	p: 91.91 	r: 87.87 	f1: 89.84 	 2977 	 3239 	 3388
ni 	p: 86.6 	r: 77.27 	f1: 81.67 	 1176 	 1358 	 1522

[32m iter_1[0m
ga 	p: 84.12 	r: 73.95 	f1: 78.71 	 4561 	 5422 	 6168
wo 	p: 92.58 	r: 86.87 	f1: 89.63 	 2943 	 3179 	 3388
ni 	p: 84.88 	r: 78.91 	f1: 81.78 	 1201 	 1415 	 1522

[32m iter_2[0m
ga 	p: 82.5 	r: 75.34 	f1: 78.76 	 4647 	 5633 	 6168
wo 	p: 92.66 	r: 86.81 	f1: 89.64 	 2941 	 3174 	 3388
ni 	p: 83.74 	r: 80.55 	f1: 82.12 	 1226 	 1464 	 1522
best_thres [[0.4, 0.33, 0.16], [0.46, 0.39, 0.11], [0.37, 0.39, 0.09]]
f [0.8248, 0.8251, 0.8253]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 13 	 [0.39, 0.49, 0.14] 	 lr: 2.5e-05 	 f: 82.75309998430387
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 27 [0m
Train...
loss: tensor(573.7989) lr: 6.25e-06 time: 2092.25
pred_count_train 41644

Test...
loss: tensor(818.5778) lr: 5e-06 time: 2107.43
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.29 	r: 76.23 	f1: 78.21 	 4702 	 5856 	 6168
wo 	p: 92.12 	r: 87.34 	f1: 89.67 	 2959 	 3212 	 3388
ni 	p: 86.54 	r: 75.62 	f1: 80.72 	 1151 	 1330 	 1522

[32m iter_1[0m
ga 	p: 81.68 	r: 75.78 	f1: 78.62 	 4674 	 5722 	 6168
wo 	p: 92.21 	r: 87.34 	f1: 89.71 	 2959 	 3209 	 3388
ni 	p: 85.0 	r: 77.07 	f1: 80.84 	 1173 	 1380 	 1522

[32m iter_2[0m
ga 	p: 81.75 	r: 75.68 	f1: 78.6 	 4668 	 5710 	 6168
wo 	p: 92.04 	r: 87.4 	f1: 89.66 	 2961 	 3217 	 3388
ni 	p: 85.77 	r: 76.81 	f1: 81.04 	 1169 	 1363 	 1522
best_thres [[0.37, 0.38, 0.2], [0.42, 0.39, 0.15], [0.42, 0.36, 0.16]]
f [0.8206, 0.822, 0.8225]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 18 	 [0.39, 0.4, 0.07] 	 lr: 1.25e-05 	 f: 82.86024844720497
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 26 [0m
Train...
loss: tensor(773.2543) lr: 5e-06 time: 2039.69
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.73 	r: 74.71 	f1: 78.51 	 4608 	 5570 	 6168
wo 	p: 91.91 	r: 87.87 	f1: 89.84 	 2977 	 3239 	 3388
ni 	p: 86.6 	r: 77.27 	f1: 81.67 	 1176 	 1358 	 1522

[32m iter_1[0m
ga 	p: 84.12 	r: 73.95 	f1: 78.71 	 4561 	 5422 	 6168
wo 	p: 92.58 	r: 86.87 	f1: 89.63 	 2943 	 3179 	 3388
ni 	p: 84.88 	r: 78.91 	f1: 81.78 	 1201 	 1415 	 1522

[32m iter_2[0m
ga 	p: 82.5 	r: 75.34 	f1: 78.76 	 4647 	 5633 	 6168
wo 	p: 92.66 	r: 86.81 	f1: 89.64 	 2941 	 3174 	 3388
ni 	p: 83.74 	r: 80.55 	f1: 82.12 	 1226 	 1464 	 1522
best_thres [[0.4, 0.33, 0.16], [0.46, 0.39, 0.11], [0.37, 0.39, 0.09]]
f [0.8248, 0.8251, 0.8253]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 13 	 [0.39, 0.49, 0.14] 	 lr: 2.5e-05 	 f: 82.75309998430387
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 27 [0m
Train...

[32m iter_0[0m
ga 	p: 82.29 	r: 74.9 	f1: 78.42 	 4620 	 5614 	 6168
wo 	p: 92.2 	r: 87.28 	f1: 89.67 	 2957 	 3207 	 3388
ni 	p: 85.63 	r: 77.53 	f1: 81.38 	 1180 	 1378 	 1522

[32m iter_1[0m
ga 	p: 82.24 	r: 75.44 	f1: 78.69 	 4653 	 5658 	 6168
wo 	p: 93.06 	r: 86.25 	f1: 89.52 	 2922 	 3140 	 3388
ni 	p: 83.11 	r: 80.49 	f1: 81.78 	 1225 	 1474 	 1522

[32m iter_2[0m
ga 	p: 82.31 	r: 75.29 	f1: 78.65 	 4644 	 5642 	 6168
wo 	p: 92.9 	r: 86.51 	f1: 89.59 	 2931 	 3155 	 3388
ni 	p: 83.17 	r: 80.22 	f1: 81.67 	 1221 	 1468 	 1522
best_thres [[0.4, 0.35, 0.15], [0.37, 0.43, 0.08], [0.38, 0.4, 0.08]]
f [0.8231, 0.8238, 0.8239]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 13 	 [0.39, 0.49, 0.14] 	 lr: 2.5e-05 	 f: 82.75309998430387
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 28 [0m
Train...
loss: tensor(536.3959) lr: 6.25e-06 time: 2073.14
pred_count_train 41644

Test...
loss: tensor(773.2543) lr: 5e-06 time: 2088.44
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.32 	r: 74.19 	f1: 78.04 	 4576 	 5559 	 6168
wo 	p: 91.72 	r: 87.63 	f1: 89.63 	 2969 	 3237 	 3388
ni 	p: 84.91 	r: 74.7 	f1: 79.48 	 1137 	 1339 	 1522

[32m iter_1[0m
ga 	p: 80.45 	r: 76.39 	f1: 78.37 	 4712 	 5857 	 6168
wo 	p: 92.28 	r: 87.13 	f1: 89.63 	 2952 	 3199 	 3388
ni 	p: 81.7 	r: 77.46 	f1: 79.53 	 1179 	 1443 	 1522

[32m iter_2[0m
ga 	p: 80.66 	r: 76.33 	f1: 78.43 	 4708 	 5837 	 6168
wo 	p: 91.99 	r: 87.46 	f1: 89.67 	 2963 	 3221 	 3388
ni 	p: 81.43 	r: 78.38 	f1: 79.88 	 1193 	 1465 	 1522
best_thres [[0.47, 0.34, 0.15], [0.32, 0.4, 0.09], [0.32, 0.35, 0.08]]
f [0.8186, 0.8191, 0.8196]
load model: epoch18
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 18 	 [0.39, 0.4, 0.07] 	 lr: 1.25e-05 	 f: 82.86024844720497
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 27 [0m
Train...
loss: tensor(735.5611) lr: 5e-06 time: 2042.99
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.29 	r: 74.9 	f1: 78.42 	 4620 	 5614 	 6168
wo 	p: 92.2 	r: 87.28 	f1: 89.67 	 2957 	 3207 	 3388
ni 	p: 85.63 	r: 77.53 	f1: 81.38 	 1180 	 1378 	 1522

[32m iter_1[0m
ga 	p: 82.24 	r: 75.44 	f1: 78.69 	 4653 	 5658 	 6168
wo 	p: 93.06 	r: 86.25 	f1: 89.52 	 2922 	 3140 	 3388
ni 	p: 83.11 	r: 80.49 	f1: 81.78 	 1225 	 1474 	 1522

[32m iter_2[0m
ga 	p: 82.31 	r: 75.29 	f1: 78.65 	 4644 	 5642 	 6168
wo 	p: 92.9 	r: 86.51 	f1: 89.59 	 2931 	 3155 	 3388
ni 	p: 83.17 	r: 80.22 	f1: 81.67 	 1221 	 1468 	 1522
best_thres [[0.4, 0.35, 0.15], [0.37, 0.43, 0.08], [0.38, 0.4, 0.08]]
f [0.8231, 0.8238, 0.8239]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 13 	 [0.39, 0.49, 0.14] 	 lr: 2.5e-05 	 f: 82.75309998430387
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 28 [0m
Train...

[32m iter_0[0m
ga 	p: 81.65 	r: 75.08 	f1: 78.23 	 4631 	 5672 	 6168
wo 	p: 91.84 	r: 87.37 	f1: 89.55 	 2960 	 3223 	 3388
ni 	p: 87.42 	r: 76.22 	f1: 81.43 	 1160 	 1327 	 1522

[32m iter_1[0m
ga 	p: 83.09 	r: 74.16 	f1: 78.37 	 4574 	 5505 	 6168
wo 	p: 93.01 	r: 86.04 	f1: 89.39 	 2915 	 3134 	 3388
ni 	p: 84.31 	r: 79.43 	f1: 81.8 	 1209 	 1434 	 1522

[32m iter_2[0m
ga 	p: 82.07 	r: 74.95 	f1: 78.35 	 4623 	 5633 	 6168
wo 	p: 92.93 	r: 86.1 	f1: 89.38 	 2917 	 3139 	 3388
ni 	p: 83.09 	r: 80.68 	f1: 81.87 	 1228 	 1478 	 1522
best_thres [[0.38, 0.33, 0.19], [0.44, 0.44, 0.1], [0.38, 0.43, 0.08]]
f [0.8217, 0.8221, 0.8221]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 13 	 [0.39, 0.49, 0.14] 	 lr: 2.5e-05 	 f: 82.75309998430387
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 29 [0m
Train...
loss: tensor(659.4768) lr: 5e-06 time: 2047.51
pred_count_train 41644

Test...
loss: tensor(735.5611) lr: 5e-06 time: 2053.78
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.23 	r: 75.45 	f1: 78.69 	 4654 	 5660 	 6168
wo 	p: 93.05 	r: 86.89 	f1: 89.87 	 2944 	 3164 	 3388
ni 	p: 84.79 	r: 79.11 	f1: 81.85 	 1204 	 1420 	 1522

[32m iter_1[0m
ga 	p: 82.88 	r: 75.44 	f1: 78.98 	 4653 	 5614 	 6168
wo 	p: 93.46 	r: 86.51 	f1: 89.85 	 2931 	 3136 	 3388
ni 	p: 83.15 	r: 80.09 	f1: 81.59 	 1219 	 1466 	 1522

[32m iter_2[0m
ga 	p: 82.99 	r: 75.31 	f1: 78.96 	 4645 	 5597 	 6168
wo 	p: 93.63 	r: 86.3 	f1: 89.82 	 2924 	 3123 	 3388
ni 	p: 83.74 	r: 79.5 	f1: 81.56 	 1210 	 1445 	 1522
best_thres [[0.45, 0.47, 0.13], [0.45, 0.48, 0.1], [0.46, 0.5, 0.11]]
f [0.8256, 0.8262, 0.8263]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 18 	 [0.39, 0.4, 0.07] 	 lr: 1.25e-05 	 f: 82.86024844720497
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 28 [0m
Train...
loss: tensor(697.1495) lr: 5e-06 time: 1985.04
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.65 	r: 75.08 	f1: 78.23 	 4631 	 5672 	 6168
wo 	p: 91.84 	r: 87.37 	f1: 89.55 	 2960 	 3223 	 3388
ni 	p: 87.42 	r: 76.22 	f1: 81.43 	 1160 	 1327 	 1522

[32m iter_1[0m
ga 	p: 83.09 	r: 74.16 	f1: 78.37 	 4574 	 5505 	 6168
wo 	p: 93.01 	r: 86.04 	f1: 89.39 	 2915 	 3134 	 3388
ni 	p: 84.31 	r: 79.43 	f1: 81.8 	 1209 	 1434 	 1522

[32m iter_2[0m
ga 	p: 82.07 	r: 74.95 	f1: 78.35 	 4623 	 5633 	 6168
wo 	p: 92.93 	r: 86.1 	f1: 89.38 	 2917 	 3139 	 3388
ni 	p: 83.09 	r: 80.68 	f1: 81.87 	 1228 	 1478 	 1522
best_thres [[0.38, 0.33, 0.19], [0.44, 0.44, 0.1], [0.38, 0.43, 0.08]]
f [0.8217, 0.8221, 0.8221]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	current best epoch 13 	 [0.39, 0.49, 0.14] 	 lr: 2.5e-05 	 f: 82.75309998430387
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse[0m [33m epoch 29 [0m
Train...

[32m iter_0[0m
ga 	p: 80.15 	r: 75.92 	f1: 77.98 	 4683 	 5843 	 6168
wo 	p: 92.08 	r: 86.87 	f1: 89.4 	 2943 	 3196 	 3388
ni 	p: 86.69 	r: 75.3 	f1: 80.59 	 1146 	 1322 	 1522

[32m iter_1[0m
ga 	p: 81.91 	r: 74.94 	f1: 78.27 	 4622 	 5643 	 6168
wo 	p: 93.05 	r: 85.8 	f1: 89.28 	 2907 	 3124 	 3388
ni 	p: 82.24 	r: 79.43 	f1: 80.82 	 1209 	 1470 	 1522

[32m iter_2[0m
ga 	p: 82.66 	r: 74.34 	f1: 78.28 	 4585 	 5547 	 6168
wo 	p: 92.97 	r: 85.83 	f1: 89.26 	 2908 	 3128 	 3388
ni 	p: 84.35 	r: 77.92 	f1: 81.01 	 1186 	 1406 	 1522
best_thres [[0.29, 0.34, 0.16], [0.34, 0.42, 0.07], [0.38, 0.41, 0.08]]
f [0.8183, 0.8191, 0.8195]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	best in epoch 13 	 [0.39, 0.49, 0.14] 	 lr: 2.5e-05 	 f: 82.75309998430387
2018 {'GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse': {'best_epoch': 13, 'best_thres': [0.57, 0.47, 0.13], 'best_lr': 2.5e-05, 'best_performance': 82.557975224129}}
2020 {'GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse': {'best_epoch': 13, 'best_thres': [0.39, 0.49, 0.14], 'best_lr': 2.5e-05, 'best_performance': 82.75309998430387}}
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/test.json
loss: tensor(622.0364) lr: 5e-06 time: 1948.54
pred_count_train 41644

Test...
6 3 2
vocab size:  29396
prediction mode: model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse test [0.5, 0.5, 0.5]
{'ga': {'nn': 6231, 'np': 2739, 'pn': 4627, 'pp': 14316},
 'ni': {'nn': 22022, 'np': 1616, 'pn': 885, 'pp': 1863},
 'o': {'nn': 15529, 'np': 766, 'pn': 1267, 'pp': 9031}}
ga:	prec: 83.94, recall: 75.57, f1: 79.54
o:	prec: 92.18, recall: 87.7, f1: 89.88
ni:	prec: 53.55, recall: 67.79, f1: 59.84
all:	prec: 83.12, recall: 78.81, f1: 80.91
pred_count_test 26367
loss: tensor(697.1495) lr: 5e-06 time: 1964.82
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.57 	r: 75.78 	f1: 78.57 	 4674 	 5730 	 6168
wo 	p: 93.0 	r: 86.69 	f1: 89.73 	 2937 	 3158 	 3388
ni 	p: 85.66 	r: 78.12 	f1: 81.72 	 1189 	 1388 	 1522

[32m iter_1[0m
ga 	p: 81.29 	r: 76.59 	f1: 78.87 	 4724 	 5811 	 6168
wo 	p: 93.29 	r: 86.57 	f1: 89.8 	 2933 	 3144 	 3388
ni 	p: 85.6 	r: 78.12 	f1: 81.69 	 1189 	 1389 	 1522

[32m iter_2[0m
ga 	p: 81.4 	r: 76.56 	f1: 78.9 	 4722 	 5801 	 6168
wo 	p: 93.43 	r: 86.42 	f1: 89.79 	 2928 	 3134 	 3388
ni 	p: 86.24 	r: 77.4 	f1: 81.58 	 1178 	 1366 	 1522
best_thres [[0.45, 0.51, 0.15], [0.39, 0.52, 0.14], [0.39, 0.53, 0.15]]
f [0.8242, 0.825, 0.8253]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 18 	 [0.39, 0.4, 0.07] 	 lr: 1.25e-05 	 f: 82.86024844720497
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 29 [0m
Train...

[32m iter_0[0m
ga 	p: 80.15 	r: 75.92 	f1: 77.98 	 4683 	 5843 	 6168
wo 	p: 92.08 	r: 86.87 	f1: 89.4 	 2943 	 3196 	 3388
ni 	p: 86.69 	r: 75.3 	f1: 80.59 	 1146 	 1322 	 1522

[32m iter_1[0m
ga 	p: 81.91 	r: 74.94 	f1: 78.27 	 4622 	 5643 	 6168
wo 	p: 93.05 	r: 85.8 	f1: 89.28 	 2907 	 3124 	 3388
ni 	p: 82.24 	r: 79.43 	f1: 80.82 	 1209 	 1470 	 1522

[32m iter_2[0m
ga 	p: 82.66 	r: 74.34 	f1: 78.28 	 4585 	 5547 	 6168
wo 	p: 92.97 	r: 85.83 	f1: 89.26 	 2908 	 3128 	 3388
ni 	p: 84.35 	r: 77.92 	f1: 81.01 	 1186 	 1406 	 1522
best_thres [[0.29, 0.34, 0.16], [0.34, 0.42, 0.07], [0.38, 0.41, 0.08]]
f [0.8183, 0.8191, 0.8195]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse 	best in epoch 13 	 [0.39, 0.49, 0.14] 	 lr: 2.5e-05 	 f: 82.75309998430387
2018 {'GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2018_preFalse': {'best_epoch': 13, 'best_thres': [0.57, 0.47, 0.13], 'best_lr': 2.5e-05, 'best_performance': 82.557975224129}}
2020 {'GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.5_it3_rs2020_preFalse': {'best_epoch': 13, 'best_thres': [0.39, 0.49, 0.14], 'best_lr': 2.5e-05, 'best_performance': 82.75309998430387}}
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/test.json
loss: tensor(586.6407) lr: 5e-06 time: 1610.89
pred_count_train 41644

Test...
6 3 2
vocab size:  29396
prediction mode: model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse test [0.5, 0.5, 0.5]
{'ga': {'nn': 6231, 'np': 2739, 'pn': 4627, 'pp': 14316},
 'ni': {'nn': 22022, 'np': 1616, 'pn': 885, 'pp': 1863},
 'o': {'nn': 15529, 'np': 766, 'pn': 1267, 'pp': 9031}}
ga:	prec: 83.94, recall: 75.57, f1: 79.54
o:	prec: 92.18, recall: 87.7, f1: 89.88
ni:	prec: 53.55, recall: 67.79, f1: 59.84
all:	prec: 83.12, recall: 78.81, f1: 80.91
pred_count_test 26367

[32m iter_0[0m
ga 	p: 81.43 	r: 75.5 	f1: 78.35 	 4657 	 5719 	 6168
wo 	p: 92.16 	r: 87.43 	f1: 89.73 	 2962 	 3214 	 3388
ni 	p: 85.21 	r: 76.48 	f1: 80.61 	 1164 	 1366 	 1522

[32m iter_1[0m
ga 	p: 80.79 	r: 76.77 	f1: 78.73 	 4735 	 5861 	 6168
wo 	p: 92.04 	r: 87.37 	f1: 89.64 	 2960 	 3216 	 3388
ni 	p: 84.58 	r: 77.46 	f1: 80.86 	 1179 	 1394 	 1522

[32m iter_2[0m
ga 	p: 80.87 	r: 76.77 	f1: 78.77 	 4735 	 5855 	 6168
wo 	p: 92.31 	r: 87.16 	f1: 89.66 	 2953 	 3199 	 3388
ni 	p: 85.37 	r: 76.68 	f1: 80.79 	 1167 	 1367 	 1522
best_thres [[0.42, 0.41, 0.14], [0.33, 0.39, 0.11], [0.33, 0.42, 0.12]]
f [0.8217, 0.8227, 0.823]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	current best epoch 18 	 [0.39, 0.4, 0.07] 	 lr: 1.25e-05 	 f: 82.86024844720497
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse[0m [33m epoch 30 [0m
Train...
loss: tensor(553.1768) lr: 5e-06 time: 1417.01
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.01 	r: 76.46 	f1: 78.2 	 4716 	 5894 	 6168
wo 	p: 92.41 	r: 86.98 	f1: 89.62 	 2947 	 3189 	 3388
ni 	p: 86.76 	r: 74.51 	f1: 80.17 	 1134 	 1307 	 1522

[32m iter_1[0m
ga 	p: 82.15 	r: 75.23 	f1: 78.54 	 4640 	 5648 	 6168
wo 	p: 93.19 	r: 86.45 	f1: 89.7 	 2929 	 3143 	 3388
ni 	p: 84.93 	r: 75.89 	f1: 80.15 	 1155 	 1360 	 1522

[32m iter_2[0m
ga 	p: 82.37 	r: 75.13 	f1: 78.58 	 4634 	 5626 	 6168
wo 	p: 93.66 	r: 85.92 	f1: 89.62 	 2911 	 3108 	 3388
ni 	p: 85.11 	r: 75.89 	f1: 80.24 	 1155 	 1357 	 1522
best_thres [[0.37, 0.47, 0.19], [0.46, 0.54, 0.14], [0.47, 0.6, 0.14]]
f [0.8195, 0.8207, 0.8211]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse 	best in epoch 18 	 [0.39, 0.4, 0.07] 	 lr: 1.25e-05 	 f: 82.86024844720497
2018 {'GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2018_preFalse': {'best_epoch': 18, 'best_thres': [0.44, 0.36, 0.16], 'best_lr': 1.25e-05, 'best_performance': 83.05538591628694}}
2020 {'GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2020_preFalse': {'best_epoch': 18, 'best_thres': [0.39, 0.4, 0.07], 'best_lr': 1.25e-05, 'best_performance': 82.86024844720497}}
model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse.h5
e2e-stack model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse
gpu: 0
############ /home/miyawaki_shumpei/PAS/NTC_Matsu_converted/test.json
6 3 2
vocab size:  29396
prediction mode: model-e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1030_th0.8_it3_rs2016_preFalse test [0.5, 0.5, 0.5]
{'ga': {'nn': 6306, 'np': 2605, 'pn': 4758, 'pp': 14185},
 'ni': {'nn': 22068, 'np': 1569, 'pn': 915, 'pp': 1833},
 'o': {'nn': 15544, 'np': 746, 'pn': 1294, 'pp': 9004}}
ga:	prec: 84.48, recall: 74.88, f1: 79.39
o:	prec: 92.35, recall: 87.43, f1: 89.82
ni:	prec: 53.88, recall: 66.7, f1: 59.61
all:	prec: 83.57, recall: 78.22, f1: 80.81
pred_count_test 26367
# Make directory: result/log
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 1 [0m
Train...
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(-39623.0156) lr: 0.0002 time: 2414.69
pred_count_train 41644

Test...
loss: tensor(-39619.6094) lr: 0.0001 time: 2430.23
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 4.4 	r: 6.1 	f1: 5.11 	 376 	 8546 	 6168
wo 	p: 0.7 	r: 1.77 	f1: 1.01 	 60 	 8546 	 3388
ni 	p: 0.62 	r: 3.48 	f1: 1.05 	 53 	 8546 	 1522

[32m iter_1[0m
ga 	p: 4.41 	r: 6.11 	f1: 5.12 	 377 	 8546 	 6168
wo 	p: 0.78 	r: 1.98 	f1: 1.12 	 67 	 8546 	 3388
ni 	p: 0.41 	r: 2.3 	f1: 0.7 	 35 	 8546 	 1522

[32m iter_2[0m
ga 	p: 4.4 	r: 6.1 	f1: 5.11 	 376 	 8546 	 6168
wo 	p: 0.83 	r: 2.1 	f1: 1.19 	 71 	 8546 	 3388
ni 	p: 0.62 	r: 3.48 	f1: 1.05 	 53 	 8546 	 1522
best_thres [[0.1, 0.2, 0.0], [0.1, 0.2, 0.0], [0.1, 0.2, 0.0]]
f [0.0266, 0.0264, 0.0267]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 1 	 [0.1, 0.2, 0.0] 	 lr: 0.0002 	 f: 2.6655045938192248
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 2 [0m
Train...

[32m iter_0[0m
ga 	p: 4.28 	r: 5.93 	f1: 4.97 	 366 	 8546 	 6168
wo 	p: 0.74 	r: 1.86 	f1: 1.06 	 63 	 8546 	 3388
ni 	p: 0.62 	r: 3.48 	f1: 1.05 	 53 	 8546 	 1522

[32m iter_1[0m
ga 	p: 0.36 	r: 0.5 	f1: 0.42 	 31 	 8546 	 6168
wo 	p: 0.0 	r: 0.0 	f1: 0.0 	 0 	 8546 	 3388
ni 	p: 0.04 	r: 0.2 	f1: 0.06 	 3 	 8546 	 1522

[32m iter_2[0m
ga 	p: 3.25 	r: 4.51 	f1: 3.78 	 278 	 8546 	 6168
wo 	p: 0.14 	r: 0.35 	f1: 0.2 	 12 	 8546 	 3388
ni 	p: 0.57 	r: 3.22 	f1: 0.97 	 49 	 8546 	 1522
best_thres [[0.1, 0.2, 0.0], [0.1, 0.2, 0.0], [0.1, 0.2, 0.0]]
f [0.0263, 0.0141, 0.0155]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 1 	 [0.1, 0.2, 0.0] 	 lr: 0.0001 	 f: 1.5524566946290448
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 2 [0m
Train...
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth4_adam_lr0.0001_du0.1_dh0.0_True_size1_sub0_th0.2_it2_rs2018_preFalse[0m [33m epoch 1 [0m
Train...
> [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/model_edit.py[0m(55)[0;36mforward[0;34m()[0m
[0;32m     54 [0;31m            [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 55 [0;31m            [0mres[0m [0;34m=[0m [0;34m[[0m[0mF[0m[0;34m.[0m[0msoftmax[0m[0;34m([0m[0mself[0m[0;34m.[0m[0moutput_layer[0m[0;34m([0m[0mout[0m[0;34m)[0m[0;34m)[0m [0;32mfor[0m [0mout[0m [0;32min[0m [0moutputs[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     56 [0;31m            [0mscores[0m[0;34m.[0m[0mappend[0m[0;34m([0m[0mres[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> tensor([[[ 0.0792,  0.2574, -0.4184,  ...,  0.7381, -0.1708, -0.5185],
         [ 0.3859,  0.3401, -0.4635,  ...,  0.3547,  0.0142, -0.4271],
         [ 0.4018,  0.1917, -0.6679,  ...,  0.3003,  0.0641, -0.2711],
         ...,
         [ 0.2916,  0.1435, -0.4648,  ...,  0.5352, -0.2761,  0.1690],
         [ 0.3343,  0.4600, -0.8805,  ...,  0.5632, -0.2833,  0.4687],
         [ 0.2303,  0.1702, -0.8940,  ...,  0.3523,  0.0826,  0.1873]],

        [[ 0.0792,  0.2575, -0.4183,  ...,  0.7382, -0.1707, -0.5184],
         [ 0.3860,  0.3402, -0.4634,  ...,  0.3547,  0.0143, -0.4270],
         [ 0.4019,  0.1919, -0.6678,  ...,  0.3004,  0.0644, -0.2708],
         ...,
         [ 0.2917,  0.1434, -0.4648,  ...,  0.5351, -0.2761,  0.1689],
         [ 0.3344,  0.4600, -0.8805,  ...,  0.5631, -0.2833,  0.4687],
         [ 0.2304,  0.1702, -0.8940,  ...,  0.3523,  0.0826,  0.1873]],

        [[ 0.0793,  0.2575, -0.4183,  ...,  0.7382, -0.1707, -0.5184],
         [ 0.3860,  0.3402, -0.4634,  ...,  0.3548,  0.0143, -0.4270],
         [ 0.4019,  0.1919, -0.6678,  ...,  0.3004,  0.0644, -0.2708],
         ...,
         [ 0.2824,  0.1391, -0.4662,  ...,  0.5385, -0.2785,  0.1631],
         [ 0.3467,  0.4472, -0.8831,  ...,  0.5409, -0.2758,  0.4229],
         [ 0.2317,  0.1604, -0.8956,  ...,  0.3426,  0.0876,  0.1696]]],
       device='cuda:0', grad_fn=<TransposeBackward0>)
ipdb> *** NameError: name 'res' is not defined
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/model_edit.py[0m(56)[0;36mforward[0;34m()[0m
[0;32m     55 [0;31m            [0mres[0m [0;34m=[0m [0;34m[[0m[0mF[0m[0;34m.[0m[0msoftmax[0m[0;34m([0m[0mself[0m[0;34m.[0m[0moutput_layer[0m[0;34m([0m[0mout[0m[0;34m)[0m[0;34m)[0m [0;32mfor[0m [0mout[0m [0;32min[0m [0moutputs[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 56 [0;31m            [0mscores[0m[0;34m.[0m[0mappend[0m[0;34m([0m[0mres[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     57 [0;31m            [0;31m#temp = self.filtering(res, temp, thres)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> *** NameError: name 'rees' is not defined
ipdb> [tensor([[0.2538, 0.2601, 0.2798, 0.2064],
        [0.2729, 0.2423, 0.3112, 0.1736],
        [0.2691, 0.2402, 0.3180, 0.1727],
        [0.2799, 0.1919, 0.3583, 0.1699],
        [0.2716, 0.2076, 0.3692, 0.1516],
        [0.2945, 0.2103, 0.3635, 0.1317],
        [0.3088, 0.1990, 0.3586, 0.1336],
        [0.2773, 0.2079, 0.3799, 0.1349],
        [0.3099, 0.1641, 0.3854, 0.1406],
        [0.2939, 0.1969, 0.3737, 0.1355],
        [0.2572, 0.2268, 0.3982, 0.1179],
        [0.3845, 0.2084, 0.2689, 0.1383],
        [0.3317, 0.2034, 0.3055, 0.1594],
        [0.2687, 0.2536, 0.3646, 0.1131],
        [0.2318, 0.2406, 0.3939, 0.1337],
        [0.2108, 0.2193, 0.3956, 0.1743],
        [0.2167, 0.2015, 0.3842, 0.1977],
        [0.2577, 0.1902, 0.3525, 0.1996],
        [0.1994, 0.2689, 0.3409, 0.1908],
        [0.2417, 0.2327, 0.3401, 0.1855],
        [0.2769, 0.2027, 0.3351, 0.1853],
        [0.2728, 0.1994, 0.3602, 0.1676],
        [0.3047, 0.1375, 0.3792, 0.1786],
        [0.2631, 0.1545, 0.4289, 0.1535],
        [0.2713, 0.1742, 0.4270, 0.1275],
        [0.2766, 0.1749, 0.4251, 0.1233],
        [0.2801, 0.1973, 0.3663, 0.1563],
        [0.2686, 0.2019, 0.3708, 0.1588]], device='cuda:0',
       grad_fn=<SoftmaxBackward>), tensor([[0.2538, 0.2601, 0.2798, 0.2064],
        [0.2729, 0.2423, 0.3112, 0.1736],
        [0.2691, 0.2402, 0.3180, 0.1727],
        [0.2800, 0.1919, 0.3583, 0.1699],
        [0.2717, 0.2076, 0.3692, 0.1516],
        [0.2946, 0.2104, 0.3634, 0.1316],
        [0.3090, 0.1990, 0.3584, 0.1336],
        [0.2776, 0.2080, 0.3796, 0.1348],
        [0.3104, 0.1641, 0.3851, 0.1404],
        [0.2946, 0.1968, 0.3733, 0.1353],
        [0.2566, 0.2263, 0.4001, 0.1169],
        [0.3853, 0.2075, 0.2700, 0.1372],
        [0.3324, 0.2026, 0.3067, 0.1583],
        [0.2688, 0.2528, 0.3658, 0.1126],
        [0.2318, 0.2398, 0.3952, 0.1333],
        [0.2150, 0.2181, 0.3930, 0.1739],
        [0.2190, 0.2013, 0.3820, 0.1977],
        [0.2589, 0.1901, 0.3512, 0.1998],
        [0.2000, 0.2688, 0.3400, 0.1912],
        [0.2421, 0.2327, 0.3395, 0.1857],
        [0.2771, 0.2028, 0.3346, 0.1856],
        [0.2728, 0.1995, 0.3599, 0.1678],
        [0.3046, 0.1375, 0.3790, 0.1788],
        [0.2630, 0.1545, 0.4288, 0.1536],
        [0.2713, 0.1743, 0.4269, 0.1275],
        [0.2766, 0.1750, 0.4251, 0.1234],
        [0.2801, 0.1973, 0.3663, 0.1563],
        [0.2685, 0.2019, 0.3707, 0.1588]], device='cuda:0',
       grad_fn=<SoftmaxBackward>), tensor([[0.2538, 0.2601, 0.2798, 0.2064],
        [0.2729, 0.2423, 0.3112, 0.1736],
        [0.2691, 0.2402, 0.3180, 0.1727],
        [0.2800, 0.1919, 0.3583, 0.1699],
        [0.2717, 0.2076, 0.3691, 0.1516],
        [0.2947, 0.2104, 0.3633, 0.1316],
        [0.3090, 0.1990, 0.3584, 0.1336],
        [0.2777, 0.2080, 0.3795, 0.1348],
        [0.3105, 0.1641, 0.3850, 0.1404],
        [0.2947, 0.1968, 0.3732, 0.1353],
        [0.2568, 0.2264, 0.3999, 0.1169],
        [0.3855, 0.2076, 0.2697, 0.1372],
        [0.3326, 0.2027, 0.3063, 0.1583],
        [0.2691, 0.2531, 0.3652, 0.1126],
        [0.2321, 0.2402, 0.3944, 0.1333],
        [0.2110, 0.2191, 0.3960, 0.1740],
        [0.2168, 0.2013, 0.3845, 0.1975],
        [0.2577, 0.1901, 0.3527, 0.1995],
        [0.1994, 0.2688, 0.3410, 0.1908],
        [0.2417, 0.2327, 0.3402, 0.1854],
        [0.2769, 0.2027, 0.3352, 0.1853],
        [0.2727, 0.1993, 0.3604, 0.1676],
        [0.3045, 0.1374, 0.3794, 0.1787],
        [0.2628, 0.1544, 0.4291, 0.1537],
        [0.2710, 0.1741, 0.4273, 0.1277],
        [0.2761, 0.1747, 0.4254, 0.1237],
        [0.2806, 0.1980, 0.3625, 0.1589],
        [0.2680, 0.2022, 0.3694, 0.1604]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)]
ipdb> tensor([[[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]]], device='cuda:0')
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/model_edit.py[0m(58)[0;36mforward[0;34m()[0m
[0;32m     57 [0;31m            [0;31m#temp = self.filtering(res, temp, thres)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 58 [0;31m            [0mtemp[0m [0;34m=[0m [0mtorch[0m[0;34m.[0m[0mstack[0m[0;34m([0m[0;34m[[0m[0mr[0m [0;34m*[0m [0;34m([0m[0mr[0m [0;34m>[0m [0mthres[0m[0;34m)[0m[0;34m.[0m[0mfloat[0m[0;34m([0m[0;34m)[0m [0;32mfor[0m [0mr[0m [0;32min[0m [0mres[0m[0;34m][0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     59 [0;31m            [0mtemp[0m[0;34m[[0m[0;34m:[0m[0;34m,[0m[0;34m:[0m[0;34m,[0m[0;34m-[0m[0;36m1[0m[0;34m][0m [0;34m=[0m [0;36m0[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> tensor([[[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]]], device='cuda:0')
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/model_edit.py[0m(59)[0;36mforward[0;34m()[0m
[0;32m     58 [0;31m            [0mtemp[0m [0;34m=[0m [0mtorch[0m[0;34m.[0m[0mstack[0m[0;34m([0m[0;34m[[0m[0mr[0m [0;34m*[0m [0;34m([0m[0mr[0m [0;34m>[0m [0mthres[0m[0;34m)[0m[0;34m.[0m[0mfloat[0m[0;34m([0m[0;34m)[0m [0;32mfor[0m [0mr[0m [0;32min[0m [0mres[0m[0;34m][0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 59 [0;31m            [0mtemp[0m[0;34m[[0m[0;34m:[0m[0;34m,[0m[0;34m:[0m[0;34m,[0m[0;34m-[0m[0;36m1[0m[0;34m][0m [0;34m=[0m [0;36m0[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     60 [0;31m        [0;32mreturn[0m [0mscores[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> tensor([[[0.2538, 0.2601, 0.2798, 0.2064],
         [0.2729, 0.2423, 0.3112, 0.0000],
         [0.2691, 0.2402, 0.3180, 0.0000],
         [0.2799, 0.0000, 0.3583, 0.0000],
         [0.2716, 0.2076, 0.3692, 0.0000],
         [0.2945, 0.2103, 0.3635, 0.0000],
         [0.3088, 0.0000, 0.3586, 0.0000],
         [0.2773, 0.2079, 0.3799, 0.0000],
         [0.3099, 0.0000, 0.3854, 0.0000],
         [0.2939, 0.0000, 0.3737, 0.0000],
         [0.2572, 0.2268, 0.3982, 0.0000],
         [0.3845, 0.2084, 0.2689, 0.0000],
         [0.3317, 0.2034, 0.3055, 0.0000],
         [0.2687, 0.2536, 0.3646, 0.0000],
         [0.2318, 0.2406, 0.3939, 0.0000],
         [0.2108, 0.2193, 0.3956, 0.0000],
         [0.2167, 0.2015, 0.3842, 0.0000],
         [0.2577, 0.0000, 0.3525, 0.0000],
         [0.0000, 0.2689, 0.3409, 0.0000],
         [0.2417, 0.2327, 0.3401, 0.0000],
         [0.2769, 0.2027, 0.3351, 0.0000],
         [0.2728, 0.0000, 0.3602, 0.0000],
         [0.3047, 0.0000, 0.3792, 0.0000],
         [0.2631, 0.0000, 0.4289, 0.0000],
         [0.2713, 0.0000, 0.4270, 0.0000],
         [0.2766, 0.0000, 0.4251, 0.0000],
         [0.2801, 0.0000, 0.3663, 0.0000],
         [0.2686, 0.2019, 0.3708, 0.0000]],

        [[0.2538, 0.2601, 0.2798, 0.2064],
         [0.2729, 0.2423, 0.3112, 0.0000],
         [0.2691, 0.2402, 0.3180, 0.0000],
         [0.2800, 0.0000, 0.3583, 0.0000],
         [0.2717, 0.2076, 0.3692, 0.0000],
         [0.2946, 0.2104, 0.3634, 0.0000],
         [0.3090, 0.0000, 0.3584, 0.0000],
         [0.2776, 0.2080, 0.3796, 0.0000],
         [0.3104, 0.0000, 0.3851, 0.0000],
         [0.2946, 0.0000, 0.3733, 0.0000],
         [0.2566, 0.2263, 0.4001, 0.0000],
         [0.3853, 0.2075, 0.2700, 0.0000],
         [0.3324, 0.2026, 0.3067, 0.0000],
         [0.2688, 0.2528, 0.3658, 0.0000],
         [0.2318, 0.2398, 0.3952, 0.0000],
         [0.2150, 0.2181, 0.3930, 0.0000],
         [0.2190, 0.2013, 0.3820, 0.0000],
         [0.2589, 0.0000, 0.3512, 0.0000],
         [0.2000, 0.2688, 0.3400, 0.0000],
         [0.2421, 0.2327, 0.3395, 0.0000],
         [0.2771, 0.2028, 0.3346, 0.0000],
         [0.2728, 0.0000, 0.3599, 0.0000],
         [0.3046, 0.0000, 0.3790, 0.0000],
         [0.2630, 0.0000, 0.4288, 0.0000],
         [0.2713, 0.0000, 0.4269, 0.0000],
         [0.2766, 0.0000, 0.4251, 0.0000],
         [0.2801, 0.0000, 0.3663, 0.0000],
         [0.2685, 0.2019, 0.3707, 0.0000]],

        [[0.2538, 0.2601, 0.2798, 0.2064],
         [0.2729, 0.2423, 0.3112, 0.0000],
         [0.2691, 0.2402, 0.3180, 0.0000],
         [0.2800, 0.0000, 0.3583, 0.0000],
         [0.2717, 0.2076, 0.3691, 0.0000],
         [0.2947, 0.2104, 0.3633, 0.0000],
         [0.3090, 0.0000, 0.3584, 0.0000],
         [0.2777, 0.2080, 0.3795, 0.0000],
         [0.3105, 0.0000, 0.3850, 0.0000],
         [0.2947, 0.0000, 0.3732, 0.0000],
         [0.2568, 0.2264, 0.3999, 0.0000],
         [0.3855, 0.2076, 0.2697, 0.0000],
         [0.3326, 0.2027, 0.3063, 0.0000],
         [0.2691, 0.2531, 0.3652, 0.0000],
         [0.2321, 0.2402, 0.3944, 0.0000],
         [0.2110, 0.2191, 0.3960, 0.0000],
         [0.2168, 0.2013, 0.3845, 0.0000],
         [0.2577, 0.0000, 0.3527, 0.0000],
         [0.0000, 0.2688, 0.3410, 0.0000],
         [0.2417, 0.2327, 0.3402, 0.0000],
         [0.2769, 0.2027, 0.3352, 0.0000],
         [0.2727, 0.0000, 0.3604, 0.0000],
         [0.3045, 0.0000, 0.3794, 0.0000],
         [0.2628, 0.0000, 0.4291, 0.0000],
         [0.2710, 0.0000, 0.4273, 0.0000],
         [0.2761, 0.0000, 0.4254, 0.0000],
         [0.2806, 0.0000, 0.3625, 0.0000],
         [0.2680, 0.2022, 0.3694, 0.0000]]], device='cuda:0',
       grad_fn=<StackBackward>)
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/model_edit.py[0m(54)[0;36mforward[0;34m()[0m
[0;32m     53 [0;31m            [0moutputs[0m[0;34m,[0m [0meach_layer_in[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgru[0m[0;34m([0m[0minputs[0m[0;34m,[0m [0meach_layer_in[0m[0;34m,[0m [0mt[0m[0;34m)[0m [0;31m# ã“ã“ã§ hidden_list ã‚’å—ã‘å–ã‚‹[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 54 [0;31m            [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     55 [0;31m            [0mres[0m [0;34m=[0m [0;34m[[0m[0mF[0m[0;34m.[0m[0msoftmax[0m[0;34m([0m[0mself[0m[0;34m.[0m[0moutput_layer[0m[0;34m([0m[0mout[0m[0;34m)[0m[0;34m)[0m [0;32mfor[0m [0mout[0m [0;32min[0m [0moutputs[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/model_edit.py[0m(55)[0;36mforward[0;34m()[0m
[0;32m     54 [0;31m            [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 55 [0;31m            [0mres[0m [0;34m=[0m [0;34m[[0m[0mF[0m[0;34m.[0m[0msoftmax[0m[0;34m([0m[0mself[0m[0;34m.[0m[0moutput_layer[0m[0;34m([0m[0mout[0m[0;34m)[0m[0;34m)[0m [0;32mfor[0m [0mout[0m [0;32min[0m [0moutputs[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     56 [0;31m            [0mscores[0m[0;34m.[0m[0mappend[0m[0;34m([0m[0mres[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/model_edit.py[0m(54)[0;36mforward[0;34m()[0m
[0;32m     53 [0;31m            [0moutputs[0m[0;34m,[0m [0meach_layer_in[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgru[0m[0;34m([0m[0minputs[0m[0;34m,[0m [0meach_layer_in[0m[0;34m,[0m [0mt[0m[0;34m)[0m [0;31m# ã“ã“ã§ hidden_list ã‚’å—ã‘å–ã‚‹[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 54 [0;31m            [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     55 [0;31m            [0mres[0m [0;34m=[0m [0;34m[[0m[0mF[0m[0;34m.[0m[0msoftmax[0m[0;34m([0m[0mself[0m[0;34m.[0m[0moutput_layer[0m[0;34m([0m[0mout[0m[0;34m)[0m[0;34m)[0m [0;32mfor[0m [0mout[0m [0;32min[0m [0moutputs[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/model_edit.py[0m(55)[0;36mforward[0;34m()[0m
[0;32m     54 [0;31m            [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 55 [0;31m            [0mres[0m [0;34m=[0m [0;34m[[0m[0mF[0m[0;34m.[0m[0msoftmax[0m[0;34m([0m[0mself[0m[0;34m.[0m[0moutput_layer[0m[0;34m([0m[0mout[0m[0;34m)[0m[0;34m)[0m [0;32mfor[0m [0mout[0m [0;32min[0m [0moutputs[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     56 [0;31m            [0mscores[0m[0;34m.[0m[0mappend[0m[0;34m([0m[0mres[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/model_edit.py[0m(54)[0;36mforward[0;34m()[0m
[0;32m     53 [0;31m            [0moutputs[0m[0;34m,[0m [0meach_layer_in[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgru[0m[0;34m([0m[0minputs[0m[0;34m,[0m [0meach_layer_in[0m[0;34m,[0m [0mt[0m[0;34m)[0m [0;31m# ã“ã“ã§ hidden_list ã‚’å—ã‘å–ã‚‹[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 54 [0;31m            [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     55 [0;31m            [0mres[0m [0;34m=[0m [0;34m[[0m[0mF[0m[0;34m.[0m[0msoftmax[0m[0;34m([0m[0mself[0m[0;34m.[0m[0moutput_layer[0m[0;34m([0m[0mout[0m[0;34m)[0m[0;34m)[0m [0;32mfor[0m [0mout[0m [0;32min[0m [0moutputs[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/model_edit.py[0m(55)[0;36mforward[0;34m()[0m
[0;32m     54 [0;31m            [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 55 [0;31m            [0mres[0m [0;34m=[0m [0;34m[[0m[0mF[0m[0;34m.[0m[0msoftmax[0m[0;34m([0m[0mself[0m[0;34m.[0m[0moutput_layer[0m[0;34m([0m[0mout[0m[0;34m)[0m[0;34m)[0m [0;32mfor[0m [0mout[0m [0;32min[0m [0moutputs[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     56 [0;31m            [0mscores[0m[0;34m.[0m[0mappend[0m[0;34m([0m[0mres[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/model_edit.py[0m(56)[0;36mforward[0;34m()[0m
[0;32m     55 [0;31m            [0mres[0m [0;34m=[0m [0;34m[[0m[0mF[0m[0;34m.[0m[0msoftmax[0m[0;34m([0m[0mself[0m[0;34m.[0m[0moutput_layer[0m[0;34m([0m[0mout[0m[0;34m)[0m[0;34m)[0m [0;32mfor[0m [0mout[0m [0;32min[0m [0moutputs[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 56 [0;31m            [0mscores[0m[0;34m.[0m[0mappend[0m[0;34m([0m[0mres[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     57 [0;31m            [0;31m#temp = self.filtering(res, temp, thres)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/model_edit.py[0m(58)[0;36mforward[0;34m()[0m
[0;32m     57 [0;31m            [0;31m#temp = self.filtering(res, temp, thres)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 58 [0;31m            [0mtemp[0m [0;34m=[0m [0mtorch[0m[0;34m.[0m[0mstack[0m[0;34m([0m[0;34m[[0m[0mr[0m [0;34m*[0m [0;34m([0m[0mr[0m [0;34m>[0m [0mthres[0m[0;34m)[0m[0;34m.[0m[0mfloat[0m[0;34m([0m[0;34m)[0m [0;32mfor[0m [0mr[0m [0;32min[0m [0mres[0m[0;34m][0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     59 [0;31m            [0mtemp[0m[0;34m[[0m[0;34m:[0m[0;34m,[0m[0;34m:[0m[0;34m,[0m[0;34m-[0m[0;36m1[0m[0;34m][0m [0;34m=[0m [0;36m0[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> tensor([[[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]]], device='cuda:0')
ipdb> [tensor([[0.2518, 0.2528, 0.2371, 0.2583],
        [0.2710, 0.2288, 0.2678, 0.2324],
        [0.2763, 0.2556, 0.2449, 0.2231],
        [0.3366, 0.1839, 0.2422, 0.2373],
        [0.2827, 0.1926, 0.2473, 0.2774],
        [0.2603, 0.1879, 0.2410, 0.3108],
        [0.2477, 0.1319, 0.3281, 0.2923],
        [0.2166, 0.1071, 0.3320, 0.3444],
        [0.2367, 0.1416, 0.2814, 0.3402],
        [0.2602, 0.1784, 0.2091, 0.3524],
        [0.2554, 0.1887, 0.2305, 0.3254],
        [0.1767, 0.1891, 0.2735, 0.3608],
        [0.1981, 0.2014, 0.2678, 0.3327],
        [0.2216, 0.2027, 0.2462, 0.3295]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)]
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/model_edit.py[0m(59)[0;36mforward[0;34m()[0m
[0;32m     58 [0;31m            [0mtemp[0m [0;34m=[0m [0mtorch[0m[0;34m.[0m[0mstack[0m[0;34m([0m[0;34m[[0m[0mr[0m [0;34m*[0m [0;34m([0m[0mr[0m [0;34m>[0m [0mthres[0m[0;34m)[0m[0;34m.[0m[0mfloat[0m[0;34m([0m[0;34m)[0m [0;32mfor[0m [0mr[0m [0;32min[0m [0mres[0m[0;34m][0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 59 [0;31m            [0mtemp[0m[0;34m[[0m[0;34m:[0m[0;34m,[0m[0;34m:[0m[0;34m,[0m[0;34m-[0m[0;36m1[0m[0;34m][0m [0;34m=[0m [0;36m0[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     60 [0;31m        [0;32mreturn[0m [0mscores[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> tensor([[[0.2518, 0.2528, 0.2371, 0.2583],
         [0.2710, 0.2288, 0.2678, 0.2324],
         [0.2763, 0.2556, 0.2449, 0.2231],
         [0.3366, 0.0000, 0.2422, 0.2373],
         [0.2827, 0.0000, 0.2473, 0.2774],
         [0.2603, 0.0000, 0.2410, 0.3108],
         [0.2477, 0.0000, 0.3281, 0.2923],
         [0.2166, 0.0000, 0.3320, 0.3444],
         [0.2367, 0.0000, 0.2814, 0.3402],
         [0.2602, 0.0000, 0.2091, 0.3524],
         [0.2554, 0.0000, 0.2305, 0.3254],
         [0.0000, 0.0000, 0.2735, 0.3608],
         [0.0000, 0.2014, 0.2678, 0.3327],
         [0.2216, 0.2027, 0.2462, 0.3295]]], device='cuda:0',
       grad_fn=<StackBackward>)
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/model_edit.py[0m(50)[0;36mforward[0;34m()[0m
[0;32m     49 [0;31m        [0mscores[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 50 [0;31m        [0;32mfor[0m [0mt[0m [0;32min[0m [0mrange[0m[0;34m([0m[0mit[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     51 [0;31m            [0mprev_temp[0m [0;34m=[0m [0mtemp[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> tensor([[[0.2518, 0.2528, 0.2371, 0.0000],
         [0.2710, 0.2288, 0.2678, 0.0000],
         [0.2763, 0.2556, 0.2449, 0.0000],
         [0.3366, 0.0000, 0.2422, 0.0000],
         [0.2827, 0.0000, 0.2473, 0.0000],
         [0.2603, 0.0000, 0.2410, 0.0000],
         [0.2477, 0.0000, 0.3281, 0.0000],
         [0.2166, 0.0000, 0.3320, 0.0000],
         [0.2367, 0.0000, 0.2814, 0.0000],
         [0.2602, 0.0000, 0.2091, 0.0000],
         [0.2554, 0.0000, 0.2305, 0.0000],
         [0.0000, 0.0000, 0.2735, 0.0000],
         [0.0000, 0.2014, 0.2678, 0.0000],
         [0.2216, 0.2027, 0.2462, 0.0000]]], device='cuda:0',
       grad_fn=<CopySlices>)
ipdb> > [0;32m/home/miyawaki_shumpei/showcase_miyawaki/nlp2019_gate/src/model_edit.py[0m(55)[0;36mforward[0;34m()[0m
[0;32m     54 [0;31m            [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 55 [0;31m            [0mres[0m [0;34m=[0m [0;34m[[0m[0mF[0m[0;34m.[0m[0msoftmax[0m[0;34m([0m[0mself[0m[0;34m.[0m[0moutput_layer[0m[0;34m([0m[0mout[0m[0;34m)[0m[0;34m)[0m [0;32mfor[0m [0mout[0m [0;32min[0m [0moutputs[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     56 [0;31m            [0mscores[0m[0;34m.[0m[0mappend[0m[0;34m([0m[0mres[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 1 [0m
Train...
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size10_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(-6375.2314) lr: 0.0001 time: 368.67
pred_count_train 6727

Test...

[32m iter_0[0m
ga 	p: 3.33 	r: 4.68 	f1: 3.89 	 49 	 1472 	 1046
wo 	p: 0.68 	r: 1.59 	f1: 0.95 	 10 	 1472 	 630
ni 	p: 0.54 	r: 3.01 	f1: 0.92 	 8 	 1472 	 266

[32m iter_1[0m
ga 	p: 0.0 	r: 0.0 	f1: 0.0 	 0 	 1472 	 1046
wo 	p: 0.0 	r: 0.0 	f1: 0.0 	 0 	 1472 	 630
ni 	p: 0.0 	r: 0.0 	f1: 0.0 	 0 	 1472 	 266

[32m iter_2[0m
ga 	p: 1.83 	r: 2.58 	f1: 2.14 	 27 	 1472 	 1046
wo 	p: 0.07 	r: 0.16 	f1: 0.1 	 1 	 1472 	 630
ni 	p: 0.2 	r: 1.13 	f1: 0.35 	 3 	 1472 	 266
best_thres [[0.1, 0.2, 0.0], [0.1, 0.2, 0.0], [0.1, 0.2, 0.0]]
f [0.0211, 0.0105, 0.0103]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size10_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 1 	 [0.1, 0.2, 0.0] 	 lr: 0.0001 	 f: 1.027576806123519
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size10_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 2 [0m
Train...
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size10_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(-6375.3032) lr: 0.0001 time: 151.81
pred_count_train 6727

Test...

[32m iter_0[0m
ga 	p: 3.33 	r: 4.68 	f1: 3.89 	 49 	 1472 	 1046
wo 	p: 0.68 	r: 1.59 	f1: 0.95 	 10 	 1472 	 630
ni 	p: 0.54 	r: 3.01 	f1: 0.92 	 8 	 1472 	 266

[32m iter_1[0m
ga 	p: 0.0 	r: 0.0 	f1: 0.0 	 0 	 1472 	 1046
wo 	p: 0.0 	r: 0.0 	f1: 0.0 	 0 	 1472 	 630
ni 	p: 0.0 	r: 0.0 	f1: 0.0 	 0 	 1472 	 266

[32m iter_2[0m
ga 	p: 1.77 	r: 2.49 	f1: 2.07 	 26 	 1472 	 1046
wo 	p: 0.07 	r: 0.16 	f1: 0.1 	 1 	 1472 	 630
ni 	p: 0.27 	r: 1.5 	f1: 0.46 	 4 	 1472 	 266
best_thres [[0.1, 0.2, 0.0], [0.1, 0.2, 0.0], [0.1, 0.2, 0.0]]
f [0.0211, 0.0105, 0.0103]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size10_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 1 	 [0.1, 0.2, 0.0] 	 lr: 0.0001 	 f: 1.027576806123519
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size10_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 2 [0m
Train...
loss: tensor(-6382.3550) lr: 0.0001 time: 153.6
pred_count_train 6727

Test...

[32m iter_0[0m
ga 	p: 3.46 	r: 4.88 	f1: 4.05 	 51 	 1472 	 1046
wo 	p: 0.75 	r: 1.75 	f1: 1.05 	 11 	 1472 	 630
ni 	p: 0.54 	r: 3.01 	f1: 0.92 	 8 	 1472 	 266

[32m iter_1[0m
ga 	p: 0.0 	r: 0.0 	f1: 0.0 	 0 	 1472 	 1046
wo 	p: 0.0 	r: 0.0 	f1: 0.0 	 0 	 1472 	 630
ni 	p: 0.0 	r: 0.0 	f1: 0.0 	 0 	 1472 	 266

[32m iter_2[0m
ga 	p: 2.38 	r: 3.35 	f1: 2.78 	 35 	 1472 	 1046
wo 	p: 0.2 	r: 0.48 	f1: 0.29 	 3 	 1472 	 630
ni 	p: 0.27 	r: 1.5 	f1: 0.46 	 4 	 1472 	 266
best_thres [[0.1, 0.2, 0.0], [0.1, 0.2, 0.0], [0.1, 0.2, 0.0]]
f [0.022, 0.011, 0.0117]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size10_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 2 	 [0.1, 0.2, 0.0] 	 lr: 0.0001 	 f: 1.174373492712593
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size10_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 3 [0m
Train...
loss: tensor(-6382.3491) lr: 0.0001 time: 154.35
pred_count_train 6727

Test...

[32m iter_0[0m
ga 	p: 3.6 	r: 5.07 	f1: 4.21 	 53 	 1472 	 1046
wo 	p: 0.75 	r: 1.75 	f1: 1.05 	 11 	 1472 	 630
ni 	p: 0.54 	r: 3.01 	f1: 0.92 	 8 	 1472 	 266

[32m iter_1[0m
ga 	p: 0.0 	r: 0.0 	f1: 0.0 	 0 	 1472 	 1046
wo 	p: 0.0 	r: 0.0 	f1: 0.0 	 0 	 1472 	 630
ni 	p: 0.0 	r: 0.0 	f1: 0.0 	 0 	 1472 	 266

[32m iter_2[0m
ga 	p: 2.85 	r: 4.02 	f1: 3.34 	 42 	 1472 	 1046
wo 	p: 0.48 	r: 1.11 	f1: 0.67 	 7 	 1472 	 630
ni 	p: 0.48 	r: 2.63 	f1: 0.81 	 7 	 1472 	 266
best_thres [[0.1, 0.2, 0.0], [0.1, 0.2, 0.0], [0.1, 0.2, 0.0]]
f [0.0226, 0.0113, 0.0134]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size10_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 3 	 [0.1, 0.2, 0.0] 	 lr: 0.0001 	 f: 1.3421411345286778
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size10_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 4 [0m
Train...
loss: tensor(-6382.3540) lr: 0.0001 time: 153.17
pred_count_train 6727

Test...

[32m iter_0[0m
ga 	p: 3.6 	r: 5.07 	f1: 4.21 	 53 	 1472 	 1046
wo 	p: 0.75 	r: 1.75 	f1: 1.05 	 11 	 1472 	 630
ni 	p: 0.54 	r: 3.01 	f1: 0.92 	 8 	 1472 	 266

[32m iter_1[0m
ga 	p: 0.0 	r: 0.0 	f1: 0.0 	 0 	 1472 	 1046
wo 	p: 0.0 	r: 0.0 	f1: 0.0 	 0 	 1472 	 630
ni 	p: 0.0 	r: 0.0 	f1: 0.0 	 0 	 1472 	 266

[32m iter_2[0m
ga 	p: 3.19 	r: 4.49 	f1: 3.73 	 47 	 1472 	 1046
wo 	p: 0.54 	r: 1.27 	f1: 0.76 	 8 	 1472 	 630
ni 	p: 0.54 	r: 3.01 	f1: 0.92 	 8 	 1472 	 266
best_thres [[0.1, 0.2, 0.0], [0.1, 0.2, 0.0], [0.1, 0.2, 0.0]]
f [0.0226, 0.0113, 0.0142]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size10_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 4 	 [0.1, 0.2, 0.0] 	 lr: 0.0001 	 f: 1.4155394778232149
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size10_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 5 [0m
Train...
loss: tensor(-6382.3501) lr: 0.0001 time: 145.18
pred_count_train 6727

Test...

[32m iter_0[0m
ga 	p: 3.6 	r: 5.07 	f1: 4.21 	 53 	 1472 	 1046
wo 	p: 0.75 	r: 1.75 	f1: 1.05 	 11 	 1472 	 630
ni 	p: 0.54 	r: 3.01 	f1: 0.92 	 8 	 1472 	 266

[32m iter_1[0m
ga 	p: 0.07 	r: 0.1 	f1: 0.08 	 1 	 1472 	 1046
wo 	p: 0.0 	r: 0.0 	f1: 0.0 	 0 	 1472 	 630
ni 	p: 0.0 	r: 0.0 	f1: 0.0 	 0 	 1472 	 266

[32m iter_2[0m
ga 	p: 3.19 	r: 4.49 	f1: 3.73 	 47 	 1472 	 1046
wo 	p: 0.61 	r: 1.43 	f1: 0.86 	 9 	 1472 	 630
ni 	p: 0.54 	r: 3.01 	f1: 0.92 	 8 	 1472 	 266
best_thres [[0.1, 0.2, 0.0], [0.1, 0.2, 0.0], [0.1, 0.2, 0.0]]
f [0.0226, 0.0115, 0.0144]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size10_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 5 	 [0.1, 0.2, 0.0] 	 lr: 0.0001 	 f: 1.4365104330502254
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size10_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 6 [0m
Train...
loss: tensor(-6382.3560) lr: 0.0001 time: 155.21
pred_count_train 6727

Test...
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 1 [0m
Train...
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 1 [0m
Train...
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 1 [0m
Train...
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(3150.6882) lr: 0.0002 time: 1746.19
pred_count_train 41644

Test...
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(3151.0034) lr: 0.0002 time: 1755.72
pred_count_train 41644

Test...
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(3150.6240) lr: 0.0002 time: 1751.49
pred_count_train 41644

Test...
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(3150.6240) lr: 0.0002 time: 1789.37
pred_count_train 41644

Test...
loss: tensor(3151.0034) lr: 0.0002 time: 1794.8
pred_count_train 41644

Test...
loss: tensor(3150.6882) lr: 0.0002 time: 1789.39
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 76.28 	r: 74.97 	f1: 75.62 	 4624 	 6062 	 6168
wo 	p: 91.73 	r: 83.77 	f1: 87.57 	 2838 	 3094 	 3388
ni 	p: 80.95 	r: 80.95 	f1: 80.95 	 1232 	 1522 	 1522

[32m iter_1[0m
ga 	p: 76.24 	r: 75.03 	f1: 75.63 	 4628 	 6070 	 6168
wo 	p: 91.14 	r: 84.39 	f1: 87.63 	 2859 	 3137 	 3388
ni 	p: 80.84 	r: 80.95 	f1: 80.89 	 1232 	 1524 	 1522

[32m iter_2[0m
ga 	p: 76.24 	r: 75.03 	f1: 75.63 	 4628 	 6070 	 6168
wo 	p: 91.11 	r: 84.39 	f1: 87.62 	 2859 	 3138 	 3388
ni 	p: 80.84 	r: 80.95 	f1: 80.89 	 1232 	 1524 	 1522
best_thres [[0.42, 0.32, 0.17], [0.42, 0.28, 0.17], [0.42, 0.28, 0.17]]
f [0.7992, 0.7994, 0.7994]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 1 	 [0.42, 0.28, 0.17] 	 lr: 0.0002 	 f: 79.94493307839389
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 2 [0m
Train...

[32m iter_0[0m
ga 	p: 76.94 	r: 74.58 	f1: 75.74 	 4600 	 5979 	 6168
wo 	p: 91.87 	r: 82.76 	f1: 87.08 	 2804 	 3052 	 3388
ni 	p: 80.92 	r: 80.81 	f1: 80.87 	 1230 	 1520 	 1522

[32m iter_1[0m
ga 	p: 77.57 	r: 73.62 	f1: 75.54 	 4541 	 5854 	 6168
wo 	p: 92.03 	r: 82.47 	f1: 86.99 	 2794 	 3036 	 3388
ni 	p: 86.88 	r: 75.69 	f1: 80.9 	 1152 	 1326 	 1522

[32m iter_2[0m
ga 	p: 77.63 	r: 73.65 	f1: 75.59 	 4543 	 5852 	 6168
wo 	p: 92.0 	r: 82.5 	f1: 86.99 	 2795 	 3038 	 3388
ni 	p: 86.81 	r: 75.69 	f1: 80.87 	 1152 	 1327 	 1522
best_thres [[0.43, 0.31, 0.18], [0.46, 0.32, 0.31], [0.46, 0.32, 0.31]]
f [0.7984, 0.7978, 0.7976]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 1 	 [0.46, 0.32, 0.31] 	 lr: 0.0002 	 f: 79.76268335980566
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 2 [0m
Train...

[32m iter_0[0m
ga 	p: 78.21 	r: 73.49 	f1: 75.78 	 4533 	 5796 	 6168
wo 	p: 93.82 	r: 81.58 	f1: 87.28 	 2764 	 2946 	 3388
ni 	p: 87.81 	r: 74.77 	f1: 80.77 	 1138 	 1296 	 1522

[32m iter_1[0m
ga 	p: 77.39 	r: 74.24 	f1: 75.78 	 4579 	 5917 	 6168
wo 	p: 90.96 	r: 83.77 	f1: 87.22 	 2838 	 3120 	 3388
ni 	p: 83.4 	r: 78.25 	f1: 80.75 	 1191 	 1428 	 1522

[32m iter_2[0m
ga 	p: 77.39 	r: 74.27 	f1: 75.8 	 4581 	 5919 	 6168
wo 	p: 91.18 	r: 83.59 	f1: 87.22 	 2832 	 3106 	 3388
ni 	p: 83.25 	r: 78.38 	f1: 80.74 	 1193 	 1433 	 1522
best_thres [[0.47, 0.42, 0.34], [0.45, 0.27, 0.22], [0.45, 0.28, 0.22]]
f [0.7989, 0.799, 0.7991]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 1 	 [0.45, 0.28, 0.22] 	 lr: 0.0002 	 f: 79.90965028429005
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 2 [0m
Train...
loss: tensor(2013.0117) lr: 0.0002 time: 1772.66
pred_count_train 41644

Test...
loss: tensor(2014.7007) lr: 0.0002 time: 1787.43
pred_count_train 41644

Test...
loss: tensor(2012.7239) lr: 0.0002 time: 1782.37
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.52 	r: 75.62 	f1: 77.99 	 4664 	 5792 	 6168
wo 	p: 92.97 	r: 85.51 	f1: 89.08 	 2897 	 3116 	 3388
ni 	p: 85.11 	r: 80.75 	f1: 82.87 	 1229 	 1444 	 1522

[32m iter_1[0m
ga 	p: 79.93 	r: 76.18 	f1: 78.01 	 4699 	 5879 	 6168
wo 	p: 93.07 	r: 85.6 	f1: 89.18 	 2900 	 3116 	 3388
ni 	p: 84.59 	r: 81.14 	f1: 82.83 	 1235 	 1460 	 1522

[32m iter_2[0m
ga 	p: 79.62 	r: 76.46 	f1: 78.01 	 4716 	 5923 	 6168
wo 	p: 93.1 	r: 85.66 	f1: 89.22 	 2902 	 3117 	 3388
ni 	p: 84.54 	r: 81.21 	f1: 82.84 	 1236 	 1462 	 1522
best_thres [[0.38, 0.35, 0.26], [0.36, 0.35, 0.25], [0.35, 0.35, 0.25]]
f [0.8203, 0.8204, 0.8205]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 2 	 [0.35, 0.35, 0.25] 	 lr: 0.0002 	 f: 82.04762716328649
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 3 [0m
Train...

[32m iter_0[0m
ga 	p: 80.45 	r: 75.57 	f1: 77.93 	 4661 	 5794 	 6168
wo 	p: 93.74 	r: 84.39 	f1: 88.82 	 2859 	 3050 	 3388
ni 	p: 83.99 	r: 82.06 	f1: 83.02 	 1249 	 1487 	 1522

[32m iter_1[0m
ga 	p: 79.1 	r: 76.52 	f1: 77.79 	 4720 	 5967 	 6168
wo 	p: 93.21 	r: 84.71 	f1: 88.76 	 2870 	 3079 	 3388
ni 	p: 82.31 	r: 83.77 	f1: 83.03 	 1275 	 1549 	 1522

[32m iter_2[0m
ga 	p: 79.09 	r: 76.49 	f1: 77.77 	 4718 	 5965 	 6168
wo 	p: 93.07 	r: 84.77 	f1: 88.72 	 2872 	 3086 	 3388
ni 	p: 82.7 	r: 83.57 	f1: 83.14 	 1272 	 1538 	 1522
best_thres [[0.37, 0.5, 0.23], [0.32, 0.45, 0.18], [0.32, 0.44, 0.19]]
f [0.8192, 0.8186, 0.8184]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 2 	 [0.32, 0.44, 0.19] 	 lr: 0.0002 	 f: 81.84219061298244
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 3 [0m
Train...

[32m iter_0[0m
ga 	p: 80.45 	r: 75.26 	f1: 77.77 	 4642 	 5770 	 6168
wo 	p: 92.34 	r: 85.74 	f1: 88.92 	 2905 	 3146 	 3388
ni 	p: 83.58 	r: 81.27 	f1: 82.41 	 1237 	 1480 	 1522

[32m iter_1[0m
ga 	p: 79.69 	r: 75.68 	f1: 77.63 	 4668 	 5858 	 6168
wo 	p: 92.39 	r: 85.66 	f1: 88.9 	 2902 	 3141 	 3388
ni 	p: 83.73 	r: 81.14 	f1: 82.42 	 1235 	 1475 	 1522

[32m iter_2[0m
ga 	p: 79.63 	r: 75.68 	f1: 77.61 	 4668 	 5862 	 6168
wo 	p: 92.33 	r: 85.68 	f1: 88.89 	 2903 	 3144 	 3388
ni 	p: 83.55 	r: 81.41 	f1: 82.46 	 1239 	 1483 	 1522
best_thres [[0.37, 0.26, 0.19], [0.36, 0.27, 0.19], [0.36, 0.26, 0.19]]
f [0.8181, 0.8176, 0.8174]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 2 	 [0.36, 0.26, 0.19] 	 lr: 0.0002 	 f: 81.73950737696035
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 3 [0m
Train...
loss: tensor(1624.2397) lr: 0.0002 time: 1758.71
pred_count_train 41644

Test...
loss: tensor(1625.1926) lr: 0.0002 time: 1774.79
pred_count_train 41644

Test...
loss: tensor(1625.7217) lr: 0.0002 time: 1768.58
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.5 	r: 74.9 	f1: 78.52 	 4620 	 5600 	 6168
wo 	p: 93.94 	r: 84.71 	f1: 89.09 	 2870 	 3055 	 3388
ni 	p: 88.3 	r: 76.35 	f1: 81.89 	 1162 	 1316 	 1522

[32m iter_1[0m
ga 	p: 82.25 	r: 75.03 	f1: 78.47 	 4628 	 5627 	 6168
wo 	p: 92.75 	r: 85.74 	f1: 89.11 	 2905 	 3132 	 3388
ni 	p: 87.33 	r: 77.0 	f1: 81.84 	 1172 	 1342 	 1522

[32m iter_2[0m
ga 	p: 82.25 	r: 75.05 	f1: 78.48 	 4629 	 5628 	 6168
wo 	p: 92.98 	r: 85.57 	f1: 89.12 	 2899 	 3118 	 3388
ni 	p: 87.2 	r: 77.0 	f1: 81.79 	 1172 	 1344 	 1522
best_thres [[0.47, 0.53, 0.24], [0.46, 0.41, 0.21], [0.46, 0.43, 0.21]]
f [0.8221, 0.8221, 0.822]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 3 	 [0.46, 0.43, 0.21] 	 lr: 0.0002 	 f: 82.20392453782573
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 4 [0m
Train...

[32m iter_0[0m
ga 	p: 82.24 	r: 74.34 	f1: 78.09 	 4585 	 5575 	 6168
wo 	p: 91.77 	r: 85.89 	f1: 88.73 	 2910 	 3171 	 3388
ni 	p: 87.22 	r: 77.6 	f1: 82.13 	 1181 	 1354 	 1522

[32m iter_1[0m
ga 	p: 83.25 	r: 73.56 	f1: 78.1 	 4537 	 5450 	 6168
wo 	p: 93.32 	r: 84.59 	f1: 88.74 	 2866 	 3071 	 3388
ni 	p: 86.63 	r: 77.92 	f1: 82.05 	 1186 	 1369 	 1522

[32m iter_2[0m
ga 	p: 83.47 	r: 73.33 	f1: 78.07 	 4523 	 5419 	 6168
wo 	p: 93.33 	r: 84.62 	f1: 88.76 	 2867 	 3072 	 3388
ni 	p: 86.7 	r: 77.92 	f1: 82.08 	 1186 	 1368 	 1522
best_thres [[0.48, 0.43, 0.2], [0.53, 0.59, 0.19], [0.55, 0.59, 0.19]]
f [0.8193, 0.8193, 0.8193]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 3 	 [0.55, 0.59, 0.19] 	 lr: 0.0002 	 f: 81.92698508314444
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 4 [0m
Train...

[32m iter_0[0m
ga 	p: 81.86 	r: 75.21 	f1: 78.39 	 4639 	 5667 	 6168
wo 	p: 94.08 	r: 84.47 	f1: 89.02 	 2862 	 3042 	 3388
ni 	p: 85.86 	r: 77.79 	f1: 81.63 	 1184 	 1379 	 1522

[32m iter_1[0m
ga 	p: 81.23 	r: 75.65 	f1: 78.34 	 4666 	 5744 	 6168
wo 	p: 93.81 	r: 84.47 	f1: 88.9 	 2862 	 3051 	 3388
ni 	p: 85.15 	r: 78.38 	f1: 81.63 	 1193 	 1401 	 1522

[32m iter_2[0m
ga 	p: 81.17 	r: 75.68 	f1: 78.33 	 4668 	 5751 	 6168
wo 	p: 93.78 	r: 84.56 	f1: 88.93 	 2865 	 3055 	 3388
ni 	p: 85.76 	r: 78.32 	f1: 81.87 	 1192 	 1390 	 1522
best_thres [[0.42, 0.54, 0.2], [0.41, 0.54, 0.18], [0.41, 0.54, 0.19]]
f [0.8207, 0.8203, 0.8203]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 3 	 [0.41, 0.54, 0.19] 	 lr: 0.0002 	 f: 82.02592836739178
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 4 [0m
Train...
loss: tensor(1279.4050) lr: 0.0002 time: 1734.32
pred_count_train 41644

Test...
loss: tensor(1284.5236) lr: 0.0002 time: 1762.14
pred_count_train 41644

Test...
loss: tensor(1286.6794) lr: 0.0002 time: 1753.9
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.79 	r: 74.14 	f1: 77.78 	 4573 	 5591 	 6168
wo 	p: 94.13 	r: 85.27 	f1: 89.48 	 2889 	 3069 	 3388
ni 	p: 85.3 	r: 80.09 	f1: 82.62 	 1219 	 1429 	 1522

[32m iter_1[0m
ga 	p: 82.08 	r: 73.96 	f1: 77.81 	 4562 	 5558 	 6168
wo 	p: 92.4 	r: 86.81 	f1: 89.51 	 2941 	 3183 	 3388
ni 	p: 85.16 	r: 79.96 	f1: 82.48 	 1217 	 1429 	 1522

[32m iter_2[0m
ga 	p: 82.12 	r: 73.99 	f1: 77.84 	 4564 	 5558 	 6168
wo 	p: 92.4 	r: 86.84 	f1: 89.53 	 2942 	 3184 	 3388
ni 	p: 84.81 	r: 80.35 	f1: 82.52 	 1223 	 1442 	 1522
best_thres [[0.29, 0.72, 0.26], [0.3, 0.56, 0.26], [0.3, 0.56, 0.25]]
f [0.8202, 0.8205, 0.8207]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 3 	 [0.46, 0.43, 0.21] 	 lr: 0.0002 	 f: 82.20392453782573
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 5 [0m
Train...

[32m iter_0[0m
ga 	p: 80.04 	r: 75.66 	f1: 77.79 	 4667 	 5831 	 6168
wo 	p: 91.97 	r: 86.57 	f1: 89.19 	 2933 	 3189 	 3388
ni 	p: 85.19 	r: 80.09 	f1: 82.56 	 1219 	 1431 	 1522

[32m iter_1[0m
ga 	p: 81.0 	r: 75.08 	f1: 77.93 	 4631 	 5717 	 6168
wo 	p: 92.34 	r: 86.13 	f1: 89.13 	 2918 	 3160 	 3388
ni 	p: 83.9 	r: 81.47 	f1: 82.67 	 1240 	 1478 	 1522

[32m iter_2[0m
ga 	p: 80.79 	r: 75.28 	f1: 77.94 	 4643 	 5747 	 6168
wo 	p: 92.4 	r: 86.07 	f1: 89.12 	 2916 	 3156 	 3388
ni 	p: 83.95 	r: 81.47 	f1: 82.69 	 1240 	 1477 	 1522
best_thres [[0.25, 0.51, 0.25], [0.28, 0.61, 0.2], [0.27, 0.63, 0.2]]
f [0.8193, 0.8197, 0.8198]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 4 	 [0.27, 0.63, 0.2] 	 lr: 0.0002 	 f: 81.98385594535858
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 5 [0m
Train...

[32m iter_0[0m
ga 	p: 78.48 	r: 77.27 	f1: 77.87 	 4766 	 6073 	 6168
wo 	p: 92.95 	r: 85.27 	f1: 88.95 	 2889 	 3108 	 3388
ni 	p: 85.64 	r: 80.35 	f1: 82.92 	 1223 	 1428 	 1522

[32m iter_1[0m
ga 	p: 79.76 	r: 75.89 	f1: 77.78 	 4681 	 5869 	 6168
wo 	p: 92.92 	r: 85.57 	f1: 89.09 	 2899 	 3120 	 3388
ni 	p: 85.08 	r: 80.95 	f1: 82.96 	 1232 	 1448 	 1522

[32m iter_2[0m
ga 	p: 77.79 	r: 77.74 	f1: 77.77 	 4795 	 6164 	 6168
wo 	p: 92.71 	r: 85.63 	f1: 89.03 	 2901 	 3129 	 3388
ni 	p: 86.14 	r: 80.03 	f1: 82.97 	 1218 	 1414 	 1522
best_thres [[0.17, 0.66, 0.3], [0.22, 0.7, 0.26], [0.15, 0.7, 0.29]]
f [0.8187, 0.8189, 0.8187]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 3 	 [0.41, 0.54, 0.19] 	 lr: 0.0002 	 f: 82.02592836739178
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 5 [0m
Train...
loss: tensor(963.4816) lr: 0.0002 time: 1712.69
pred_count_train 41644

Test...
loss: tensor(974.0670) lr: 0.0002 time: 1746.24
pred_count_train 41644

Test...
loss: tensor(974.3977) lr: 0.0002 time: 1736.93
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.78 	r: 75.62 	f1: 78.58 	 4664 	 5703 	 6168
wo 	p: 92.72 	r: 86.45 	f1: 89.48 	 2929 	 3159 	 3388
ni 	p: 81.01 	r: 77.66 	f1: 79.3 	 1182 	 1459 	 1522

[32m iter_1[0m
ga 	p: 80.34 	r: 76.8 	f1: 78.53 	 4737 	 5896 	 6168
wo 	p: 92.6 	r: 86.39 	f1: 89.39 	 2927 	 3161 	 3388
ni 	p: 80.84 	r: 78.19 	f1: 79.49 	 1190 	 1472 	 1522

[32m iter_2[0m
ga 	p: 80.51 	r: 76.69 	f1: 78.55 	 4730 	 5875 	 6168
wo 	p: 92.57 	r: 86.42 	f1: 89.39 	 2928 	 3163 	 3388
ni 	p: 80.73 	r: 78.19 	f1: 79.44 	 1190 	 1474 	 1522
best_thres [[0.41, 0.51, 0.11], [0.33, 0.5, 0.1], [0.34, 0.5, 0.1]]
f [0.8201, 0.8198, 0.8198]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 3 	 [0.46, 0.43, 0.21] 	 lr: 0.0002 	 f: 82.20392453782573
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 6 [0m
Train...

[32m iter_0[0m
ga 	p: 81.8 	r: 76.05 	f1: 78.82 	 4691 	 5735 	 6168
wo 	p: 93.65 	r: 85.77 	f1: 89.54 	 2906 	 3103 	 3388
ni 	p: 86.37 	r: 75.36 	f1: 80.49 	 1147 	 1328 	 1522

[32m iter_1[0m
ga 	p: 82.62 	r: 75.08 	f1: 78.67 	 4631 	 5605 	 6168
wo 	p: 93.67 	r: 85.57 	f1: 89.43 	 2899 	 3095 	 3388
ni 	p: 84.88 	r: 76.74 	f1: 80.61 	 1168 	 1376 	 1522

[32m iter_2[0m
ga 	p: 82.65 	r: 75.15 	f1: 78.72 	 4635 	 5608 	 6168
wo 	p: 93.46 	r: 85.6 	f1: 89.35 	 2900 	 3103 	 3388
ni 	p: 84.87 	r: 76.68 	f1: 80.57 	 1167 	 1375 	 1522
best_thres [[0.42, 0.67, 0.21], [0.49, 0.78, 0.16], [0.49, 0.78, 0.16]]
f [0.8232, 0.8228, 0.8226]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 5 	 [0.49, 0.78, 0.16] 	 lr: 0.0002 	 f: 82.26298731946761
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 6 [0m
Train...

[32m iter_0[0m
ga 	p: 81.83 	r: 76.43 	f1: 79.03 	 4714 	 5761 	 6168
wo 	p: 91.46 	r: 87.19 	f1: 89.27 	 2954 	 3230 	 3388
ni 	p: 84.56 	r: 75.95 	f1: 80.03 	 1156 	 1367 	 1522

[32m iter_1[0m
ga 	p: 82.68 	r: 75.29 	f1: 78.81 	 4644 	 5617 	 6168
wo 	p: 92.33 	r: 86.75 	f1: 89.45 	 2939 	 3183 	 3388
ni 	p: 83.26 	r: 76.81 	f1: 79.9 	 1169 	 1404 	 1522

[32m iter_2[0m
ga 	p: 81.85 	r: 75.97 	f1: 78.8 	 4686 	 5725 	 6168
wo 	p: 92.44 	r: 86.92 	f1: 89.6 	 2945 	 3186 	 3388
ni 	p: 82.83 	r: 77.33 	f1: 79.99 	 1177 	 1421 	 1522
best_thres [[0.39, 0.33, 0.22], [0.5, 0.46, 0.18], [0.46, 0.45, 0.17]]
f [0.8233, 0.8229, 0.8229]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 5 	 [0.46, 0.45, 0.17] 	 lr: 0.0002 	 f: 82.28542914171658
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 6 [0m
Train...
loss: tensor(716.2055) lr: 0.0002 time: 1682.73
pred_count_train 41644

Test...
loss: tensor(727.5516) lr: 0.0002 time: 1730.46
pred_count_train 41644

Test...
loss: tensor(733.7824) lr: 0.0002 time: 1717.23
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.77 	r: 75.29 	f1: 78.85 	 4644 	 5611 	 6168
wo 	p: 92.46 	r: 86.51 	f1: 89.39 	 2931 	 3170 	 3388
ni 	p: 87.62 	r: 74.9 	f1: 80.77 	 1140 	 1301 	 1522

[32m iter_1[0m
ga 	p: 82.96 	r: 75.32 	f1: 78.96 	 4646 	 5600 	 6168
wo 	p: 93.88 	r: 85.15 	f1: 89.31 	 2885 	 3073 	 3388
ni 	p: 84.73 	r: 76.94 	f1: 80.65 	 1171 	 1382 	 1522

[32m iter_2[0m
ga 	p: 80.37 	r: 77.53 	f1: 78.92 	 4782 	 5950 	 6168
wo 	p: 91.51 	r: 87.22 	f1: 89.32 	 2955 	 3229 	 3388
ni 	p: 84.77 	r: 76.81 	f1: 80.59 	 1169 	 1379 	 1522
best_thres [[0.61, 0.45, 0.18], [0.61, 0.65, 0.11], [0.42, 0.34, 0.11]]
f [0.8237, 0.8236, 0.8235]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 6 	 [0.42, 0.34, 0.11] 	 lr: 0.0002 	 f: 82.3507328442491
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 7 [0m
Train...

[32m iter_0[0m
ga 	p: 80.57 	r: 76.3 	f1: 78.37 	 4706 	 5841 	 6168
wo 	p: 93.21 	r: 86.33 	f1: 89.64 	 2925 	 3138 	 3388
ni 	p: 85.13 	r: 69.58 	f1: 76.57 	 1059 	 1244 	 1522

[32m iter_1[0m
ga 	p: 82.74 	r: 74.12 	f1: 78.19 	 4572 	 5526 	 6168
wo 	p: 93.53 	r: 86.13 	f1: 89.67 	 2918 	 3120 	 3388
ni 	p: 83.09 	r: 71.68 	f1: 76.97 	 1091 	 1313 	 1522

[32m iter_2[0m
ga 	p: 81.71 	r: 74.98 	f1: 78.2 	 4625 	 5660 	 6168
wo 	p: 93.47 	r: 86.22 	f1: 89.7 	 2921 	 3125 	 3388
ni 	p: 83.08 	r: 71.62 	f1: 76.92 	 1090 	 1312 	 1522
best_thres [[0.37, 0.65, 0.06], [0.58, 0.79, 0.04], [0.45, 0.8, 0.04]]
f [0.8159, 0.8159, 0.8158]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 5 	 [0.49, 0.78, 0.16] 	 lr: 0.0002 	 f: 82.26298731946761
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 7 [0m
Train...

[32m iter_0[0m
ga 	p: 82.37 	r: 75.47 	f1: 78.77 	 4655 	 5651 	 6168
wo 	p: 92.63 	r: 85.71 	f1: 89.04 	 2904 	 3135 	 3388
ni 	p: 87.26 	r: 73.32 	f1: 79.69 	 1116 	 1279 	 1522

[32m iter_1[0m
ga 	p: 82.82 	r: 74.77 	f1: 78.59 	 4612 	 5569 	 6168
wo 	p: 92.74 	r: 85.92 	f1: 89.2 	 2911 	 3139 	 3388
ni 	p: 87.36 	r: 74.05 	f1: 80.16 	 1127 	 1290 	 1522

[32m iter_2[0m
ga 	p: 82.57 	r: 75.03 	f1: 78.62 	 4628 	 5605 	 6168
wo 	p: 92.79 	r: 85.86 	f1: 89.19 	 2909 	 3135 	 3388
ni 	p: 87.43 	r: 74.05 	f1: 80.18 	 1127 	 1289 	 1522
best_thres [[0.58, 0.57, 0.16], [0.7, 0.64, 0.14], [0.7, 0.65, 0.14]]
f [0.8206, 0.8207, 0.8208]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 5 	 [0.46, 0.45, 0.17] 	 lr: 0.0002 	 f: 82.28542914171658
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 7 [0m
Train...
loss: tensor(562.3495) lr: 0.0002 time: 1645.03
pred_count_train 41644

Test...
loss: tensor(560.2393) lr: 0.0002 time: 1706.61
pred_count_train 41644

Test...
loss: tensor(572.3937) lr: 0.0002 time: 1694.5
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.47 	r: 75.21 	f1: 78.22 	 4639 	 5694 	 6168
wo 	p: 92.47 	r: 86.63 	f1: 89.45 	 2935 	 3174 	 3388
ni 	p: 85.27 	r: 69.97 	f1: 76.87 	 1065 	 1249 	 1522

[32m iter_1[0m
ga 	p: 81.64 	r: 75.11 	f1: 78.24 	 4633 	 5675 	 6168
wo 	p: 92.89 	r: 86.36 	f1: 89.51 	 2926 	 3150 	 3388
ni 	p: 84.54 	r: 70.04 	f1: 76.61 	 1066 	 1261 	 1522

[32m iter_2[0m
ga 	p: 81.56 	r: 75.06 	f1: 78.18 	 4630 	 5677 	 6168
wo 	p: 92.01 	r: 87.04 	f1: 89.46 	 2949 	 3205 	 3388
ni 	p: 82.17 	r: 71.75 	f1: 76.6 	 1092 	 1329 	 1522
best_thres [[0.7, 0.5, 0.14], [0.7, 0.56, 0.12], [0.7, 0.42, 0.08]]
f [0.8152, 0.8151, 0.815]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 6 	 [0.42, 0.34, 0.11] 	 lr: 0.0002 	 f: 82.3507328442491
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 8 [0m
Train...

[32m iter_0[0m
ga 	p: 82.25 	r: 74.4 	f1: 78.13 	 4589 	 5579 	 6168
wo 	p: 92.5 	r: 86.95 	f1: 89.64 	 2946 	 3185 	 3388
ni 	p: 81.88 	r: 73.32 	f1: 77.37 	 1116 	 1363 	 1522

[32m iter_1[0m
ga 	p: 81.22 	r: 75.5 	f1: 78.26 	 4657 	 5734 	 6168
wo 	p: 92.57 	r: 86.75 	f1: 89.56 	 2939 	 3175 	 3388
ni 	p: 84.79 	r: 71.42 	f1: 77.53 	 1087 	 1282 	 1522

[32m iter_2[0m
ga 	p: 81.6 	r: 75.23 	f1: 78.29 	 4640 	 5686 	 6168
wo 	p: 92.0 	r: 87.28 	f1: 89.58 	 2957 	 3214 	 3388
ni 	p: 85.32 	r: 71.02 	f1: 77.52 	 1081 	 1267 	 1522
best_thres [[0.57, 0.69, 0.09], [0.51, 0.82, 0.12], [0.61, 0.79, 0.13]]
f [0.8159, 0.8162, 0.8165]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 5 	 [0.49, 0.78, 0.16] 	 lr: 0.0002 	 f: 82.26298731946761
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 8 [0m
Train...

[32m iter_0[0m
ga 	p: 82.72 	r: 74.87 	f1: 78.6 	 4618 	 5583 	 6168
wo 	p: 93.09 	r: 85.95 	f1: 89.38 	 2912 	 3128 	 3388
ni 	p: 86.07 	r: 71.02 	f1: 77.83 	 1081 	 1256 	 1522

[32m iter_1[0m
ga 	p: 81.63 	r: 75.73 	f1: 78.57 	 4671 	 5722 	 6168
wo 	p: 92.03 	r: 86.95 	f1: 89.42 	 2946 	 3201 	 3388
ni 	p: 83.52 	r: 73.59 	f1: 78.24 	 1120 	 1341 	 1522

[32m iter_2[0m
ga 	p: 81.44 	r: 76.04 	f1: 78.65 	 4690 	 5759 	 6168
wo 	p: 91.98 	r: 86.98 	f1: 89.41 	 2947 	 3204 	 3388
ni 	p: 82.31 	r: 74.57 	f1: 78.25 	 1135 	 1379 	 1522
best_thres [[0.63, 0.74, 0.12], [0.67, 0.7, 0.07], [0.67, 0.72, 0.06]]
f [0.8183, 0.8186, 0.8187]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 5 	 [0.46, 0.45, 0.17] 	 lr: 0.0002 	 f: 82.28542914171658
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 8 [0m
Train...
loss: tensor(451.3966) lr: 0.0002 time: 1600.74
pred_count_train 41644

Test...
loss: tensor(456.0934) lr: 0.0002 time: 1680.31
pred_count_train 41644

Test...
loss: tensor(483.1526) lr: 0.0002 time: 1669.35
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.1 	r: 75.19 	f1: 78.03 	 4638 	 5719 	 6168
wo 	p: 92.81 	r: 86.13 	f1: 89.34 	 2918 	 3144 	 3388
ni 	p: 84.92 	r: 76.61 	f1: 80.55 	 1166 	 1373 	 1522

[32m iter_1[0m
ga 	p: 80.12 	r: 76.07 	f1: 78.04 	 4692 	 5856 	 6168
wo 	p: 92.57 	r: 86.39 	f1: 89.37 	 2927 	 3162 	 3388
ni 	p: 86.33 	r: 75.49 	f1: 80.55 	 1149 	 1331 	 1522

[32m iter_2[0m
ga 	p: 80.07 	r: 76.17 	f1: 78.07 	 4698 	 5867 	 6168
wo 	p: 93.01 	r: 86.01 	f1: 89.37 	 2914 	 3133 	 3388
ni 	p: 85.93 	r: 75.82 	f1: 80.56 	 1154 	 1343 	 1522
best_thres [[0.57, 0.53, 0.23], [0.48, 0.49, 0.27], [0.47, 0.55, 0.26]]
f [0.8184, 0.8184, 0.8184]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 6 	 [0.42, 0.34, 0.11] 	 lr: 0.0002 	 f: 82.3507328442491
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 9 [0m
Train...

[32m iter_0[0m
ga 	p: 80.87 	r: 74.94 	f1: 77.79 	 4622 	 5715 	 6168
wo 	p: 93.78 	r: 85.06 	f1: 89.21 	 2882 	 3073 	 3388
ni 	p: 83.11 	r: 74.05 	f1: 78.32 	 1127 	 1356 	 1522

[32m iter_1[0m
ga 	p: 80.99 	r: 74.64 	f1: 77.68 	 4604 	 5685 	 6168
wo 	p: 92.31 	r: 86.42 	f1: 89.27 	 2928 	 3172 	 3388
ni 	p: 79.01 	r: 77.4 	f1: 78.19 	 1178 	 1491 	 1522

[32m iter_2[0m
ga 	p: 80.27 	r: 75.32 	f1: 77.72 	 4646 	 5788 	 6168
wo 	p: 92.11 	r: 86.54 	f1: 89.24 	 2932 	 3183 	 3388
ni 	p: 84.23 	r: 73.0 	f1: 78.21 	 1111 	 1319 	 1522
best_thres [[0.52, 0.85, 0.24], [0.65, 0.82, 0.11], [0.53, 0.83, 0.26]]
f [0.8134, 0.8132, 0.8132]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 5 	 [0.49, 0.78, 0.16] 	 lr: 0.0002 	 f: 82.26298731946761
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 9 [0m
Train...

[32m iter_0[0m
ga 	p: 79.85 	r: 76.31 	f1: 78.04 	 4707 	 5895 	 6168
wo 	p: 92.3 	r: 85.98 	f1: 89.03 	 2913 	 3156 	 3388
ni 	p: 84.2 	r: 72.14 	f1: 77.71 	 1098 	 1304 	 1522

[32m iter_1[0m
ga 	p: 79.81 	r: 76.52 	f1: 78.13 	 4720 	 5914 	 6168
wo 	p: 92.34 	r: 86.07 	f1: 89.09 	 2916 	 3158 	 3388
ni 	p: 83.09 	r: 74.24 	f1: 78.42 	 1130 	 1360 	 1522

[32m iter_2[0m
ga 	p: 79.54 	r: 76.62 	f1: 78.05 	 4726 	 5942 	 6168
wo 	p: 92.2 	r: 86.19 	f1: 89.09 	 2920 	 3167 	 3388
ni 	p: 83.17 	r: 74.05 	f1: 78.35 	 1127 	 1355 	 1522
best_thres [[0.46, 0.47, 0.12], [0.55, 0.56, 0.08], [0.56, 0.56, 0.08]]
f [0.8135, 0.8143, 0.8144]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 5 	 [0.46, 0.45, 0.17] 	 lr: 0.0002 	 f: 82.28542914171658
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 9 [0m
Train...
loss: tensor(390.7694) lr: 0.0002 time: 1552.75
pred_count_train 41644

Test...
loss: tensor(403.9572) lr: 0.0002 time: 1656.82
pred_count_train 41644

Test...
loss: tensor(419.9448) lr: 0.0002 time: 1642.94
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.38 	r: 73.69 	f1: 77.79 	 4545 	 5517 	 6168
wo 	p: 93.21 	r: 85.09 	f1: 88.97 	 2883 	 3093 	 3388
ni 	p: 87.42 	r: 69.84 	f1: 77.65 	 1063 	 1216 	 1522

[32m iter_1[0m
ga 	p: 82.48 	r: 73.8 	f1: 77.9 	 4552 	 5519 	 6168
wo 	p: 92.07 	r: 86.01 	f1: 88.94 	 2914 	 3165 	 3388
ni 	p: 86.26 	r: 70.96 	f1: 77.87 	 1080 	 1252 	 1522

[32m iter_2[0m
ga 	p: 81.93 	r: 74.25 	f1: 77.9 	 4580 	 5590 	 6168
wo 	p: 91.99 	r: 86.1 	f1: 88.95 	 2917 	 3171 	 3388
ni 	p: 86.25 	r: 70.89 	f1: 77.82 	 1079 	 1251 	 1522
best_thres [[0.57, 0.81, 0.21], [0.57, 0.66, 0.17], [0.53, 0.65, 0.17]]
f [0.8124, 0.8129, 0.813]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 6 	 [0.42, 0.34, 0.11] 	 lr: 0.0002 	 f: 82.3507328442491
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 10 [0m
Train...

[32m iter_0[0m
ga 	p: 81.7 	r: 74.42 	f1: 77.89 	 4590 	 5618 	 6168
wo 	p: 92.4 	r: 85.42 	f1: 88.77 	 2894 	 3132 	 3388
ni 	p: 80.43 	r: 72.08 	f1: 76.02 	 1097 	 1364 	 1522

[32m iter_1[0m
ga 	p: 81.74 	r: 74.38 	f1: 77.89 	 4588 	 5613 	 6168
wo 	p: 92.69 	r: 85.39 	f1: 88.89 	 2893 	 3121 	 3388
ni 	p: 82.73 	r: 70.17 	f1: 75.93 	 1068 	 1291 	 1522

[32m iter_2[0m
ga 	p: 81.73 	r: 74.43 	f1: 77.91 	 4591 	 5617 	 6168
wo 	p: 92.69 	r: 85.39 	f1: 88.89 	 2893 	 3121 	 3388
ni 	p: 81.57 	r: 70.96 	f1: 75.9 	 1080 	 1324 	 1522
best_thres [[0.39, 0.42, 0.11], [0.41, 0.45, 0.15], [0.4, 0.45, 0.12]]
f [0.8098, 0.81, 0.8101]
load model: epoch5
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 5 	 [0.49, 0.78, 0.16] 	 lr: 0.0002 	 f: 82.26298731946761
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 10 [0m
Train...

[32m iter_0[0m
ga 	p: 80.92 	r: 75.76 	f1: 78.26 	 4673 	 5775 	 6168
wo 	p: 92.12 	r: 85.27 	f1: 88.57 	 2889 	 3136 	 3388
ni 	p: 88.21 	r: 71.29 	f1: 78.85 	 1085 	 1230 	 1522

[32m iter_1[0m
ga 	p: 81.11 	r: 75.81 	f1: 78.37 	 4676 	 5765 	 6168
wo 	p: 92.88 	r: 84.77 	f1: 88.64 	 2872 	 3092 	 3388
ni 	p: 87.07 	r: 72.14 	f1: 78.91 	 1098 	 1261 	 1522

[32m iter_2[0m
ga 	p: 80.99 	r: 75.91 	f1: 78.37 	 4682 	 5781 	 6168
wo 	p: 92.69 	r: 84.92 	f1: 88.63 	 2877 	 3104 	 3388
ni 	p: 88.12 	r: 71.62 	f1: 79.01 	 1090 	 1237 	 1522
best_thres [[0.29, 0.41, 0.15], [0.36, 0.64, 0.1], [0.37, 0.66, 0.12]]
f [0.815, 0.8154, 0.8156]
load model: epoch5
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 5 	 [0.46, 0.45, 0.17] 	 lr: 0.0002 	 f: 82.28542914171658
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 10 [0m
Train...
loss: tensor(359.6439) lr: 0.0002 time: 1499.52
pred_count_train 41644

Test...
loss: tensor(528.8675) lr: 0.0001 time: 1625.65
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.19 	r: 74.53 	f1: 77.72 	 4597 	 5662 	 6168
wo 	p: 91.92 	r: 85.6 	f1: 88.64 	 2900 	 3155 	 3388
ni 	p: 81.05 	r: 72.21 	f1: 76.37 	 1099 	 1356 	 1522

[32m iter_1[0m
ga 	p: 81.24 	r: 74.43 	f1: 77.69 	 4591 	 5651 	 6168
wo 	p: 92.07 	r: 85.71 	f1: 88.78 	 2904 	 3154 	 3388
ni 	p: 81.42 	r: 72.27 	f1: 76.58 	 1100 	 1351 	 1522

[32m iter_2[0m
ga 	p: 80.69 	r: 74.95 	f1: 77.72 	 4623 	 5729 	 6168
wo 	p: 92.08 	r: 85.77 	f1: 88.81 	 2906 	 3156 	 3388
ni 	p: 79.67 	r: 73.39 	f1: 76.4 	 1117 	 1402 	 1522
best_thres [[0.6, 0.64, 0.17], [0.61, 0.65, 0.15], [0.55, 0.65, 0.11]]
f [0.809, 0.8093, 0.8093]
load model: epoch6
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 6 	 [0.42, 0.34, 0.11] 	 lr: 0.0002 	 f: 82.3507328442491
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 11 [0m
Train...
loss: tensor(537.5540) lr: 0.0001 time: 1615.42
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.09 	r: 76.02 	f1: 78.94 	 4689 	 5712 	 6168
wo 	p: 93.42 	r: 86.33 	f1: 89.74 	 2925 	 3131 	 3388
ni 	p: 89.2 	r: 74.9 	f1: 81.43 	 1140 	 1278 	 1522

[32m iter_1[0m
ga 	p: 82.85 	r: 75.57 	f1: 79.04 	 4661 	 5626 	 6168
wo 	p: 93.51 	r: 86.39 	f1: 89.81 	 2927 	 3130 	 3388
ni 	p: 89.14 	r: 74.97 	f1: 81.44 	 1141 	 1280 	 1522

[32m iter_2[0m
ga 	p: 82.82 	r: 75.66 	f1: 79.08 	 4667 	 5635 	 6168
wo 	p: 93.6 	r: 86.36 	f1: 89.84 	 2926 	 3126 	 3388
ni 	p: 89.27 	r: 74.9 	f1: 81.46 	 1140 	 1277 	 1522
best_thres [[0.52, 0.57, 0.32], [0.68, 0.66, 0.3], [0.69, 0.69, 0.3]]
f [0.8259, 0.8264, 0.8266]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 10 	 [0.69, 0.69, 0.3] 	 lr: 0.0001 	 f: 82.66250453262704
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 11 [0m
Train...

[32m iter_0[0m
ga 	p: 82.92 	r: 75.34 	f1: 78.95 	 4647 	 5604 	 6168
wo 	p: 93.53 	r: 85.33 	f1: 89.24 	 2891 	 3091 	 3388
ni 	p: 85.26 	r: 77.92 	f1: 81.43 	 1186 	 1391 	 1522

[32m iter_1[0m
ga 	p: 83.27 	r: 75.03 	f1: 78.94 	 4628 	 5558 	 6168
wo 	p: 93.32 	r: 85.39 	f1: 89.18 	 2893 	 3100 	 3388
ni 	p: 85.7 	r: 77.6 	f1: 81.45 	 1181 	 1378 	 1522

[32m iter_2[0m
ga 	p: 83.23 	r: 75.02 	f1: 78.91 	 4627 	 5559 	 6168
wo 	p: 93.46 	r: 85.24 	f1: 89.16 	 2888 	 3090 	 3388
ni 	p: 85.3 	r: 77.79 	f1: 81.37 	 1184 	 1388 	 1522
best_thres [[0.46, 0.51, 0.15], [0.56, 0.58, 0.15], [0.58, 0.64, 0.14]]
f [0.8244, 0.8244, 0.8242]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 10 	 [0.58, 0.64, 0.14] 	 lr: 0.0001 	 f: 82.42234947076174
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 11 [0m
Train...
loss: tensor(379.1225) lr: 0.0001 time: 1447.38
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.51 	r: 75.23 	f1: 79.15 	 4640 	 5556 	 6168
wo 	p: 92.9 	r: 86.92 	f1: 89.81 	 2945 	 3170 	 3388
ni 	p: 87.65 	r: 75.56 	f1: 81.16 	 1150 	 1312 	 1522

[32m iter_1[0m
ga 	p: 82.66 	r: 75.91 	f1: 79.14 	 4682 	 5664 	 6168
wo 	p: 93.17 	r: 86.57 	f1: 89.75 	 2933 	 3148 	 3388
ni 	p: 85.34 	r: 77.27 	f1: 81.1 	 1176 	 1378 	 1522

[32m iter_2[0m
ga 	p: 82.7 	r: 75.97 	f1: 79.2 	 4686 	 5666 	 6168
wo 	p: 93.17 	r: 86.6 	f1: 89.77 	 2934 	 3149 	 3388
ni 	p: 85.28 	r: 77.27 	f1: 81.08 	 1176 	 1379 	 1522
best_thres [[0.54, 0.54, 0.17], [0.47, 0.59, 0.11], [0.47, 0.59, 0.11]]
f [0.8273, 0.827, 0.827]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 11 	 [0.47, 0.59, 0.11] 	 lr: 0.0001 	 f: 82.70076662058567
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 12 [0m
Train...
loss: tensor(294.9534) lr: 0.0001 time: 1632.6
pred_count_train 41644

Test...
loss: tensor(307.6218) lr: 0.0001 time: 1626.54
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.39 	r: 76.17 	f1: 78.22 	 4698 	 5844 	 6168
wo 	p: 93.82 	r: 85.63 	f1: 89.54 	 2901 	 3092 	 3388
ni 	p: 86.02 	r: 73.59 	f1: 79.32 	 1120 	 1302 	 1522

[32m iter_1[0m
ga 	p: 81.93 	r: 75.06 	f1: 78.35 	 4630 	 5651 	 6168
wo 	p: 93.8 	r: 85.77 	f1: 89.61 	 2906 	 3098 	 3388
ni 	p: 83.12 	r: 76.02 	f1: 79.41 	 1157 	 1392 	 1522

[32m iter_2[0m
ga 	p: 81.88 	r: 75.03 	f1: 78.31 	 4628 	 5652 	 6168
wo 	p: 93.5 	r: 86.13 	f1: 89.66 	 2918 	 3121 	 3388
ni 	p: 82.61 	r: 76.48 	f1: 79.43 	 1164 	 1409 	 1522
best_thres [[0.34, 0.72, 0.17], [0.48, 0.85, 0.09], [0.48, 0.81, 0.08]]
f [0.8181, 0.8187, 0.8189]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 10 	 [0.69, 0.69, 0.3] 	 lr: 0.0001 	 f: 82.66250453262704
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 12 [0m
Train...

[32m iter_0[0m
ga 	p: 83.18 	r: 74.42 	f1: 78.56 	 4590 	 5518 	 6168
wo 	p: 93.0 	r: 86.33 	f1: 89.55 	 2925 	 3145 	 3388
ni 	p: 87.08 	r: 74.38 	f1: 80.23 	 1132 	 1300 	 1522

[32m iter_1[0m
ga 	p: 82.56 	r: 74.77 	f1: 78.48 	 4612 	 5586 	 6168
wo 	p: 91.94 	r: 87.25 	f1: 89.54 	 2956 	 3215 	 3388
ni 	p: 85.81 	r: 75.89 	f1: 80.54 	 1155 	 1346 	 1522

[32m iter_2[0m
ga 	p: 82.23 	r: 75.1 	f1: 78.5 	 4632 	 5633 	 6168
wo 	p: 91.71 	r: 87.51 	f1: 89.56 	 2965 	 3233 	 3388
ni 	p: 86.39 	r: 75.49 	f1: 80.58 	 1149 	 1330 	 1522
best_thres [[0.43, 0.69, 0.4], [0.49, 0.67, 0.32], [0.49, 0.64, 0.35]]
f [0.8219, 0.8219, 0.822]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 10 	 [0.58, 0.64, 0.14] 	 lr: 0.0001 	 f: 82.42234947076174
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 12 [0m
Train...
loss: tensor(200.2000) lr: 0.0001 time: 1442.27
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.37 	r: 75.32 	f1: 79.14 	 4646 	 5573 	 6168
wo 	p: 93.27 	r: 86.72 	f1: 89.87 	 2938 	 3150 	 3388
ni 	p: 84.55 	r: 73.72 	f1: 78.76 	 1122 	 1327 	 1522

[32m iter_1[0m
ga 	p: 82.18 	r: 75.96 	f1: 78.95 	 4685 	 5701 	 6168
wo 	p: 92.88 	r: 87.01 	f1: 89.85 	 2948 	 3174 	 3388
ni 	p: 82.37 	r: 75.49 	f1: 78.78 	 1149 	 1395 	 1522

[32m iter_2[0m
ga 	p: 82.14 	r: 75.99 	f1: 78.95 	 4687 	 5706 	 6168
wo 	p: 93.41 	r: 86.57 	f1: 89.86 	 2933 	 3140 	 3388
ni 	p: 82.48 	r: 75.49 	f1: 78.83 	 1149 	 1393 	 1522
best_thres [[0.57, 0.75, 0.36], [0.48, 0.69, 0.23], [0.48, 0.79, 0.24]]
f [0.8241, 0.8234, 0.8232]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 11 	 [0.47, 0.59, 0.11] 	 lr: 0.0001 	 f: 82.70076662058567
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 13 [0m
Train...
loss: tensor(216.6532) lr: 0.0001 time: 1614.05
pred_count_train 41644

Test...
loss: tensor(221.4130) lr: 0.0001 time: 1611.45
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.34 	r: 76.44 	f1: 78.81 	 4715 	 5797 	 6168
wo 	p: 93.19 	r: 86.89 	f1: 89.93 	 2944 	 3159 	 3388
ni 	p: 80.19 	r: 76.87 	f1: 78.5 	 1170 	 1459 	 1522

[32m iter_1[0m
ga 	p: 82.16 	r: 75.32 	f1: 78.59 	 4646 	 5655 	 6168
wo 	p: 92.18 	r: 87.66 	f1: 89.86 	 2970 	 3222 	 3388
ni 	p: 82.69 	r: 75.03 	f1: 78.68 	 1142 	 1381 	 1522

[32m iter_2[0m
ga 	p: 82.18 	r: 75.37 	f1: 78.63 	 4649 	 5657 	 6168
wo 	p: 91.9 	r: 87.78 	f1: 89.79 	 2974 	 3236 	 3388
ni 	p: 82.75 	r: 75.03 	f1: 78.7 	 1142 	 1380 	 1522
best_thres [[0.27, 0.79, 0.15], [0.36, 0.83, 0.18], [0.36, 0.85, 0.18]]
f [0.8216, 0.8213, 0.8212]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 10 	 [0.69, 0.69, 0.3] 	 lr: 0.0001 	 f: 82.66250453262704
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 13 [0m
Train...

[32m iter_0[0m
ga 	p: 82.35 	r: 75.58 	f1: 78.82 	 4662 	 5661 	 6168
wo 	p: 94.65 	r: 85.57 	f1: 89.88 	 2899 	 3063 	 3388
ni 	p: 85.45 	r: 73.72 	f1: 79.15 	 1122 	 1313 	 1522

[32m iter_1[0m
ga 	p: 81.37 	r: 76.31 	f1: 78.76 	 4707 	 5785 	 6168
wo 	p: 93.61 	r: 86.42 	f1: 89.87 	 2928 	 3128 	 3388
ni 	p: 87.72 	r: 72.27 	f1: 79.25 	 1100 	 1254 	 1522

[32m iter_2[0m
ga 	p: 81.0 	r: 76.33 	f1: 78.6 	 4708 	 5812 	 6168
wo 	p: 93.27 	r: 86.66 	f1: 89.84 	 2936 	 3148 	 3388
ni 	p: 85.61 	r: 73.85 	f1: 79.29 	 1124 	 1313 	 1522
best_thres [[0.59, 0.79, 0.17], [0.67, 0.84, 0.23], [0.7, 0.84, 0.13]]
f [0.8224, 0.8224, 0.822]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 10 	 [0.58, 0.64, 0.14] 	 lr: 0.0001 	 f: 82.42234947076174
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 13 [0m
Train...
loss: tensor(152.6161) lr: 0.0001 time: 1441.68
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.3 	r: 75.92 	f1: 78.98 	 4683 	 5690 	 6168
wo 	p: 92.93 	r: 86.95 	f1: 89.84 	 2946 	 3170 	 3388
ni 	p: 85.9 	r: 72.47 	f1: 78.62 	 1103 	 1284 	 1522

[32m iter_1[0m
ga 	p: 83.0 	r: 75.54 	f1: 79.09 	 4659 	 5613 	 6168
wo 	p: 92.67 	r: 86.95 	f1: 89.72 	 2946 	 3179 	 3388
ni 	p: 85.57 	r: 72.47 	f1: 78.48 	 1103 	 1289 	 1522

[32m iter_2[0m
ga 	p: 83.0 	r: 75.57 	f1: 79.11 	 4661 	 5616 	 6168
wo 	p: 92.67 	r: 86.89 	f1: 89.69 	 2944 	 3177 	 3388
ni 	p: 85.85 	r: 72.54 	f1: 78.63 	 1104 	 1286 	 1522
best_thres [[0.48, 0.51, 0.12], [0.53, 0.47, 0.1], [0.53, 0.48, 0.1]]
f [0.8229, 0.823, 0.8231]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 11 	 [0.47, 0.59, 0.11] 	 lr: 0.0001 	 f: 82.70076662058567
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 14 [0m
Train...
loss: tensor(171.1344) lr: 0.0001 time: 1624.16
pred_count_train 41644

Test...
loss: tensor(178.8318) lr: 0.0001 time: 1618.63
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.43 	r: 75.86 	f1: 78.55 	 4679 	 5746 	 6168
wo 	p: 93.39 	r: 86.33 	f1: 89.72 	 2925 	 3132 	 3388
ni 	p: 87.47 	r: 72.01 	f1: 78.99 	 1096 	 1253 	 1522

[32m iter_1[0m
ga 	p: 81.3 	r: 76.01 	f1: 78.57 	 4688 	 5766 	 6168
wo 	p: 92.18 	r: 87.34 	f1: 89.69 	 2959 	 3210 	 3388
ni 	p: 86.79 	r: 73.39 	f1: 79.53 	 1117 	 1287 	 1522

[32m iter_2[0m
ga 	p: 81.35 	r: 76.04 	f1: 78.61 	 4690 	 5765 	 6168
wo 	p: 92.1 	r: 87.46 	f1: 89.72 	 2963 	 3217 	 3388
ni 	p: 86.99 	r: 73.39 	f1: 79.62 	 1117 	 1284 	 1522
best_thres [[0.21, 0.79, 0.05], [0.19, 0.73, 0.03], [0.19, 0.8, 0.03]]
f [0.8204, 0.8209, 0.8212]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 10 	 [0.69, 0.69, 0.3] 	 lr: 0.0001 	 f: 82.66250453262704
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 14 [0m
Train...
loss: tensor(122.3368) lr: 0.0001 time: 1433.82
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.1 	r: 74.51 	f1: 78.57 	 4596 	 5531 	 6168
wo 	p: 93.07 	r: 86.36 	f1: 89.59 	 2926 	 3144 	 3388
ni 	p: 85.67 	r: 74.24 	f1: 79.55 	 1130 	 1319 	 1522

[32m iter_1[0m
ga 	p: 81.37 	r: 76.28 	f1: 78.74 	 4705 	 5782 	 6168
wo 	p: 91.97 	r: 87.22 	f1: 89.53 	 2955 	 3213 	 3388
ni 	p: 86.23 	r: 74.05 	f1: 79.67 	 1127 	 1307 	 1522

[32m iter_2[0m
ga 	p: 81.96 	r: 75.92 	f1: 78.83 	 4683 	 5714 	 6168
wo 	p: 91.81 	r: 87.4 	f1: 89.55 	 2961 	 3225 	 3388
ni 	p: 85.7 	r: 74.44 	f1: 79.68 	 1133 	 1322 	 1522
best_thres [[0.58, 0.52, 0.06], [0.52, 0.4, 0.05], [0.68, 0.39, 0.04]]
f [0.8212, 0.8216, 0.8219]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 10 	 [0.58, 0.64, 0.14] 	 lr: 0.0001 	 f: 82.42234947076174
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 14 [0m
Train...

[32m iter_0[0m
ga 	p: 82.14 	r: 75.16 	f1: 78.5 	 4636 	 5644 	 6168
wo 	p: 93.1 	r: 86.81 	f1: 89.84 	 2941 	 3159 	 3388
ni 	p: 84.5 	r: 73.78 	f1: 78.78 	 1123 	 1329 	 1522

[32m iter_1[0m
ga 	p: 81.81 	r: 75.47 	f1: 78.51 	 4655 	 5690 	 6168
wo 	p: 93.05 	r: 86.6 	f1: 89.71 	 2934 	 3153 	 3388
ni 	p: 84.88 	r: 73.78 	f1: 78.95 	 1123 	 1323 	 1522

[32m iter_2[0m
ga 	p: 81.66 	r: 75.6 	f1: 78.51 	 4663 	 5710 	 6168
wo 	p: 92.92 	r: 86.75 	f1: 89.73 	 2939 	 3163 	 3388
ni 	p: 86.16 	r: 72.8 	f1: 78.92 	 1108 	 1286 	 1522
best_thres [[0.36, 0.78, 0.1], [0.33, 0.85, 0.09], [0.31, 0.79, 0.13]]
f [0.8204, 0.8203, 0.8203]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 11 	 [0.47, 0.59, 0.11] 	 lr: 0.0001 	 f: 82.70076662058567
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 15 [0m
Train...
loss: tensor(146.2945) lr: 0.0001 time: 1611.9
pred_count_train 41644

Test...
loss: tensor(154.7829) lr: 0.0001 time: 1604.84
pred_count_train 41644

Test...
loss: tensor(106.5939) lr: 0.0001 time: 1500.54
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.96 	r: 73.83 	f1: 78.57 	 4554 	 5424 	 6168
wo 	p: 91.99 	r: 87.1 	f1: 89.48 	 2951 	 3208 	 3388
ni 	p: 84.92 	r: 74.38 	f1: 79.3 	 1132 	 1333 	 1522

[32m iter_1[0m
ga 	p: 81.61 	r: 75.55 	f1: 78.46 	 4660 	 5710 	 6168
wo 	p: 91.02 	r: 87.63 	f1: 89.29 	 2969 	 3262 	 3388
ni 	p: 86.75 	r: 73.59 	f1: 79.63 	 1120 	 1291 	 1522

[32m iter_2[0m
ga 	p: 81.54 	r: 75.49 	f1: 78.4 	 4656 	 5710 	 6168
wo 	p: 90.69 	r: 87.66 	f1: 89.15 	 2970 	 3275 	 3388
ni 	p: 86.58 	r: 73.78 	f1: 79.67 	 1123 	 1297 	 1522
best_thres [[0.65, 0.77, 0.15], [0.43, 0.85, 0.17], [0.43, 0.84, 0.16]]
f [0.8209, 0.8204, 0.82]
load model: epoch10
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 10 	 [0.69, 0.69, 0.3] 	 lr: 0.0001 	 f: 82.66250453262704
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 15 [0m
Train...

[32m iter_0[0m
ga 	p: 82.41 	r: 75.13 	f1: 78.6 	 4634 	 5623 	 6168
wo 	p: 94.08 	r: 84.95 	f1: 89.28 	 2878 	 3059 	 3388
ni 	p: 86.18 	r: 70.04 	f1: 77.27 	 1066 	 1237 	 1522

[32m iter_1[0m
ga 	p: 81.4 	r: 76.22 	f1: 78.72 	 4701 	 5775 	 6168
wo 	p: 93.79 	r: 85.18 	f1: 89.28 	 2886 	 3077 	 3388
ni 	p: 86.87 	r: 70.43 	f1: 77.79 	 1072 	 1234 	 1522

[32m iter_2[0m
ga 	p: 80.14 	r: 77.09 	f1: 78.59 	 4755 	 5933 	 6168
wo 	p: 93.31 	r: 85.6 	f1: 89.29 	 2900 	 3108 	 3388
ni 	p: 88.78 	r: 69.65 	f1: 78.06 	 1060 	 1194 	 1522
best_thres [[0.61, 0.58, 0.16], [0.7, 0.74, 0.15], [0.59, 0.72, 0.26]]
f [0.8171, 0.8177, 0.8177]
load model: epoch10
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 10 	 [0.58, 0.64, 0.14] 	 lr: 0.0001 	 f: 82.42234947076174
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 15 [0m
Train...

[32m iter_0[0m
ga 	p: 81.7 	r: 75.63 	f1: 78.55 	 4665 	 5710 	 6168
wo 	p: 92.44 	r: 87.31 	f1: 89.8 	 2958 	 3200 	 3388
ni 	p: 88.15 	r: 73.32 	f1: 80.06 	 1116 	 1266 	 1522

[32m iter_1[0m
ga 	p: 81.73 	r: 75.57 	f1: 78.53 	 4661 	 5703 	 6168
wo 	p: 92.3 	r: 87.75 	f1: 89.97 	 2973 	 3221 	 3388
ni 	p: 86.8 	r: 74.31 	f1: 80.07 	 1131 	 1303 	 1522

[32m iter_2[0m
ga 	p: 81.74 	r: 75.6 	f1: 78.55 	 4663 	 5705 	 6168
wo 	p: 92.3 	r: 87.75 	f1: 89.97 	 2973 	 3221 	 3388
ni 	p: 87.06 	r: 74.24 	f1: 80.14 	 1130 	 1298 	 1522
best_thres [[0.42, 0.72, 0.1], [0.44, 0.64, 0.05], [0.44, 0.64, 0.05]]
f [0.8223, 0.8226, 0.8227]
load model: epoch11
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 11 	 [0.47, 0.59, 0.11] 	 lr: 0.0001 	 f: 82.70076662058567
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 16 [0m
Train...
loss: tensor(231.4104) lr: 5e-05 time: 1619.18
pred_count_train 41644

Test...
loss: tensor(239.5210) lr: 5e-05 time: 1640.0
pred_count_train 41644

Test...
loss: tensor(147.2597) lr: 5e-05 time: 1536.43
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.58 	r: 74.51 	f1: 78.79 	 4596 	 5499 	 6168
wo 	p: 93.05 	r: 86.95 	f1: 89.9 	 2946 	 3166 	 3388
ni 	p: 82.9 	r: 75.82 	f1: 79.2 	 1154 	 1392 	 1522

[32m iter_1[0m
ga 	p: 82.59 	r: 75.47 	f1: 78.87 	 4655 	 5636 	 6168
wo 	p: 93.14 	r: 86.89 	f1: 89.91 	 2944 	 3161 	 3388
ni 	p: 83.07 	r: 76.08 	f1: 79.42 	 1158 	 1394 	 1522

[32m iter_2[0m
ga 	p: 83.93 	r: 74.37 	f1: 78.86 	 4587 	 5465 	 6168
wo 	p: 93.32 	r: 86.66 	f1: 89.87 	 2936 	 3146 	 3388
ni 	p: 83.59 	r: 75.95 	f1: 79.59 	 1156 	 1383 	 1522
best_thres [[0.46, 0.57, 0.11], [0.37, 0.72, 0.1], [0.58, 0.79, 0.1]]
f [0.8229, 0.8232, 0.8234]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 10 	 [0.69, 0.69, 0.3] 	 lr: 0.0001 	 f: 82.66250453262704
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 16 [0m
Train...

[32m iter_0[0m
ga 	p: 82.61 	r: 75.63 	f1: 78.97 	 4665 	 5647 	 6168
wo 	p: 92.72 	r: 86.89 	f1: 89.72 	 2944 	 3175 	 3388
ni 	p: 84.64 	r: 75.69 	f1: 79.92 	 1152 	 1361 	 1522

[32m iter_1[0m
ga 	p: 83.38 	r: 75.02 	f1: 78.98 	 4627 	 5549 	 6168
wo 	p: 92.88 	r: 87.04 	f1: 89.87 	 2949 	 3175 	 3388
ni 	p: 85.46 	r: 75.69 	f1: 80.28 	 1152 	 1348 	 1522

[32m iter_2[0m
ga 	p: 82.79 	r: 75.73 	f1: 79.1 	 4671 	 5642 	 6168
wo 	p: 92.8 	r: 87.16 	f1: 89.89 	 2953 	 3182 	 3388
ni 	p: 85.58 	r: 75.62 	f1: 80.29 	 1151 	 1345 	 1522
best_thres [[0.46, 0.59, 0.1], [0.66, 0.74, 0.09], [0.65, 0.77, 0.09]]
f [0.8241, 0.8247, 0.8252]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 15 	 [0.65, 0.77, 0.09] 	 lr: 5e-05 	 f: 82.51594457884319
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 16 [0m
Train...

[32m iter_0[0m
ga 	p: 83.08 	r: 75.47 	f1: 79.09 	 4655 	 5603 	 6168
wo 	p: 92.59 	r: 87.75 	f1: 90.1 	 2973 	 3211 	 3388
ni 	p: 87.93 	r: 75.62 	f1: 81.31 	 1151 	 1309 	 1522

[32m iter_1[0m
ga 	p: 82.86 	r: 75.65 	f1: 79.09 	 4666 	 5631 	 6168
wo 	p: 92.57 	r: 87.49 	f1: 89.95 	 2964 	 3202 	 3388
ni 	p: 86.95 	r: 76.15 	f1: 81.19 	 1159 	 1333 	 1522

[32m iter_2[0m
ga 	p: 83.19 	r: 75.42 	f1: 79.12 	 4652 	 5592 	 6168
wo 	p: 93.53 	r: 86.63 	f1: 89.95 	 2935 	 3138 	 3388
ni 	p: 86.88 	r: 76.15 	f1: 81.16 	 1159 	 1334 	 1522
best_thres [[0.33, 0.41, 0.15], [0.32, 0.44, 0.11], [0.35, 0.63, 0.11]]
f [0.8282, 0.8278, 0.8277]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 16 	 [0.35, 0.63, 0.11] 	 lr: 5e-05 	 f: 82.76534511771274
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 17 [0m
Train...
loss: tensor(113.5945) lr: 5e-05 time: 1640.75
pred_count_train 41644

Test...
loss: tensor(122.8458) lr: 5e-05 time: 1652.42
pred_count_train 41644

Test...
loss: tensor(71.3890) lr: 5e-05 time: 1578.24
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.61 	r: 74.84 	f1: 78.53 	 4616 	 5588 	 6168
wo 	p: 92.51 	r: 87.51 	f1: 89.94 	 2965 	 3205 	 3388
ni 	p: 84.1 	r: 75.43 	f1: 79.53 	 1148 	 1365 	 1522

[32m iter_1[0m
ga 	p: 82.01 	r: 75.39 	f1: 78.56 	 4650 	 5670 	 6168
wo 	p: 92.44 	r: 87.37 	f1: 89.83 	 2960 	 3202 	 3388
ni 	p: 84.79 	r: 75.1 	f1: 79.65 	 1143 	 1348 	 1522

[32m iter_2[0m
ga 	p: 81.92 	r: 75.45 	f1: 78.56 	 4654 	 5681 	 6168
wo 	p: 92.59 	r: 87.4 	f1: 89.92 	 2961 	 3198 	 3388
ni 	p: 85.15 	r: 74.97 	f1: 79.73 	 1141 	 1340 	 1522
best_thres [[0.28, 0.37, 0.08], [0.23, 0.35, 0.08], [0.22, 0.36, 0.08]]
f [0.8221, 0.822, 0.8221]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 10 	 [0.69, 0.69, 0.3] 	 lr: 0.0001 	 f: 82.66250453262704
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 17 [0m
Train...

[32m iter_0[0m
ga 	p: 83.05 	r: 74.98 	f1: 78.81 	 4625 	 5569 	 6168
wo 	p: 92.26 	r: 86.95 	f1: 89.53 	 2946 	 3193 	 3388
ni 	p: 84.71 	r: 74.64 	f1: 79.36 	 1136 	 1341 	 1522

[32m iter_1[0m
ga 	p: 82.49 	r: 75.29 	f1: 78.73 	 4644 	 5630 	 6168
wo 	p: 91.88 	r: 87.51 	f1: 89.64 	 2965 	 3227 	 3388
ni 	p: 88.54 	r: 72.6 	f1: 79.78 	 1105 	 1248 	 1522

[32m iter_2[0m
ga 	p: 83.68 	r: 74.59 	f1: 78.88 	 4601 	 5498 	 6168
wo 	p: 92.52 	r: 86.92 	f1: 89.64 	 2945 	 3183 	 3388
ni 	p: 88.69 	r: 72.67 	f1: 79.88 	 1106 	 1247 	 1522
best_thres [[0.34, 0.48, 0.09], [0.39, 0.49, 0.16], [0.65, 0.7, 0.15]]
f [0.8222, 0.8224, 0.8229]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 15 	 [0.65, 0.77, 0.09] 	 lr: 5e-05 	 f: 82.51594457884319
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 17 [0m
Train...

[32m iter_0[0m
ga 	p: 82.46 	r: 75.99 	f1: 79.09 	 4687 	 5684 	 6168
wo 	p: 93.41 	r: 86.19 	f1: 89.65 	 2920 	 3126 	 3388
ni 	p: 88.17 	r: 72.93 	f1: 79.83 	 1110 	 1259 	 1522

[32m iter_1[0m
ga 	p: 82.32 	r: 76.07 	f1: 79.07 	 4692 	 5700 	 6168
wo 	p: 93.55 	r: 85.98 	f1: 89.6 	 2913 	 3114 	 3388
ni 	p: 86.27 	r: 74.7 	f1: 80.07 	 1137 	 1318 	 1522

[32m iter_2[0m
ga 	p: 82.71 	r: 75.79 	f1: 79.1 	 4675 	 5652 	 6168
wo 	p: 93.55 	r: 85.98 	f1: 89.6 	 2913 	 3114 	 3388
ni 	p: 86.33 	r: 74.7 	f1: 80.1 	 1137 	 1317 	 1522
best_thres [[0.42, 0.75, 0.14], [0.42, 0.8, 0.07], [0.47, 0.8, 0.07]]
f [0.8244, 0.8244, 0.8245]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 16 	 [0.35, 0.63, 0.11] 	 lr: 5e-05 	 f: 82.76534511771274
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 18 [0m
Train...
loss: tensor(84.0625) lr: 5e-05 time: 1627.65
pred_count_train 41644

Test...
loss: tensor(87.5651) lr: 5e-05 time: 1675.55
pred_count_train 41644

Test...
loss: tensor(52.6445) lr: 5e-05 time: 1611.38
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.25 	r: 76.39 	f1: 78.27 	 4712 	 5872 	 6168
wo 	p: 92.92 	r: 86.72 	f1: 89.71 	 2938 	 3162 	 3388
ni 	p: 83.91 	r: 71.94 	f1: 77.47 	 1095 	 1305 	 1522

[32m iter_1[0m
ga 	p: 80.16 	r: 76.44 	f1: 78.26 	 4715 	 5882 	 6168
wo 	p: 92.63 	r: 86.78 	f1: 89.61 	 2940 	 3174 	 3388
ni 	p: 85.7 	r: 70.89 	f1: 77.6 	 1079 	 1259 	 1522

[32m iter_2[0m
ga 	p: 80.25 	r: 76.46 	f1: 78.31 	 4716 	 5877 	 6168
wo 	p: 92.34 	r: 87.13 	f1: 89.66 	 2952 	 3197 	 3388
ni 	p: 85.37 	r: 71.29 	f1: 77.69 	 1085 	 1271 	 1522
best_thres [[0.21, 0.68, 0.09], [0.2, 0.85, 0.09], [0.2, 0.8, 0.08]]
f [0.8166, 0.8166, 0.8168]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 10 	 [0.69, 0.69, 0.3] 	 lr: 0.0001 	 f: 82.66250453262704
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 18 [0m
Train...

[32m iter_0[0m
ga 	p: 81.67 	r: 76.26 	f1: 78.87 	 4704 	 5760 	 6168
wo 	p: 91.77 	r: 87.51 	f1: 89.59 	 2965 	 3231 	 3388
ni 	p: 88.44 	r: 71.35 	f1: 78.98 	 1086 	 1228 	 1522

[32m iter_1[0m
ga 	p: 82.11 	r: 75.84 	f1: 78.85 	 4678 	 5697 	 6168
wo 	p: 92.19 	r: 87.43 	f1: 89.74 	 2962 	 3213 	 3388
ni 	p: 86.5 	r: 73.26 	f1: 79.33 	 1115 	 1289 	 1522

[32m iter_2[0m
ga 	p: 81.46 	r: 76.38 	f1: 78.84 	 4711 	 5783 	 6168
wo 	p: 91.83 	r: 87.6 	f1: 89.67 	 2968 	 3232 	 3388
ni 	p: 86.95 	r: 73.13 	f1: 79.44 	 1113 	 1280 	 1522
best_thres [[0.24, 0.41, 0.13], [0.43, 0.64, 0.06], [0.37, 0.63, 0.06]]
f [0.8222, 0.8226, 0.8226]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 15 	 [0.65, 0.77, 0.09] 	 lr: 5e-05 	 f: 82.51594457884319
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 18 [0m
Train...

[32m iter_0[0m
ga 	p: 83.34 	r: 74.87 	f1: 78.88 	 4618 	 5541 	 6168
wo 	p: 92.82 	r: 86.6 	f1: 89.6 	 2934 	 3161 	 3388
ni 	p: 87.03 	r: 75.36 	f1: 80.77 	 1147 	 1318 	 1522

[32m iter_1[0m
ga 	p: 83.38 	r: 75.06 	f1: 79.0 	 4630 	 5553 	 6168
wo 	p: 93.08 	r: 86.6 	f1: 89.72 	 2934 	 3152 	 3388
ni 	p: 87.36 	r: 74.9 	f1: 80.65 	 1140 	 1305 	 1522

[32m iter_2[0m
ga 	p: 83.38 	r: 75.05 	f1: 78.99 	 4629 	 5552 	 6168
wo 	p: 92.91 	r: 86.69 	f1: 89.69 	 2937 	 3161 	 3388
ni 	p: 87.42 	r: 74.9 	f1: 80.68 	 1140 	 1304 	 1522
best_thres [[0.43, 0.55, 0.1], [0.42, 0.61, 0.09], [0.42, 0.58, 0.09]]
f [0.8246, 0.8251, 0.8252]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 16 	 [0.35, 0.63, 0.11] 	 lr: 5e-05 	 f: 82.76534511771274
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 19 [0m
Train...
loss: tensor(60.8948) lr: 5e-05 time: 1640.05
pred_count_train 41644

Test...
loss: tensor(69.6387) lr: 5e-05 time: 1666.65
pred_count_train 41644

Test...
loss: tensor(41.8380) lr: 5e-05 time: 1626.95
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.6 	r: 76.09 	f1: 78.75 	 4693 	 5751 	 6168
wo 	p: 92.68 	r: 86.72 	f1: 89.6 	 2938 	 3170 	 3388
ni 	p: 86.23 	r: 72.8 	f1: 78.95 	 1108 	 1285 	 1522

[32m iter_1[0m
ga 	p: 82.11 	r: 75.45 	f1: 78.64 	 4654 	 5668 	 6168
wo 	p: 92.98 	r: 86.36 	f1: 89.55 	 2926 	 3147 	 3388
ni 	p: 83.86 	r: 75.1 	f1: 79.24 	 1143 	 1363 	 1522

[32m iter_2[0m
ga 	p: 82.15 	r: 75.44 	f1: 78.65 	 4653 	 5664 	 6168
wo 	p: 92.92 	r: 86.39 	f1: 89.54 	 2927 	 3150 	 3388
ni 	p: 85.98 	r: 73.72 	f1: 79.38 	 1122 	 1305 	 1522
best_thres [[0.23, 0.48, 0.14], [0.29, 0.71, 0.06], [0.29, 0.75, 0.09]]
f [0.8212, 0.821, 0.821]
load model: epoch10
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 10 	 [0.69, 0.69, 0.3] 	 lr: 0.0001 	 f: 82.66250453262704
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 19 [0m
Train...

[32m iter_0[0m
ga 	p: 81.31 	r: 76.05 	f1: 78.6 	 4691 	 5769 	 6168
wo 	p: 93.42 	r: 85.92 	f1: 89.51 	 2911 	 3116 	 3388
ni 	p: 86.88 	r: 73.98 	f1: 79.91 	 1126 	 1296 	 1522

[32m iter_1[0m
ga 	p: 83.85 	r: 73.9 	f1: 78.56 	 4558 	 5436 	 6168
wo 	p: 92.43 	r: 86.84 	f1: 89.54 	 2942 	 3183 	 3388
ni 	p: 86.34 	r: 75.16 	f1: 80.37 	 1144 	 1325 	 1522

[32m iter_2[0m
ga 	p: 83.36 	r: 74.4 	f1: 78.63 	 4589 	 5505 	 6168
wo 	p: 92.22 	r: 87.1 	f1: 89.59 	 2951 	 3200 	 3388
ni 	p: 86.26 	r: 75.1 	f1: 80.3 	 1143 	 1325 	 1522
best_thres [[0.16, 0.79, 0.05], [0.6, 0.79, 0.03], [0.61, 0.84, 0.03]]
f [0.8211, 0.8217, 0.8221]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 15 	 [0.65, 0.77, 0.09] 	 lr: 5e-05 	 f: 82.51594457884319
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 19 [0m
Train...

[32m iter_0[0m
ga 	p: 82.46 	r: 75.83 	f1: 79.0 	 4677 	 5672 	 6168
wo 	p: 92.02 	r: 87.46 	f1: 89.68 	 2963 	 3220 	 3388
ni 	p: 89.15 	r: 72.86 	f1: 80.19 	 1109 	 1244 	 1522

[32m iter_1[0m
ga 	p: 82.78 	r: 75.54 	f1: 78.99 	 4659 	 5628 	 6168
wo 	p: 93.46 	r: 86.04 	f1: 89.6 	 2915 	 3119 	 3388
ni 	p: 89.13 	r: 73.26 	f1: 80.42 	 1115 	 1251 	 1522

[32m iter_2[0m
ga 	p: 82.41 	r: 75.79 	f1: 78.96 	 4675 	 5673 	 6168
wo 	p: 93.46 	r: 86.04 	f1: 89.6 	 2915 	 3119 	 3388
ni 	p: 89.29 	r: 73.39 	f1: 80.56 	 1117 	 1251 	 1522
best_thres [[0.47, 0.39, 0.1], [0.53, 0.71, 0.07], [0.49, 0.69, 0.07]]
f [0.8248, 0.8247, 0.8246]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 16 	 [0.35, 0.63, 0.11] 	 lr: 5e-05 	 f: 82.76534511771274
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 20 [0m
Train...
loss: tensor(215.5505) lr: 2.5e-05 time: 1609.43
pred_count_train 41644

Test...
loss: tensor(50.1553) lr: 5e-05 time: 1681.08
pred_count_train 41644

Test...
loss: tensor(33.2942) lr: 5e-05 time: 1663.48
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.19 	r: 75.34 	f1: 79.07 	 4647 	 5586 	 6168
wo 	p: 92.63 	r: 87.57 	f1: 90.03 	 2967 	 3203 	 3388
ni 	p: 83.6 	r: 76.68 	f1: 79.99 	 1167 	 1396 	 1522

[32m iter_1[0m
ga 	p: 82.96 	r: 75.47 	f1: 79.04 	 4655 	 5611 	 6168
wo 	p: 92.42 	r: 87.75 	f1: 90.02 	 2973 	 3217 	 3388
ni 	p: 84.1 	r: 77.14 	f1: 80.47 	 1174 	 1396 	 1522

[32m iter_2[0m
ga 	p: 82.1 	r: 76.05 	f1: 78.96 	 4691 	 5714 	 6168
wo 	p: 93.32 	r: 86.95 	f1: 90.02 	 2946 	 3157 	 3388
ni 	p: 84.28 	r: 77.14 	f1: 80.55 	 1174 	 1393 	 1522
best_thres [[0.43, 0.54, 0.09], [0.43, 0.51, 0.08], [0.36, 0.81, 0.08]]
f [0.8259, 0.8262, 0.826]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 10 	 [0.69, 0.69, 0.3] 	 lr: 0.0001 	 f: 82.66250453262704
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 20 [0m
Train...

[32m iter_0[0m
ga 	p: 83.31 	r: 74.76 	f1: 78.8 	 4611 	 5535 	 6168
wo 	p: 93.53 	r: 86.13 	f1: 89.67 	 2918 	 3120 	 3388
ni 	p: 86.76 	r: 72.34 	f1: 78.9 	 1101 	 1269 	 1522

[32m iter_1[0m
ga 	p: 83.03 	r: 74.95 	f1: 78.78 	 4623 	 5568 	 6168
wo 	p: 92.74 	r: 86.69 	f1: 89.61 	 2937 	 3167 	 3388
ni 	p: 86.82 	r: 72.73 	f1: 79.16 	 1107 	 1275 	 1522

[32m iter_2[0m
ga 	p: 82.98 	r: 74.94 	f1: 78.75 	 4622 	 5570 	 6168
wo 	p: 92.24 	r: 86.98 	f1: 89.53 	 2947 	 3195 	 3388
ni 	p: 87.36 	r: 72.67 	f1: 79.34 	 1106 	 1266 	 1522
best_thres [[0.36, 0.76, 0.1], [0.55, 0.84, 0.07], [0.69, 0.84, 0.07]]
f [0.8218, 0.8219, 0.8219]
load model: epoch15
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 15 	 [0.65, 0.77, 0.09] 	 lr: 5e-05 	 f: 82.51594457884319
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 20 [0m
Train...

[32m iter_0[0m
ga 	p: 82.94 	r: 75.36 	f1: 78.97 	 4648 	 5604 	 6168
wo 	p: 93.88 	r: 85.98 	f1: 89.76 	 2913 	 3103 	 3388
ni 	p: 86.51 	r: 73.72 	f1: 79.6 	 1122 	 1297 	 1522

[32m iter_1[0m
ga 	p: 83.26 	r: 74.98 	f1: 78.9 	 4625 	 5555 	 6168
wo 	p: 93.44 	r: 86.57 	f1: 89.87 	 2933 	 3139 	 3388
ni 	p: 87.17 	r: 73.65 	f1: 79.84 	 1121 	 1286 	 1522

[32m iter_2[0m
ga 	p: 83.33 	r: 74.9 	f1: 78.89 	 4620 	 5544 	 6168
wo 	p: 93.55 	r: 86.45 	f1: 89.86 	 2929 	 3131 	 3388
ni 	p: 87.09 	r: 73.59 	f1: 79.77 	 1120 	 1286 	 1522
best_thres [[0.6, 0.78, 0.08], [0.65, 0.64, 0.07], [0.67, 0.67, 0.07]]
f [0.8237, 0.824, 0.824]
load model: epoch16
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 16 	 [0.35, 0.63, 0.11] 	 lr: 5e-05 	 f: 82.76534511771274
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 21 [0m
Train...
loss: tensor(109.2781) lr: 2.5e-05 time: 1620.82
pred_count_train 41644

Test...
loss: tensor(107.0453) lr: 2.5e-05 time: 1655.95
pred_count_train 41644

Test...
loss: tensor(59.1232) lr: 2.5e-05 time: 1634.41
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.6 	r: 74.14 	f1: 78.59 	 4573 	 5470 	 6168
wo 	p: 92.46 	r: 87.25 	f1: 89.78 	 2956 	 3197 	 3388
ni 	p: 82.08 	r: 76.15 	f1: 79.0 	 1159 	 1412 	 1522

[32m iter_1[0m
ga 	p: 83.08 	r: 74.51 	f1: 78.56 	 4596 	 5532 	 6168
wo 	p: 92.54 	r: 87.1 	f1: 89.74 	 2951 	 3189 	 3388
ni 	p: 82.85 	r: 76.48 	f1: 79.54 	 1164 	 1405 	 1522

[32m iter_2[0m
ga 	p: 82.82 	r: 74.63 	f1: 78.51 	 4603 	 5558 	 6168
wo 	p: 92.65 	r: 87.01 	f1: 89.74 	 2948 	 3182 	 3388
ni 	p: 83.23 	r: 76.61 	f1: 79.78 	 1166 	 1401 	 1522
best_thres [[0.56, 0.46, 0.07], [0.67, 0.54, 0.06], [0.68, 0.67, 0.06]]
f [0.8213, 0.8215, 0.8215]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 10 	 [0.69, 0.69, 0.3] 	 lr: 0.0001 	 f: 82.66250453262704
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 21 [0m
Train...

[32m iter_0[0m
ga 	p: 82.29 	r: 76.01 	f1: 79.02 	 4688 	 5697 	 6168
wo 	p: 93.77 	r: 85.71 	f1: 89.56 	 2904 	 3097 	 3388
ni 	p: 87.66 	r: 72.34 	f1: 79.27 	 1101 	 1256 	 1522

[32m iter_1[0m
ga 	p: 80.73 	r: 77.16 	f1: 78.9 	 4759 	 5895 	 6168
wo 	p: 91.99 	r: 87.16 	f1: 89.51 	 2953 	 3210 	 3388
ni 	p: 88.77 	r: 71.68 	f1: 79.32 	 1091 	 1229 	 1522

[32m iter_2[0m
ga 	p: 80.78 	r: 77.04 	f1: 78.86 	 4752 	 5883 	 6168
wo 	p: 91.94 	r: 87.25 	f1: 89.54 	 2956 	 3215 	 3388
ni 	p: 88.93 	r: 71.75 	f1: 79.42 	 1092 	 1228 	 1522
best_thres [[0.36, 0.75, 0.18], [0.29, 0.58, 0.21], [0.33, 0.61, 0.21]]
f [0.8229, 0.8226, 0.8225]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 15 	 [0.65, 0.77, 0.09] 	 lr: 5e-05 	 f: 82.51594457884319
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 21 [0m
Train...

[32m iter_0[0m
ga 	p: 83.78 	r: 75.11 	f1: 79.21 	 4633 	 5530 	 6168
wo 	p: 93.19 	r: 86.78 	f1: 89.87 	 2940 	 3155 	 3388
ni 	p: 86.25 	r: 76.22 	f1: 80.92 	 1160 	 1345 	 1522

[32m iter_1[0m
ga 	p: 82.84 	r: 75.91 	f1: 79.22 	 4682 	 5652 	 6168
wo 	p: 92.81 	r: 86.89 	f1: 89.76 	 2944 	 3172 	 3388
ni 	p: 85.68 	r: 76.68 	f1: 80.93 	 1167 	 1362 	 1522

[32m iter_2[0m
ga 	p: 83.47 	r: 75.41 	f1: 79.23 	 4651 	 5572 	 6168
wo 	p: 92.84 	r: 86.84 	f1: 89.74 	 2942 	 3169 	 3388
ni 	p: 87.07 	r: 75.62 	f1: 80.94 	 1151 	 1322 	 1522
best_thres [[0.65, 0.68, 0.07], [0.54, 0.59, 0.05], [0.61, 0.59, 0.07]]
f [0.8275, 0.8272, 0.8272]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 16 	 [0.35, 0.63, 0.11] 	 lr: 5e-05 	 f: 82.76534511771274
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 22 [0m
Train...
loss: tensor(64.6184) lr: 2.5e-05 time: 1576.27
pred_count_train 41644

Test...
loss: tensor(52.9940) lr: 2.5e-05 time: 1658.79
pred_count_train 41644

Test...
loss: tensor(25.4596) lr: 2.5e-05 time: 1657.0
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.74 	r: 76.65 	f1: 78.64 	 4728 	 5856 	 6168
wo 	p: 91.96 	r: 87.72 	f1: 89.79 	 2972 	 3232 	 3388
ni 	p: 85.91 	r: 74.11 	f1: 79.58 	 1128 	 1313 	 1522

[32m iter_1[0m
ga 	p: 82.67 	r: 75.02 	f1: 78.66 	 4627 	 5597 	 6168
wo 	p: 92.12 	r: 87.69 	f1: 89.85 	 2971 	 3225 	 3388
ni 	p: 85.68 	r: 74.31 	f1: 79.59 	 1131 	 1320 	 1522

[32m iter_2[0m
ga 	p: 82.67 	r: 75.02 	f1: 78.66 	 4627 	 5597 	 6168
wo 	p: 92.1 	r: 87.72 	f1: 89.86 	 2972 	 3227 	 3388
ni 	p: 86.07 	r: 74.31 	f1: 79.76 	 1131 	 1314 	 1522
best_thres [[0.24, 0.53, 0.1], [0.47, 0.73, 0.08], [0.47, 0.77, 0.08]]
f [0.822, 0.8224, 0.8226]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 10 	 [0.69, 0.69, 0.3] 	 lr: 0.0001 	 f: 82.66250453262704
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 22 [0m
Train...

[32m iter_0[0m
ga 	p: 81.85 	r: 76.13 	f1: 78.89 	 4696 	 5737 	 6168
wo 	p: 92.5 	r: 86.6 	f1: 89.45 	 2934 	 3172 	 3388
ni 	p: 86.03 	r: 75.23 	f1: 80.27 	 1145 	 1331 	 1522

[32m iter_1[0m
ga 	p: 82.58 	r: 75.62 	f1: 78.94 	 4664 	 5648 	 6168
wo 	p: 91.28 	r: 87.49 	f1: 89.34 	 2964 	 3247 	 3388
ni 	p: 87.2 	r: 75.16 	f1: 80.73 	 1144 	 1312 	 1522

[32m iter_2[0m
ga 	p: 82.04 	r: 76.01 	f1: 78.91 	 4688 	 5714 	 6168
wo 	p: 91.36 	r: 87.37 	f1: 89.32 	 2960 	 3240 	 3388
ni 	p: 86.49 	r: 75.69 	f1: 80.73 	 1152 	 1332 	 1522
best_thres [[0.38, 0.61, 0.12], [0.67, 0.5, 0.12], [0.7, 0.6, 0.09]]
f [0.8232, 0.8237, 0.8238]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 15 	 [0.65, 0.77, 0.09] 	 lr: 5e-05 	 f: 82.51594457884319
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 22 [0m
Train...

[32m iter_0[0m
ga 	p: 82.14 	r: 76.41 	f1: 79.17 	 4713 	 5738 	 6168
wo 	p: 92.6 	r: 87.16 	f1: 89.8 	 2953 	 3189 	 3388
ni 	p: 88.39 	r: 74.51 	f1: 80.86 	 1134 	 1283 	 1522

[32m iter_1[0m
ga 	p: 82.63 	r: 76.28 	f1: 79.33 	 4705 	 5694 	 6168
wo 	p: 93.99 	r: 85.89 	f1: 89.76 	 2910 	 3096 	 3388
ni 	p: 86.52 	r: 75.49 	f1: 80.63 	 1149 	 1328 	 1522

[32m iter_2[0m
ga 	p: 82.69 	r: 76.26 	f1: 79.35 	 4704 	 5689 	 6168
wo 	p: 94.05 	r: 85.86 	f1: 89.77 	 2909 	 3093 	 3388
ni 	p: 88.6 	r: 74.05 	f1: 80.67 	 1127 	 1272 	 1522
best_thres [[0.32, 0.47, 0.09], [0.36, 0.74, 0.05], [0.36, 0.77, 0.09]]
f [0.8268, 0.8269, 0.827]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 16 	 [0.35, 0.63, 0.11] 	 lr: 5e-05 	 f: 82.76534511771274
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 23 [0m
Train...
loss: tensor(43.8652) lr: 2.5e-05 time: 1557.9
pred_count_train 41644

Test...
loss: tensor(39.1265) lr: 2.5e-05 time: 1628.43
pred_count_train 41644

Test...
loss: tensor(24.0757) lr: 2.5e-05 time: 1623.26
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.5 	r: 75.92 	f1: 78.61 	 4683 	 5746 	 6168
wo 	p: 93.83 	r: 86.13 	f1: 89.81 	 2918 	 3110 	 3388
ni 	p: 86.2 	r: 74.31 	f1: 79.82 	 1131 	 1312 	 1522

[32m iter_1[0m
ga 	p: 79.99 	r: 77.01 	f1: 78.47 	 4750 	 5938 	 6168
wo 	p: 93.09 	r: 86.66 	f1: 89.76 	 2936 	 3154 	 3388
ni 	p: 81.6 	r: 78.65 	f1: 80.09 	 1197 	 1467 	 1522

[32m iter_2[0m
ga 	p: 79.95 	r: 76.93 	f1: 78.41 	 4745 	 5935 	 6168
wo 	p: 93.01 	r: 86.78 	f1: 89.78 	 2940 	 3161 	 3388
ni 	p: 86.16 	r: 75.23 	f1: 80.32 	 1145 	 1329 	 1522
best_thres [[0.31, 0.7, 0.08], [0.17, 0.79, 0.02], [0.17, 0.83, 0.05]]
f [0.822, 0.8215, 0.8215]
load model: epoch10
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 10 	 [0.69, 0.69, 0.3] 	 lr: 0.0001 	 f: 82.66250453262704
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 23 [0m
Train...

[32m iter_0[0m
ga 	p: 82.53 	r: 76.65 	f1: 79.48 	 4728 	 5729 	 6168
wo 	p: 92.51 	r: 87.51 	f1: 89.94 	 2965 	 3205 	 3388
ni 	p: 87.6 	r: 75.16 	f1: 80.91 	 1144 	 1306 	 1522

[32m iter_1[0m
ga 	p: 82.67 	r: 76.56 	f1: 79.49 	 4722 	 5712 	 6168
wo 	p: 93.01 	r: 87.16 	f1: 89.99 	 2953 	 3175 	 3388
ni 	p: 86.06 	r: 76.28 	f1: 80.88 	 1161 	 1349 	 1522

[32m iter_2[0m
ga 	p: 82.73 	r: 76.64 	f1: 79.57 	 4727 	 5714 	 6168
wo 	p: 93.04 	r: 87.19 	f1: 90.02 	 2954 	 3175 	 3388
ni 	p: 85.8 	r: 76.22 	f1: 80.72 	 1160 	 1352 	 1522
best_thres [[0.27, 0.63, 0.14], [0.28, 0.74, 0.08], [0.28, 0.74, 0.08]]
f [0.8291, 0.8291, 0.8292]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 23 	 [0.28, 0.74, 0.08] 	 lr: 2.5e-05 	 f: 82.9197354224328
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 24 [0m
Train...

[32m iter_0[0m
ga 	p: 83.86 	r: 74.11 	f1: 78.68 	 4571 	 5451 	 6168
wo 	p: 93.85 	r: 85.63 	f1: 89.55 	 2901 	 3091 	 3388
ni 	p: 89.48 	r: 72.08 	f1: 79.84 	 1097 	 1226 	 1522

[32m iter_1[0m
ga 	p: 80.12 	r: 77.32 	f1: 78.7 	 4769 	 5952 	 6168
wo 	p: 93.7 	r: 85.66 	f1: 89.5 	 2902 	 3097 	 3388
ni 	p: 88.2 	r: 73.65 	f1: 80.27 	 1121 	 1271 	 1522

[32m iter_2[0m
ga 	p: 81.5 	r: 76.07 	f1: 78.69 	 4692 	 5757 	 6168
wo 	p: 93.26 	r: 85.83 	f1: 89.39 	 2908 	 3118 	 3388
ni 	p: 88.45 	r: 73.46 	f1: 80.26 	 1118 	 1264 	 1522
best_thres [[0.5, 0.63, 0.16], [0.14, 0.81, 0.08], [0.43, 0.83, 0.08]]
f [0.8221, 0.8219, 0.8219]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 15 	 [0.65, 0.77, 0.09] 	 lr: 5e-05 	 f: 82.51594457884319
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 23 [0m
Train...
loss: tensor(216.1125) lr: 1.25e-05 time: 1522.96
pred_count_train 41644

Test...
loss: tensor(29.3264) lr: 2.5e-05 time: 1632.04
pred_count_train 41644

Test...
loss: tensor(15.0861) lr: 2.5e-05 time: 1636.08
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.35 	r: 75.1 	f1: 79.01 	 4632 	 5557 	 6168
wo 	p: 94.16 	r: 86.13 	f1: 89.96 	 2918 	 3099 	 3388
ni 	p: 83.79 	r: 77.07 	f1: 80.29 	 1173 	 1400 	 1522

[32m iter_1[0m
ga 	p: 84.07 	r: 74.51 	f1: 79.0 	 4596 	 5467 	 6168
wo 	p: 93.99 	r: 86.33 	f1: 90.0 	 2925 	 3112 	 3388
ni 	p: 84.18 	r: 77.27 	f1: 80.58 	 1176 	 1397 	 1522

[32m iter_2[0m
ga 	p: 83.77 	r: 74.72 	f1: 78.99 	 4609 	 5502 	 6168
wo 	p: 93.94 	r: 86.42 	f1: 90.02 	 2928 	 3117 	 3388
ni 	p: 85.61 	r: 76.22 	f1: 80.64 	 1160 	 1355 	 1522
best_thres [[0.51, 0.74, 0.12], [0.7, 0.83, 0.11], [0.7, 0.84, 0.14]]
f [0.8255, 0.8258, 0.826]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 10 	 [0.69, 0.69, 0.3] 	 lr: 0.0001 	 f: 82.66250453262704
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 24 [0m
Train...

[32m iter_0[0m
ga 	p: 83.3 	r: 75.13 	f1: 79.0 	 4634 	 5563 	 6168
wo 	p: 93.74 	r: 86.22 	f1: 89.82 	 2921 	 3116 	 3388
ni 	p: 88.76 	r: 72.67 	f1: 79.91 	 1106 	 1246 	 1522

[32m iter_1[0m
ga 	p: 82.38 	r: 75.97 	f1: 79.05 	 4686 	 5688 	 6168
wo 	p: 93.68 	r: 86.19 	f1: 89.78 	 2920 	 3117 	 3388
ni 	p: 86.8 	r: 73.85 	f1: 79.8 	 1124 	 1295 	 1522

[32m iter_2[0m
ga 	p: 82.38 	r: 75.96 	f1: 79.04 	 4685 	 5687 	 6168
wo 	p: 93.71 	r: 86.19 	f1: 89.79 	 2920 	 3116 	 3388
ni 	p: 86.98 	r: 73.72 	f1: 79.8 	 1122 	 1290 	 1522
best_thres [[0.49, 0.77, 0.12], [0.38, 0.79, 0.06], [0.38, 0.79, 0.06]]
f [0.8247, 0.8246, 0.8245]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 23 	 [0.28, 0.74, 0.08] 	 lr: 2.5e-05 	 f: 82.9197354224328
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 25 [0m
Train...

[32m iter_0[0m
ga 	p: 82.1 	r: 76.23 	f1: 79.06 	 4702 	 5727 	 6168
wo 	p: 92.81 	r: 86.42 	f1: 89.5 	 2928 	 3155 	 3388
ni 	p: 87.29 	r: 73.52 	f1: 79.81 	 1119 	 1282 	 1522

[32m iter_1[0m
ga 	p: 81.8 	r: 76.67 	f1: 79.15 	 4729 	 5781 	 6168
wo 	p: 92.28 	r: 87.13 	f1: 89.63 	 2952 	 3199 	 3388
ni 	p: 85.92 	r: 74.97 	f1: 80.07 	 1141 	 1328 	 1522

[32m iter_2[0m
ga 	p: 82.16 	r: 76.51 	f1: 79.23 	 4719 	 5744 	 6168
wo 	p: 92.42 	r: 87.07 	f1: 89.67 	 2950 	 3192 	 3388
ni 	p: 85.21 	r: 75.69 	f1: 80.17 	 1152 	 1352 	 1522
best_thres [[0.25, 0.6, 0.17], [0.25, 0.66, 0.09], [0.42, 0.75, 0.06]]
f [0.8237, 0.8244, 0.8248]
load model: epoch15
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 15 	 [0.65, 0.77, 0.09] 	 lr: 5e-05 	 f: 82.51594457884319
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 24 [0m
Train...
loss: tensor(136.1157) lr: 1.25e-05 time: 1455.41
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.64 	r: 74.04 	f1: 78.55 	 4567 	 5460 	 6168
wo 	p: 92.57 	r: 87.49 	f1: 89.95 	 2964 	 3202 	 3388
ni 	p: 83.31 	r: 75.76 	f1: 79.35 	 1153 	 1384 	 1522

[32m iter_1[0m
ga 	p: 79.09 	r: 77.98 	f1: 78.53 	 4810 	 6082 	 6168
wo 	p: 92.71 	r: 87.43 	f1: 89.99 	 2962 	 3195 	 3388
ni 	p: 83.06 	r: 76.68 	f1: 79.74 	 1167 	 1405 	 1522

[32m iter_2[0m
ga 	p: 80.46 	r: 76.65 	f1: 78.51 	 4728 	 5876 	 6168
wo 	p: 92.65 	r: 87.46 	f1: 89.98 	 2963 	 3198 	 3388
ni 	p: 85.4 	r: 74.97 	f1: 79.85 	 1141 	 1336 	 1522
best_thres [[0.55, 0.48, 0.09], [0.17, 0.51, 0.07], [0.28, 0.49, 0.1]]
f [0.8222, 0.8219, 0.8219]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 10 	 [0.69, 0.69, 0.3] 	 lr: 0.0001 	 f: 82.66250453262704
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 25 [0m
Train...
loss: tensor(15.6217) lr: 2.5e-05 time: 1614.58
pred_count_train 41644

Test...
loss: tensor(102.4998) lr: 1.25e-05 time: 1609.04
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.33 	r: 76.43 	f1: 79.27 	 4714 	 5726 	 6168
wo 	p: 92.57 	r: 87.49 	f1: 89.95 	 2964 	 3202 	 3388
ni 	p: 85.01 	r: 74.9 	f1: 79.64 	 1140 	 1341 	 1522

[32m iter_1[0m
ga 	p: 82.23 	r: 76.61 	f1: 79.32 	 4725 	 5746 	 6168
wo 	p: 92.46 	r: 87.57 	f1: 89.95 	 2967 	 3209 	 3388
ni 	p: 88.06 	r: 73.19 	f1: 79.94 	 1114 	 1265 	 1522

[32m iter_2[0m
ga 	p: 82.23 	r: 76.75 	f1: 79.4 	 4734 	 5757 	 6168
wo 	p: 92.54 	r: 87.51 	f1: 89.96 	 2965 	 3204 	 3388
ni 	p: 86.83 	r: 74.05 	f1: 79.93 	 1127 	 1298 	 1522
best_thres [[0.28, 0.52, 0.04], [0.25, 0.45, 0.06], [0.24, 0.48, 0.04]]
f [0.8262, 0.8265, 0.8268]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 23 	 [0.28, 0.74, 0.08] 	 lr: 2.5e-05 	 f: 82.9197354224328
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 26 [0m
Train...

[32m iter_0[0m
ga 	p: 81.15 	r: 77.11 	f1: 79.08 	 4756 	 5861 	 6168
wo 	p: 92.95 	r: 86.33 	f1: 89.52 	 2925 	 3147 	 3388
ni 	p: 90.28 	r: 72.6 	f1: 80.48 	 1105 	 1224 	 1522

[32m iter_1[0m
ga 	p: 81.73 	r: 76.49 	f1: 79.02 	 4718 	 5773 	 6168
wo 	p: 92.21 	r: 87.04 	f1: 89.55 	 2949 	 3198 	 3388
ni 	p: 90.22 	r: 73.32 	f1: 80.9 	 1116 	 1237 	 1522

[32m iter_2[0m
ga 	p: 82.51 	r: 75.89 	f1: 79.06 	 4681 	 5673 	 6168
wo 	p: 92.09 	r: 87.25 	f1: 89.6 	 2956 	 3210 	 3388
ni 	p: 90.41 	r: 73.13 	f1: 80.86 	 1113 	 1231 	 1522
best_thres [[0.24, 0.5, 0.24], [0.37, 0.41, 0.19], [0.5, 0.37, 0.21]]
f [0.8246, 0.8249, 0.8252]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 24 	 [0.5, 0.37, 0.21] 	 lr: 1.25e-05 	 f: 82.52022323948077
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 25 [0m
Train...
loss: tensor(88.6021) lr: 1.25e-05 time: 1464.16
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 84.13 	r: 73.77 	f1: 78.61 	 4550 	 5408 	 6168
wo 	p: 92.51 	r: 87.54 	f1: 89.96 	 2966 	 3206 	 3388
ni 	p: 84.47 	r: 76.48 	f1: 80.28 	 1164 	 1378 	 1522

[32m iter_1[0m
ga 	p: 81.85 	r: 75.58 	f1: 78.59 	 4662 	 5696 	 6168
wo 	p: 92.62 	r: 87.49 	f1: 89.98 	 2964 	 3200 	 3388
ni 	p: 80.37 	r: 80.42 	f1: 80.39 	 1224 	 1523 	 1522

[32m iter_2[0m
ga 	p: 81.94 	r: 75.62 	f1: 78.65 	 4664 	 5692 	 6168
wo 	p: 92.85 	r: 87.37 	f1: 90.02 	 2960 	 3188 	 3388
ni 	p: 80.63 	r: 80.42 	f1: 80.53 	 1224 	 1518 	 1522
best_thres [[0.58, 0.48, 0.08], [0.37, 0.5, 0.03], [0.37, 0.65, 0.03]]
f [0.8239, 0.8236, 0.8238]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 10 	 [0.69, 0.69, 0.3] 	 lr: 0.0001 	 f: 82.66250453262704
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 26 [0m
Train...
loss: tensor(9.4639) lr: 2.5e-05 time: 1595.47
pred_count_train 41644

Test...
loss: tensor(57.1288) lr: 1.25e-05 time: 1588.93
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.11 	r: 75.78 	f1: 79.27 	 4674 	 5624 	 6168
wo 	p: 93.11 	r: 86.51 	f1: 89.69 	 2931 	 3148 	 3388
ni 	p: 87.27 	r: 75.23 	f1: 80.8 	 1145 	 1312 	 1522

[32m iter_1[0m
ga 	p: 82.59 	r: 76.3 	f1: 79.32 	 4706 	 5698 	 6168
wo 	p: 93.13 	r: 86.42 	f1: 89.65 	 2928 	 3144 	 3388
ni 	p: 85.1 	r: 77.33 	f1: 81.03 	 1177 	 1383 	 1522

[32m iter_2[0m
ga 	p: 82.58 	r: 76.25 	f1: 79.29 	 4703 	 5695 	 6168
wo 	p: 92.23 	r: 87.22 	f1: 89.65 	 2955 	 3204 	 3388
ni 	p: 85.16 	r: 77.27 	f1: 81.02 	 1176 	 1381 	 1522
best_thres [[0.41, 0.63, 0.06], [0.31, 0.66, 0.02], [0.31, 0.38, 0.02]]
f [0.827, 0.8271, 0.8271]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 23 	 [0.28, 0.74, 0.08] 	 lr: 2.5e-05 	 f: 82.9197354224328
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 27 [0m
Train...

[32m iter_0[0m
ga 	p: 82.2 	r: 76.38 	f1: 79.18 	 4711 	 5731 	 6168
wo 	p: 93.52 	r: 86.01 	f1: 89.61 	 2914 	 3116 	 3388
ni 	p: 87.69 	r: 73.46 	f1: 79.94 	 1118 	 1275 	 1522

[32m iter_1[0m
ga 	p: 81.86 	r: 76.61 	f1: 79.15 	 4725 	 5772 	 6168
wo 	p: 92.88 	r: 86.63 	f1: 89.65 	 2935 	 3160 	 3388
ni 	p: 86.01 	r: 75.56 	f1: 80.45 	 1150 	 1337 	 1522

[32m iter_2[0m
ga 	p: 81.5 	r: 76.91 	f1: 79.14 	 4744 	 5821 	 6168
wo 	p: 92.79 	r: 86.63 	f1: 89.6 	 2935 	 3163 	 3388
ni 	p: 86.22 	r: 75.62 	f1: 80.57 	 1151 	 1335 	 1522
best_thres [[0.33, 0.66, 0.16], [0.42, 0.7, 0.08], [0.41, 0.75, 0.07]]
f [0.8248, 0.8251, 0.8252]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 24 	 [0.5, 0.37, 0.21] 	 lr: 1.25e-05 	 f: 82.52022323948077
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 26 [0m
Train...
loss: tensor(56.5554) lr: 1.25e-05 time: 1427.28
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.4 	r: 76.82 	f1: 78.57 	 4738 	 5893 	 6168
wo 	p: 92.7 	r: 87.28 	f1: 89.91 	 2957 	 3190 	 3388
ni 	p: 81.17 	r: 77.33 	f1: 79.21 	 1177 	 1450 	 1522

[32m iter_1[0m
ga 	p: 80.31 	r: 76.93 	f1: 78.59 	 4745 	 5908 	 6168
wo 	p: 92.12 	r: 87.93 	f1: 89.97 	 2979 	 3234 	 3388
ni 	p: 83.23 	r: 76.61 	f1: 79.78 	 1166 	 1401 	 1522

[32m iter_2[0m
ga 	p: 79.91 	r: 77.33 	f1: 78.6 	 4770 	 5969 	 6168
wo 	p: 92.06 	r: 87.9 	f1: 89.93 	 2978 	 3235 	 3388
ni 	p: 82.49 	r: 77.4 	f1: 79.86 	 1178 	 1428 	 1522
best_thres [[0.21, 0.49, 0.05], [0.19, 0.3, 0.05], [0.16, 0.3, 0.04]]
f [0.8211, 0.8217, 0.8219]
load model: epoch10
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 10 	 [0.69, 0.69, 0.3] 	 lr: 0.0001 	 f: 82.66250453262704
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 27 [0m
Train...
loss: tensor(9.9507) lr: 2.5e-05 time: 1632.45
pred_count_train 41644

Test...
loss: tensor(36.1476) lr: 1.25e-05 time: 1633.5
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.07 	r: 77.03 	f1: 79.47 	 4751 	 5789 	 6168
wo 	p: 92.76 	r: 86.95 	f1: 89.76 	 2946 	 3176 	 3388
ni 	p: 88.49 	r: 73.78 	f1: 80.47 	 1123 	 1269 	 1522

[32m iter_1[0m
ga 	p: 83.44 	r: 75.79 	f1: 79.43 	 4675 	 5603 	 6168
wo 	p: 93.28 	r: 86.48 	f1: 89.75 	 2930 	 3141 	 3388
ni 	p: 87.84 	r: 74.51 	f1: 80.63 	 1134 	 1291 	 1522

[32m iter_2[0m
ga 	p: 83.32 	r: 75.91 	f1: 79.44 	 4682 	 5619 	 6168
wo 	p: 93.31 	r: 86.48 	f1: 89.77 	 2930 	 3140 	 3388
ni 	p: 87.9 	r: 74.44 	f1: 80.61 	 1133 	 1289 	 1522
best_thres [[0.23, 0.44, 0.09], [0.44, 0.51, 0.05], [0.42, 0.51, 0.05]]
f [0.8277, 0.8278, 0.8278]
load model: epoch23
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 23 	 [0.28, 0.74, 0.08] 	 lr: 2.5e-05 	 f: 82.9197354224328
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 28 [0m
Train...

[32m iter_0[0m
ga 	p: 84.31 	r: 74.16 	f1: 78.91 	 4574 	 5425 	 6168
wo 	p: 92.75 	r: 86.48 	f1: 89.51 	 2930 	 3159 	 3388
ni 	p: 87.27 	r: 73.0 	f1: 79.5 	 1111 	 1273 	 1522

[32m iter_1[0m
ga 	p: 81.07 	r: 76.85 	f1: 78.9 	 4740 	 5847 	 6168
wo 	p: 92.85 	r: 86.57 	f1: 89.6 	 2933 	 3159 	 3388
ni 	p: 86.99 	r: 73.78 	f1: 79.84 	 1123 	 1291 	 1522

[32m iter_2[0m
ga 	p: 82.77 	r: 75.39 	f1: 78.91 	 4650 	 5618 	 6168
wo 	p: 92.98 	r: 86.36 	f1: 89.55 	 2926 	 3147 	 3388
ni 	p: 90.36 	r: 71.42 	f1: 79.78 	 1087 	 1203 	 1522
best_thres [[0.54, 0.54, 0.09], [0.22, 0.68, 0.06], [0.66, 0.8, 0.13]]
f [0.823, 0.823, 0.8231]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 24 	 [0.5, 0.37, 0.21] 	 lr: 1.25e-05 	 f: 82.52022323948077
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 27 [0m
Train...
loss: tensor(217.9685) lr: 1e-05 time: 1429.58
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.1 	r: 75.44 	f1: 79.09 	 4653 	 5599 	 6168
wo 	p: 93.65 	r: 86.63 	f1: 90.0 	 2935 	 3134 	 3388
ni 	p: 82.75 	r: 78.19 	f1: 80.41 	 1190 	 1438 	 1522

[32m iter_1[0m
ga 	p: 83.54 	r: 75.13 	f1: 79.11 	 4634 	 5547 	 6168
wo 	p: 94.46 	r: 86.04 	f1: 90.05 	 2915 	 3086 	 3388
ni 	p: 84.05 	r: 77.92 	f1: 80.87 	 1186 	 1411 	 1522

[32m iter_2[0m
ga 	p: 83.84 	r: 74.79 	f1: 79.06 	 4613 	 5502 	 6168
wo 	p: 94.22 	r: 86.13 	f1: 89.99 	 2918 	 3097 	 3388
ni 	p: 84.15 	r: 77.79 	f1: 80.85 	 1184 	 1407 	 1522
best_thres [[0.47, 0.63, 0.09], [0.58, 0.85, 0.09], [0.69, 0.85, 0.09]]
f [0.8262, 0.8267, 0.8267]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 27 	 [0.69, 0.85, 0.09] 	 lr: 1e-05 	 f: 82.66645654400756
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 28 [0m
Train...
loss: tensor(13.4413) lr: 1.25e-05 time: 1576.03
pred_count_train 41644

Test...
loss: tensor(24.3645) lr: 1.25e-05 time: 1570.25
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.02 	r: 76.86 	f1: 79.36 	 4741 	 5780 	 6168
wo 	p: 92.27 	r: 87.72 	f1: 89.94 	 2972 	 3221 	 3388
ni 	p: 88.1 	r: 73.46 	f1: 80.11 	 1118 	 1269 	 1522

[32m iter_1[0m
ga 	p: 83.02 	r: 76.12 	f1: 79.42 	 4695 	 5655 	 6168
wo 	p: 92.64 	r: 87.25 	f1: 89.86 	 2956 	 3191 	 3388
ni 	p: 86.64 	r: 74.57 	f1: 80.16 	 1135 	 1310 	 1522

[32m iter_2[0m
ga 	p: 82.92 	r: 76.2 	f1: 79.42 	 4700 	 5668 	 6168
wo 	p: 92.33 	r: 87.46 	f1: 89.83 	 2963 	 3209 	 3388
ni 	p: 86.71 	r: 74.57 	f1: 80.18 	 1135 	 1309 	 1522
best_thres [[0.21, 0.39, 0.1], [0.33, 0.47, 0.05], [0.32, 0.4, 0.05]]
f [0.8273, 0.8274, 0.8275]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 23 	 [0.28, 0.74, 0.08] 	 lr: 2.5e-05 	 f: 82.9197354224328
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 29 [0m
Train...

[32m iter_0[0m
ga 	p: 82.46 	r: 75.52 	f1: 78.84 	 4658 	 5649 	 6168
wo 	p: 93.5 	r: 85.83 	f1: 89.5 	 2908 	 3110 	 3388
ni 	p: 87.97 	r: 73.0 	f1: 79.78 	 1111 	 1263 	 1522

[32m iter_1[0m
ga 	p: 81.3 	r: 76.52 	f1: 78.84 	 4720 	 5806 	 6168
wo 	p: 93.15 	r: 86.25 	f1: 89.56 	 2922 	 3137 	 3388
ni 	p: 88.99 	r: 72.73 	f1: 80.04 	 1107 	 1244 	 1522

[32m iter_2[0m
ga 	p: 81.21 	r: 76.44 	f1: 78.75 	 4715 	 5806 	 6168
wo 	p: 92.4 	r: 86.87 	f1: 89.55 	 2943 	 3185 	 3388
ni 	p: 84.23 	r: 76.48 	f1: 80.17 	 1164 	 1382 	 1522
best_thres [[0.29, 0.68, 0.15], [0.21, 0.79, 0.16], [0.2, 0.68, 0.04]]
f [0.8225, 0.8227, 0.8226]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 24 	 [0.5, 0.37, 0.21] 	 lr: 1.25e-05 	 f: 82.52022323948077
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 28 [0m
Train...
loss: tensor(145.6451) lr: 1e-05 time: 1466.35
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.71 	r: 74.58 	f1: 78.88 	 4600 	 5495 	 6168
wo 	p: 93.8 	r: 86.57 	f1: 90.04 	 2933 	 3127 	 3388
ni 	p: 85.42 	r: 76.22 	f1: 80.56 	 1160 	 1358 	 1522

[32m iter_1[0m
ga 	p: 83.5 	r: 74.72 	f1: 78.87 	 4609 	 5520 	 6168
wo 	p: 92.55 	r: 87.57 	f1: 89.99 	 2967 	 3206 	 3388
ni 	p: 83.66 	r: 78.06 	f1: 80.76 	 1188 	 1420 	 1522

[32m iter_2[0m
ga 	p: 82.61 	r: 75.42 	f1: 78.85 	 4652 	 5631 	 6168
wo 	p: 92.51 	r: 87.54 	f1: 89.96 	 2966 	 3206 	 3388
ni 	p: 83.95 	r: 77.99 	f1: 80.86 	 1187 	 1414 	 1522
best_thres [[0.54, 0.67, 0.13], [0.65, 0.48, 0.08], [0.47, 0.48, 0.08]]
f [0.8256, 0.8257, 0.8257]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 27 	 [0.69, 0.85, 0.09] 	 lr: 1e-05 	 f: 82.66645654400756
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 29 [0m
Train...
loss: tensor(4.9376) lr: 1.25e-05 time: 1567.68
pred_count_train 41644

Test...
loss: tensor(18.4901) lr: 1.25e-05 time: 1576.68
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.94 	r: 76.73 	f1: 79.25 	 4733 	 5776 	 6168
wo 	p: 93.41 	r: 86.66 	f1: 89.91 	 2936 	 3143 	 3388
ni 	p: 87.88 	r: 74.77 	f1: 80.8 	 1138 	 1295 	 1522

[32m iter_1[0m
ga 	p: 83.65 	r: 75.39 	f1: 79.3 	 4650 	 5559 	 6168
wo 	p: 93.22 	r: 86.87 	f1: 89.93 	 2943 	 3157 	 3388
ni 	p: 87.55 	r: 74.84 	f1: 80.69 	 1139 	 1301 	 1522

[32m iter_2[0m
ga 	p: 83.7 	r: 75.44 	f1: 79.36 	 4653 	 5559 	 6168
wo 	p: 93.25 	r: 86.84 	f1: 89.93 	 2942 	 3155 	 3388
ni 	p: 87.81 	r: 74.77 	f1: 80.77 	 1138 	 1296 	 1522
best_thres [[0.26, 0.64, 0.07], [0.5, 0.61, 0.05], [0.5, 0.62, 0.05]]
f [0.8273, 0.8276, 0.8278]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 23 	 [0.28, 0.74, 0.08] 	 lr: 2.5e-05 	 f: 82.9197354224328
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 30 [0m
Train...
loss: tensor(100.7614) lr: 1e-05 time: 1442.04
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.27 	r: 76.56 	f1: 78.84 	 4722 	 5810 	 6168
wo 	p: 93.69 	r: 85.39 	f1: 89.35 	 2893 	 3088 	 3388
ni 	p: 87.68 	r: 73.39 	f1: 79.9 	 1117 	 1274 	 1522

[32m iter_1[0m
ga 	p: 81.57 	r: 76.13 	f1: 78.76 	 4696 	 5757 	 6168
wo 	p: 91.26 	r: 87.57 	f1: 89.38 	 2967 	 3251 	 3388
ni 	p: 85.64 	r: 74.84 	f1: 79.87 	 1139 	 1330 	 1522

[32m iter_2[0m
ga 	p: 81.43 	r: 76.26 	f1: 78.76 	 4704 	 5777 	 6168
wo 	p: 91.26 	r: 87.57 	f1: 89.38 	 2967 	 3251 	 3388
ni 	p: 86.16 	r: 74.84 	f1: 80.1 	 1139 	 1322 	 1522
best_thres [[0.14, 0.81, 0.13], [0.17, 0.38, 0.06], [0.15, 0.46, 0.06]]
f [0.8218, 0.8219, 0.822]
load model: epoch24
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 24 	 [0.5, 0.37, 0.21] 	 lr: 1.25e-05 	 f: 82.52022323948077
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 29 [0m
Train...

[32m iter_0[0m
ga 	p: 80.52 	r: 76.65 	f1: 78.54 	 4728 	 5872 	 6168
wo 	p: 92.25 	r: 87.78 	f1: 89.96 	 2974 	 3224 	 3388
ni 	p: 84.2 	r: 76.35 	f1: 80.08 	 1162 	 1380 	 1522

[32m iter_1[0m
ga 	p: 80.22 	r: 77.11 	f1: 78.63 	 4756 	 5929 	 6168
wo 	p: 93.14 	r: 86.98 	f1: 89.96 	 2947 	 3164 	 3388
ni 	p: 82.72 	r: 77.99 	f1: 80.28 	 1187 	 1435 	 1522

[32m iter_2[0m
ga 	p: 80.29 	r: 77.14 	f1: 78.68 	 4758 	 5926 	 6168
wo 	p: 93.28 	r: 86.84 	f1: 89.94 	 2942 	 3154 	 3388
ni 	p: 83.63 	r: 77.2 	f1: 80.29 	 1175 	 1405 	 1522
best_thres [[0.25, 0.38, 0.11], [0.22, 0.64, 0.07], [0.22, 0.75, 0.08]]
f [0.8225, 0.8227, 0.8229]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 27 	 [0.69, 0.85, 0.09] 	 lr: 1e-05 	 f: 82.66645654400756
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 30 [0m
Train...
loss: tensor(5.2841) lr: 1.25e-05 time: 1585.38
pred_count_train 41644

Test...
loss: tensor(56.3032) lr: 1e-05 time: 1576.79
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.63 	r: 75.11 	f1: 79.14 	 4633 	 5540 	 6168
wo 	p: 92.47 	r: 87.34 	f1: 89.83 	 2959 	 3200 	 3388
ni 	p: 90.31 	r: 72.27 	f1: 80.29 	 1100 	 1218 	 1522

[32m iter_1[0m
ga 	p: 82.19 	r: 76.23 	f1: 79.1 	 4702 	 5721 	 6168
wo 	p: 93.32 	r: 86.57 	f1: 89.82 	 2933 	 3143 	 3388
ni 	p: 86.36 	r: 74.9 	f1: 80.23 	 1140 	 1320 	 1522

[32m iter_2[0m
ga 	p: 81.68 	r: 76.78 	f1: 79.16 	 4736 	 5798 	 6168
wo 	p: 93.15 	r: 86.66 	f1: 89.79 	 2936 	 3152 	 3388
ni 	p: 86.49 	r: 74.9 	f1: 80.28 	 1140 	 1318 	 1522
best_thres [[0.49, 0.41, 0.15], [0.25, 0.68, 0.03], [0.18, 0.64, 0.03]]
f [0.8264, 0.8259, 0.8258]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 23 	 [0.28, 0.74, 0.08] 	 lr: 2.5e-05 	 f: 82.9197354224328
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 31 [0m
Train...
loss: tensor(68.9336) lr: 1e-05 time: 1478.53
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.31 	r: 75.28 	f1: 79.09 	 4643 	 5573 	 6168
wo 	p: 93.49 	r: 86.01 	f1: 89.59 	 2914 	 3117 	 3388
ni 	p: 88.89 	r: 73.06 	f1: 80.2 	 1112 	 1251 	 1522

[32m iter_1[0m
ga 	p: 81.81 	r: 76.56 	f1: 79.1 	 4722 	 5772 	 6168
wo 	p: 92.9 	r: 86.57 	f1: 89.63 	 2933 	 3157 	 3388
ni 	p: 86.84 	r: 74.57 	f1: 80.24 	 1135 	 1307 	 1522

[32m iter_2[0m
ga 	p: 81.81 	r: 76.64 	f1: 79.14 	 4727 	 5778 	 6168
wo 	p: 92.9 	r: 86.48 	f1: 89.58 	 2930 	 3154 	 3388
ni 	p: 85.99 	r: 75.43 	f1: 80.36 	 1148 	 1335 	 1522
best_thres [[0.37, 0.61, 0.23], [0.29, 0.67, 0.12], [0.32, 0.72, 0.09]]
f [0.8249, 0.8248, 0.8249]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 24 	 [0.5, 0.37, 0.21] 	 lr: 1.25e-05 	 f: 82.52022323948077
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 30 [0m
Train...

[32m iter_0[0m
ga 	p: 84.16 	r: 73.67 	f1: 78.57 	 4544 	 5399 	 6168
wo 	p: 91.84 	r: 88.02 	f1: 89.89 	 2982 	 3247 	 3388
ni 	p: 86.6 	r: 73.46 	f1: 79.49 	 1118 	 1291 	 1522

[32m iter_1[0m
ga 	p: 80.6 	r: 76.51 	f1: 78.5 	 4719 	 5855 	 6168
wo 	p: 92.69 	r: 87.51 	f1: 90.03 	 2965 	 3199 	 3388
ni 	p: 81.43 	r: 78.38 	f1: 79.88 	 1193 	 1465 	 1522

[32m iter_2[0m
ga 	p: 80.59 	r: 76.46 	f1: 78.47 	 4716 	 5852 	 6168
wo 	p: 92.45 	r: 87.49 	f1: 89.9 	 2964 	 3206 	 3388
ni 	p: 81.7 	r: 78.32 	f1: 79.97 	 1192 	 1459 	 1522
best_thres [[0.62, 0.36, 0.14], [0.26, 0.58, 0.04], [0.26, 0.54, 0.04]]
f [0.8227, 0.8224, 0.8221]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 27 	 [0.69, 0.85, 0.09] 	 lr: 1e-05 	 f: 82.66645654400756
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 31 [0m
Train...
loss: tensor(4.3695) lr: 1.25e-05 time: 1540.89
pred_count_train 41644

Test...
loss: tensor(37.1469) lr: 1e-05 time: 1572.29
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.52 	r: 76.86 	f1: 79.12 	 4741 	 5816 	 6168
wo 	p: 93.46 	r: 86.51 	f1: 89.85 	 2931 	 3136 	 3388
ni 	p: 89.05 	r: 72.67 	f1: 80.03 	 1106 	 1242 	 1522

[32m iter_1[0m
ga 	p: 81.76 	r: 76.8 	f1: 79.2 	 4737 	 5794 	 6168
wo 	p: 92.61 	r: 87.28 	f1: 89.86 	 2957 	 3193 	 3388
ni 	p: 89.34 	r: 72.67 	f1: 80.14 	 1106 	 1238 	 1522

[32m iter_2[0m
ga 	p: 81.67 	r: 76.88 	f1: 79.2 	 4742 	 5806 	 6168
wo 	p: 93.8 	r: 86.19 	f1: 89.83 	 2920 	 3113 	 3388
ni 	p: 87.98 	r: 73.59 	f1: 80.14 	 1120 	 1273 	 1522
best_thres [[0.12, 0.51, 0.07], [0.13, 0.26, 0.05], [0.12, 0.58, 0.03]]
f [0.8253, 0.8257, 0.8257]
load model: epoch23
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 23 	 [0.28, 0.74, 0.08] 	 lr: 2.5e-05 	 f: 82.9197354224328
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 32 [0m
Train...
loss: tensor(46.6181) lr: 1e-05 time: 1515.53
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.92 	r: 77.09 	f1: 78.96 	 4755 	 5876 	 6168
wo 	p: 93.32 	r: 86.19 	f1: 89.61 	 2920 	 3129 	 3388
ni 	p: 86.3 	r: 74.11 	f1: 79.75 	 1128 	 1307 	 1522

[32m iter_1[0m
ga 	p: 81.99 	r: 76.23 	f1: 79.01 	 4702 	 5735 	 6168
wo 	p: 93.23 	r: 86.19 	f1: 89.57 	 2920 	 3132 	 3388
ni 	p: 85.82 	r: 75.16 	f1: 80.14 	 1144 	 1333 	 1522

[32m iter_2[0m
ga 	p: 81.88 	r: 76.26 	f1: 78.97 	 4704 	 5745 	 6168
wo 	p: 92.24 	r: 87.04 	f1: 89.57 	 2949 	 3197 	 3388
ni 	p: 86.43 	r: 74.9 	f1: 80.25 	 1140 	 1319 	 1522
best_thres [[0.19, 0.6, 0.1], [0.41, 0.78, 0.07], [0.47, 0.54, 0.07]]
f [0.8231, 0.8235, 0.8237]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 24 	 [0.5, 0.37, 0.21] 	 lr: 1.25e-05 	 f: 82.52022323948077
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 31 [0m
Train...

[32m iter_0[0m
ga 	p: 83.33 	r: 74.08 	f1: 78.43 	 4569 	 5483 	 6168
wo 	p: 91.83 	r: 87.93 	f1: 89.84 	 2979 	 3244 	 3388
ni 	p: 81.24 	r: 76.81 	f1: 78.96 	 1169 	 1439 	 1522

[32m iter_1[0m
ga 	p: 80.76 	r: 76.15 	f1: 78.39 	 4697 	 5816 	 6168
wo 	p: 92.7 	r: 87.28 	f1: 89.91 	 2957 	 3190 	 3388
ni 	p: 82.43 	r: 77.07 	f1: 79.66 	 1173 	 1423 	 1522

[32m iter_2[0m
ga 	p: 80.87 	r: 76.17 	f1: 78.45 	 4698 	 5809 	 6168
wo 	p: 92.72 	r: 87.28 	f1: 89.92 	 2957 	 3189 	 3388
ni 	p: 82.63 	r: 76.87 	f1: 79.65 	 1170 	 1416 	 1522
best_thres [[0.48, 0.3, 0.06], [0.23, 0.47, 0.05], [0.23, 0.47, 0.05]]
f [0.8207, 0.8208, 0.8209]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	best in epoch 27 	 [0.69, 0.85, 0.09] 	 lr: 1e-05 	 f: 82.66645654400756
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(13.3585) lr: 1e-05 time: 1555.12
pred_count_train 41644

Test...
loss: tensor(23.4131) lr: 1e-05 time: 1596.49
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.2 	r: 76.02 	f1: 79.45 	 4689 	 5636 	 6168
wo 	p: 91.73 	r: 88.08 	f1: 89.87 	 2984 	 3253 	 3388
ni 	p: 87.37 	r: 74.51 	f1: 80.43 	 1134 	 1298 	 1522

[32m iter_1[0m
ga 	p: 83.37 	r: 75.84 	f1: 79.43 	 4678 	 5611 	 6168
wo 	p: 93.74 	r: 86.19 	f1: 89.8 	 2920 	 3115 	 3388
ni 	p: 88.37 	r: 73.92 	f1: 80.5 	 1125 	 1273 	 1522

[32m iter_2[0m
ga 	p: 83.4 	r: 75.86 	f1: 79.45 	 4679 	 5610 	 6168
wo 	p: 92.33 	r: 87.4 	f1: 89.8 	 2961 	 3207 	 3388
ni 	p: 88.12 	r: 74.11 	f1: 80.51 	 1128 	 1280 	 1522
best_thres [[0.39, 0.22, 0.09], [0.41, 0.71, 0.1], [0.41, 0.34, 0.09]]
f [0.8283, 0.828, 0.8281]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 23 	 [0.28, 0.74, 0.08] 	 lr: 2.5e-05 	 f: 82.9197354224328
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 33 [0m
Train...
loss: tensor(3144.9846) lr: 0.0002 time: 1522.93
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.24 	r: 75.6 	f1: 78.78 	 4663 	 5670 	 6168
wo 	p: 92.68 	r: 86.33 	f1: 89.39 	 2925 	 3156 	 3388
ni 	p: 88.2 	r: 72.67 	f1: 79.68 	 1106 	 1254 	 1522

[32m iter_1[0m
ga 	p: 81.55 	r: 76.56 	f1: 78.98 	 4722 	 5790 	 6168
wo 	p: 92.23 	r: 86.87 	f1: 89.47 	 2943 	 3191 	 3388
ni 	p: 88.09 	r: 73.39 	f1: 80.07 	 1117 	 1268 	 1522

[32m iter_2[0m
ga 	p: 82.06 	r: 76.18 	f1: 79.01 	 4699 	 5726 	 6168
wo 	p: 92.93 	r: 86.1 	f1: 89.38 	 2917 	 3139 	 3388
ni 	p: 88.55 	r: 73.19 	f1: 80.14 	 1114 	 1258 	 1522
best_thres [[0.27, 0.41, 0.14], [0.23, 0.32, 0.11], [0.41, 0.68, 0.11]]
f [0.8218, 0.8227, 0.823]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 24 	 [0.5, 0.37, 0.21] 	 lr: 1.25e-05 	 f: 82.52022323948077
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 32 [0m
Train...

[32m iter_0[0m
ga 	p: 80.06 	r: 72.13 	f1: 75.89 	 4449 	 5557 	 6168
wo 	p: 92.29 	r: 84.8 	f1: 88.39 	 2873 	 3113 	 3388
ni 	p: 81.3 	r: 78.84 	f1: 80.05 	 1200 	 1476 	 1522

[32m iter_1[0m
ga 	p: 79.25 	r: 72.88 	f1: 75.93 	 4495 	 5672 	 6168
wo 	p: 92.81 	r: 84.21 	f1: 88.3 	 2853 	 3074 	 3388
ni 	p: 82.34 	r: 77.79 	f1: 80.0 	 1184 	 1438 	 1522

[32m iter_2[0m
ga 	p: 77.95 	r: 73.98 	f1: 75.91 	 4563 	 5854 	 6168
wo 	p: 92.03 	r: 84.89 	f1: 88.32 	 2876 	 3125 	 3388
ni 	p: 82.34 	r: 77.79 	f1: 80.0 	 1184 	 1438 	 1522
best_thres [[0.5, 0.36, 0.22], [0.47, 0.39, 0.24], [0.42, 0.35, 0.24]]
f [0.8031, 0.8028, 0.8026]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 1 	 [0.42, 0.35, 0.24] 	 lr: 0.0002 	 f: 80.26445350963566
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 2 [0m
Train...
loss: tensor(4.0996) lr: 1e-05 time: 1521.33
pred_count_train 41644

Test...
loss: tensor(18.2361) lr: 1e-05 time: 1579.12
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.59 	r: 75.55 	f1: 79.37 	 4660 	 5575 	 6168
wo 	p: 92.75 	r: 87.19 	f1: 89.88 	 2954 	 3185 	 3388
ni 	p: 88.06 	r: 73.65 	f1: 80.21 	 1121 	 1273 	 1522

[32m iter_1[0m
ga 	p: 83.74 	r: 75.47 	f1: 79.39 	 4655 	 5559 	 6168
wo 	p: 92.34 	r: 87.51 	f1: 89.86 	 2965 	 3211 	 3388
ni 	p: 87.5 	r: 74.05 	f1: 80.21 	 1127 	 1288 	 1522

[32m iter_2[0m
ga 	p: 84.87 	r: 74.59 	f1: 79.4 	 4601 	 5421 	 6168
wo 	p: 92.37 	r: 87.51 	f1: 89.88 	 2965 	 3210 	 3388
ni 	p: 86.7 	r: 74.97 	f1: 80.41 	 1141 	 1316 	 1522
best_thres [[0.42, 0.49, 0.07], [0.42, 0.36, 0.04], [0.6, 0.36, 0.03]]
f [0.8275, 0.8276, 0.8278]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 23 	 [0.28, 0.74, 0.08] 	 lr: 2.5e-05 	 f: 82.9197354224328
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 34 [0m
Train...
loss: tensor(2023.6516) lr: 0.0002 time: 1553.7
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.4 	r: 75.45 	f1: 78.77 	 4654 	 5648 	 6168
wo 	p: 94.68 	r: 84.65 	f1: 89.39 	 2868 	 3029 	 3388
ni 	p: 87.57 	r: 72.67 	f1: 79.43 	 1106 	 1263 	 1522

[32m iter_1[0m
ga 	p: 82.38 	r: 75.58 	f1: 78.84 	 4662 	 5659 	 6168
wo 	p: 91.66 	r: 87.28 	f1: 89.42 	 2957 	 3226 	 3388
ni 	p: 86.99 	r: 73.78 	f1: 79.84 	 1123 	 1291 	 1522

[32m iter_2[0m
ga 	p: 82.19 	r: 75.58 	f1: 78.75 	 4662 	 5672 	 6168
wo 	p: 92.4 	r: 86.54 	f1: 89.38 	 2932 	 3173 	 3388
ni 	p: 83.49 	r: 76.74 	f1: 79.97 	 1168 	 1399 	 1522
best_thres [[0.3, 0.85, 0.14], [0.48, 0.34, 0.09], [0.58, 0.75, 0.03]]
f [0.821, 0.8218, 0.8218]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	best in epoch 24 	 [0.5, 0.37, 0.21] 	 lr: 1.25e-05 	 f: 82.52022323948077
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 1 [0m
Train...

[32m iter_0[0m
ga 	p: 82.85 	r: 74.64 	f1: 78.53 	 4604 	 5557 	 6168
wo 	p: 93.42 	r: 85.48 	f1: 89.27 	 2896 	 3100 	 3388
ni 	p: 84.5 	r: 80.22 	f1: 82.31 	 1221 	 1445 	 1522

[32m iter_1[0m
ga 	p: 81.7 	r: 75.65 	f1: 78.56 	 4666 	 5711 	 6168
wo 	p: 92.91 	r: 85.8 	f1: 89.21 	 2907 	 3129 	 3388
ni 	p: 84.59 	r: 80.09 	f1: 82.28 	 1219 	 1441 	 1522

[32m iter_2[0m
ga 	p: 81.72 	r: 75.65 	f1: 78.57 	 4666 	 5710 	 6168
wo 	p: 92.91 	r: 85.8 	f1: 89.21 	 2907 	 3129 	 3388
ni 	p: 84.6 	r: 80.16 	f1: 82.32 	 1220 	 1442 	 1522
best_thres [[0.34, 0.44, 0.13], [0.3, 0.42, 0.13], [0.3, 0.42, 0.13]]
f [0.8235, 0.8234, 0.8234]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 2 	 [0.3, 0.42, 0.13] 	 lr: 0.0002 	 f: 82.33747535134121
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 3 [0m
Train...
loss: tensor(4.6166) lr: 1e-05 time: 1484.24
pred_count_train 41644

Test...
loss: tensor(3144.2241) lr: 0.0002 time: 1610.68
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.09 	r: 76.33 	f1: 79.11 	 4708 	 5735 	 6168
wo 	p: 93.44 	r: 86.6 	f1: 89.89 	 2934 	 3140 	 3388
ni 	p: 89.12 	r: 73.19 	f1: 80.38 	 1114 	 1250 	 1522

[32m iter_1[0m
ga 	p: 82.62 	r: 75.97 	f1: 79.16 	 4686 	 5672 	 6168
wo 	p: 91.79 	r: 87.81 	f1: 89.76 	 2975 	 3241 	 3388
ni 	p: 86.45 	r: 75.43 	f1: 80.56 	 1148 	 1328 	 1522

[32m iter_2[0m
ga 	p: 83.03 	r: 75.7 	f1: 79.2 	 4669 	 5623 	 6168
wo 	p: 92.46 	r: 87.25 	f1: 89.78 	 2956 	 3197 	 3388
ni 	p: 86.45 	r: 75.43 	f1: 80.56 	 1148 	 1328 	 1522
best_thres [[0.26, 0.66, 0.1], [0.31, 0.27, 0.03], [0.37, 0.41, 0.03]]
f [0.8259, 0.8262, 0.8263]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 23 	 [0.28, 0.74, 0.08] 	 lr: 2.5e-05 	 f: 82.9197354224328
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 35 [0m
Train...
loss: tensor(1616.8424) lr: 0.0002 time: 1539.48
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 78.97 	r: 73.59 	f1: 76.18 	 4539 	 5748 	 6168
wo 	p: 91.57 	r: 85.33 	f1: 88.34 	 2891 	 3157 	 3388
ni 	p: 81.93 	r: 78.06 	f1: 79.95 	 1188 	 1450 	 1522

[32m iter_1[0m
ga 	p: 79.56 	r: 73.05 	f1: 76.17 	 4506 	 5664 	 6168
wo 	p: 91.64 	r: 85.12 	f1: 88.26 	 2884 	 3147 	 3388
ni 	p: 83.66 	r: 76.02 	f1: 79.66 	 1157 	 1383 	 1522

[32m iter_2[0m
ga 	p: 79.53 	r: 73.07 	f1: 76.16 	 4507 	 5667 	 6168
wo 	p: 91.67 	r: 85.12 	f1: 88.28 	 2884 	 3146 	 3388
ni 	p: 81.2 	r: 78.32 	f1: 79.73 	 1192 	 1468 	 1522
best_thres [[0.45, 0.34, 0.25], [0.48, 0.35, 0.28], [0.48, 0.35, 0.23]]
f [0.8042, 0.8039, 0.8038]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 1 	 [0.48, 0.35, 0.23] 	 lr: 0.0002 	 f: 80.38211788211788
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 2 [0m
Train...

[32m iter_0[0m
ga 	p: 82.87 	r: 74.45 	f1: 78.44 	 4592 	 5541 	 6168
wo 	p: 91.8 	r: 85.95 	f1: 88.78 	 2912 	 3172 	 3388
ni 	p: 84.3 	r: 81.47 	f1: 82.86 	 1240 	 1471 	 1522

[32m iter_1[0m
ga 	p: 83.73 	r: 73.67 	f1: 78.38 	 4544 	 5427 	 6168
wo 	p: 92.81 	r: 85.01 	f1: 88.74 	 2880 	 3103 	 3388
ni 	p: 84.08 	r: 81.21 	f1: 82.62 	 1236 	 1470 	 1522

[32m iter_2[0m
ga 	p: 80.48 	r: 76.28 	f1: 78.33 	 4705 	 5846 	 6168
wo 	p: 92.87 	r: 84.98 	f1: 88.75 	 2879 	 3100 	 3388
ni 	p: 84.08 	r: 81.21 	f1: 82.62 	 1236 	 1470 	 1522
best_thres [[0.37, 0.66, 0.12], [0.4, 0.78, 0.12], [0.27, 0.79, 0.12]]
f [0.8225, 0.8221, 0.8216]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 2 	 [0.3, 0.42, 0.13] 	 lr: 0.0002 	 f: 82.33747535134121
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 4 [0m
Train...
loss: tensor(3.1080) lr: 1e-05 time: 1504.59
pred_count_train 41644

Test...
loss: tensor(2024.4930) lr: 0.0002 time: 1595.8
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.02 	r: 76.83 	f1: 79.34 	 4739 	 5778 	 6168
wo 	p: 93.38 	r: 86.66 	f1: 89.9 	 2936 	 3144 	 3388
ni 	p: 87.7 	r: 73.52 	f1: 79.99 	 1119 	 1276 	 1522

[32m iter_1[0m
ga 	p: 82.7 	r: 76.17 	f1: 79.3 	 4698 	 5681 	 6168
wo 	p: 93.26 	r: 86.63 	f1: 89.82 	 2935 	 3147 	 3388
ni 	p: 88.32 	r: 73.52 	f1: 80.24 	 1119 	 1267 	 1522

[32m iter_2[0m
ga 	p: 82.69 	r: 76.26 	f1: 79.35 	 4704 	 5689 	 6168
wo 	p: 93.46 	r: 86.48 	f1: 89.84 	 2930 	 3135 	 3388
ni 	p: 88.44 	r: 73.39 	f1: 80.22 	 1117 	 1263 	 1522
best_thres [[0.19, 0.61, 0.07], [0.28, 0.57, 0.06], [0.27, 0.62, 0.06]]
f [0.8267, 0.8267, 0.8268]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	best in epoch 23 	 [0.28, 0.74, 0.08] 	 lr: 2.5e-05 	 f: 82.9197354224328
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(1281.9600) lr: 0.0002 time: 1567.53
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.32 	r: 76.88 	f1: 78.56 	 4742 	 5904 	 6168
wo 	p: 92.4 	r: 85.74 	f1: 88.95 	 2905 	 3144 	 3388
ni 	p: 83.54 	r: 80.03 	f1: 81.74 	 1218 	 1458 	 1522

[32m iter_1[0m
ga 	p: 80.83 	r: 76.56 	f1: 78.63 	 4722 	 5842 	 6168
wo 	p: 92.91 	r: 85.42 	f1: 89.01 	 2894 	 3115 	 3388
ni 	p: 84.12 	r: 79.37 	f1: 81.68 	 1208 	 1436 	 1522

[32m iter_2[0m
ga 	p: 80.8 	r: 76.56 	f1: 78.62 	 4722 	 5844 	 6168
wo 	p: 92.85 	r: 85.42 	f1: 88.98 	 2894 	 3117 	 3388
ni 	p: 84.01 	r: 79.37 	f1: 81.62 	 1208 	 1438 	 1522
best_thres [[0.3, 0.3, 0.17], [0.32, 0.33, 0.18], [0.32, 0.33, 0.18]]
f [0.8214, 0.8217, 0.8217]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 2 	 [0.32, 0.33, 0.18] 	 lr: 0.0002 	 f: 82.17008615880493
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 3 [0m
Train...

[32m iter_0[0m
ga 	p: 82.6 	r: 75.73 	f1: 79.02 	 4671 	 5655 	 6168
wo 	p: 92.58 	r: 86.92 	f1: 89.66 	 2945 	 3181 	 3388
ni 	p: 85.31 	r: 77.07 	f1: 80.98 	 1173 	 1375 	 1522

[32m iter_1[0m
ga 	p: 83.33 	r: 75.03 	f1: 78.96 	 4628 	 5554 	 6168
wo 	p: 93.09 	r: 86.28 	f1: 89.55 	 2923 	 3140 	 3388
ni 	p: 83.24 	r: 78.32 	f1: 80.7 	 1192 	 1432 	 1522

[32m iter_2[0m
ga 	p: 83.17 	r: 75.1 	f1: 78.93 	 4632 	 5569 	 6168
wo 	p: 92.78 	r: 86.48 	f1: 89.52 	 2930 	 3158 	 3388
ni 	p: 83.18 	r: 78.32 	f1: 80.68 	 1192 	 1433 	 1522
best_thres [[0.47, 0.27, 0.11], [0.51, 0.34, 0.09], [0.5, 0.31, 0.09]]
f [0.8257, 0.8252, 0.8249]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 4 	 [0.5, 0.31, 0.09] 	 lr: 0.0002 	 f: 82.49046774725018
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 5 [0m
Train...
loss: tensor(3144.4648) lr: 0.0002 time: 1486.28
pred_count_train 41644

Test...
loss: tensor(1628.1495) lr: 0.0002 time: 1645.89
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 78.59 	r: 73.51 	f1: 75.97 	 4534 	 5769 	 6168
wo 	p: 92.1 	r: 84.95 	f1: 88.38 	 2878 	 3125 	 3388
ni 	p: 83.08 	r: 76.81 	f1: 79.82 	 1169 	 1407 	 1522

[32m iter_1[0m
ga 	p: 78.26 	r: 73.7 	f1: 75.91 	 4546 	 5809 	 6168
wo 	p: 92.03 	r: 84.92 	f1: 88.33 	 2877 	 3126 	 3388
ni 	p: 82.32 	r: 77.4 	f1: 79.78 	 1178 	 1431 	 1522

[32m iter_2[0m
ga 	p: 78.29 	r: 73.74 	f1: 75.95 	 4548 	 5809 	 6168
wo 	p: 92.06 	r: 84.92 	f1: 88.35 	 2877 	 3125 	 3388
ni 	p: 82.32 	r: 77.4 	f1: 79.78 	 1178 	 1431 	 1522
best_thres [[0.44, 0.39, 0.27], [0.43, 0.39, 0.25], [0.43, 0.39, 0.25]]
f [0.8028, 0.8025, 0.8024]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 1 	 [0.43, 0.39, 0.25] 	 lr: 0.0002 	 f: 80.24460834655962
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 2 [0m
Train...
loss: tensor(966.6246) lr: 0.0002 time: 1576.3
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.37 	r: 76.88 	f1: 78.59 	 4742 	 5900 	 6168
wo 	p: 92.72 	r: 86.51 	f1: 89.51 	 2931 	 3161 	 3388
ni 	p: 84.78 	r: 82.0 	f1: 83.37 	 1248 	 1472 	 1522

[32m iter_1[0m
ga 	p: 80.47 	r: 76.86 	f1: 78.62 	 4741 	 5892 	 6168
wo 	p: 92.59 	r: 86.69 	f1: 89.54 	 2937 	 3172 	 3388
ni 	p: 85.46 	r: 81.47 	f1: 83.42 	 1240 	 1451 	 1522

[32m iter_2[0m
ga 	p: 80.45 	r: 76.85 	f1: 78.61 	 4740 	 5892 	 6168
wo 	p: 92.29 	r: 86.92 	f1: 89.53 	 2945 	 3191 	 3388
ni 	p: 85.46 	r: 81.47 	f1: 83.42 	 1240 	 1451 	 1522
best_thres [[0.28, 0.58, 0.16], [0.29, 0.6, 0.17], [0.29, 0.57, 0.17]]
f [0.8256, 0.8258, 0.8258]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 3 	 [0.29, 0.57, 0.17] 	 lr: 0.0002 	 f: 82.58454702542582
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 4 [0m
Train...

[32m iter_0[0m
ga 	p: 82.29 	r: 74.81 	f1: 78.37 	 4614 	 5607 	 6168
wo 	p: 93.17 	r: 86.13 	f1: 89.51 	 2918 	 3132 	 3388
ni 	p: 85.71 	r: 75.3 	f1: 80.17 	 1146 	 1337 	 1522

[32m iter_1[0m
ga 	p: 82.72 	r: 74.51 	f1: 78.4 	 4596 	 5556 	 6168
wo 	p: 92.38 	r: 86.89 	f1: 89.55 	 2944 	 3187 	 3388
ni 	p: 85.39 	r: 75.62 	f1: 80.21 	 1151 	 1348 	 1522

[32m iter_2[0m
ga 	p: 82.58 	r: 74.64 	f1: 78.41 	 4604 	 5575 	 6168
wo 	p: 92.37 	r: 86.84 	f1: 89.52 	 2942 	 3185 	 3388
ni 	p: 84.78 	r: 76.15 	f1: 80.24 	 1159 	 1367 	 1522
best_thres [[0.45, 0.45, 0.39], [0.47, 0.35, 0.37], [0.46, 0.35, 0.34]]
f [0.8205, 0.8208, 0.8209]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 4 	 [0.5, 0.31, 0.09] 	 lr: 0.0002 	 f: 82.49046774725018
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 6 [0m
Train...
loss: tensor(2023.8010) lr: 0.0002 time: 1487.88
pred_count_train 41644

Test...
loss: tensor(1286.3705) lr: 0.0002 time: 1536.46
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.4 	r: 77.29 	f1: 78.81 	 4767 	 5929 	 6168
wo 	p: 92.08 	r: 86.48 	f1: 89.19 	 2930 	 3182 	 3388
ni 	p: 85.45 	r: 79.11 	f1: 82.16 	 1204 	 1409 	 1522

[32m iter_1[0m
ga 	p: 80.17 	r: 77.5 	f1: 78.81 	 4780 	 5962 	 6168
wo 	p: 92.57 	r: 86.07 	f1: 89.2 	 2916 	 3150 	 3388
ni 	p: 85.37 	r: 78.98 	f1: 82.05 	 1202 	 1408 	 1522

[32m iter_2[0m
ga 	p: 80.18 	r: 77.51 	f1: 78.82 	 4781 	 5963 	 6168
wo 	p: 92.54 	r: 86.07 	f1: 89.19 	 2916 	 3151 	 3388
ni 	p: 85.37 	r: 78.98 	f1: 82.05 	 1202 	 1408 	 1522
best_thres [[0.28, 0.36, 0.17], [0.27, 0.39, 0.17], [0.27, 0.39, 0.17]]
f [0.8242, 0.8241, 0.8241]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 2 	 [0.27, 0.39, 0.17] 	 lr: 0.0002 	 f: 82.40632137786282
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 3 [0m
Train...
loss: tensor(716.7601) lr: 0.0002 time: 1538.08
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.91 	r: 77.25 	f1: 79.04 	 4765 	 5889 	 6168
wo 	p: 93.05 	r: 86.51 	f1: 89.66 	 2931 	 3150 	 3388
ni 	p: 88.68 	r: 76.68 	f1: 82.24 	 1167 	 1316 	 1522

[32m iter_1[0m
ga 	p: 82.27 	r: 75.89 	f1: 78.95 	 4681 	 5690 	 6168
wo 	p: 93.13 	r: 86.48 	f1: 89.68 	 2930 	 3146 	 3388
ni 	p: 88.8 	r: 76.54 	f1: 82.22 	 1165 	 1312 	 1522

[32m iter_2[0m
ga 	p: 82.42 	r: 75.78 	f1: 78.96 	 4674 	 5671 	 6168
wo 	p: 93.17 	r: 86.51 	f1: 89.72 	 2931 	 3146 	 3388
ni 	p: 88.88 	r: 76.68 	f1: 82.33 	 1167 	 1313 	 1522
best_thres [[0.46, 0.28, 0.2], [0.56, 0.3, 0.19], [0.57, 0.3, 0.19]]
f [0.827, 0.827, 0.8271]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 4 	 [0.57, 0.3, 0.19] 	 lr: 0.0002 	 f: 82.70624892354425
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 5 [0m
Train...
loss: tensor(1615.1250) lr: 0.0002 time: 1445.36
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.9 	r: 74.68 	f1: 78.57 	 4606 	 5556 	 6168
wo 	p: 92.46 	r: 86.51 	f1: 89.39 	 2931 	 3170 	 3388
ni 	p: 83.05 	r: 75.03 	f1: 78.84 	 1142 	 1375 	 1522

[32m iter_1[0m
ga 	p: 80.66 	r: 76.46 	f1: 78.5 	 4716 	 5847 	 6168
wo 	p: 93.24 	r: 85.95 	f1: 89.45 	 2912 	 3123 	 3388
ni 	p: 84.93 	r: 74.05 	f1: 79.12 	 1127 	 1327 	 1522

[32m iter_2[0m
ga 	p: 80.75 	r: 76.31 	f1: 78.47 	 4707 	 5829 	 6168
wo 	p: 92.49 	r: 86.57 	f1: 89.43 	 2933 	 3171 	 3388
ni 	p: 85.03 	r: 73.92 	f1: 79.09 	 1125 	 1323 	 1522
best_thres [[0.58, 0.56, 0.15], [0.45, 0.77, 0.16], [0.46, 0.7, 0.16]]
f [0.8196, 0.8194, 0.8193]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 4 	 [0.5, 0.31, 0.09] 	 lr: 0.0002 	 f: 82.49046774725018
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 7 [0m
Train...

[32m iter_0[0m
ga 	p: 79.67 	r: 77.27 	f1: 78.45 	 4766 	 5982 	 6168
wo 	p: 93.12 	r: 85.54 	f1: 89.17 	 2898 	 3112 	 3388
ni 	p: 83.17 	r: 82.13 	f1: 82.64 	 1250 	 1503 	 1522

[32m iter_1[0m
ga 	p: 79.62 	r: 77.25 	f1: 78.42 	 4765 	 5985 	 6168
wo 	p: 93.98 	r: 84.83 	f1: 89.17 	 2874 	 3058 	 3388
ni 	p: 84.94 	r: 80.42 	f1: 82.62 	 1224 	 1441 	 1522

[32m iter_2[0m
ga 	p: 79.6 	r: 77.25 	f1: 78.41 	 4765 	 5986 	 6168
wo 	p: 93.0 	r: 85.54 	f1: 89.11 	 2898 	 3116 	 3388
ni 	p: 84.44 	r: 80.95 	f1: 82.66 	 1232 	 1459 	 1522
best_thres [[0.25, 0.7, 0.12], [0.25, 0.76, 0.15], [0.25, 0.69, 0.14]]
f [0.8225, 0.8223, 0.8222]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 2 	 [0.27, 0.39, 0.17] 	 lr: 0.0002 	 f: 82.40632137786282
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 4 [0m
Train...
loss: tensor(969.2543) lr: 0.0002 time: 1619.6
pred_count_train 41644

Test...
loss: tensor(563.2576) lr: 0.0002 time: 1587.91
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.67 	r: 75.11 	f1: 78.71 	 4633 	 5604 	 6168
wo 	p: 92.3 	r: 86.36 	f1: 89.23 	 2926 	 3170 	 3388
ni 	p: 83.15 	r: 77.79 	f1: 80.38 	 1184 	 1424 	 1522

[32m iter_1[0m
ga 	p: 80.94 	r: 76.75 	f1: 78.79 	 4734 	 5849 	 6168
wo 	p: 93.03 	r: 85.86 	f1: 89.3 	 2909 	 3127 	 3388
ni 	p: 82.87 	r: 77.86 	f1: 80.28 	 1185 	 1430 	 1522

[32m iter_2[0m
ga 	p: 81.09 	r: 76.62 	f1: 78.79 	 4726 	 5828 	 6168
wo 	p: 92.86 	r: 85.98 	f1: 89.29 	 2913 	 3137 	 3388
ni 	p: 82.91 	r: 77.79 	f1: 80.27 	 1184 	 1428 	 1522
best_thres [[0.39, 0.43, 0.22], [0.31, 0.59, 0.21], [0.32, 0.58, 0.21]]
f [0.8219, 0.8218, 0.8218]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 4 	 [0.57, 0.3, 0.19] 	 lr: 0.0002 	 f: 82.70624892354425
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 6 [0m
Train...
loss: tensor(1276.6908) lr: 0.0002 time: 1466.59
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.21 	r: 73.9 	f1: 78.28 	 4558 	 5478 	 6168
wo 	p: 93.96 	r: 84.42 	f1: 88.93 	 2860 	 3044 	 3388
ni 	p: 83.32 	r: 71.55 	f1: 76.99 	 1089 	 1307 	 1522

[32m iter_1[0m
ga 	p: 81.66 	r: 75.08 	f1: 78.23 	 4631 	 5671 	 6168
wo 	p: 92.95 	r: 85.27 	f1: 88.95 	 2889 	 3108 	 3388
ni 	p: 85.29 	r: 70.11 	f1: 76.96 	 1067 	 1251 	 1522

[32m iter_2[0m
ga 	p: 82.66 	r: 74.19 	f1: 78.2 	 4576 	 5536 	 6168
wo 	p: 94.25 	r: 84.12 	f1: 88.9 	 2850 	 3024 	 3388
ni 	p: 85.04 	r: 70.24 	f1: 76.93 	 1069 	 1257 	 1522
best_thres [[0.53, 0.65, 0.2], [0.42, 0.58, 0.25], [0.54, 0.84, 0.24]]
f [0.8138, 0.8137, 0.8135]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 4 	 [0.5, 0.31, 0.09] 	 lr: 0.0002 	 f: 82.49046774725018
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 8 [0m
Train...

[32m iter_0[0m
ga 	p: 80.8 	r: 76.57 	f1: 78.63 	 4723 	 5845 	 6168
wo 	p: 94.47 	r: 84.65 	f1: 89.29 	 2868 	 3036 	 3388
ni 	p: 86.71 	r: 77.6 	f1: 81.9 	 1181 	 1362 	 1522

[32m iter_1[0m
ga 	p: 82.05 	r: 75.62 	f1: 78.7 	 4664 	 5684 	 6168
wo 	p: 94.55 	r: 84.56 	f1: 89.28 	 2865 	 3030 	 3388
ni 	p: 88.15 	r: 76.74 	f1: 82.05 	 1168 	 1325 	 1522

[32m iter_2[0m
ga 	p: 82.04 	r: 75.63 	f1: 78.71 	 4665 	 5686 	 6168
wo 	p: 93.76 	r: 85.21 	f1: 89.28 	 2887 	 3079 	 3388
ni 	p: 88.15 	r: 76.74 	f1: 82.05 	 1168 	 1325 	 1522
best_thres [[0.41, 0.51, 0.12], [0.47, 0.51, 0.14], [0.47, 0.43, 0.14]]
f [0.8229, 0.8233, 0.8235]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 2 	 [0.27, 0.39, 0.17] 	 lr: 0.0002 	 f: 82.40632137786282
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 5 [0m
Train...
loss: tensor(736.0727) lr: 0.0002 time: 1568.02
pred_count_train 41644

Test...
loss: tensor(471.9155) lr: 0.0002 time: 1601.74
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.43 	r: 73.8 	f1: 78.32 	 4552 	 5456 	 6168
wo 	p: 93.99 	r: 83.94 	f1: 88.68 	 2844 	 3026 	 3388
ni 	p: 86.45 	r: 75.89 	f1: 80.83 	 1155 	 1336 	 1522

[32m iter_1[0m
ga 	p: 82.52 	r: 74.63 	f1: 78.38 	 4603 	 5578 	 6168
wo 	p: 94.11 	r: 83.91 	f1: 88.72 	 2843 	 3021 	 3388
ni 	p: 88.36 	r: 74.31 	f1: 80.73 	 1131 	 1280 	 1522

[32m iter_2[0m
ga 	p: 82.53 	r: 74.61 	f1: 78.37 	 4602 	 5576 	 6168
wo 	p: 93.73 	r: 84.24 	f1: 88.73 	 2854 	 3045 	 3388
ni 	p: 88.77 	r: 74.24 	f1: 80.86 	 1130 	 1273 	 1522
best_thres [[0.6, 0.74, 0.16], [0.61, 0.85, 0.2], [0.62, 0.81, 0.21]]
f [0.8184, 0.8185, 0.8186]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 4 	 [0.57, 0.3, 0.19] 	 lr: 0.0002 	 f: 82.70624892354425
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 7 [0m
Train...
loss: tensor(952.7979) lr: 0.0002 time: 1473.77
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.26 	r: 74.56 	f1: 78.22 	 4599 	 5591 	 6168
wo 	p: 92.36 	r: 85.66 	f1: 88.88 	 2902 	 3142 	 3388
ni 	p: 83.56 	r: 79.5 	f1: 81.48 	 1210 	 1448 	 1522

[32m iter_1[0m
ga 	p: 81.78 	r: 74.9 	f1: 78.19 	 4620 	 5649 	 6168
wo 	p: 92.94 	r: 85.42 	f1: 89.02 	 2894 	 3114 	 3388
ni 	p: 83.88 	r: 79.3 	f1: 81.53 	 1207 	 1439 	 1522

[32m iter_2[0m
ga 	p: 81.97 	r: 74.74 	f1: 78.19 	 4610 	 5624 	 6168
wo 	p: 92.99 	r: 85.39 	f1: 89.03 	 2893 	 3111 	 3388
ni 	p: 84.01 	r: 79.37 	f1: 81.62 	 1208 	 1438 	 1522
best_thres [[0.55, 0.61, 0.09], [0.58, 0.79, 0.08], [0.64, 0.83, 0.08]]
f [0.8195, 0.8196, 0.8197]
load model: epoch4
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 4 	 [0.5, 0.31, 0.09] 	 lr: 0.0002 	 f: 82.49046774725018
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 9 [0m
Train...

[32m iter_0[0m
ga 	p: 81.66 	r: 75.42 	f1: 78.42 	 4652 	 5697 	 6168
wo 	p: 93.3 	r: 86.28 	f1: 89.65 	 2923 	 3133 	 3388
ni 	p: 85.58 	r: 76.41 	f1: 80.74 	 1163 	 1359 	 1522

[32m iter_1[0m
ga 	p: 82.74 	r: 74.4 	f1: 78.35 	 4589 	 5546 	 6168
wo 	p: 93.09 	r: 86.3 	f1: 89.57 	 2924 	 3141 	 3388
ni 	p: 85.51 	r: 76.74 	f1: 80.89 	 1168 	 1366 	 1522

[32m iter_2[0m
ga 	p: 82.78 	r: 74.42 	f1: 78.37 	 4590 	 5545 	 6168
wo 	p: 94.53 	r: 85.12 	f1: 89.58 	 2884 	 3051 	 3388
ni 	p: 85.44 	r: 76.74 	f1: 80.86 	 1168 	 1367 	 1522
best_thres [[0.35, 0.47, 0.3], [0.41, 0.46, 0.3], [0.41, 0.64, 0.3]]
f [0.8217, 0.8217, 0.8216]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 2 	 [0.27, 0.39, 0.17] 	 lr: 0.0002 	 f: 82.40632137786282
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 6 [0m
Train...
loss: tensor(564.8152) lr: 0.0002 time: 1590.29
pred_count_train 41644

Test...
loss: tensor(750.0646) lr: 0.0001 time: 1630.24
pred_count_train 41644

Test...
loss: tensor(709.3897) lr: 0.0002 time: 1534.79
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.41 	r: 75.57 	f1: 78.38 	 4661 	 5725 	 6168
wo 	p: 92.3 	r: 85.92 	f1: 88.99 	 2911 	 3154 	 3388
ni 	p: 84.53 	r: 70.37 	f1: 76.8 	 1071 	 1267 	 1522

[32m iter_1[0m
ga 	p: 81.9 	r: 74.97 	f1: 78.28 	 4624 	 5646 	 6168
wo 	p: 93.09 	r: 85.45 	f1: 89.1 	 2895 	 3110 	 3388
ni 	p: 84.05 	r: 70.96 	f1: 76.95 	 1080 	 1285 	 1522

[32m iter_2[0m
ga 	p: 82.02 	r: 75.08 	f1: 78.4 	 4631 	 5646 	 6168
wo 	p: 93.12 	r: 85.54 	f1: 89.17 	 2898 	 3112 	 3388
ni 	p: 83.92 	r: 70.96 	f1: 76.9 	 1080 	 1287 	 1522
best_thres [[0.38, 0.38, 0.27], [0.5, 0.57, 0.27], [0.52, 0.6, 0.27]]
f [0.8145, 0.8144, 0.8146]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 4 	 [0.57, 0.3, 0.19] 	 lr: 0.0002 	 f: 82.70624892354425
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 8 [0m
Train...

[32m iter_0[0m
ga 	p: 83.98 	r: 74.59 	f1: 79.01 	 4601 	 5479 	 6168
wo 	p: 91.64 	r: 87.72 	f1: 89.64 	 2972 	 3243 	 3388
ni 	p: 82.74 	r: 76.87 	f1: 79.7 	 1170 	 1414 	 1522

[32m iter_1[0m
ga 	p: 83.59 	r: 75.15 	f1: 79.14 	 4635 	 5545 	 6168
wo 	p: 92.43 	r: 87.19 	f1: 89.73 	 2954 	 3196 	 3388
ni 	p: 84.16 	r: 75.03 	f1: 79.33 	 1142 	 1357 	 1522

[32m iter_2[0m
ga 	p: 84.3 	r: 74.63 	f1: 79.17 	 4603 	 5460 	 6168
wo 	p: 92.43 	r: 87.19 	f1: 89.73 	 2954 	 3196 	 3388
ni 	p: 84.14 	r: 74.97 	f1: 79.29 	 1141 	 1356 	 1522
best_thres [[0.52, 0.37, 0.1], [0.51, 0.46, 0.12], [0.6, 0.46, 0.12]]
f [0.8243, 0.8244, 0.8246]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 4 	 [0.5, 0.31, 0.09] 	 lr: 0.0002 	 f: 82.49046774725018
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 10 [0m
Train...

[32m iter_0[0m
ga 	p: 81.76 	r: 76.04 	f1: 78.8 	 4690 	 5736 	 6168
wo 	p: 93.51 	r: 85.48 	f1: 89.31 	 2896 	 3097 	 3388
ni 	p: 85.68 	r: 75.89 	f1: 80.49 	 1155 	 1348 	 1522

[32m iter_1[0m
ga 	p: 81.89 	r: 75.86 	f1: 78.76 	 4679 	 5714 	 6168
wo 	p: 93.62 	r: 85.36 	f1: 89.3 	 2892 	 3089 	 3388
ni 	p: 85.78 	r: 75.69 	f1: 80.42 	 1152 	 1343 	 1522

[32m iter_2[0m
ga 	p: 82.09 	r: 75.71 	f1: 78.77 	 4670 	 5689 	 6168
wo 	p: 93.62 	r: 85.36 	f1: 89.3 	 2892 	 3089 	 3388
ni 	p: 85.77 	r: 75.62 	f1: 80.38 	 1151 	 1342 	 1522
best_thres [[0.54, 0.71, 0.14], [0.55, 0.72, 0.14], [0.56, 0.72, 0.14]]
f [0.8223, 0.8222, 0.8221]
load model: epoch2
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 2 	 [0.27, 0.39, 0.17] 	 lr: 0.0002 	 f: 82.40632137786282
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 7 [0m
Train...
loss: tensor(470.5060) lr: 0.0002 time: 1587.01
pred_count_train 41644

Test...
loss: tensor(460.4485) lr: 0.0001 time: 1663.68
pred_count_train 41644

Test...
loss: tensor(1440.2898) lr: 0.0001 time: 1534.79
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.25 	r: 73.87 	f1: 78.28 	 4556 	 5473 	 6168
wo 	p: 90.38 	r: 86.84 	f1: 88.57 	 2942 	 3255 	 3388
ni 	p: 81.62 	r: 70.89 	f1: 75.88 	 1079 	 1322 	 1522

[32m iter_1[0m
ga 	p: 82.97 	r: 74.08 	f1: 78.27 	 4569 	 5507 	 6168
wo 	p: 92.88 	r: 84.74 	f1: 88.62 	 2871 	 3091 	 3388
ni 	p: 83.66 	r: 69.65 	f1: 76.01 	 1060 	 1267 	 1522

[32m iter_2[0m
ga 	p: 83.12 	r: 73.93 	f1: 78.26 	 4560 	 5486 	 6168
wo 	p: 92.46 	r: 85.06 	f1: 88.61 	 2882 	 3117 	 3388
ni 	p: 83.98 	r: 69.58 	f1: 76.1 	 1059 	 1261 	 1522
best_thres [[0.47, 0.36, 0.05], [0.55, 0.85, 0.06], [0.59, 0.84, 0.06]]
f [0.8119, 0.8118, 0.8118]
load model: epoch4
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 4 	 [0.57, 0.3, 0.19] 	 lr: 0.0002 	 f: 82.70624892354425
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 9 [0m
Train...

[32m iter_0[0m
ga 	p: 83.23 	r: 75.08 	f1: 78.95 	 4631 	 5564 	 6168
wo 	p: 93.53 	r: 86.22 	f1: 89.73 	 2921 	 3123 	 3388
ni 	p: 85.96 	r: 74.44 	f1: 79.79 	 1133 	 1318 	 1522

[32m iter_1[0m
ga 	p: 83.41 	r: 74.97 	f1: 78.96 	 4624 	 5544 	 6168
wo 	p: 93.25 	r: 86.48 	f1: 89.74 	 2930 	 3142 	 3388
ni 	p: 85.32 	r: 74.84 	f1: 79.73 	 1139 	 1335 	 1522

[32m iter_2[0m
ga 	p: 83.21 	r: 75.03 	f1: 78.91 	 4628 	 5562 	 6168
wo 	p: 93.28 	r: 86.51 	f1: 89.77 	 2931 	 3142 	 3388
ni 	p: 86.16 	r: 74.05 	f1: 79.65 	 1127 	 1308 	 1522
best_thres [[0.5, 0.49, 0.2], [0.59, 0.45, 0.16], [0.59, 0.45, 0.19]]
f [0.8239, 0.824, 0.8239]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 4 	 [0.5, 0.31, 0.09] 	 lr: 0.0002 	 f: 82.49046774725018
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 11 [0m
Train...

[32m iter_0[0m
ga 	p: 83.23 	r: 76.46 	f1: 79.7 	 4716 	 5666 	 6168
wo 	p: 91.94 	r: 87.93 	f1: 89.89 	 2979 	 3240 	 3388
ni 	p: 85.53 	r: 78.45 	f1: 81.84 	 1194 	 1396 	 1522

[32m iter_1[0m
ga 	p: 83.3 	r: 76.51 	f1: 79.76 	 4719 	 5665 	 6168
wo 	p: 91.82 	r: 87.81 	f1: 89.77 	 2975 	 3240 	 3388
ni 	p: 85.7 	r: 78.38 	f1: 81.88 	 1193 	 1392 	 1522

[32m iter_2[0m
ga 	p: 83.3 	r: 76.51 	f1: 79.76 	 4719 	 5665 	 6168
wo 	p: 91.82 	r: 87.81 	f1: 89.77 	 2975 	 3240 	 3388
ni 	p: 85.64 	r: 78.38 	f1: 81.85 	 1193 	 1393 	 1522
best_thres [[0.39, 0.32, 0.15], [0.39, 0.32, 0.15], [0.39, 0.32, 0.15]]
f [0.8315, 0.8315, 0.8315]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 7 	 [0.39, 0.32, 0.15] 	 lr: 0.0001 	 f: 83.15167391745021
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 8 [0m
Train...
loss: tensor(761.9582) lr: 0.0001 time: 1568.73
pred_count_train 41644

Test...
loss: tensor(308.2337) lr: 0.0001 time: 1643.04
pred_count_train 41644

Test...
loss: tensor(1093.6465) lr: 0.0001 time: 1578.98
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 84.78 	r: 74.51 	f1: 79.32 	 4596 	 5421 	 6168
wo 	p: 94.54 	r: 85.39 	f1: 89.73 	 2893 	 3060 	 3388
ni 	p: 85.93 	r: 75.43 	f1: 80.34 	 1148 	 1336 	 1522

[32m iter_1[0m
ga 	p: 84.29 	r: 75.06 	f1: 79.41 	 4630 	 5493 	 6168
wo 	p: 93.62 	r: 86.13 	f1: 89.72 	 2918 	 3117 	 3388
ni 	p: 86.63 	r: 74.9 	f1: 80.34 	 1140 	 1316 	 1522

[32m iter_2[0m
ga 	p: 84.21 	r: 75.23 	f1: 79.47 	 4640 	 5510 	 6168
wo 	p: 93.64 	r: 86.1 	f1: 89.71 	 2917 	 3115 	 3388
ni 	p: 86.69 	r: 74.9 	f1: 80.37 	 1140 	 1315 	 1522
best_thres [[0.55, 0.73, 0.16], [0.57, 0.73, 0.16], [0.57, 0.75, 0.16]]
f [0.8267, 0.827, 0.8272]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 9 	 [0.57, 0.75, 0.16] 	 lr: 0.0001 	 f: 82.71850215363098
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 10 [0m
Train...

[32m iter_0[0m
ga 	p: 83.58 	r: 74.21 	f1: 78.62 	 4577 	 5476 	 6168
wo 	p: 92.15 	r: 87.01 	f1: 89.51 	 2948 	 3199 	 3388
ni 	p: 86.07 	r: 67.41 	f1: 75.61 	 1026 	 1192 	 1522

[32m iter_1[0m
ga 	p: 81.52 	r: 75.96 	f1: 78.64 	 4685 	 5747 	 6168
wo 	p: 92.48 	r: 86.72 	f1: 89.5 	 2938 	 3177 	 3388
ni 	p: 86.0 	r: 67.41 	f1: 75.58 	 1026 	 1193 	 1522

[32m iter_2[0m
ga 	p: 81.44 	r: 76.1 	f1: 78.68 	 4694 	 5764 	 6168
wo 	p: 91.56 	r: 87.4 	f1: 89.43 	 2961 	 3234 	 3388
ni 	p: 86.24 	r: 67.15 	f1: 75.51 	 1022 	 1185 	 1522
best_thres [[0.69, 0.49, 0.18], [0.64, 0.77, 0.16], [0.66, 0.37, 0.17]]
f [0.8165, 0.8163, 0.8163]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 4 	 [0.5, 0.31, 0.09] 	 lr: 0.0002 	 f: 82.49046774725018
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 12 [0m
Train...

[32m iter_0[0m
ga 	p: 83.33 	r: 75.68 	f1: 79.32 	 4668 	 5602 	 6168
wo 	p: 93.25 	r: 86.1 	f1: 89.53 	 2917 	 3128 	 3388
ni 	p: 87.44 	r: 79.57 	f1: 83.32 	 1211 	 1385 	 1522

[32m iter_1[0m
ga 	p: 84.03 	r: 75.16 	f1: 79.35 	 4636 	 5517 	 6168
wo 	p: 91.89 	r: 87.31 	f1: 89.54 	 2958 	 3219 	 3388
ni 	p: 86.53 	r: 79.76 	f1: 83.01 	 1214 	 1403 	 1522

[32m iter_2[0m
ga 	p: 83.77 	r: 75.39 	f1: 79.36 	 4650 	 5551 	 6168
wo 	p: 91.89 	r: 87.31 	f1: 89.54 	 2958 	 3219 	 3388
ni 	p: 85.72 	r: 80.49 	f1: 83.02 	 1225 	 1429 	 1522
best_thres [[0.5, 0.5, 0.19], [0.54, 0.34, 0.17], [0.53, 0.34, 0.15]]
f [0.8301, 0.8302, 0.8302]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 7 	 [0.39, 0.32, 0.15] 	 lr: 0.0001 	 f: 83.15167391745021
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 9 [0m
Train...
loss: tensor(464.4282) lr: 0.0001 time: 1591.82
pred_count_train 41644

Test...
loss: tensor(235.4300) lr: 0.0001 time: 1767.8
pred_count_train 41644

Test...
loss: tensor(792.6487) lr: 0.0001 time: 1746.17
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 84.08 	r: 74.24 	f1: 78.85 	 4579 	 5446 	 6168
wo 	p: 92.31 	r: 86.78 	f1: 89.46 	 2940 	 3185 	 3388
ni 	p: 87.87 	r: 75.23 	f1: 81.06 	 1145 	 1303 	 1522

[32m iter_1[0m
ga 	p: 84.49 	r: 74.09 	f1: 78.95 	 4570 	 5409 	 6168
wo 	p: 94.34 	r: 85.12 	f1: 89.5 	 2884 	 3057 	 3388
ni 	p: 88.6 	r: 75.03 	f1: 81.25 	 1142 	 1289 	 1522

[32m iter_2[0m
ga 	p: 83.25 	r: 75.03 	f1: 78.93 	 4628 	 5559 	 6168
wo 	p: 93.56 	r: 85.71 	f1: 89.46 	 2904 	 3104 	 3388
ni 	p: 89.3 	r: 74.57 	f1: 81.27 	 1135 	 1271 	 1522
best_thres [[0.46, 0.41, 0.17], [0.59, 0.82, 0.17], [0.52, 0.78, 0.19]]
f [0.8247, 0.8249, 0.825]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 9 	 [0.57, 0.75, 0.16] 	 lr: 0.0001 	 f: 82.71850215363098
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 11 [0m
Train...

[32m iter_0[0m
ga 	p: 80.65 	r: 75.89 	f1: 78.2 	 4681 	 5804 	 6168
wo 	p: 93.07 	r: 85.98 	f1: 89.38 	 2913 	 3130 	 3388
ni 	p: 85.07 	r: 69.65 	f1: 76.59 	 1060 	 1246 	 1522

[32m iter_1[0m
ga 	p: 80.87 	r: 76.02 	f1: 78.37 	 4689 	 5798 	 6168
wo 	p: 92.89 	r: 86.01 	f1: 89.32 	 2914 	 3137 	 3388
ni 	p: 77.63 	r: 75.69 	f1: 76.65 	 1152 	 1484 	 1522

[32m iter_2[0m
ga 	p: 80.75 	r: 76.17 	f1: 78.39 	 4698 	 5818 	 6168
wo 	p: 92.66 	r: 86.07 	f1: 89.24 	 2916 	 3147 	 3388
ni 	p: 78.59 	r: 74.77 	f1: 76.63 	 1138 	 1448 	 1522
best_thres [[0.37, 0.65, 0.25], [0.38, 0.84, 0.05], [0.36, 0.85, 0.06]]
f [0.8142, 0.8144, 0.8144]
load model: epoch4
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 4 	 [0.5, 0.31, 0.09] 	 lr: 0.0002 	 f: 82.49046774725018
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 13 [0m
Train...

[32m iter_0[0m
ga 	p: 83.04 	r: 75.31 	f1: 78.98 	 4645 	 5594 	 6168
wo 	p: 94.25 	r: 85.18 	f1: 89.49 	 2886 	 3062 	 3388
ni 	p: 85.52 	r: 77.6 	f1: 81.36 	 1181 	 1381 	 1522

[32m iter_1[0m
ga 	p: 83.12 	r: 75.31 	f1: 79.02 	 4645 	 5588 	 6168
wo 	p: 94.28 	r: 85.06 	f1: 89.43 	 2882 	 3057 	 3388
ni 	p: 85.52 	r: 77.99 	f1: 81.58 	 1187 	 1388 	 1522

[32m iter_2[0m
ga 	p: 83.31 	r: 75.16 	f1: 79.02 	 4636 	 5565 	 6168
wo 	p: 94.24 	r: 85.06 	f1: 89.42 	 2882 	 3058 	 3388
ni 	p: 85.52 	r: 77.99 	f1: 81.58 	 1187 	 1388 	 1522
best_thres [[0.54, 0.69, 0.24], [0.54, 0.69, 0.23], [0.55, 0.69, 0.23]]
f [0.8252, 0.8254, 0.8254]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 7 	 [0.39, 0.32, 0.15] 	 lr: 0.0001 	 f: 83.15167391745021
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 10 [0m
Train...
loss: tensor(311.9604) lr: 0.0001 time: 1525.77
pred_count_train 41644

Test...
loss: tensor(689.3998) lr: 5e-05 time: 1646.8
pred_count_train 41644

Test...
loss: tensor(557.0753) lr: 0.0001 time: 1568.29
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.89 	r: 74.76 	f1: 78.61 	 4611 	 5563 	 6168
wo 	p: 93.38 	r: 85.8 	f1: 89.43 	 2907 	 3113 	 3388
ni 	p: 84.82 	r: 70.5 	f1: 77.0 	 1073 	 1265 	 1522

[32m iter_1[0m
ga 	p: 82.34 	r: 74.97 	f1: 78.48 	 4624 	 5616 	 6168
wo 	p: 93.18 	r: 85.86 	f1: 89.37 	 2909 	 3122 	 3388
ni 	p: 82.18 	r: 72.73 	f1: 77.17 	 1107 	 1347 	 1522

[32m iter_2[0m
ga 	p: 82.0 	r: 75.18 	f1: 78.44 	 4637 	 5655 	 6168
wo 	p: 92.84 	r: 86.1 	f1: 89.34 	 2917 	 3142 	 3388
ni 	p: 81.72 	r: 73.13 	f1: 77.18 	 1113 	 1362 	 1522
best_thres [[0.53, 0.65, 0.13], [0.67, 0.82, 0.07], [0.7, 0.84, 0.06]]
f [0.8175, 0.817, 0.8167]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 9 	 [0.57, 0.75, 0.16] 	 lr: 0.0001 	 f: 82.71850215363098
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 12 [0m
Train...

[32m iter_0[0m
ga 	p: 82.82 	r: 76.44 	f1: 79.5 	 4715 	 5693 	 6168
wo 	p: 93.77 	r: 86.6 	f1: 90.04 	 2934 	 3129 	 3388
ni 	p: 87.77 	r: 74.51 	f1: 80.6 	 1134 	 1292 	 1522

[32m iter_1[0m
ga 	p: 82.66 	r: 76.65 	f1: 79.54 	 4728 	 5720 	 6168
wo 	p: 93.26 	r: 87.04 	f1: 90.05 	 2949 	 3162 	 3388
ni 	p: 86.72 	r: 75.1 	f1: 80.49 	 1143 	 1318 	 1522

[32m iter_2[0m
ga 	p: 82.72 	r: 76.69 	f1: 79.59 	 4730 	 5718 	 6168
wo 	p: 93.1 	r: 87.16 	f1: 90.03 	 2953 	 3172 	 3388
ni 	p: 85.46 	r: 76.08 	f1: 80.5 	 1158 	 1355 	 1522
best_thres [[0.48, 0.6, 0.2], [0.47, 0.63, 0.17], [0.47, 0.61, 0.14]]
f [0.8289, 0.829, 0.8291]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 13 	 [0.47, 0.61, 0.14] 	 lr: 5e-05 	 f: 82.90564795510478
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 14 [0m
Train...

[32m iter_0[0m
ga 	p: 81.59 	r: 76.23 	f1: 78.82 	 4702 	 5763 	 6168
wo 	p: 93.47 	r: 84.95 	f1: 89.01 	 2878 	 3079 	 3388
ni 	p: 84.09 	r: 77.46 	f1: 80.64 	 1179 	 1402 	 1522

[32m iter_1[0m
ga 	p: 81.12 	r: 76.83 	f1: 78.92 	 4739 	 5842 	 6168
wo 	p: 93.7 	r: 84.71 	f1: 88.98 	 2870 	 3063 	 3388
ni 	p: 84.86 	r: 76.94 	f1: 80.7 	 1171 	 1380 	 1522

[32m iter_2[0m
ga 	p: 80.97 	r: 76.98 	f1: 78.92 	 4748 	 5864 	 6168
wo 	p: 93.07 	r: 85.27 	f1: 89.0 	 2889 	 3104 	 3388
ni 	p: 84.79 	r: 76.94 	f1: 80.68 	 1171 	 1381 	 1522
best_thres [[0.39, 0.55, 0.14], [0.35, 0.6, 0.15], [0.34, 0.5, 0.15]]
f [0.8216, 0.8218, 0.8219]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 7 	 [0.39, 0.32, 0.15] 	 lr: 0.0001 	 f: 83.15167391745021
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 11 [0m
Train...
loss: tensor(233.9638) lr: 0.0001 time: 1525.24
pred_count_train 41644

Test...
loss: tensor(423.7678) lr: 5e-05 time: 1627.31
pred_count_train 41644

Test...
loss: tensor(389.5428) lr: 0.0001 time: 1598.9
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.14 	r: 76.85 	f1: 78.46 	 4740 	 5915 	 6168
wo 	p: 92.23 	r: 86.51 	f1: 89.28 	 2931 	 3178 	 3388
ni 	p: 84.56 	r: 75.56 	f1: 79.81 	 1150 	 1360 	 1522

[32m iter_1[0m
ga 	p: 81.18 	r: 76.01 	f1: 78.51 	 4688 	 5775 	 6168
wo 	p: 91.64 	r: 87.07 	f1: 89.3 	 2950 	 3219 	 3388
ni 	p: 84.63 	r: 76.35 	f1: 80.28 	 1162 	 1373 	 1522

[32m iter_2[0m
ga 	p: 81.45 	r: 75.81 	f1: 78.53 	 4676 	 5741 	 6168
wo 	p: 92.23 	r: 86.54 	f1: 89.29 	 2932 	 3179 	 3388
ni 	p: 83.44 	r: 77.46 	f1: 80.34 	 1179 	 1413 	 1522
best_thres [[0.21, 0.38, 0.15], [0.36, 0.37, 0.12], [0.43, 0.62, 0.09]]
f [0.8194, 0.82, 0.8203]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 9 	 [0.57, 0.75, 0.16] 	 lr: 0.0001 	 f: 82.71850215363098
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 13 [0m
Train...

[32m iter_0[0m
ga 	p: 82.71 	r: 75.92 	f1: 79.17 	 4683 	 5662 	 6168
wo 	p: 93.95 	r: 86.13 	f1: 89.87 	 2918 	 3106 	 3388
ni 	p: 81.65 	r: 73.98 	f1: 77.63 	 1126 	 1379 	 1522

[32m iter_1[0m
ga 	p: 82.16 	r: 76.44 	f1: 79.2 	 4715 	 5739 	 6168
wo 	p: 92.91 	r: 87.04 	f1: 89.88 	 2949 	 3174 	 3388
ni 	p: 83.22 	r: 72.67 	f1: 77.59 	 1106 	 1329 	 1522

[32m iter_2[0m
ga 	p: 82.73 	r: 76.04 	f1: 79.24 	 4690 	 5669 	 6168
wo 	p: 92.91 	r: 87.07 	f1: 89.9 	 2950 	 3175 	 3388
ni 	p: 82.12 	r: 73.65 	f1: 77.66 	 1121 	 1365 	 1522
best_thres [[0.41, 0.64, 0.13], [0.38, 0.49, 0.14], [0.41, 0.49, 0.12]]
f [0.8223, 0.8225, 0.8227]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 13 	 [0.47, 0.61, 0.14] 	 lr: 5e-05 	 f: 82.90564795510478
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 15 [0m
Train...

[32m iter_0[0m
ga 	p: 81.44 	r: 74.5 	f1: 77.82 	 4595 	 5642 	 6168
wo 	p: 93.67 	r: 85.15 	f1: 89.21 	 2885 	 3080 	 3388
ni 	p: 79.81 	r: 72.21 	f1: 75.82 	 1099 	 1377 	 1522

[32m iter_1[0m
ga 	p: 82.05 	r: 74.25 	f1: 77.96 	 4580 	 5582 	 6168
wo 	p: 93.63 	r: 85.09 	f1: 89.16 	 2883 	 3079 	 3388
ni 	p: 80.33 	r: 71.35 	f1: 75.57 	 1086 	 1352 	 1522

[32m iter_2[0m
ga 	p: 82.09 	r: 74.29 	f1: 77.99 	 4582 	 5582 	 6168
wo 	p: 93.52 	r: 85.15 	f1: 89.14 	 2885 	 3085 	 3388
ni 	p: 80.4 	r: 71.42 	f1: 75.64 	 1087 	 1352 	 1522
best_thres [[0.54, 0.64, 0.09], [0.58, 0.67, 0.09], [0.58, 0.65, 0.09]]
f [0.8102, 0.8104, 0.8106]
load model: epoch7
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 7 	 [0.39, 0.32, 0.15] 	 lr: 0.0001 	 f: 83.15167391745021
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 12 [0m
Train...
loss: tensor(194.9894) lr: 0.0001 time: 1543.34
pred_count_train 41644

Test...
loss: tensor(261.3515) lr: 5e-05 time: 1673.22
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.5 	r: 73.87 	f1: 78.39 	 4556 	 5456 	 6168
wo 	p: 93.89 	r: 85.66 	f1: 89.58 	 2902 	 3091 	 3388
ni 	p: 84.2 	r: 73.19 	f1: 78.31 	 1114 	 1323 	 1522

[32m iter_1[0m
ga 	p: 81.04 	r: 76.09 	f1: 78.48 	 4693 	 5791 	 6168
wo 	p: 93.46 	r: 86.01 	f1: 89.58 	 2914 	 3118 	 3388
ni 	p: 85.1 	r: 72.8 	f1: 78.47 	 1108 	 1302 	 1522

[32m iter_2[0m
ga 	p: 80.62 	r: 76.39 	f1: 78.45 	 4712 	 5845 	 6168
wo 	p: 93.31 	r: 86.07 	f1: 89.54 	 2916 	 3125 	 3388
ni 	p: 85.67 	r: 72.67 	f1: 78.63 	 1106 	 1291 	 1522
best_thres [[0.56, 0.64, 0.04], [0.49, 0.78, 0.04], [0.5, 0.85, 0.04]]
f [0.8184, 0.8186, 0.8186]
load model: epoch9
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 9 	 [0.57, 0.75, 0.16] 	 lr: 0.0001 	 f: 82.71850215363098
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 14 [0m
Train...
loss: tensor(985.2833) lr: 5e-05 time: 1619.32
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 85.65 	r: 72.8 	f1: 78.7 	 4490 	 5242 	 6168
wo 	p: 92.48 	r: 87.49 	f1: 89.91 	 2964 	 3205 	 3388
ni 	p: 86.5 	r: 72.4 	f1: 78.83 	 1102 	 1274 	 1522

[32m iter_1[0m
ga 	p: 82.21 	r: 75.58 	f1: 78.76 	 4662 	 5671 	 6168
wo 	p: 92.87 	r: 87.31 	f1: 90.0 	 2958 	 3185 	 3388
ni 	p: 85.27 	r: 73.78 	f1: 79.11 	 1123 	 1317 	 1522

[32m iter_2[0m
ga 	p: 82.3 	r: 75.6 	f1: 78.81 	 4663 	 5666 	 6168
wo 	p: 93.01 	r: 87.22 	f1: 90.02 	 2955 	 3177 	 3388
ni 	p: 83.3 	r: 75.36 	f1: 79.13 	 1147 	 1377 	 1522
best_thres [[0.68, 0.38, 0.2], [0.42, 0.4, 0.13], [0.42, 0.41, 0.09]]
f [0.8227, 0.8228, 0.8229]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 13 	 [0.47, 0.61, 0.14] 	 lr: 5e-05 	 f: 82.90564795510478
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 16 [0m
Train...

[32m iter_0[0m
ga 	p: 80.68 	r: 78.18 	f1: 79.41 	 4822 	 5977 	 6168
wo 	p: 92.53 	r: 87.01 	f1: 89.69 	 2948 	 3186 	 3388
ni 	p: 85.29 	r: 78.45 	f1: 81.72 	 1194 	 1400 	 1522

[32m iter_1[0m
ga 	p: 84.29 	r: 75.36 	f1: 79.58 	 4648 	 5514 	 6168
wo 	p: 92.71 	r: 87.07 	f1: 89.8 	 2950 	 3182 	 3388
ni 	p: 87.04 	r: 77.2 	f1: 81.82 	 1175 	 1350 	 1522

[32m iter_2[0m
ga 	p: 84.33 	r: 75.32 	f1: 79.58 	 4646 	 5509 	 6168
wo 	p: 92.74 	r: 87.07 	f1: 89.82 	 2950 	 3181 	 3388
ni 	p: 86.97 	r: 77.2 	f1: 81.8 	 1175 	 1351 	 1522
best_thres [[0.34, 0.35, 0.23], [0.51, 0.36, 0.27], [0.51, 0.36, 0.27]]
f [0.8284, 0.8295, 0.8299]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 7 	 [0.39, 0.32, 0.15] 	 lr: 0.0001 	 f: 83.15167391745021
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 13 [0m
Train...
loss: tensor(385.9503) lr: 5e-05 time: 1540.97
pred_count_train 41644

Test...
loss: tensor(166.9965) lr: 5e-05 time: 1655.2
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.65 	r: 75.81 	f1: 79.54 	 4676 	 5590 	 6168
wo 	p: 94.36 	r: 85.36 	f1: 89.63 	 2892 	 3065 	 3388
ni 	p: 81.75 	r: 75.62 	f1: 78.57 	 1151 	 1408 	 1522

[32m iter_1[0m
ga 	p: 82.37 	r: 76.78 	f1: 79.48 	 4736 	 5750 	 6168
wo 	p: 93.81 	r: 85.86 	f1: 89.66 	 2909 	 3101 	 3388
ni 	p: 85.59 	r: 73.0 	f1: 78.79 	 1111 	 1298 	 1522

[32m iter_2[0m
ga 	p: 82.68 	r: 76.49 	f1: 79.47 	 4718 	 5706 	 6168
wo 	p: 93.73 	r: 86.07 	f1: 89.74 	 2916 	 3111 	 3388
ni 	p: 85.73 	r: 73.0 	f1: 78.85 	 1111 	 1296 	 1522
best_thres [[0.44, 0.66, 0.07], [0.42, 0.73, 0.1], [0.47, 0.74, 0.1]]
f [0.8248, 0.8249, 0.8251]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 9 	 [0.57, 0.75, 0.16] 	 lr: 0.0001 	 f: 82.71850215363098
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 15 [0m
Train...
loss: tensor(738.5162) lr: 5e-05 time: 1628.08
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.67 	r: 76.8 	f1: 78.69 	 4737 	 5872 	 6168
wo 	p: 93.35 	r: 86.25 	f1: 89.66 	 2922 	 3130 	 3388
ni 	p: 82.21 	r: 71.35 	f1: 76.4 	 1086 	 1321 	 1522

[32m iter_1[0m
ga 	p: 81.79 	r: 76.18 	f1: 78.89 	 4699 	 5745 	 6168
wo 	p: 93.0 	r: 86.33 	f1: 89.55 	 2925 	 3145 	 3388
ni 	p: 80.11 	r: 73.32 	f1: 76.57 	 1116 	 1393 	 1522

[32m iter_2[0m
ga 	p: 81.75 	r: 76.2 	f1: 78.88 	 4700 	 5749 	 6168
wo 	p: 92.7 	r: 86.57 	f1: 89.53 	 2933 	 3164 	 3388
ni 	p: 80.33 	r: 73.26 	f1: 76.63 	 1115 	 1388 	 1522
best_thres [[0.23, 0.66, 0.16], [0.29, 0.84, 0.08], [0.29, 0.84, 0.08]]
f [0.8173, 0.8178, 0.818]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 13 	 [0.47, 0.61, 0.14] 	 lr: 5e-05 	 f: 82.90564795510478
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 17 [0m
Train...

[32m iter_0[0m
ga 	p: 82.97 	r: 74.32 	f1: 78.41 	 4584 	 5525 	 6168
wo 	p: 92.34 	r: 86.81 	f1: 89.49 	 2941 	 3185 	 3388
ni 	p: 85.76 	r: 76.35 	f1: 80.78 	 1162 	 1355 	 1522

[32m iter_1[0m
ga 	p: 82.73 	r: 74.72 	f1: 78.52 	 4609 	 5571 	 6168
wo 	p: 92.07 	r: 87.07 	f1: 89.5 	 2950 	 3204 	 3388
ni 	p: 86.28 	r: 76.02 	f1: 80.82 	 1157 	 1341 	 1522

[32m iter_2[0m
ga 	p: 82.71 	r: 74.74 	f1: 78.52 	 4610 	 5574 	 6168
wo 	p: 92.07 	r: 87.07 	f1: 89.5 	 2950 	 3204 	 3388
ni 	p: 86.34 	r: 76.02 	f1: 80.85 	 1157 	 1340 	 1522
best_thres [[0.65, 0.36, 0.15], [0.63, 0.33, 0.15], [0.63, 0.33, 0.15]]
f [0.8217, 0.8221, 0.8222]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 7 	 [0.39, 0.32, 0.15] 	 lr: 0.0001 	 f: 83.15167391745021
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 14 [0m
Train...
loss: tensor(219.5123) lr: 5e-05 time: 1494.74
pred_count_train 41644

Test...
loss: tensor(124.2113) lr: 5e-05 time: 1621.57
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.69 	r: 74.94 	f1: 79.07 	 4622 	 5523 	 6168
wo 	p: 92.39 	r: 86.75 	f1: 89.48 	 2939 	 3181 	 3388
ni 	p: 83.82 	r: 74.18 	f1: 78.7 	 1129 	 1347 	 1522

[32m iter_1[0m
ga 	p: 83.34 	r: 75.32 	f1: 79.13 	 4646 	 5575 	 6168
wo 	p: 93.52 	r: 86.07 	f1: 89.64 	 2916 	 3118 	 3388
ni 	p: 82.31 	r: 75.82 	f1: 78.93 	 1154 	 1402 	 1522

[32m iter_2[0m
ga 	p: 82.87 	r: 75.76 	f1: 79.16 	 4673 	 5639 	 6168
wo 	p: 93.57 	r: 85.92 	f1: 89.58 	 2911 	 3111 	 3388
ni 	p: 82.42 	r: 75.76 	f1: 78.95 	 1153 	 1399 	 1522
best_thres [[0.5, 0.44, 0.1], [0.64, 0.78, 0.06], [0.64, 0.85, 0.06]]
f [0.8226, 0.8229, 0.823]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 9 	 [0.57, 0.75, 0.16] 	 lr: 0.0001 	 f: 82.71850215363098
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 16 [0m
Train...
loss: tensor(539.4324) lr: 5e-05 time: 1619.8
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.25 	r: 74.95 	f1: 78.88 	 4623 	 5553 	 6168
wo 	p: 92.02 	r: 87.19 	f1: 89.54 	 2954 	 3210 	 3388
ni 	p: 85.98 	r: 67.28 	f1: 75.49 	 1024 	 1191 	 1522

[32m iter_1[0m
ga 	p: 83.29 	r: 75.06 	f1: 78.96 	 4630 	 5559 	 6168
wo 	p: 92.2 	r: 87.22 	f1: 89.64 	 2955 	 3205 	 3388
ni 	p: 83.05 	r: 69.51 	f1: 75.68 	 1058 	 1274 	 1522

[32m iter_2[0m
ga 	p: 83.13 	r: 75.0 	f1: 78.85 	 4626 	 5565 	 6168
wo 	p: 92.18 	r: 87.31 	f1: 89.68 	 2958 	 3209 	 3388
ni 	p: 84.44 	r: 68.46 	f1: 75.62 	 1042 	 1234 	 1522
best_thres [[0.49, 0.41, 0.19], [0.56, 0.4, 0.08], [0.7, 0.38, 0.1]]
f [0.8179, 0.8183, 0.8182]
load model: epoch13
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 13 	 [0.47, 0.61, 0.14] 	 lr: 5e-05 	 f: 82.90564795510478
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 18 [0m
Train...

[32m iter_0[0m
ga 	p: 81.51 	r: 75.81 	f1: 78.56 	 4676 	 5737 	 6168
wo 	p: 92.31 	r: 86.51 	f1: 89.32 	 2931 	 3175 	 3388
ni 	p: 82.16 	r: 73.52 	f1: 77.6 	 1119 	 1362 	 1522

[32m iter_1[0m
ga 	p: 82.01 	r: 75.41 	f1: 78.57 	 4651 	 5671 	 6168
wo 	p: 92.37 	r: 86.45 	f1: 89.31 	 2929 	 3171 	 3388
ni 	p: 79.97 	r: 75.03 	f1: 77.42 	 1142 	 1428 	 1522

[32m iter_2[0m
ga 	p: 82.04 	r: 75.39 	f1: 78.57 	 4650 	 5668 	 6168
wo 	p: 92.37 	r: 86.42 	f1: 89.3 	 2928 	 3170 	 3388
ni 	p: 79.92 	r: 75.03 	f1: 77.4 	 1142 	 1429 	 1522
best_thres [[0.47, 0.52, 0.09], [0.51, 0.54, 0.06], [0.51, 0.54, 0.06]]
f [0.8173, 0.8172, 0.8172]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 7 	 [0.39, 0.32, 0.15] 	 lr: 0.0001 	 f: 83.15167391745021
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 15 [0m
Train...
loss: tensor(141.1557) lr: 5e-05 time: 1463.83
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 84.38 	r: 74.55 	f1: 79.16 	 4598 	 5449 	 6168
wo 	p: 93.85 	r: 85.98 	f1: 89.74 	 2913 	 3104 	 3388
ni 	p: 84.76 	r: 74.57 	f1: 79.34 	 1135 	 1339 	 1522

[32m iter_1[0m
ga 	p: 83.43 	r: 75.19 	f1: 79.1 	 4638 	 5559 	 6168
wo 	p: 93.39 	r: 86.3 	f1: 89.71 	 2924 	 3131 	 3388
ni 	p: 85.92 	r: 74.18 	f1: 79.62 	 1129 	 1314 	 1522

[32m iter_2[0m
ga 	p: 83.08 	r: 75.54 	f1: 79.13 	 4659 	 5608 	 6168
wo 	p: 92.86 	r: 86.78 	f1: 89.72 	 2940 	 3166 	 3388
ni 	p: 86.24 	r: 74.11 	f1: 79.72 	 1128 	 1308 	 1522
best_thres [[0.46, 0.63, 0.07], [0.51, 0.77, 0.07], [0.53, 0.76, 0.07]]
f [0.8246, 0.8246, 0.8247]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 9 	 [0.57, 0.75, 0.16] 	 lr: 0.0001 	 f: 82.71850215363098
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 17 [0m
Train...
loss: tensor(391.2976) lr: 2.5e-05 time: 1610.28
pred_count_train 41644

Test...
loss: tensor(373.0005) lr: 5e-05 time: 1604.49
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.74 	r: 75.24 	f1: 79.27 	 4641 	 5542 	 6168
wo 	p: 93.26 	r: 87.04 	f1: 90.05 	 2949 	 3162 	 3388
ni 	p: 86.65 	r: 74.18 	f1: 79.93 	 1129 	 1303 	 1522

[32m iter_1[0m
ga 	p: 83.64 	r: 75.42 	f1: 79.32 	 4652 	 5562 	 6168
wo 	p: 93.27 	r: 87.19 	f1: 90.13 	 2954 	 3167 	 3388
ni 	p: 87.16 	r: 74.05 	f1: 80.07 	 1127 	 1293 	 1522

[32m iter_2[0m
ga 	p: 83.62 	r: 75.41 	f1: 79.3 	 4651 	 5562 	 6168
wo 	p: 93.33 	r: 87.19 	f1: 90.16 	 2954 	 3165 	 3388
ni 	p: 87.23 	r: 74.05 	f1: 80.1 	 1127 	 1292 	 1522
best_thres [[0.45, 0.5, 0.18], [0.45, 0.5, 0.17], [0.45, 0.54, 0.17]]
f [0.827, 0.8274, 0.8275]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 13 	 [0.47, 0.61, 0.14] 	 lr: 5e-05 	 f: 82.90564795510478
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 19 [0m
Train...
loss: tensor(100.4533) lr: 5e-05 time: 1472.3
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.39 	r: 77.16 	f1: 78.74 	 4759 	 5920 	 6168
wo 	p: 92.9 	r: 85.77 	f1: 89.2 	 2906 	 3128 	 3388
ni 	p: 81.55 	r: 73.46 	f1: 77.29 	 1118 	 1371 	 1522

[32m iter_1[0m
ga 	p: 81.42 	r: 76.36 	f1: 78.81 	 4710 	 5785 	 6168
wo 	p: 91.39 	r: 87.13 	f1: 89.21 	 2952 	 3230 	 3388
ni 	p: 84.32 	r: 71.35 	f1: 77.3 	 1086 	 1288 	 1522

[32m iter_2[0m
ga 	p: 81.55 	r: 76.3 	f1: 78.83 	 4706 	 5771 	 6168
wo 	p: 91.23 	r: 87.19 	f1: 89.16 	 2954 	 3238 	 3388
ni 	p: 81.86 	r: 73.26 	f1: 77.32 	 1115 	 1362 	 1522
best_thres [[0.29, 0.53, 0.08], [0.35, 0.3, 0.1], [0.36, 0.29, 0.07]]
f [0.8171, 0.8177, 0.8179]
load model: epoch7
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 7 	 [0.39, 0.32, 0.15] 	 lr: 0.0001 	 f: 83.15167391745021
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 16 [0m
Train...

[32m iter_0[0m
ga 	p: 82.28 	r: 76.35 	f1: 79.2 	 4709 	 5723 	 6168
wo 	p: 93.8 	r: 85.71 	f1: 89.57 	 2904 	 3096 	 3388
ni 	p: 82.44 	r: 75.56 	f1: 78.85 	 1150 	 1395 	 1522

[32m iter_1[0m
ga 	p: 83.23 	r: 75.15 	f1: 78.98 	 4635 	 5569 	 6168
wo 	p: 94.34 	r: 85.54 	f1: 89.72 	 2898 	 3072 	 3388
ni 	p: 85.05 	r: 73.98 	f1: 79.13 	 1126 	 1324 	 1522

[32m iter_2[0m
ga 	p: 81.65 	r: 76.39 	f1: 78.93 	 4712 	 5771 	 6168
wo 	p: 93.78 	r: 85.92 	f1: 89.68 	 2911 	 3104 	 3388
ni 	p: 83.33 	r: 75.56 	f1: 79.26 	 1150 	 1380 	 1522
best_thres [[0.35, 0.54, 0.07], [0.7, 0.85, 0.09], [0.58, 0.85, 0.06]]
f [0.8231, 0.8231, 0.8229]
load model: epoch9
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 9 	 [0.57, 0.75, 0.16] 	 lr: 0.0001 	 f: 82.71850215363098
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 18 [0m
Train...
loss: tensor(265.7165) lr: 2.5e-05 time: 1607.16
pred_count_train 41644

Test...
loss: tensor(950.5060) lr: 2.5e-05 time: 1642.28
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 84.86 	r: 73.88 	f1: 78.99 	 4557 	 5370 	 6168
wo 	p: 92.79 	r: 87.34 	f1: 89.98 	 2959 	 3189 	 3388
ni 	p: 82.64 	r: 75.69 	f1: 79.01 	 1152 	 1394 	 1522

[32m iter_1[0m
ga 	p: 84.36 	r: 74.4 	f1: 79.07 	 4589 	 5440 	 6168
wo 	p: 92.93 	r: 87.31 	f1: 90.03 	 2958 	 3183 	 3388
ni 	p: 84.58 	r: 74.57 	f1: 79.26 	 1135 	 1342 	 1522

[32m iter_2[0m
ga 	p: 84.01 	r: 74.64 	f1: 79.05 	 4604 	 5480 	 6168
wo 	p: 92.85 	r: 87.4 	f1: 90.04 	 2961 	 3189 	 3388
ni 	p: 84.63 	r: 74.51 	f1: 79.25 	 1134 	 1340 	 1522
best_thres [[0.58, 0.52, 0.13], [0.69, 0.69, 0.14], [0.69, 0.71, 0.14]]
f [0.8243, 0.8247, 0.8248]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 13 	 [0.47, 0.61, 0.14] 	 lr: 5e-05 	 f: 82.90564795510478
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 20 [0m
Train...
loss: tensor(363.2510) lr: 2.5e-05 time: 1493.81
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.15 	r: 76.98 	f1: 79.95 	 4748 	 5710 	 6168
wo 	p: 92.85 	r: 87.37 	f1: 90.02 	 2960 	 3188 	 3388
ni 	p: 85.07 	r: 80.49 	f1: 82.71 	 1225 	 1440 	 1522

[32m iter_1[0m
ga 	p: 82.99 	r: 77.06 	f1: 79.92 	 4753 	 5727 	 6168
wo 	p: 93.18 	r: 87.13 	f1: 90.05 	 2952 	 3168 	 3388
ni 	p: 88.05 	r: 77.92 	f1: 82.68 	 1186 	 1347 	 1522

[32m iter_2[0m
ga 	p: 83.02 	r: 77.04 	f1: 79.92 	 4752 	 5724 	 6168
wo 	p: 93.18 	r: 87.16 	f1: 90.07 	 2953 	 3169 	 3388
ni 	p: 88.05 	r: 77.92 	f1: 82.68 	 1186 	 1347 	 1522
best_thres [[0.42, 0.45, 0.13], [0.41, 0.49, 0.17], [0.41, 0.48, 0.17]]
f [0.8342, 0.8341, 0.8341]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 16 	 [0.41, 0.48, 0.17] 	 lr: 2.5e-05 	 f: 83.4139944421894
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 17 [0m
Train...

[32m iter_0[0m
ga 	p: 83.07 	r: 76.05 	f1: 79.41 	 4691 	 5647 	 6168
wo 	p: 93.68 	r: 86.16 	f1: 89.76 	 2919 	 3116 	 3388
ni 	p: 86.64 	r: 77.14 	f1: 81.61 	 1174 	 1355 	 1522

[32m iter_1[0m
ga 	p: 83.16 	r: 76.12 	f1: 79.48 	 4695 	 5646 	 6168
wo 	p: 94.25 	r: 85.66 	f1: 89.75 	 2902 	 3079 	 3388
ni 	p: 86.71 	r: 77.6 	f1: 81.9 	 1181 	 1362 	 1522

[32m iter_2[0m
ga 	p: 83.91 	r: 75.31 	f1: 79.37 	 4645 	 5536 	 6168
wo 	p: 94.25 	r: 85.66 	f1: 89.75 	 2902 	 3079 	 3388
ni 	p: 86.76 	r: 77.53 	f1: 81.89 	 1180 	 1360 	 1522
best_thres [[0.4, 0.64, 0.14], [0.48, 0.83, 0.12], [0.57, 0.85, 0.12]]
f [0.8288, 0.8292, 0.8291]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 18 	 [0.57, 0.85, 0.12] 	 lr: 2.5e-05 	 f: 82.91229066136815
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 19 [0m
Train...
loss: tensor(177.1757) lr: 2.5e-05 time: 1607.79
pred_count_train 41644

Test...
loss: tensor(780.4482) lr: 2.5e-05 time: 1652.59
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 84.98 	r: 73.56 	f1: 78.86 	 4537 	 5339 	 6168
wo 	p: 93.58 	r: 86.48 	f1: 89.89 	 2930 	 3131 	 3388
ni 	p: 83.25 	r: 74.77 	f1: 78.78 	 1138 	 1367 	 1522

[32m iter_1[0m
ga 	p: 82.86 	r: 75.31 	f1: 78.9 	 4645 	 5606 	 6168
wo 	p: 93.65 	r: 86.63 	f1: 90.0 	 2935 	 3134 	 3388
ni 	p: 84.7 	r: 74.18 	f1: 79.09 	 1129 	 1333 	 1522

[32m iter_2[0m
ga 	p: 83.68 	r: 74.72 	f1: 78.95 	 4609 	 5508 	 6168
wo 	p: 93.54 	r: 86.69 	f1: 89.98 	 2937 	 3140 	 3388
ni 	p: 84.75 	r: 74.11 	f1: 79.07 	 1128 	 1331 	 1522
best_thres [[0.61, 0.63, 0.09], [0.41, 0.83, 0.09], [0.64, 0.85, 0.09]]
f [0.8229, 0.8232, 0.8234]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 13 	 [0.47, 0.61, 0.14] 	 lr: 5e-05 	 f: 82.90564795510478
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 21 [0m
Train...
loss: tensor(228.9917) lr: 2.5e-05 time: 1497.42
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 84.41 	r: 75.47 	f1: 79.69 	 4655 	 5515 	 6168
wo 	p: 92.54 	r: 87.16 	f1: 89.77 	 2953 	 3191 	 3388
ni 	p: 85.32 	r: 77.92 	f1: 81.46 	 1186 	 1390 	 1522

[32m iter_1[0m
ga 	p: 83.88 	r: 75.86 	f1: 79.67 	 4679 	 5578 	 6168
wo 	p: 92.38 	r: 87.31 	f1: 89.77 	 2958 	 3202 	 3388
ni 	p: 85.14 	r: 77.92 	f1: 81.37 	 1186 	 1393 	 1522

[32m iter_2[0m
ga 	p: 83.75 	r: 75.97 	f1: 79.67 	 4686 	 5595 	 6168
wo 	p: 92.51 	r: 87.19 	f1: 89.77 	 2954 	 3193 	 3388
ni 	p: 85.08 	r: 77.92 	f1: 81.34 	 1186 	 1394 	 1522
best_thres [[0.56, 0.51, 0.17], [0.53, 0.49, 0.16], [0.52, 0.51, 0.16]]
f [0.8306, 0.8305, 0.8304]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 16 	 [0.41, 0.48, 0.17] 	 lr: 2.5e-05 	 f: 83.4139944421894
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 18 [0m
Train...

[32m iter_0[0m
ga 	p: 83.86 	r: 75.24 	f1: 79.32 	 4641 	 5534 	 6168
wo 	p: 93.95 	r: 85.71 	f1: 89.64 	 2904 	 3091 	 3388
ni 	p: 81.62 	r: 78.19 	f1: 79.87 	 1190 	 1458 	 1522

[32m iter_1[0m
ga 	p: 82.13 	r: 76.67 	f1: 79.31 	 4729 	 5758 	 6168
wo 	p: 94.06 	r: 85.57 	f1: 89.61 	 2899 	 3082 	 3388
ni 	p: 81.11 	r: 78.71 	f1: 79.89 	 1198 	 1477 	 1522

[32m iter_2[0m
ga 	p: 82.6 	r: 76.22 	f1: 79.28 	 4701 	 5691 	 6168
wo 	p: 93.58 	r: 85.98 	f1: 89.62 	 2913 	 3113 	 3388
ni 	p: 83.29 	r: 76.94 	f1: 79.99 	 1171 	 1406 	 1522
best_thres [[0.49, 0.63, 0.1], [0.43, 0.79, 0.08], [0.52, 0.77, 0.11]]
f [0.8256, 0.8253, 0.8253]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 18 	 [0.57, 0.85, 0.12] 	 lr: 2.5e-05 	 f: 82.91229066136815
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 20 [0m
Train...
loss: tensor(117.3512) lr: 2.5e-05 time: 1605.3
pred_count_train 41644

Test...
loss: tensor(637.3534) lr: 2.5e-05 time: 1650.27
pred_count_train 41644

Test...
loss: tensor(147.0478) lr: 2.5e-05 time: 1531.99
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.16 	r: 74.77 	f1: 78.74 	 4612 	 5546 	 6168
wo 	p: 93.68 	r: 86.13 	f1: 89.74 	 2918 	 3115 	 3388
ni 	p: 81.53 	r: 72.8 	f1: 76.92 	 1108 	 1359 	 1522

[32m iter_1[0m
ga 	p: 82.1 	r: 75.68 	f1: 78.76 	 4668 	 5686 	 6168
wo 	p: 93.07 	r: 86.84 	f1: 89.85 	 2942 	 3161 	 3388
ni 	p: 81.08 	r: 74.05 	f1: 77.4 	 1127 	 1390 	 1522

[32m iter_2[0m
ga 	p: 81.92 	r: 75.81 	f1: 78.75 	 4676 	 5708 	 6168
wo 	p: 92.93 	r: 86.89 	f1: 89.81 	 2944 	 3168 	 3388
ni 	p: 82.85 	r: 72.67 	f1: 77.42 	 1106 	 1335 	 1522
best_thres [[0.38, 0.58, 0.09], [0.29, 0.48, 0.06], [0.27, 0.45, 0.08]]
f [0.8188, 0.8193, 0.8195]
load model: epoch13
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 13 	 [0.47, 0.61, 0.14] 	 lr: 5e-05 	 f: 82.90564795510478
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 22 [0m
Train...

[32m iter_0[0m
ga 	p: 84.13 	r: 74.84 	f1: 79.21 	 4616 	 5487 	 6168
wo 	p: 92.31 	r: 87.19 	f1: 89.68 	 2954 	 3200 	 3388
ni 	p: 85.37 	r: 77.46 	f1: 81.23 	 1179 	 1381 	 1522

[32m iter_1[0m
ga 	p: 84.3 	r: 74.87 	f1: 79.31 	 4618 	 5478 	 6168
wo 	p: 93.1 	r: 86.48 	f1: 89.67 	 2930 	 3147 	 3388
ni 	p: 85.17 	r: 77.33 	f1: 81.06 	 1177 	 1382 	 1522

[32m iter_2[0m
ga 	p: 84.3 	r: 74.85 	f1: 79.3 	 4617 	 5477 	 6168
wo 	p: 93.1 	r: 86.48 	f1: 89.67 	 2930 	 3147 	 3388
ni 	p: 85.23 	r: 77.33 	f1: 81.09 	 1177 	 1381 	 1522
best_thres [[0.53, 0.43, 0.15], [0.53, 0.52, 0.14], [0.53, 0.52, 0.14]]
f [0.8275, 0.8275, 0.8276]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 16 	 [0.41, 0.48, 0.17] 	 lr: 2.5e-05 	 f: 83.4139944421894
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 19 [0m
Train...

[32m iter_0[0m
ga 	p: 83.41 	r: 75.5 	f1: 79.26 	 4657 	 5583 	 6168
wo 	p: 93.3 	r: 86.28 	f1: 89.65 	 2923 	 3133 	 3388
ni 	p: 85.05 	r: 73.65 	f1: 78.94 	 1121 	 1318 	 1522

[32m iter_1[0m
ga 	p: 83.66 	r: 75.29 	f1: 79.26 	 4644 	 5551 	 6168
wo 	p: 92.52 	r: 86.92 	f1: 89.64 	 2945 	 3183 	 3388
ni 	p: 84.37 	r: 74.84 	f1: 79.32 	 1139 	 1350 	 1522

[32m iter_2[0m
ga 	p: 83.44 	r: 75.42 	f1: 79.23 	 4652 	 5575 	 6168
wo 	p: 92.26 	r: 87.25 	f1: 89.68 	 2956 	 3204 	 3388
ni 	p: 83.13 	r: 76.08 	f1: 79.45 	 1158 	 1393 	 1522
best_thres [[0.43, 0.54, 0.12], [0.65, 0.55, 0.08], [0.7, 0.53, 0.06]]
f [0.8243, 0.8246, 0.8247]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 18 	 [0.57, 0.85, 0.12] 	 lr: 2.5e-05 	 f: 82.91229066136815
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 21 [0m
Train...
loss: tensor(383.2442) lr: 1.25e-05 time: 1592.16
pred_count_train 41644

Test...
loss: tensor(506.3449) lr: 2.5e-05 time: 1674.85
pred_count_train 41644

Test...
loss: tensor(94.5815) lr: 2.5e-05 time: 1553.67
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.58 	r: 76.15 	f1: 79.69 	 4697 	 5620 	 6168
wo 	p: 92.7 	r: 87.66 	f1: 90.11 	 2970 	 3204 	 3388
ni 	p: 85.92 	r: 74.18 	f1: 79.62 	 1129 	 1314 	 1522

[32m iter_1[0m
ga 	p: 83.08 	r: 76.64 	f1: 79.73 	 4727 	 5690 	 6168
wo 	p: 92.71 	r: 87.78 	f1: 90.18 	 2974 	 3208 	 3388
ni 	p: 84.4 	r: 75.69 	f1: 79.81 	 1152 	 1365 	 1522

[32m iter_2[0m
ga 	p: 83.47 	r: 76.31 	f1: 79.73 	 4707 	 5639 	 6168
wo 	p: 92.7 	r: 87.75 	f1: 90.16 	 2973 	 3207 	 3388
ni 	p: 84.58 	r: 75.69 	f1: 79.89 	 1152 	 1362 	 1522
best_thres [[0.43, 0.45, 0.19], [0.39, 0.44, 0.14], [0.42, 0.44, 0.14]]
f [0.8292, 0.8294, 0.8296]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 22 	 [0.42, 0.44, 0.14] 	 lr: 1.25e-05 	 f: 82.95662797800856
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 23 [0m
Train...

[32m iter_0[0m
ga 	p: 81.52 	r: 75.97 	f1: 78.65 	 4686 	 5748 	 6168
wo 	p: 92.01 	r: 86.72 	f1: 89.29 	 2938 	 3193 	 3388
ni 	p: 84.05 	r: 77.2 	f1: 80.48 	 1175 	 1398 	 1522

[32m iter_1[0m
ga 	p: 81.51 	r: 76.1 	f1: 78.71 	 4694 	 5759 	 6168
wo 	p: 92.13 	r: 86.69 	f1: 89.32 	 2937 	 3188 	 3388
ni 	p: 83.87 	r: 77.2 	f1: 80.4 	 1175 	 1401 	 1522

[32m iter_2[0m
ga 	p: 81.54 	r: 76.13 	f1: 78.75 	 4696 	 5759 	 6168
wo 	p: 92.24 	r: 86.63 	f1: 89.35 	 2935 	 3182 	 3388
ni 	p: 84.17 	r: 76.87 	f1: 80.36 	 1170 	 1390 	 1522
best_thres [[0.42, 0.49, 0.16], [0.41, 0.5, 0.15], [0.41, 0.51, 0.16]]
f [0.8217, 0.8218, 0.822]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 16 	 [0.41, 0.48, 0.17] 	 lr: 2.5e-05 	 f: 83.4139944421894
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 20 [0m
Train...

[32m iter_0[0m
ga 	p: 80.49 	r: 77.79 	f1: 79.12 	 4798 	 5961 	 6168
wo 	p: 93.66 	r: 85.45 	f1: 89.37 	 2895 	 3091 	 3388
ni 	p: 86.28 	r: 71.48 	f1: 78.19 	 1088 	 1261 	 1522

[32m iter_1[0m
ga 	p: 83.11 	r: 75.49 	f1: 79.12 	 4656 	 5602 	 6168
wo 	p: 91.36 	r: 87.43 	f1: 89.35 	 2962 	 3242 	 3388
ni 	p: 84.52 	r: 73.52 	f1: 78.64 	 1119 	 1324 	 1522

[32m iter_2[0m
ga 	p: 80.86 	r: 77.45 	f1: 79.12 	 4777 	 5908 	 6168
wo 	p: 91.35 	r: 87.31 	f1: 89.28 	 2958 	 3238 	 3388
ni 	p: 85.03 	r: 73.52 	f1: 78.86 	 1119 	 1316 	 1522
best_thres [[0.14, 0.7, 0.13], [0.49, 0.43, 0.07], [0.15, 0.48, 0.07]]
f [0.821, 0.8217, 0.8219]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 18 	 [0.57, 0.85, 0.12] 	 lr: 2.5e-05 	 f: 82.91229066136815
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 22 [0m
Train...
loss: tensor(301.5426) lr: 1.25e-05 time: 1592.24
pred_count_train 41644

Test...
loss: tensor(389.8503) lr: 2.5e-05 time: 1658.39
pred_count_train 41644

Test...
loss: tensor(64.5483) lr: 2.5e-05 time: 1565.54
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.87 	r: 75.26 	f1: 79.33 	 4642 	 5535 	 6168
wo 	p: 93.54 	r: 86.72 	f1: 90.0 	 2938 	 3141 	 3388
ni 	p: 88.99 	r: 71.68 	f1: 79.4 	 1091 	 1226 	 1522

[32m iter_1[0m
ga 	p: 82.84 	r: 76.02 	f1: 79.29 	 4689 	 5660 	 6168
wo 	p: 93.29 	r: 87.04 	f1: 90.06 	 2949 	 3161 	 3388
ni 	p: 87.02 	r: 73.13 	f1: 79.47 	 1113 	 1279 	 1522

[32m iter_2[0m
ga 	p: 83.75 	r: 75.31 	f1: 79.31 	 4645 	 5546 	 6168
wo 	p: 93.15 	r: 87.1 	f1: 90.02 	 2951 	 3168 	 3388
ni 	p: 83.87 	r: 75.49 	f1: 79.46 	 1149 	 1370 	 1522
best_thres [[0.51, 0.56, 0.28], [0.45, 0.64, 0.2], [0.64, 0.64, 0.13]]
f [0.8266, 0.8265, 0.8265]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 22 	 [0.42, 0.44, 0.14] 	 lr: 1.25e-05 	 f: 82.95662797800856
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 24 [0m
Train...

[32m iter_0[0m
ga 	p: 82.75 	r: 74.98 	f1: 78.68 	 4625 	 5589 	 6168
wo 	p: 92.58 	r: 85.83 	f1: 89.08 	 2908 	 3141 	 3388
ni 	p: 84.64 	r: 74.97 	f1: 79.51 	 1141 	 1348 	 1522

[32m iter_1[0m
ga 	p: 81.62 	r: 75.94 	f1: 78.68 	 4684 	 5739 	 6168
wo 	p: 92.57 	r: 85.71 	f1: 89.01 	 2904 	 3137 	 3388
ni 	p: 84.98 	r: 74.38 	f1: 79.33 	 1132 	 1332 	 1522

[32m iter_2[0m
ga 	p: 81.88 	r: 75.76 	f1: 78.7 	 4673 	 5707 	 6168
wo 	p: 92.6 	r: 85.71 	f1: 89.03 	 2904 	 3136 	 3388
ni 	p: 84.42 	r: 74.77 	f1: 79.3 	 1138 	 1348 	 1522
best_thres [[0.46, 0.65, 0.12], [0.37, 0.66, 0.11], [0.39, 0.66, 0.1]]
f [0.82, 0.8197, 0.8196]
load model: epoch16
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 16 	 [0.41, 0.48, 0.17] 	 lr: 2.5e-05 	 f: 83.4139944421894
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 21 [0m
Train...

[32m iter_0[0m
ga 	p: 81.53 	r: 77.35 	f1: 79.38 	 4771 	 5852 	 6168
wo 	p: 92.56 	r: 86.72 	f1: 89.55 	 2938 	 3174 	 3388
ni 	p: 86.8 	r: 71.75 	f1: 78.56 	 1092 	 1258 	 1522

[32m iter_1[0m
ga 	p: 82.02 	r: 76.83 	f1: 79.34 	 4739 	 5778 	 6168
wo 	p: 92.65 	r: 86.72 	f1: 89.59 	 2938 	 3171 	 3388
ni 	p: 81.4 	r: 76.48 	f1: 78.86 	 1164 	 1430 	 1522

[32m iter_2[0m
ga 	p: 82.02 	r: 76.72 	f1: 79.28 	 4732 	 5769 	 6168
wo 	p: 92.6 	r: 86.75 	f1: 89.58 	 2939 	 3174 	 3388
ni 	p: 81.91 	r: 76.48 	f1: 79.1 	 1164 	 1421 	 1522
best_thres [[0.16, 0.5, 0.13], [0.19, 0.75, 0.03], [0.19, 0.84, 0.03]]
f [0.824, 0.824, 0.824]
load model: epoch18
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 18 	 [0.57, 0.85, 0.12] 	 lr: 2.5e-05 	 f: 82.91229066136815
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 23 [0m
Train...
loss: tensor(235.8145) lr: 1.25e-05 time: 1579.25
pred_count_train 41644

Test...
loss: tensor(758.2691) lr: 1.25e-05 time: 1665.28
pred_count_train 41644

Test...
loss: tensor(220.7832) lr: 1.25e-05 time: 1564.97
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.97 	r: 75.49 	f1: 79.05 	 4656 	 5612 	 6168
wo 	p: 94.02 	r: 86.33 	f1: 90.01 	 2925 	 3111 	 3388
ni 	p: 84.45 	r: 74.24 	f1: 79.02 	 1130 	 1338 	 1522

[32m iter_1[0m
ga 	p: 82.65 	r: 75.94 	f1: 79.16 	 4684 	 5667 	 6168
wo 	p: 92.71 	r: 87.49 	f1: 90.02 	 2964 	 3197 	 3388
ni 	p: 85.1 	r: 73.19 	f1: 78.7 	 1114 	 1309 	 1522

[32m iter_2[0m
ga 	p: 82.77 	r: 75.79 	f1: 79.13 	 4675 	 5648 	 6168
wo 	p: 92.74 	r: 87.46 	f1: 90.02 	 2963 	 3195 	 3388
ni 	p: 85.38 	r: 73.26 	f1: 78.85 	 1115 	 1306 	 1522
best_thres [[0.4, 0.6, 0.17], [0.37, 0.41, 0.17], [0.38, 0.41, 0.17]]
f [0.8242, 0.8244, 0.8245]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 22 	 [0.42, 0.44, 0.14] 	 lr: 1.25e-05 	 f: 82.95662797800856
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 25 [0m
Train...

[32m iter_0[0m
ga 	p: 83.75 	r: 76.18 	f1: 79.79 	 4699 	 5611 	 6168
wo 	p: 92.57 	r: 87.57 	f1: 90.0 	 2967 	 3205 	 3388
ni 	p: 85.28 	r: 79.96 	f1: 82.54 	 1217 	 1427 	 1522

[32m iter_1[0m
ga 	p: 83.35 	r: 76.62 	f1: 79.84 	 4726 	 5670 	 6168
wo 	p: 93.57 	r: 86.81 	f1: 90.06 	 2941 	 3143 	 3388
ni 	p: 84.11 	r: 80.68 	f1: 82.36 	 1228 	 1460 	 1522

[32m iter_2[0m
ga 	p: 83.23 	r: 76.78 	f1: 79.88 	 4736 	 5690 	 6168
wo 	p: 93.57 	r: 86.81 	f1: 90.06 	 2941 	 3143 	 3388
ni 	p: 84.05 	r: 80.68 	f1: 82.33 	 1228 	 1461 	 1522
best_thres [[0.51, 0.45, 0.16], [0.48, 0.56, 0.13], [0.47, 0.56, 0.13]]
f [0.8333, 0.8332, 0.8333]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 16 	 [0.41, 0.48, 0.17] 	 lr: 2.5e-05 	 f: 83.4139944421894
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 22 [0m
Train...

[32m iter_0[0m
ga 	p: 84.22 	r: 75.26 	f1: 79.49 	 4642 	 5512 	 6168
wo 	p: 93.06 	r: 86.63 	f1: 89.73 	 2935 	 3154 	 3388
ni 	p: 84.33 	r: 77.46 	f1: 80.75 	 1179 	 1398 	 1522

[32m iter_1[0m
ga 	p: 84.91 	r: 74.64 	f1: 79.45 	 4604 	 5422 	 6168
wo 	p: 93.7 	r: 85.98 	f1: 89.67 	 2913 	 3109 	 3388
ni 	p: 84.24 	r: 78.65 	f1: 81.35 	 1197 	 1421 	 1522

[32m iter_2[0m
ga 	p: 84.37 	r: 75.11 	f1: 79.48 	 4633 	 5491 	 6168
wo 	p: 93.96 	r: 85.83 	f1: 89.71 	 2908 	 3095 	 3388
ni 	p: 84.33 	r: 78.52 	f1: 81.32 	 1195 	 1417 	 1522
best_thres [[0.5, 0.51, 0.11], [0.69, 0.78, 0.08], [0.68, 0.83, 0.08]]
f [0.8283, 0.8285, 0.8286]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 18 	 [0.57, 0.85, 0.12] 	 lr: 2.5e-05 	 f: 82.91229066136815
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 24 [0m
Train...
loss: tensor(182.8565) lr: 1.25e-05 time: 1569.21
pred_count_train 41644

Test...
loss: tensor(674.8832) lr: 1.25e-05 time: 1673.52
pred_count_train 41644

Test...
loss: tensor(163.0326) lr: 1.25e-05 time: 1567.06
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 84.06 	r: 74.66 	f1: 79.08 	 4605 	 5478 	 6168
wo 	p: 94.97 	r: 85.21 	f1: 89.83 	 2887 	 3040 	 3388
ni 	p: 83.25 	r: 74.11 	f1: 78.42 	 1128 	 1355 	 1522

[32m iter_1[0m
ga 	p: 83.21 	r: 75.37 	f1: 79.1 	 4649 	 5587 	 6168
wo 	p: 93.55 	r: 86.42 	f1: 89.84 	 2928 	 3130 	 3388
ni 	p: 85.06 	r: 72.93 	f1: 78.53 	 1110 	 1305 	 1522

[32m iter_2[0m
ga 	p: 83.27 	r: 75.36 	f1: 79.11 	 4648 	 5582 	 6168
wo 	p: 93.49 	r: 86.45 	f1: 89.83 	 2929 	 3133 	 3388
ni 	p: 83.49 	r: 74.44 	f1: 78.71 	 1133 	 1357 	 1522
best_thres [[0.51, 0.79, 0.14], [0.44, 0.79, 0.16], [0.44, 0.82, 0.12]]
f [0.8229, 0.8231, 0.8233]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 22 	 [0.42, 0.44, 0.14] 	 lr: 1.25e-05 	 f: 82.95662797800856
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 26 [0m
Train...

[32m iter_0[0m
ga 	p: 84.47 	r: 75.19 	f1: 79.56 	 4638 	 5491 	 6168
wo 	p: 91.7 	r: 87.75 	f1: 89.68 	 2973 	 3242 	 3388
ni 	p: 87.7 	r: 76.35 	f1: 81.63 	 1162 	 1325 	 1522

[32m iter_1[0m
ga 	p: 83.7 	r: 75.83 	f1: 79.57 	 4677 	 5588 	 6168
wo 	p: 92.11 	r: 87.57 	f1: 89.79 	 2967 	 3221 	 3388
ni 	p: 83.51 	r: 79.83 	f1: 81.63 	 1215 	 1455 	 1522

[32m iter_2[0m
ga 	p: 83.69 	r: 75.79 	f1: 79.55 	 4675 	 5586 	 6168
wo 	p: 92.06 	r: 87.6 	f1: 89.78 	 2968 	 3224 	 3388
ni 	p: 84.11 	r: 79.3 	f1: 81.64 	 1207 	 1435 	 1522
best_thres [[0.53, 0.39, 0.22], [0.48, 0.42, 0.11], [0.48, 0.41, 0.12]]
f [0.8301, 0.8302, 0.8301]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 16 	 [0.41, 0.48, 0.17] 	 lr: 2.5e-05 	 f: 83.4139944421894
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 23 [0m
Train...

[32m iter_0[0m
ga 	p: 82.23 	r: 76.46 	f1: 79.24 	 4716 	 5735 	 6168
wo 	p: 92.89 	r: 86.42 	f1: 89.54 	 2928 	 3152 	 3388
ni 	p: 82.64 	r: 76.94 	f1: 79.69 	 1171 	 1417 	 1522

[32m iter_1[0m
ga 	p: 82.27 	r: 76.38 	f1: 79.22 	 4711 	 5726 	 6168
wo 	p: 92.34 	r: 86.87 	f1: 89.52 	 2943 	 3187 	 3388
ni 	p: 84.36 	r: 76.22 	f1: 80.08 	 1160 	 1375 	 1522

[32m iter_2[0m
ga 	p: 83.27 	r: 75.62 	f1: 79.26 	 4664 	 5601 	 6168
wo 	p: 92.16 	r: 87.07 	f1: 89.54 	 2950 	 3201 	 3388
ni 	p: 84.5 	r: 76.28 	f1: 80.18 	 1161 	 1374 	 1522
best_thres [[0.33, 0.54, 0.1], [0.44, 0.61, 0.1], [0.6, 0.61, 0.1]]
f [0.8245, 0.8248, 0.8251]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 18 	 [0.57, 0.85, 0.12] 	 lr: 2.5e-05 	 f: 82.91229066136815
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 25 [0m
Train...
loss: tensor(135.4270) lr: 1.25e-05 time: 1537.61
pred_count_train 41644

Test...
loss: tensor(594.0471) lr: 1.25e-05 time: 1651.68
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 84.5 	r: 73.43 	f1: 78.57 	 4529 	 5360 	 6168
wo 	p: 93.9 	r: 85.89 	f1: 89.72 	 2910 	 3099 	 3388
ni 	p: 81.84 	r: 74.05 	f1: 77.75 	 1127 	 1377 	 1522

[32m iter_1[0m
ga 	p: 83.25 	r: 74.37 	f1: 78.56 	 4587 	 5510 	 6168
wo 	p: 93.68 	r: 86.16 	f1: 89.76 	 2919 	 3116 	 3388
ni 	p: 80.66 	r: 75.89 	f1: 78.2 	 1155 	 1432 	 1522

[32m iter_2[0m
ga 	p: 83.07 	r: 74.55 	f1: 78.58 	 4598 	 5535 	 6168
wo 	p: 93.28 	r: 86.45 	f1: 89.74 	 2929 	 3140 	 3388
ni 	p: 80.98 	r: 75.82 	f1: 78.32 	 1154 	 1425 	 1522
best_thres [[0.61, 0.63, 0.13], [0.67, 0.79, 0.08], [0.7, 0.74, 0.08]]
f [0.8192, 0.8194, 0.8195]
load model: epoch22
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 22 	 [0.42, 0.44, 0.14] 	 lr: 1.25e-05 	 f: 82.95662797800856
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 27 [0m
Train...
loss: tensor(118.9120) lr: 1.25e-05 time: 1589.3
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 85.78 	r: 73.82 	f1: 79.35 	 4553 	 5308 	 6168
wo 	p: 92.25 	r: 87.16 	f1: 89.63 	 2953 	 3201 	 3388
ni 	p: 85.33 	r: 79.11 	f1: 82.1 	 1204 	 1411 	 1522

[32m iter_1[0m
ga 	p: 83.98 	r: 75.31 	f1: 79.41 	 4645 	 5531 	 6168
wo 	p: 93.39 	r: 86.28 	f1: 89.69 	 2923 	 3130 	 3388
ni 	p: 84.7 	r: 79.3 	f1: 81.91 	 1207 	 1425 	 1522

[32m iter_2[0m
ga 	p: 84.13 	r: 75.19 	f1: 79.41 	 4638 	 5513 	 6168
wo 	p: 93.39 	r: 86.28 	f1: 89.69 	 2923 	 3130 	 3388
ni 	p: 84.58 	r: 79.3 	f1: 81.86 	 1207 	 1427 	 1522
best_thres [[0.64, 0.42, 0.16], [0.53, 0.55, 0.14], [0.54, 0.55, 0.14]]
f [0.8296, 0.8294, 0.8293]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 16 	 [0.41, 0.48, 0.17] 	 lr: 2.5e-05 	 f: 83.4139944421894
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 24 [0m
Train...

[32m iter_0[0m
ga 	p: 81.05 	r: 77.55 	f1: 79.26 	 4783 	 5901 	 6168
wo 	p: 92.67 	r: 86.57 	f1: 89.52 	 2933 	 3165 	 3388
ni 	p: 85.43 	r: 73.59 	f1: 79.07 	 1120 	 1311 	 1522

[32m iter_1[0m
ga 	p: 80.86 	r: 77.51 	f1: 79.15 	 4781 	 5913 	 6168
wo 	p: 92.48 	r: 86.72 	f1: 89.5 	 2938 	 3177 	 3388
ni 	p: 86.72 	r: 73.39 	f1: 79.5 	 1117 	 1288 	 1522

[32m iter_2[0m
ga 	p: 82.8 	r: 75.76 	f1: 79.12 	 4673 	 5644 	 6168
wo 	p: 92.22 	r: 86.81 	f1: 89.43 	 2941 	 3189 	 3388
ni 	p: 86.14 	r: 73.92 	f1: 79.56 	 1125 	 1306 	 1522
best_thres [[0.21, 0.58, 0.14], [0.21, 0.7, 0.14], [0.57, 0.72, 0.12]]
f [0.8237, 0.8237, 0.8237]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 18 	 [0.57, 0.85, 0.12] 	 lr: 2.5e-05 	 f: 82.91229066136815
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 26 [0m
Train...
loss: tensor(300.3429) lr: 1e-05 time: 1529.39
pred_count_train 41644

Test...
loss: tensor(521.1367) lr: 1.25e-05 time: 1640.41
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.46 	r: 76.62 	f1: 79.44 	 4726 	 5731 	 6168
wo 	p: 92.95 	r: 87.57 	f1: 90.18 	 2967 	 3192 	 3388
ni 	p: 85.07 	r: 74.51 	f1: 79.44 	 1134 	 1333 	 1522

[32m iter_1[0m
ga 	p: 83.57 	r: 75.79 	f1: 79.49 	 4675 	 5594 	 6168
wo 	p: 92.93 	r: 87.66 	f1: 90.22 	 2970 	 3196 	 3388
ni 	p: 87.02 	r: 73.13 	f1: 79.47 	 1113 	 1279 	 1522

[32m iter_2[0m
ga 	p: 83.43 	r: 75.89 	f1: 79.48 	 4681 	 5611 	 6168
wo 	p: 93.15 	r: 87.49 	f1: 90.23 	 2964 	 3182 	 3388
ni 	p: 83.93 	r: 75.49 	f1: 79.49 	 1149 	 1369 	 1522
best_thres [[0.4, 0.48, 0.19], [0.48, 0.49, 0.22], [0.47, 0.6, 0.15]]
f [0.8275, 0.8279, 0.828]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 22 	 [0.42, 0.44, 0.14] 	 lr: 1.25e-05 	 f: 82.95662797800856
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 28 [0m
Train...
loss: tensor(84.1270) lr: 1.25e-05 time: 1597.44
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.87 	r: 75.39 	f1: 78.95 	 4650 	 5611 	 6168
wo 	p: 92.14 	r: 87.22 	f1: 89.61 	 2955 	 3207 	 3388
ni 	p: 81.63 	r: 78.84 	f1: 80.21 	 1200 	 1470 	 1522

[32m iter_1[0m
ga 	p: 81.82 	r: 76.54 	f1: 79.09 	 4721 	 5770 	 6168
wo 	p: 93.02 	r: 86.57 	f1: 89.68 	 2933 	 3153 	 3388
ni 	p: 85.22 	r: 75.76 	f1: 80.21 	 1153 	 1353 	 1522

[32m iter_2[0m
ga 	p: 81.83 	r: 76.54 	f1: 79.1 	 4721 	 5769 	 6168
wo 	p: 93.11 	r: 86.57 	f1: 89.72 	 2933 	 3150 	 3388
ni 	p: 84.55 	r: 76.22 	f1: 80.17 	 1160 	 1372 	 1522
best_thres [[0.48, 0.43, 0.11], [0.39, 0.54, 0.16], [0.39, 0.54, 0.15]]
f [0.8242, 0.8245, 0.8247]
load model: epoch16
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 16 	 [0.41, 0.48, 0.17] 	 lr: 2.5e-05 	 f: 83.4139944421894
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 25 [0m
Train...

[32m iter_0[0m
ga 	p: 81.74 	r: 76.26 	f1: 78.91 	 4704 	 5755 	 6168
wo 	p: 91.83 	r: 86.6 	f1: 89.14 	 2934 	 3195 	 3388
ni 	p: 85.24 	r: 73.26 	f1: 78.8 	 1115 	 1308 	 1522

[32m iter_1[0m
ga 	p: 81.03 	r: 76.82 	f1: 78.87 	 4738 	 5847 	 6168
wo 	p: 92.62 	r: 85.92 	f1: 89.14 	 2911 	 3143 	 3388
ni 	p: 85.39 	r: 73.32 	f1: 78.9 	 1116 	 1307 	 1522

[32m iter_2[0m
ga 	p: 83.02 	r: 75.13 	f1: 78.88 	 4634 	 5582 	 6168
wo 	p: 93.13 	r: 85.6 	f1: 89.2 	 2900 	 3114 	 3388
ni 	p: 85.7 	r: 73.26 	f1: 78.99 	 1115 	 1301 	 1522
best_thres [[0.27, 0.37, 0.12], [0.25, 0.71, 0.1], [0.68, 0.85, 0.1]]
f [0.8205, 0.8203, 0.8205]
load model: epoch18
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 18 	 [0.57, 0.85, 0.12] 	 lr: 2.5e-05 	 f: 82.91229066136815
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 27 [0m
Train...
loss: tensor(245.7184) lr: 1e-05 time: 1516.84
pred_count_train 41644

Test...
loss: tensor(753.7164) lr: 1e-05 time: 1635.98
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 84.58 	r: 74.24 	f1: 79.07 	 4579 	 5414 	 6168
wo 	p: 93.13 	r: 87.22 	f1: 90.08 	 2955 	 3173 	 3388
ni 	p: 86.55 	r: 73.13 	f1: 79.27 	 1113 	 1286 	 1522

[32m iter_1[0m
ga 	p: 83.28 	r: 75.34 	f1: 79.11 	 4647 	 5580 	 6168
wo 	p: 92.89 	r: 87.54 	f1: 90.14 	 2966 	 3193 	 3388
ni 	p: 82.66 	r: 76.74 	f1: 79.59 	 1168 	 1413 	 1522

[32m iter_2[0m
ga 	p: 83.23 	r: 75.37 	f1: 79.1 	 4649 	 5586 	 6168
wo 	p: 92.81 	r: 87.6 	f1: 90.13 	 2968 	 3198 	 3388
ni 	p: 82.71 	r: 76.68 	f1: 79.58 	 1167 	 1411 	 1522
best_thres [[0.53, 0.48, 0.2], [0.44, 0.43, 0.1], [0.43, 0.42, 0.1]]
f [0.8254, 0.8257, 0.8257]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 22 	 [0.42, 0.44, 0.14] 	 lr: 1.25e-05 	 f: 82.95662797800856
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 29 [0m
Train...
loss: tensor(219.3887) lr: 1e-05 time: 1574.15
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.99 	r: 75.89 	f1: 79.74 	 4681 	 5573 	 6168
wo 	p: 93.3 	r: 87.07 	f1: 90.08 	 2950 	 3162 	 3388
ni 	p: 86.8 	r: 78.19 	f1: 82.27 	 1190 	 1371 	 1522

[32m iter_1[0m
ga 	p: 84.82 	r: 75.29 	f1: 79.77 	 4644 	 5475 	 6168
wo 	p: 93.14 	r: 87.37 	f1: 90.16 	 2960 	 3178 	 3388
ni 	p: 82.5 	r: 81.8 	f1: 82.15 	 1245 	 1509 	 1522

[32m iter_2[0m
ga 	p: 84.83 	r: 75.26 	f1: 79.76 	 4642 	 5472 	 6168
wo 	p: 93.14 	r: 87.37 	f1: 90.16 	 2960 	 3178 	 3388
ni 	p: 83.37 	r: 81.01 	f1: 82.17 	 1233 	 1479 	 1522
best_thres [[0.52, 0.51, 0.2], [0.57, 0.48, 0.11], [0.57, 0.48, 0.12]]
f [0.8328, 0.833, 0.8331]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 16 	 [0.41, 0.48, 0.17] 	 lr: 2.5e-05 	 f: 83.4139944421894
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 26 [0m
Train...

[32m iter_0[0m
ga 	p: 82.18 	r: 77.03 	f1: 79.52 	 4751 	 5781 	 6168
wo 	p: 94.48 	r: 85.45 	f1: 89.74 	 2895 	 3064 	 3388
ni 	p: 84.69 	r: 77.79 	f1: 81.1 	 1184 	 1398 	 1522

[32m iter_1[0m
ga 	p: 82.27 	r: 77.04 	f1: 79.57 	 4752 	 5776 	 6168
wo 	p: 94.4 	r: 85.6 	f1: 89.78 	 2900 	 3072 	 3388
ni 	p: 84.68 	r: 78.06 	f1: 81.23 	 1188 	 1403 	 1522

[32m iter_2[0m
ga 	p: 82.27 	r: 77.01 	f1: 79.55 	 4750 	 5774 	 6168
wo 	p: 94.1 	r: 85.63 	f1: 89.66 	 2901 	 3083 	 3388
ni 	p: 84.79 	r: 77.99 	f1: 81.25 	 1187 	 1400 	 1522
best_thres [[0.37, 0.72, 0.13], [0.45, 0.85, 0.11], [0.48, 0.85, 0.11]]
f [0.8283, 0.8286, 0.8286]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 18 	 [0.57, 0.85, 0.12] 	 lr: 2.5e-05 	 f: 82.91229066136815
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 28 [0m
Train...
loss: tensor(196.2638) lr: 1e-05 time: 1479.51
pred_count_train 41644

Test...
loss: tensor(685.3633) lr: 1e-05 time: 1630.69
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.29 	r: 75.31 	f1: 79.1 	 4645 	 5577 	 6168
wo 	p: 93.14 	r: 86.92 	f1: 89.92 	 2945 	 3162 	 3388
ni 	p: 83.07 	r: 75.1 	f1: 78.88 	 1143 	 1376 	 1522

[32m iter_1[0m
ga 	p: 83.89 	r: 75.13 	f1: 79.27 	 4634 	 5524 	 6168
wo 	p: 92.62 	r: 87.43 	f1: 89.95 	 2962 	 3198 	 3388
ni 	p: 83.14 	r: 75.16 	f1: 78.95 	 1144 	 1376 	 1522

[32m iter_2[0m
ga 	p: 83.91 	r: 75.1 	f1: 79.26 	 4632 	 5520 	 6168
wo 	p: 93.17 	r: 86.98 	f1: 89.97 	 2947 	 3163 	 3388
ni 	p: 83.38 	r: 75.16 	f1: 79.06 	 1144 	 1372 	 1522
best_thres [[0.44, 0.53, 0.16], [0.5, 0.45, 0.14], [0.55, 0.71, 0.14]]
f [0.8241, 0.8248, 0.825]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 22 	 [0.42, 0.44, 0.14] 	 lr: 1.25e-05 	 f: 82.95662797800856
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 30 [0m
Train...
loss: tensor(170.0125) lr: 1e-05 time: 1603.26
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 84.11 	r: 75.44 	f1: 79.54 	 4653 	 5532 	 6168
wo 	p: 91.69 	r: 87.96 	f1: 89.79 	 2980 	 3250 	 3388
ni 	p: 84.93 	r: 79.96 	f1: 82.37 	 1217 	 1433 	 1522

[32m iter_1[0m
ga 	p: 84.14 	r: 75.41 	f1: 79.53 	 4651 	 5528 	 6168
wo 	p: 91.97 	r: 87.84 	f1: 89.86 	 2976 	 3236 	 3388
ni 	p: 85.08 	r: 80.16 	f1: 82.54 	 1220 	 1434 	 1522

[32m iter_2[0m
ga 	p: 84.13 	r: 75.44 	f1: 79.55 	 4653 	 5531 	 6168
wo 	p: 91.99 	r: 87.81 	f1: 89.85 	 2975 	 3234 	 3388
ni 	p: 84.97 	r: 80.22 	f1: 82.53 	 1221 	 1437 	 1522
best_thres [[0.54, 0.32, 0.15], [0.54, 0.35, 0.14], [0.54, 0.35, 0.14]]
f [0.8313, 0.8315, 0.8315]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 16 	 [0.41, 0.48, 0.17] 	 lr: 2.5e-05 	 f: 83.4139944421894
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 27 [0m
Train...
loss: tensor(155.4422) lr: 1e-05 time: 1469.43
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 85.03 	r: 74.51 	f1: 79.43 	 4596 	 5405 	 6168
wo 	p: 92.61 	r: 86.87 	f1: 89.64 	 2943 	 3178 	 3388
ni 	p: 83.03 	r: 77.79 	f1: 80.33 	 1184 	 1426 	 1522

[32m iter_1[0m
ga 	p: 82.95 	r: 76.1 	f1: 79.38 	 4694 	 5659 	 6168
wo 	p: 92.29 	r: 86.95 	f1: 89.54 	 2946 	 3192 	 3388
ni 	p: 85.37 	r: 76.28 	f1: 80.57 	 1161 	 1360 	 1522

[32m iter_2[0m
ga 	p: 83.09 	r: 75.92 	f1: 79.35 	 4683 	 5636 	 6168
wo 	p: 92.47 	r: 86.69 	f1: 89.49 	 2937 	 3176 	 3388
ni 	p: 85.57 	r: 76.35 	f1: 80.69 	 1162 	 1358 	 1522
best_thres [[0.57, 0.51, 0.09], [0.51, 0.6, 0.11], [0.59, 0.7, 0.11]]
f [0.8273, 0.8271, 0.8269]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 18 	 [0.57, 0.85, 0.12] 	 lr: 2.5e-05 	 f: 82.91229066136815
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 29 [0m
Train...
loss: tensor(620.0196) lr: 1e-05 time: 1588.06
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.7 	r: 74.5 	f1: 78.83 	 4595 	 5490 	 6168
wo 	p: 94.34 	r: 85.63 	f1: 89.77 	 2901 	 3075 	 3388
ni 	p: 84.25 	r: 72.4 	f1: 77.88 	 1102 	 1308 	 1522

[32m iter_1[0m
ga 	p: 84.31 	r: 74.16 	f1: 78.91 	 4574 	 5425 	 6168
wo 	p: 92.49 	r: 87.19 	f1: 89.76 	 2954 	 3194 	 3388
ni 	p: 86.44 	r: 71.62 	f1: 78.33 	 1090 	 1261 	 1522

[32m iter_2[0m
ga 	p: 84.01 	r: 74.43 	f1: 78.93 	 4591 	 5465 	 6168
wo 	p: 93.59 	r: 86.19 	f1: 89.74 	 2920 	 3120 	 3388
ni 	p: 85.93 	r: 71.81 	f1: 78.24 	 1093 	 1272 	 1522
best_thres [[0.48, 0.67, 0.16], [0.7, 0.38, 0.17], [0.7, 0.8, 0.16]]
f [0.8208, 0.8216, 0.8217]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	best in epoch 22 	 [0.42, 0.44, 0.14] 	 lr: 1.25e-05 	 f: 82.95662797800856
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(129.6357) lr: 1e-05 time: 1597.65
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 84.23 	r: 75.26 	f1: 79.49 	 4642 	 5511 	 6168
wo 	p: 92.36 	r: 87.07 	f1: 89.64 	 2950 	 3194 	 3388
ni 	p: 82.66 	r: 80.81 	f1: 81.73 	 1230 	 1488 	 1522

[32m iter_1[0m
ga 	p: 83.0 	r: 76.38 	f1: 79.55 	 4711 	 5676 	 6168
wo 	p: 94.13 	r: 85.74 	f1: 89.74 	 2905 	 3086 	 3388
ni 	p: 84.13 	r: 79.76 	f1: 81.89 	 1214 	 1443 	 1522

[32m iter_2[0m
ga 	p: 83.17 	r: 76.28 	f1: 79.58 	 4705 	 5657 	 6168
wo 	p: 94.13 	r: 85.74 	f1: 89.74 	 2905 	 3086 	 3388
ni 	p: 84.14 	r: 79.83 	f1: 81.93 	 1215 	 1444 	 1522
best_thres [[0.55, 0.45, 0.12], [0.47, 0.63, 0.13], [0.48, 0.63, 0.13]]
f [0.8295, 0.8296, 0.8298]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 16 	 [0.41, 0.48, 0.17] 	 lr: 2.5e-05 	 f: 83.4139944421894
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 28 [0m
Train...
loss: tensor(3195.0515) lr: 0.0002 time: 1467.81
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.53 	r: 76.26 	f1: 79.27 	 4704 	 5700 	 6168
wo 	p: 92.58 	r: 86.48 	f1: 89.42 	 2930 	 3165 	 3388
ni 	p: 83.78 	r: 75.36 	f1: 79.35 	 1147 	 1369 	 1522

[32m iter_1[0m
ga 	p: 80.71 	r: 77.72 	f1: 79.19 	 4794 	 5940 	 6168
wo 	p: 92.58 	r: 86.57 	f1: 89.48 	 2933 	 3168 	 3388
ni 	p: 85.49 	r: 74.7 	f1: 79.73 	 1137 	 1330 	 1522

[32m iter_2[0m
ga 	p: 83.41 	r: 75.31 	f1: 79.15 	 4645 	 5569 	 6168
wo 	p: 92.27 	r: 86.69 	f1: 89.39 	 2937 	 3183 	 3388
ni 	p: 85.29 	r: 75.03 	f1: 79.83 	 1142 	 1339 	 1522
best_thres [[0.31, 0.58, 0.11], [0.17, 0.72, 0.12], [0.62, 0.73, 0.11]]
f [0.824, 0.824, 0.8241]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 18 	 [0.57, 0.85, 0.12] 	 lr: 2.5e-05 	 f: 82.91229066136815
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 30 [0m
Train...

[32m iter_0[0m
ga 	p: 78.62 	r: 73.22 	f1: 75.82 	 4516 	 5744 	 6168
wo 	p: 92.54 	r: 82.76 	f1: 87.38 	 2804 	 3030 	 3388
ni 	p: 83.36 	r: 79.63 	f1: 81.45 	 1212 	 1454 	 1522

[32m iter_1[0m
ga 	p: 78.38 	r: 73.41 	f1: 75.81 	 4528 	 5777 	 6168
wo 	p: 93.92 	r: 81.64 	f1: 87.35 	 2766 	 2945 	 3388
ni 	p: 83.62 	r: 79.5 	f1: 81.51 	 1210 	 1447 	 1522

[32m iter_2[0m
ga 	p: 78.42 	r: 73.41 	f1: 75.83 	 4528 	 5774 	 6168
wo 	p: 94.11 	r: 81.52 	f1: 87.36 	 2762 	 2935 	 3388
ni 	p: 83.62 	r: 79.5 	f1: 81.51 	 1210 	 1447 	 1522
best_thres [[0.38, 0.27, 0.14], [0.37, 0.33, 0.14], [0.37, 0.34, 0.14]]
f [0.8009, 0.8007, 0.8007]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 1 	 [0.37, 0.34, 0.14] 	 lr: 0.0002 	 f: 80.06647122454419
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 2 [0m
Train...
loss: tensor(557.0665) lr: 1e-05 time: 1609.69
pred_count_train 41644

Test...
loss: tensor(98.0686) lr: 1e-05 time: 1594.77
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 85.26 	r: 74.16 	f1: 79.32 	 4574 	 5365 	 6168
wo 	p: 92.42 	r: 87.04 	f1: 89.65 	 2949 	 3191 	 3388
ni 	p: 83.94 	r: 80.03 	f1: 81.94 	 1218 	 1451 	 1522

[32m iter_1[0m
ga 	p: 84.57 	r: 74.71 	f1: 79.33 	 4608 	 5449 	 6168
wo 	p: 92.52 	r: 87.22 	f1: 89.79 	 2955 	 3194 	 3388
ni 	p: 85.04 	r: 79.17 	f1: 82.0 	 1205 	 1417 	 1522

[32m iter_2[0m
ga 	p: 84.6 	r: 74.71 	f1: 79.35 	 4608 	 5447 	 6168
wo 	p: 92.49 	r: 87.22 	f1: 89.78 	 2955 	 3195 	 3388
ni 	p: 84.92 	r: 79.17 	f1: 81.94 	 1205 	 1419 	 1522
best_thres [[0.61, 0.47, 0.12], [0.56, 0.47, 0.13], [0.56, 0.47, 0.13]]
f [0.8291, 0.8294, 0.8294]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	best in epoch 16 	 [0.41, 0.48, 0.17] 	 lr: 2.5e-05 	 f: 83.4139944421894
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(2024.1469) lr: 0.0002 time: 1480.85
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.08 	r: 77.11 	f1: 79.04 	 4756 	 5866 	 6168
wo 	p: 91.62 	r: 87.13 	f1: 89.32 	 2952 	 3222 	 3388
ni 	p: 83.47 	r: 73.65 	f1: 78.25 	 1121 	 1343 	 1522

[32m iter_1[0m
ga 	p: 82.35 	r: 75.86 	f1: 78.97 	 4679 	 5682 	 6168
wo 	p: 92.23 	r: 86.54 	f1: 89.29 	 2932 	 3179 	 3388
ni 	p: 85.88 	r: 72.73 	f1: 78.76 	 1107 	 1289 	 1522

[32m iter_2[0m
ga 	p: 82.26 	r: 75.88 	f1: 78.94 	 4680 	 5689 	 6168
wo 	p: 92.09 	r: 86.57 	f1: 89.24 	 2933 	 3185 	 3388
ni 	p: 84.07 	r: 74.18 	f1: 78.81 	 1129 	 1343 	 1522
best_thres [[0.19, 0.37, 0.08], [0.41, 0.66, 0.09], [0.46, 0.69, 0.06]]
f [0.821, 0.8212, 0.8211]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	best in epoch 18 	 [0.57, 0.85, 0.12] 	 lr: 2.5e-05 	 f: 82.91229066136815
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 1 [0m
Train...

[32m iter_0[0m
ga 	p: 78.72 	r: 75.13 	f1: 76.88 	 4634 	 5887 	 6168
wo 	p: 93.5 	r: 83.59 	f1: 88.27 	 2832 	 3029 	 3388
ni 	p: 85.04 	r: 81.08 	f1: 83.01 	 1234 	 1451 	 1522

[32m iter_1[0m
ga 	p: 78.69 	r: 75.06 	f1: 76.83 	 4630 	 5884 	 6168
wo 	p: 91.42 	r: 85.21 	f1: 88.21 	 2887 	 3158 	 3388
ni 	p: 84.1 	r: 82.0 	f1: 83.03 	 1248 	 1484 	 1522

[32m iter_2[0m
ga 	p: 78.66 	r: 75.06 	f1: 76.82 	 4630 	 5886 	 6168
wo 	p: 91.45 	r: 85.21 	f1: 88.22 	 2887 	 3157 	 3388
ni 	p: 84.11 	r: 82.06 	f1: 83.07 	 1249 	 1485 	 1522
best_thres [[0.37, 0.59, 0.34], [0.37, 0.42, 0.3], [0.37, 0.42, 0.3]]
f [0.8114, 0.8114, 0.8114]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 2 	 [0.37, 0.42, 0.3] 	 lr: 0.0002 	 f: 81.1414430438481
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 3 [0m
Train...
loss: tensor(3194.8721) lr: 0.0002 time: 1447.92
pred_count_train 41644

Test...
loss: tensor(3192.7908) lr: 0.0002 time: 1441.95
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 78.81 	r: 73.02 	f1: 75.81 	 4504 	 5715 	 6168
wo 	p: 92.07 	r: 83.26 	f1: 87.45 	 2821 	 3064 	 3388
ni 	p: 83.03 	r: 80.03 	f1: 81.5 	 1218 	 1467 	 1522

[32m iter_1[0m
ga 	p: 79.31 	r: 72.63 	f1: 75.82 	 4480 	 5649 	 6168
wo 	p: 91.99 	r: 83.35 	f1: 87.46 	 2824 	 3070 	 3388
ni 	p: 83.03 	r: 80.03 	f1: 81.5 	 1218 	 1467 	 1522

[32m iter_2[0m
ga 	p: 78.94 	r: 72.91 	f1: 75.8 	 4497 	 5697 	 6168
wo 	p: 92.02 	r: 83.35 	f1: 87.47 	 2824 	 3069 	 3388
ni 	p: 83.03 	r: 80.03 	f1: 81.5 	 1218 	 1467 	 1522
best_thres [[0.38, 0.25, 0.12], [0.39, 0.24, 0.12], [0.38, 0.24, 0.12]]
f [0.8013, 0.8014, 0.8014]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 1 	 [0.38, 0.24, 0.12] 	 lr: 0.0002 	 f: 80.13896931094384
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 2 [0m
Train...
loss: tensor(1628.6233) lr: 0.0002 time: 1349.71
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 78.8 	r: 72.84 	f1: 75.7 	 4493 	 5702 	 6168
wo 	p: 92.02 	r: 83.71 	f1: 87.67 	 2836 	 3082 	 3388
ni 	p: 82.89 	r: 79.89 	f1: 81.37 	 1216 	 1467 	 1522

[32m iter_1[0m
ga 	p: 80.04 	r: 71.76 	f1: 75.67 	 4426 	 5530 	 6168
wo 	p: 92.5 	r: 83.35 	f1: 87.69 	 2824 	 3053 	 3388
ni 	p: 83.67 	r: 79.11 	f1: 81.32 	 1204 	 1439 	 1522

[32m iter_2[0m
ga 	p: 79.67 	r: 72.03 	f1: 75.66 	 4443 	 5577 	 6168
wo 	p: 92.25 	r: 83.56 	f1: 87.69 	 2831 	 3069 	 3388
ni 	p: 83.67 	r: 79.11 	f1: 81.32 	 1204 	 1439 	 1522
best_thres [[0.42, 0.23, 0.13], [0.46, 0.25, 0.15], [0.45, 0.24, 0.15]]
f [0.8013, 0.8013, 0.8013]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 1 	 [0.45, 0.24, 0.15] 	 lr: 0.0002 	 f: 80.12643099760976
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 2 [0m
Train...

[32m iter_0[0m
ga 	p: 79.35 	r: 77.3 	f1: 78.31 	 4768 	 6009 	 6168
wo 	p: 92.45 	r: 86.72 	f1: 89.49 	 2938 	 3178 	 3388
ni 	p: 87.63 	r: 79.11 	f1: 83.15 	 1204 	 1374 	 1522

[32m iter_1[0m
ga 	p: 79.87 	r: 76.95 	f1: 78.38 	 4746 	 5942 	 6168
wo 	p: 92.39 	r: 86.75 	f1: 89.48 	 2939 	 3181 	 3388
ni 	p: 86.11 	r: 80.62 	f1: 83.27 	 1227 	 1425 	 1522

[32m iter_2[0m
ga 	p: 79.9 	r: 76.95 	f1: 78.39 	 4746 	 5940 	 6168
wo 	p: 92.27 	r: 86.72 	f1: 89.41 	 2938 	 3184 	 3388
ni 	p: 85.53 	r: 81.14 	f1: 83.28 	 1235 	 1444 	 1522
best_thres [[0.36, 0.37, 0.36], [0.38, 0.36, 0.29], [0.38, 0.36, 0.27]]
f [0.8235, 0.8239, 0.8239]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 3 	 [0.38, 0.36, 0.27] 	 lr: 0.0002 	 f: 82.3928147771564
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 4 [0m
Train...
loss: tensor(2024.0875) lr: 0.0002 time: 1410.43
pred_count_train 41644

Test...
loss: tensor(2025.4304) lr: 0.0002 time: 1455.25
pred_count_train 41644

Test...
loss: tensor(1270.4557) lr: 0.0002 time: 1356.1
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 78.54 	r: 74.55 	f1: 76.49 	 4598 	 5854 	 6168
wo 	p: 93.3 	r: 84.71 	f1: 88.8 	 2870 	 3076 	 3388
ni 	p: 84.89 	r: 81.21 	f1: 83.01 	 1236 	 1456 	 1522

[32m iter_1[0m
ga 	p: 76.92 	r: 76.02 	f1: 76.47 	 4689 	 6096 	 6168
wo 	p: 93.58 	r: 84.39 	f1: 88.75 	 2859 	 3055 	 3388
ni 	p: 85.31 	r: 80.88 	f1: 83.04 	 1231 	 1443 	 1522

[32m iter_2[0m
ga 	p: 77.12 	r: 75.86 	f1: 76.49 	 4679 	 6067 	 6168
wo 	p: 93.62 	r: 84.42 	f1: 88.78 	 2860 	 3055 	 3388
ni 	p: 85.16 	r: 81.08 	f1: 83.07 	 1234 	 1449 	 1522
best_thres [[0.34, 0.51, 0.34], [0.27, 0.53, 0.37], [0.28, 0.53, 0.36]]
f [0.811, 0.8106, 0.8106]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 2 	 [0.28, 0.53, 0.36] 	 lr: 0.0002 	 f: 81.055799953693
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 3 [0m
Train...

[32m iter_0[0m
ga 	p: 79.75 	r: 74.66 	f1: 77.12 	 4605 	 5774 	 6168
wo 	p: 91.53 	r: 85.51 	f1: 88.42 	 2897 	 3165 	 3388
ni 	p: 83.74 	r: 82.59 	f1: 83.16 	 1257 	 1501 	 1522

[32m iter_1[0m
ga 	p: 80.75 	r: 73.98 	f1: 77.21 	 4563 	 5651 	 6168
wo 	p: 92.0 	r: 84.89 	f1: 88.3 	 2876 	 3126 	 3388
ni 	p: 84.18 	r: 82.13 	f1: 83.14 	 1250 	 1485 	 1522

[32m iter_2[0m
ga 	p: 80.73 	r: 73.98 	f1: 77.21 	 4563 	 5652 	 6168
wo 	p: 92.2 	r: 84.74 	f1: 88.31 	 2871 	 3114 	 3388
ni 	p: 84.19 	r: 82.19 	f1: 83.18 	 1251 	 1486 	 1522
best_thres [[0.39, 0.35, 0.24], [0.43, 0.39, 0.24], [0.43, 0.41, 0.24]]
f [0.8141, 0.8142, 0.8143]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 2 	 [0.43, 0.41, 0.24] 	 lr: 0.0002 	 f: 81.42643484763506
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 3 [0m
Train...

[32m iter_0[0m
ga 	p: 82.73 	r: 74.4 	f1: 78.34 	 4589 	 5547 	 6168
wo 	p: 92.79 	r: 85.89 	f1: 89.21 	 2910 	 3136 	 3388
ni 	p: 85.62 	r: 80.22 	f1: 82.84 	 1221 	 1426 	 1522

[32m iter_1[0m
ga 	p: 83.03 	r: 74.17 	f1: 78.35 	 4575 	 5510 	 6168
wo 	p: 92.86 	r: 86.04 	f1: 89.32 	 2915 	 3139 	 3388
ni 	p: 83.43 	r: 82.72 	f1: 83.07 	 1259 	 1509 	 1522

[32m iter_2[0m
ga 	p: 83.13 	r: 74.21 	f1: 78.41 	 4577 	 5506 	 6168
wo 	p: 92.8 	r: 86.04 	f1: 89.29 	 2915 	 3141 	 3388
ni 	p: 83.79 	r: 82.19 	f1: 82.99 	 1251 	 1493 	 1522
best_thres [[0.43, 0.52, 0.29], [0.44, 0.55, 0.22], [0.44, 0.55, 0.23]]
f [0.8231, 0.8236, 0.8237]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 3 	 [0.38, 0.36, 0.27] 	 lr: 0.0002 	 f: 82.3928147771564
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 5 [0m
Train...
loss: tensor(1632.0322) lr: 0.0002 time: 1395.64
pred_count_train 41644

Test...
loss: tensor(1627.6959) lr: 0.0002 time: 1452.94
pred_count_train 41644

Test...
loss: tensor(961.0621) lr: 0.0002 time: 1412.94
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.2 	r: 75.42 	f1: 78.2 	 4652 	 5729 	 6168
wo 	p: 94.23 	r: 84.8 	f1: 89.27 	 2873 	 3049 	 3388
ni 	p: 86.26 	r: 80.03 	f1: 83.03 	 1218 	 1412 	 1522

[32m iter_1[0m
ga 	p: 81.04 	r: 75.6 	f1: 78.23 	 4663 	 5754 	 6168
wo 	p: 93.31 	r: 85.68 	f1: 89.34 	 2903 	 3111 	 3388
ni 	p: 85.22 	r: 80.68 	f1: 82.89 	 1228 	 1441 	 1522

[32m iter_2[0m
ga 	p: 81.03 	r: 75.62 	f1: 78.23 	 4664 	 5756 	 6168
wo 	p: 94.12 	r: 85.01 	f1: 89.33 	 2880 	 3060 	 3388
ni 	p: 85.23 	r: 80.75 	f1: 82.93 	 1229 	 1442 	 1522
best_thres [[0.49, 0.63, 0.3], [0.48, 0.54, 0.27], [0.48, 0.62, 0.27]]
f [0.8222, 0.8223, 0.8223]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 3 	 [0.48, 0.62, 0.27] 	 lr: 0.0002 	 f: 82.23416890667
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 4 [0m
Train...

[32m iter_0[0m
ga 	p: 80.1 	r: 77.03 	f1: 78.54 	 4751 	 5931 	 6168
wo 	p: 91.13 	r: 87.04 	f1: 89.04 	 2949 	 3236 	 3388
ni 	p: 86.5 	r: 80.81 	f1: 83.56 	 1230 	 1422 	 1522

[32m iter_1[0m
ga 	p: 82.58 	r: 74.95 	f1: 78.58 	 4623 	 5598 	 6168
wo 	p: 91.65 	r: 86.81 	f1: 89.16 	 2941 	 3209 	 3388
ni 	p: 86.4 	r: 80.95 	f1: 83.58 	 1232 	 1426 	 1522

[32m iter_2[0m
ga 	p: 82.58 	r: 74.94 	f1: 78.57 	 4622 	 5597 	 6168
wo 	p: 91.63 	r: 86.89 	f1: 89.2 	 2944 	 3213 	 3388
ni 	p: 87.21 	r: 80.22 	f1: 83.57 	 1221 	 1400 	 1522
best_thres [[0.42, 0.36, 0.29], [0.52, 0.43, 0.27], [0.52, 0.42, 0.3]]
f [0.8243, 0.8249, 0.8251]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 3 	 [0.52, 0.42, 0.3] 	 lr: 0.0002 	 f: 82.51019201443998
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 4 [0m
Train...

[32m iter_0[0m
ga 	p: 79.88 	r: 76.49 	f1: 78.15 	 4718 	 5906 	 6168
wo 	p: 93.54 	r: 85.95 	f1: 89.59 	 2912 	 3113 	 3388
ni 	p: 82.61 	r: 77.4 	f1: 79.92 	 1178 	 1426 	 1522

[32m iter_1[0m
ga 	p: 80.13 	r: 76.18 	f1: 78.11 	 4699 	 5864 	 6168
wo 	p: 94.0 	r: 85.48 	f1: 89.53 	 2896 	 3081 	 3388
ni 	p: 82.38 	r: 77.4 	f1: 79.81 	 1178 	 1430 	 1522

[32m iter_2[0m
ga 	p: 80.12 	r: 76.12 	f1: 78.07 	 4695 	 5860 	 6168
wo 	p: 93.96 	r: 85.42 	f1: 89.49 	 2894 	 3080 	 3388
ni 	p: 82.62 	r: 77.46 	f1: 79.96 	 1179 	 1427 	 1522
best_thres [[0.24, 0.53, 0.22], [0.25, 0.66, 0.21], [0.25, 0.68, 0.21]]
f [0.8185, 0.8182, 0.818]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 3 	 [0.38, 0.36, 0.27] 	 lr: 0.0002 	 f: 82.3928147771564
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 6 [0m
Train...
loss: tensor(1273.8560) lr: 0.0002 time: 1455.4
pred_count_train 41644

Test...
loss: tensor(1283.2324) lr: 0.0002 time: 1485.06
pred_count_train 41644

Test...
loss: tensor(716.1812) lr: 0.0002 time: 1417.62
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.67 	r: 74.37 	f1: 77.39 	 4587 	 5686 	 6168
wo 	p: 92.76 	r: 85.8 	f1: 89.14 	 2907 	 3134 	 3388
ni 	p: 88.53 	r: 78.58 	f1: 83.26 	 1196 	 1351 	 1522

[32m iter_1[0m
ga 	p: 81.38 	r: 73.99 	f1: 77.51 	 4564 	 5608 	 6168
wo 	p: 92.58 	r: 85.83 	f1: 89.08 	 2908 	 3141 	 3388
ni 	p: 85.84 	r: 80.88 	f1: 83.29 	 1231 	 1434 	 1522

[32m iter_2[0m
ga 	p: 81.36 	r: 74.01 	f1: 77.51 	 4565 	 5611 	 6168
wo 	p: 92.44 	r: 85.95 	f1: 89.08 	 2912 	 3150 	 3388
ni 	p: 85.84 	r: 80.88 	f1: 83.29 	 1231 	 1434 	 1522
best_thres [[0.4, 0.46, 0.31], [0.42, 0.46, 0.22], [0.42, 0.45, 0.22]]
f [0.8179, 0.8183, 0.8184]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 3 	 [0.48, 0.62, 0.27] 	 lr: 0.0002 	 f: 82.23416890667
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 5 [0m
Train...

[32m iter_0[0m
ga 	p: 81.49 	r: 75.92 	f1: 78.61 	 4683 	 5747 	 6168
wo 	p: 92.73 	r: 86.19 	f1: 89.34 	 2920 	 3149 	 3388
ni 	p: 86.11 	r: 80.62 	f1: 83.27 	 1227 	 1425 	 1522

[32m iter_1[0m
ga 	p: 81.57 	r: 75.71 	f1: 78.53 	 4670 	 5725 	 6168
wo 	p: 93.06 	r: 85.89 	f1: 89.33 	 2910 	 3127 	 3388
ni 	p: 85.41 	r: 80.75 	f1: 83.01 	 1229 	 1439 	 1522

[32m iter_2[0m
ga 	p: 81.93 	r: 75.42 	f1: 78.54 	 4652 	 5678 	 6168
wo 	p: 93.21 	r: 85.89 	f1: 89.4 	 2910 	 3122 	 3388
ni 	p: 85.1 	r: 81.08 	f1: 83.04 	 1234 	 1450 	 1522
best_thres [[0.42, 0.46, 0.28], [0.45, 0.51, 0.24], [0.47, 0.52, 0.23]]
f [0.8253, 0.8249, 0.8249]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 3 	 [0.52, 0.42, 0.3] 	 lr: 0.0002 	 f: 82.51019201443998
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 5 [0m
Train...

[32m iter_0[0m
ga 	p: 82.99 	r: 73.56 	f1: 77.99 	 4537 	 5467 	 6168
wo 	p: 92.07 	r: 86.3 	f1: 89.09 	 2924 	 3176 	 3388
ni 	p: 85.87 	r: 75.49 	f1: 80.35 	 1149 	 1338 	 1522

[32m iter_1[0m
ga 	p: 82.95 	r: 73.28 	f1: 77.82 	 4520 	 5449 	 6168
wo 	p: 92.15 	r: 86.28 	f1: 89.12 	 2923 	 3172 	 3388
ni 	p: 85.7 	r: 76.02 	f1: 80.57 	 1157 	 1350 	 1522

[32m iter_2[0m
ga 	p: 83.23 	r: 73.14 	f1: 77.86 	 4511 	 5420 	 6168
wo 	p: 92.12 	r: 86.28 	f1: 89.1 	 2923 	 3173 	 3388
ni 	p: 85.87 	r: 75.89 	f1: 80.57 	 1155 	 1345 	 1522
best_thres [[0.54, 0.51, 0.21], [0.61, 0.58, 0.18], [0.65, 0.58, 0.18]]
f [0.8177, 0.8174, 0.8174]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 3 	 [0.38, 0.36, 0.27] 	 lr: 0.0002 	 f: 82.3928147771564
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 7 [0m
Train...
loss: tensor(964.6636) lr: 0.0002 time: 1430.4
pred_count_train 41644

Test...
loss: tensor(966.7579) lr: 0.0002 time: 1505.84
pred_count_train 41644

Test...
loss: tensor(555.3890) lr: 0.0002 time: 1432.86
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.99 	r: 77.27 	f1: 78.61 	 4766 	 5958 	 6168
wo 	p: 92.28 	r: 86.45 	f1: 89.27 	 2929 	 3174 	 3388
ni 	p: 86.12 	r: 76.22 	f1: 80.86 	 1160 	 1347 	 1522

[32m iter_1[0m
ga 	p: 80.37 	r: 77.19 	f1: 78.75 	 4761 	 5924 	 6168
wo 	p: 93.43 	r: 85.66 	f1: 89.37 	 2902 	 3106 	 3388
ni 	p: 86.66 	r: 75.95 	f1: 80.95 	 1156 	 1334 	 1522

[32m iter_2[0m
ga 	p: 80.38 	r: 77.24 	f1: 78.78 	 4764 	 5927 	 6168
wo 	p: 93.54 	r: 85.54 	f1: 89.36 	 2898 	 3098 	 3388
ni 	p: 86.66 	r: 75.95 	f1: 80.95 	 1156 	 1334 	 1522
best_thres [[0.27, 0.37, 0.27], [0.28, 0.49, 0.29], [0.28, 0.5, 0.29]]
f [0.8215, 0.8221, 0.8223]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 3 	 [0.48, 0.62, 0.27] 	 lr: 0.0002 	 f: 82.23416890667
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 6 [0m
Train...

[32m iter_0[0m
ga 	p: 82.15 	r: 74.4 	f1: 78.08 	 4589 	 5586 	 6168
wo 	p: 94.86 	r: 83.88 	f1: 89.04 	 2842 	 2996 	 3388
ni 	p: 87.89 	r: 75.82 	f1: 81.41 	 1154 	 1313 	 1522

[32m iter_1[0m
ga 	p: 82.68 	r: 73.85 	f1: 78.02 	 4555 	 5509 	 6168
wo 	p: 94.09 	r: 84.53 	f1: 89.05 	 2864 	 3044 	 3388
ni 	p: 86.4 	r: 76.81 	f1: 81.32 	 1169 	 1353 	 1522

[32m iter_2[0m
ga 	p: 82.85 	r: 73.88 	f1: 78.11 	 4557 	 5500 	 6168
wo 	p: 93.94 	r: 84.68 	f1: 89.07 	 2869 	 3054 	 3388
ni 	p: 84.09 	r: 78.84 	f1: 81.38 	 1200 	 1427 	 1522
best_thres [[0.26, 0.71, 0.35], [0.3, 0.7, 0.28], [0.31, 0.69, 0.2]]
f [0.8187, 0.8186, 0.8188]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 3 	 [0.52, 0.42, 0.3] 	 lr: 0.0002 	 f: 82.51019201443998
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 6 [0m
Train...

[32m iter_0[0m
ga 	p: 80.03 	r: 75.49 	f1: 77.69 	 4656 	 5818 	 6168
wo 	p: 93.39 	r: 84.62 	f1: 88.79 	 2867 	 3070 	 3388
ni 	p: 84.84 	r: 77.92 	f1: 81.23 	 1186 	 1398 	 1522

[32m iter_1[0m
ga 	p: 80.58 	r: 75.19 	f1: 77.79 	 4638 	 5756 	 6168
wo 	p: 93.5 	r: 84.53 	f1: 88.79 	 2864 	 3063 	 3388
ni 	p: 84.94 	r: 78.19 	f1: 81.42 	 1190 	 1401 	 1522

[32m iter_2[0m
ga 	p: 80.28 	r: 75.5 	f1: 77.82 	 4657 	 5801 	 6168
wo 	p: 93.01 	r: 84.86 	f1: 88.75 	 2875 	 3091 	 3388
ni 	p: 85.05 	r: 78.12 	f1: 81.44 	 1189 	 1398 	 1522
best_thres [[0.42, 0.52, 0.27], [0.47, 0.65, 0.26], [0.44, 0.48, 0.26]]
f [0.8153, 0.8158, 0.8159]
load model: epoch3
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 3 	 [0.38, 0.36, 0.27] 	 lr: 0.0002 	 f: 82.3928147771564
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 8 [0m
Train...
loss: tensor(700.8818) lr: 0.0002 time: 1412.37
pred_count_train 41644

Test...
loss: tensor(721.2546) lr: 0.0002 time: 1481.43
pred_count_train 41644

Test...
loss: tensor(1062.8715) lr: 0.0001 time: 1442.28
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.13 	r: 76.25 	f1: 78.61 	 4703 	 5797 	 6168
wo 	p: 94.27 	r: 85.01 	f1: 89.4 	 2880 	 3055 	 3388
ni 	p: 80.91 	r: 74.9 	f1: 77.79 	 1140 	 1409 	 1522

[32m iter_1[0m
ga 	p: 83.17 	r: 74.29 	f1: 78.48 	 4582 	 5509 	 6168
wo 	p: 93.89 	r: 85.3 	f1: 89.39 	 2890 	 3078 	 3388
ni 	p: 81.17 	r: 74.77 	f1: 77.84 	 1138 	 1402 	 1522

[32m iter_2[0m
ga 	p: 82.86 	r: 74.56 	f1: 78.49 	 4599 	 5550 	 6168
wo 	p: 93.92 	r: 85.33 	f1: 89.42 	 2891 	 3078 	 3388
ni 	p: 81.85 	r: 74.38 	f1: 77.93 	 1132 	 1383 	 1522
best_thres [[0.53, 0.82, 0.2], [0.67, 0.8, 0.18], [0.65, 0.81, 0.2]]
f [0.8176, 0.8175, 0.8175]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 3 	 [0.48, 0.62, 0.27] 	 lr: 0.0002 	 f: 82.23416890667
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 7 [0m
Train...

[32m iter_0[0m
ga 	p: 83.36 	r: 73.85 	f1: 78.32 	 4555 	 5464 	 6168
wo 	p: 92.64 	r: 86.57 	f1: 89.5 	 2933 	 3166 	 3388
ni 	p: 81.99 	r: 75.1 	f1: 78.4 	 1143 	 1394 	 1522

[32m iter_1[0m
ga 	p: 82.57 	r: 74.37 	f1: 78.26 	 4587 	 5555 	 6168
wo 	p: 93.12 	r: 85.86 	f1: 89.34 	 2909 	 3124 	 3388
ni 	p: 81.33 	r: 76.15 	f1: 78.66 	 1159 	 1425 	 1522

[32m iter_2[0m
ga 	p: 82.53 	r: 74.38 	f1: 78.25 	 4588 	 5559 	 6168
wo 	p: 93.03 	r: 85.86 	f1: 89.3 	 2909 	 3127 	 3388
ni 	p: 82.6 	r: 75.16 	f1: 78.71 	 1144 	 1385 	 1522
best_thres [[0.52, 0.43, 0.21], [0.53, 0.6, 0.17], [0.54, 0.61, 0.2]]
f [0.818, 0.8176, 0.8175]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 3 	 [0.52, 0.42, 0.3] 	 lr: 0.0002 	 f: 82.51019201443998
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 7 [0m
Train...

[32m iter_0[0m
ga 	p: 80.06 	r: 78.31 	f1: 79.17 	 4830 	 6033 	 6168
wo 	p: 93.84 	r: 85.86 	f1: 89.67 	 2909 	 3100 	 3388
ni 	p: 87.68 	r: 78.58 	f1: 82.88 	 1196 	 1364 	 1522

[32m iter_1[0m
ga 	p: 80.53 	r: 78.11 	f1: 79.3 	 4818 	 5983 	 6168
wo 	p: 92.38 	r: 86.95 	f1: 89.58 	 2946 	 3189 	 3388
ni 	p: 86.26 	r: 79.63 	f1: 82.82 	 1212 	 1405 	 1522

[32m iter_2[0m
ga 	p: 80.51 	r: 78.08 	f1: 79.28 	 4816 	 5982 	 6168
wo 	p: 93.67 	r: 86.01 	f1: 89.68 	 2914 	 3111 	 3388
ni 	p: 88.61 	r: 77.73 	f1: 82.81 	 1183 	 1335 	 1522
best_thres [[0.33, 0.66, 0.18], [0.36, 0.47, 0.14], [0.36, 0.71, 0.19]]
f [0.8283, 0.8286, 0.8287]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.36, 0.71, 0.19] 	 lr: 0.0001 	 f: 82.8719723183391
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 9 [0m
Train...
loss: tensor(542.5286) lr: 0.0002 time: 1418.02
pred_count_train 41644

Test...
loss: tensor(567.1170) lr: 0.0002 time: 1476.75
pred_count_train 41644

Test...
loss: tensor(720.6741) lr: 0.0001 time: 1428.66
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 78.39 	r: 76.98 	f1: 77.68 	 4748 	 6057 	 6168
wo 	p: 93.57 	r: 84.59 	f1: 88.85 	 2866 	 3063 	 3388
ni 	p: 85.63 	r: 76.74 	f1: 80.94 	 1168 	 1364 	 1522

[32m iter_1[0m
ga 	p: 78.8 	r: 76.64 	f1: 77.7 	 4727 	 5999 	 6168
wo 	p: 92.05 	r: 86.1 	f1: 88.97 	 2917 	 3169 	 3388
ni 	p: 85.58 	r: 76.02 	f1: 80.51 	 1157 	 1352 	 1522

[32m iter_2[0m
ga 	p: 78.79 	r: 76.65 	f1: 77.71 	 4728 	 6001 	 6168
wo 	p: 91.53 	r: 86.45 	f1: 88.92 	 2929 	 3200 	 3388
ni 	p: 87.9 	r: 74.44 	f1: 80.61 	 1133 	 1289 	 1522
best_thres [[0.31, 0.85, 0.48], [0.34, 0.7, 0.48], [0.34, 0.65, 0.6]]
f [0.8146, 0.8148, 0.8149]
load model: epoch3
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 3 	 [0.48, 0.62, 0.27] 	 lr: 0.0002 	 f: 82.23416890667
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 8 [0m
Train...

[32m iter_0[0m
ga 	p: 80.15 	r: 76.12 	f1: 78.08 	 4695 	 5858 	 6168
wo 	p: 92.42 	r: 85.3 	f1: 88.72 	 2890 	 3127 	 3388
ni 	p: 84.62 	r: 78.06 	f1: 81.2 	 1188 	 1404 	 1522

[32m iter_1[0m
ga 	p: 79.92 	r: 76.39 	f1: 78.12 	 4712 	 5896 	 6168
wo 	p: 93.47 	r: 84.53 	f1: 88.78 	 2864 	 3064 	 3388
ni 	p: 84.6 	r: 77.99 	f1: 81.16 	 1187 	 1403 	 1522

[32m iter_2[0m
ga 	p: 79.6 	r: 76.56 	f1: 78.05 	 4722 	 5932 	 6168
wo 	p: 93.0 	r: 85.04 	f1: 88.84 	 2881 	 3098 	 3388
ni 	p: 84.63 	r: 78.12 	f1: 81.24 	 1189 	 1405 	 1522
best_thres [[0.56, 0.5, 0.43], [0.62, 0.73, 0.42], [0.62, 0.68, 0.42]]
f [0.8173, 0.8174, 0.8174]
load model: epoch3
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 3 	 [0.52, 0.42, 0.3] 	 lr: 0.0002 	 f: 82.51019201443998
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 8 [0m
Train...

[32m iter_0[0m
ga 	p: 80.91 	r: 77.09 	f1: 78.95 	 4755 	 5877 	 6168
wo 	p: 92.96 	r: 86.48 	f1: 89.6 	 2930 	 3152 	 3388
ni 	p: 87.23 	r: 76.28 	f1: 81.39 	 1161 	 1331 	 1522

[32m iter_1[0m
ga 	p: 83.33 	r: 75.39 	f1: 79.16 	 4650 	 5580 	 6168
wo 	p: 93.24 	r: 86.33 	f1: 89.66 	 2925 	 3137 	 3388
ni 	p: 87.87 	r: 75.69 	f1: 81.33 	 1152 	 1311 	 1522

[32m iter_2[0m
ga 	p: 83.55 	r: 75.28 	f1: 79.2 	 4643 	 5557 	 6168
wo 	p: 93.21 	r: 86.3 	f1: 89.62 	 2924 	 3137 	 3388
ni 	p: 87.94 	r: 75.69 	f1: 81.36 	 1152 	 1310 	 1522
best_thres [[0.3, 0.43, 0.19], [0.43, 0.46, 0.2], [0.44, 0.46, 0.2]]
f [0.8253, 0.8261, 0.8265]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.36, 0.71, 0.19] 	 lr: 0.0001 	 f: 82.8719723183391
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 10 [0m
Train...
loss: tensor(1064.4636) lr: 0.0001 time: 1391.52
pred_count_train 41644

Test...
loss: tensor(1066.1549) lr: 0.0001 time: 1493.35
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.33 	r: 77.48 	f1: 79.36 	 4779 	 5876 	 6168
wo 	p: 93.75 	r: 86.39 	f1: 89.92 	 2927 	 3122 	 3388
ni 	p: 87.46 	r: 80.16 	f1: 83.65 	 1220 	 1395 	 1522

[32m iter_1[0m
ga 	p: 81.56 	r: 77.25 	f1: 79.35 	 4765 	 5842 	 6168
wo 	p: 93.21 	r: 86.72 	f1: 89.85 	 2938 	 3152 	 3388
ni 	p: 87.66 	r: 80.29 	f1: 83.81 	 1222 	 1394 	 1522

[32m iter_2[0m
ga 	p: 81.1 	r: 77.58 	f1: 79.3 	 4785 	 5900 	 6168
wo 	p: 93.21 	r: 86.69 	f1: 89.83 	 2937 	 3151 	 3388
ni 	p: 87.72 	r: 80.22 	f1: 83.8 	 1221 	 1392 	 1522
best_thres [[0.42, 0.59, 0.2], [0.43, 0.54, 0.2], [0.41, 0.54, 0.2]]
f [0.8314, 0.8315, 0.8314]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 8 	 [0.41, 0.54, 0.2] 	 lr: 0.0001 	 f: 83.13630581153619
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 9 [0m
Train...
loss: tensor(494.8519) lr: 0.0001 time: 1437.4
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.74 	r: 77.22 	f1: 79.42 	 4763 	 5827 	 6168
wo 	p: 92.71 	r: 87.07 	f1: 89.8 	 2950 	 3182 	 3388
ni 	p: 86.79 	r: 79.83 	f1: 83.16 	 1215 	 1400 	 1522

[32m iter_1[0m
ga 	p: 83.58 	r: 75.78 	f1: 79.49 	 4674 	 5592 	 6168
wo 	p: 92.09 	r: 87.57 	f1: 89.77 	 2967 	 3222 	 3388
ni 	p: 86.3 	r: 80.29 	f1: 83.19 	 1222 	 1416 	 1522

[32m iter_2[0m
ga 	p: 83.63 	r: 75.7 	f1: 79.47 	 4669 	 5583 	 6168
wo 	p: 92.32 	r: 87.31 	f1: 89.75 	 2958 	 3204 	 3388
ni 	p: 86.35 	r: 80.22 	f1: 83.17 	 1221 	 1414 	 1522
best_thres [[0.47, 0.52, 0.13], [0.59, 0.49, 0.12], [0.6, 0.53, 0.12]]
f [0.831, 0.8315, 0.8315]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 8 	 [0.6, 0.53, 0.12] 	 lr: 0.0001 	 f: 83.15073196616413
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 9 [0m
Train...

[32m iter_0[0m
ga 	p: 82.26 	r: 75.55 	f1: 78.76 	 4660 	 5665 	 6168
wo 	p: 93.6 	r: 85.95 	f1: 89.61 	 2912 	 3111 	 3388
ni 	p: 86.51 	r: 75.82 	f1: 80.81 	 1154 	 1334 	 1522

[32m iter_1[0m
ga 	p: 82.22 	r: 75.63 	f1: 78.79 	 4665 	 5674 	 6168
wo 	p: 93.33 	r: 85.86 	f1: 89.44 	 2909 	 3117 	 3388
ni 	p: 86.94 	r: 75.69 	f1: 80.93 	 1152 	 1325 	 1522

[32m iter_2[0m
ga 	p: 81.84 	r: 76.01 	f1: 78.82 	 4688 	 5728 	 6168
wo 	p: 92.62 	r: 86.36 	f1: 89.38 	 2926 	 3159 	 3388
ni 	p: 87.21 	r: 75.69 	f1: 81.04 	 1152 	 1321 	 1522
best_thres [[0.54, 0.64, 0.27], [0.66, 0.78, 0.26], [0.66, 0.69, 0.26]]
f [0.8237, 0.8236, 0.8236]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.36, 0.71, 0.19] 	 lr: 0.0001 	 f: 82.8719723183391
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 11 [0m
Train...
loss: tensor(720.7991) lr: 0.0001 time: 1374.16
pred_count_train 41644

Test...
loss: tensor(730.8992) lr: 0.0001 time: 1463.99
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.94 	r: 75.96 	f1: 79.29 	 4685 	 5649 	 6168
wo 	p: 93.57 	r: 85.48 	f1: 89.34 	 2896 	 3095 	 3388
ni 	p: 83.52 	r: 78.25 	f1: 80.8 	 1191 	 1426 	 1522

[32m iter_1[0m
ga 	p: 81.82 	r: 76.64 	f1: 79.15 	 4727 	 5777 	 6168
wo 	p: 93.13 	r: 85.66 	f1: 89.24 	 2902 	 3116 	 3388
ni 	p: 81.22 	r: 80.68 	f1: 80.95 	 1228 	 1512 	 1522

[32m iter_2[0m
ga 	p: 82.72 	r: 75.84 	f1: 79.13 	 4678 	 5655 	 6168
wo 	p: 92.96 	r: 85.77 	f1: 89.22 	 2906 	 3126 	 3388
ni 	p: 81.16 	r: 80.68 	f1: 80.92 	 1228 	 1513 	 1522
best_thres [[0.44, 0.56, 0.19], [0.37, 0.52, 0.13], [0.43, 0.51, 0.13]]
f [0.8257, 0.8251, 0.825]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 8 	 [0.41, 0.54, 0.2] 	 lr: 0.0001 	 f: 83.13630581153619
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 10 [0m
Train...
loss: tensor(343.7079) lr: 0.0001 time: 1450.95
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.85 	r: 75.55 	f1: 78.58 	 4660 	 5693 	 6168
wo 	p: 93.99 	r: 85.45 	f1: 89.52 	 2895 	 3080 	 3388
ni 	p: 83.02 	r: 80.62 	f1: 81.8 	 1227 	 1478 	 1522

[32m iter_1[0m
ga 	p: 83.7 	r: 74.34 	f1: 78.74 	 4585 	 5478 	 6168
wo 	p: 94.05 	r: 85.45 	f1: 89.55 	 2895 	 3078 	 3388
ni 	p: 84.79 	r: 78.78 	f1: 81.68 	 1199 	 1414 	 1522

[32m iter_2[0m
ga 	p: 81.93 	r: 75.86 	f1: 78.78 	 4679 	 5711 	 6168
wo 	p: 92.79 	r: 86.63 	f1: 89.6 	 2935 	 3163 	 3388
ni 	p: 84.75 	r: 78.84 	f1: 81.69 	 1200 	 1416 	 1522
best_thres [[0.4, 0.72, 0.13], [0.57, 0.8, 0.15], [0.45, 0.68, 0.15]]
f [0.8235, 0.8241, 0.8244]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 8 	 [0.6, 0.53, 0.12] 	 lr: 0.0001 	 f: 83.15073196616413
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 10 [0m
Train...

[32m iter_0[0m
ga 	p: 81.64 	r: 75.28 	f1: 78.33 	 4643 	 5687 	 6168
wo 	p: 93.52 	r: 85.66 	f1: 89.42 	 2902 	 3103 	 3388
ni 	p: 86.73 	r: 75.16 	f1: 80.54 	 1144 	 1319 	 1522

[32m iter_1[0m
ga 	p: 81.9 	r: 75.28 	f1: 78.45 	 4643 	 5669 	 6168
wo 	p: 92.33 	r: 86.75 	f1: 89.45 	 2939 	 3183 	 3388
ni 	p: 87.96 	r: 74.9 	f1: 80.91 	 1140 	 1296 	 1522

[32m iter_2[0m
ga 	p: 81.84 	r: 75.39 	f1: 78.48 	 4650 	 5682 	 6168
wo 	p: 92.31 	r: 86.78 	f1: 89.46 	 2940 	 3185 	 3388
ni 	p: 88.07 	r: 74.7 	f1: 80.84 	 1137 	 1291 	 1522
best_thres [[0.42, 0.72, 0.09], [0.45, 0.56, 0.09], [0.44, 0.64, 0.09]]
f [0.8202, 0.821, 0.8213]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.36, 0.71, 0.19] 	 lr: 0.0001 	 f: 82.8719723183391
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 12 [0m
Train...
loss: tensor(482.4860) lr: 0.0001 time: 1353.08
pred_count_train 41644

Test...
loss: tensor(503.2966) lr: 0.0001 time: 1409.77
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.6 	r: 76.59 	f1: 79.02 	 4724 	 5789 	 6168
wo 	p: 93.05 	r: 86.1 	f1: 89.44 	 2917 	 3135 	 3388
ni 	p: 85.82 	r: 77.53 	f1: 81.46 	 1180 	 1375 	 1522

[32m iter_1[0m
ga 	p: 82.62 	r: 75.47 	f1: 78.88 	 4655 	 5634 	 6168
wo 	p: 93.06 	r: 86.28 	f1: 89.54 	 2923 	 3141 	 3388
ni 	p: 84.6 	r: 78.71 	f1: 81.55 	 1198 	 1416 	 1522

[32m iter_2[0m
ga 	p: 80.95 	r: 76.95 	f1: 78.9 	 4746 	 5863 	 6168
wo 	p: 93.14 	r: 86.19 	f1: 89.53 	 2920 	 3135 	 3388
ni 	p: 84.81 	r: 78.52 	f1: 81.54 	 1195 	 1409 	 1522
best_thres [[0.48, 0.79, 0.27], [0.59, 0.79, 0.2], [0.43, 0.8, 0.21]]
f [0.8253, 0.8253, 0.8251]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 8 	 [0.41, 0.54, 0.2] 	 lr: 0.0001 	 f: 83.13630581153619
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 11 [0m
Train...
loss: tensor(253.1915) lr: 0.0001 time: 1385.31
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.45 	r: 74.79 	f1: 78.88 	 4613 	 5528 	 6168
wo 	p: 93.26 	r: 86.54 	f1: 89.77 	 2932 	 3144 	 3388
ni 	p: 87.01 	r: 75.23 	f1: 80.69 	 1145 	 1316 	 1522

[32m iter_1[0m
ga 	p: 83.38 	r: 74.74 	f1: 78.82 	 4610 	 5529 	 6168
wo 	p: 93.0 	r: 86.69 	f1: 89.73 	 2937 	 3158 	 3388
ni 	p: 84.42 	r: 77.6 	f1: 80.86 	 1181 	 1399 	 1522

[32m iter_2[0m
ga 	p: 82.98 	r: 75.08 	f1: 78.83 	 4631 	 5581 	 6168
wo 	p: 93.87 	r: 85.89 	f1: 89.7 	 2910 	 3100 	 3388
ni 	p: 85.39 	r: 76.81 	f1: 80.87 	 1169 	 1369 	 1522
best_thres [[0.59, 0.48, 0.26], [0.7, 0.55, 0.15], [0.7, 0.74, 0.18]]
f [0.825, 0.8249, 0.8248]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 8 	 [0.6, 0.53, 0.12] 	 lr: 0.0001 	 f: 83.15073196616413
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 11 [0m
Train...

[32m iter_0[0m
ga 	p: 82.07 	r: 75.02 	f1: 78.38 	 4627 	 5638 	 6168
wo 	p: 94.37 	r: 84.59 	f1: 89.21 	 2866 	 3037 	 3388
ni 	p: 85.78 	r: 74.51 	f1: 79.75 	 1134 	 1322 	 1522

[32m iter_1[0m
ga 	p: 82.3 	r: 74.77 	f1: 78.36 	 4612 	 5604 	 6168
wo 	p: 92.93 	r: 85.71 	f1: 89.18 	 2904 	 3125 	 3388
ni 	p: 86.48 	r: 74.38 	f1: 79.97 	 1132 	 1309 	 1522

[32m iter_2[0m
ga 	p: 82.1 	r: 74.95 	f1: 78.36 	 4623 	 5631 	 6168
wo 	p: 93.27 	r: 85.48 	f1: 89.2 	 2896 	 3105 	 3388
ni 	p: 86.32 	r: 74.64 	f1: 80.06 	 1136 	 1316 	 1522
best_thres [[0.5, 0.78, 0.15], [0.67, 0.74, 0.14], [0.7, 0.85, 0.13]]
f [0.8187, 0.8189, 0.819]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.36, 0.71, 0.19] 	 lr: 0.0001 	 f: 82.8719723183391
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 13 [0m
Train...
loss: tensor(328.7398) lr: 0.0001 time: 1270.31
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.89 	r: 76.01 	f1: 78.84 	 4688 	 5725 	 6168
wo 	p: 93.19 	r: 85.98 	f1: 89.44 	 2913 	 3126 	 3388
ni 	p: 87.37 	r: 74.57 	f1: 80.47 	 1135 	 1299 	 1522

[32m iter_1[0m
ga 	p: 81.02 	r: 76.73 	f1: 78.82 	 4733 	 5842 	 6168
wo 	p: 93.21 	r: 85.83 	f1: 89.37 	 2908 	 3120 	 3388
ni 	p: 86.34 	r: 75.16 	f1: 80.37 	 1144 	 1325 	 1522

[32m iter_2[0m
ga 	p: 81.07 	r: 76.73 	f1: 78.84 	 4733 	 5838 	 6168
wo 	p: 93.23 	r: 85.77 	f1: 89.35 	 2906 	 3117 	 3388
ni 	p: 86.19 	r: 75.03 	f1: 80.22 	 1142 	 1325 	 1522
best_thres [[0.34, 0.63, 0.2], [0.28, 0.64, 0.16], [0.28, 0.64, 0.16]]
f [0.8231, 0.8227, 0.8226]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 8 	 [0.41, 0.54, 0.2] 	 lr: 0.0001 	 f: 83.13630581153619
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 12 [0m
Train...
loss: tensor(352.4475) lr: 0.0001 time: 1391.82
pred_count_train 41644

Test...
loss: tensor(627.8751) lr: 5e-05 time: 1375.68
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.61 	r: 74.51 	f1: 77.9 	 4596 	 5632 	 6168
wo 	p: 92.39 	r: 87.07 	f1: 89.65 	 2950 	 3193 	 3388
ni 	p: 83.61 	r: 75.43 	f1: 79.31 	 1148 	 1373 	 1522

[32m iter_1[0m
ga 	p: 81.61 	r: 74.48 	f1: 77.88 	 4594 	 5629 	 6168
wo 	p: 91.63 	r: 87.57 	f1: 89.56 	 2967 	 3238 	 3388
ni 	p: 85.11 	r: 74.38 	f1: 79.38 	 1132 	 1330 	 1522

[32m iter_2[0m
ga 	p: 79.92 	r: 75.96 	f1: 77.89 	 4685 	 5862 	 6168
wo 	p: 91.66 	r: 87.57 	f1: 89.57 	 2967 	 3237 	 3388
ni 	p: 84.7 	r: 74.9 	f1: 79.5 	 1140 	 1346 	 1522
best_thres [[0.44, 0.42, 0.08], [0.58, 0.38, 0.09], [0.4, 0.41, 0.08]]
f [0.8173, 0.8172, 0.8171]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 8 	 [0.6, 0.53, 0.12] 	 lr: 0.0001 	 f: 83.15073196616413
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 12 [0m
Train...
loss: tensor(247.2244) lr: 0.0001 time: 1261.39
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.37 	r: 75.91 	f1: 79.01 	 4682 	 5684 	 6168
wo 	p: 92.91 	r: 87.07 	f1: 89.9 	 2950 	 3175 	 3388
ni 	p: 85.21 	r: 79.5 	f1: 82.26 	 1210 	 1420 	 1522

[32m iter_1[0m
ga 	p: 83.33 	r: 75.31 	f1: 79.12 	 4645 	 5574 	 6168
wo 	p: 93.19 	r: 86.78 	f1: 89.87 	 2940 	 3155 	 3388
ni 	p: 87.0 	r: 78.71 	f1: 82.65 	 1198 	 1377 	 1522

[32m iter_2[0m
ga 	p: 80.17 	r: 78.13 	f1: 79.14 	 4819 	 6011 	 6168
wo 	p: 93.1 	r: 86.87 	f1: 89.88 	 2943 	 3161 	 3388
ni 	p: 88.28 	r: 77.73 	f1: 82.67 	 1183 	 1340 	 1522
best_thres [[0.45, 0.39, 0.11], [0.54, 0.41, 0.12], [0.31, 0.4, 0.15]]
f [0.828, 0.8286, 0.8286]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.36, 0.71, 0.19] 	 lr: 0.0001 	 f: 82.8719723183391
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 14 [0m
Train...

[32m iter_0[0m
ga 	p: 81.04 	r: 74.48 	f1: 77.62 	 4594 	 5669 	 6168
wo 	p: 92.89 	r: 86.01 	f1: 89.32 	 2914 	 3137 	 3388
ni 	p: 84.21 	r: 70.76 	f1: 76.9 	 1077 	 1279 	 1522

[32m iter_1[0m
ga 	p: 82.78 	r: 73.28 	f1: 77.74 	 4520 	 5460 	 6168
wo 	p: 93.58 	r: 85.66 	f1: 89.44 	 2902 	 3101 	 3388
ni 	p: 89.63 	r: 67.02 	f1: 76.69 	 1020 	 1138 	 1522

[32m iter_2[0m
ga 	p: 82.8 	r: 73.27 	f1: 77.74 	 4519 	 5458 	 6168
wo 	p: 92.85 	r: 86.28 	f1: 89.44 	 2923 	 3148 	 3388
ni 	p: 84.09 	r: 70.5 	f1: 76.7 	 1073 	 1276 	 1522
best_thres [[0.4, 0.67, 0.24], [0.54, 0.77, 0.46], [0.54, 0.63, 0.22]]
f [0.8113, 0.812, 0.8121]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 8 	 [0.41, 0.54, 0.2] 	 lr: 0.0001 	 f: 83.13630581153619
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 13 [0m
Train...
loss: tensor(262.3350) lr: 0.0001 time: 1364.57
pred_count_train 41644

Test...
loss: tensor(406.8441) lr: 5e-05 time: 1372.77
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.98 	r: 71.74 	f1: 77.38 	 4425 	 5269 	 6168
wo 	p: 93.47 	r: 85.33 	f1: 89.21 	 2891 	 3093 	 3388
ni 	p: 88.07 	r: 71.29 	f1: 78.79 	 1085 	 1232 	 1522

[32m iter_1[0m
ga 	p: 83.33 	r: 71.89 	f1: 77.19 	 4434 	 5321 	 6168
wo 	p: 92.52 	r: 85.77 	f1: 89.02 	 2906 	 3141 	 3388
ni 	p: 84.42 	r: 74.05 	f1: 78.89 	 1127 	 1335 	 1522

[32m iter_2[0m
ga 	p: 81.32 	r: 73.39 	f1: 77.15 	 4527 	 5567 	 6168
wo 	p: 92.19 	r: 86.01 	f1: 88.99 	 2914 	 3161 	 3388
ni 	p: 84.79 	r: 73.98 	f1: 79.02 	 1126 	 1328 	 1522
best_thres [[0.49, 0.64, 0.1], [0.62, 0.72, 0.04], [0.43, 0.75, 0.04]]
f [0.8128, 0.812, 0.8116]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 8 	 [0.6, 0.53, 0.12] 	 lr: 0.0001 	 f: 83.15073196616413
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 13 [0m
Train...
loss: tensor(628.1199) lr: 5e-05 time: 1282.63
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.05 	r: 76.33 	f1: 78.62 	 4708 	 5809 	 6168
wo 	p: 94.54 	r: 85.33 	f1: 89.7 	 2891 	 3058 	 3388
ni 	p: 85.07 	r: 76.35 	f1: 80.47 	 1162 	 1366 	 1522

[32m iter_1[0m
ga 	p: 81.22 	r: 76.28 	f1: 78.67 	 4705 	 5793 	 6168
wo 	p: 93.87 	r: 85.86 	f1: 89.69 	 2909 	 3099 	 3388
ni 	p: 86.63 	r: 75.36 	f1: 80.6 	 1147 	 1324 	 1522

[32m iter_2[0m
ga 	p: 81.2 	r: 76.33 	f1: 78.69 	 4708 	 5798 	 6168
wo 	p: 93.55 	r: 86.04 	f1: 89.64 	 2915 	 3116 	 3388
ni 	p: 86.85 	r: 75.1 	f1: 80.55 	 1143 	 1316 	 1522
best_thres [[0.38, 0.77, 0.14], [0.4, 0.85, 0.15], [0.39, 0.85, 0.16]]
f [0.8222, 0.8225, 0.8226]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.36, 0.71, 0.19] 	 lr: 0.0001 	 f: 82.8719723183391
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 15 [0m
Train...

[32m iter_0[0m
ga 	p: 82.29 	r: 76.18 	f1: 79.12 	 4699 	 5710 	 6168
wo 	p: 91.82 	r: 87.75 	f1: 89.74 	 2973 	 3238 	 3388
ni 	p: 86.64 	r: 80.55 	f1: 83.49 	 1226 	 1415 	 1522

[32m iter_1[0m
ga 	p: 83.06 	r: 75.62 	f1: 79.16 	 4664 	 5615 	 6168
wo 	p: 91.99 	r: 87.49 	f1: 89.68 	 2964 	 3222 	 3388
ni 	p: 87.26 	r: 80.09 	f1: 83.52 	 1219 	 1397 	 1522

[32m iter_2[0m
ga 	p: 83.08 	r: 75.63 	f1: 79.18 	 4665 	 5615 	 6168
wo 	p: 92.0 	r: 87.51 	f1: 89.7 	 2965 	 3223 	 3388
ni 	p: 87.32 	r: 80.09 	f1: 83.55 	 1219 	 1396 	 1522
best_thres [[0.42, 0.41, 0.15], [0.46, 0.44, 0.16], [0.46, 0.44, 0.16]]
f [0.83, 0.8301, 0.8302]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 8 	 [0.41, 0.54, 0.2] 	 lr: 0.0001 	 f: 83.13630581153619
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 14 [0m
Train...
loss: tensor(638.8522) lr: 5e-05 time: 1369.68
pred_count_train 41644

Test...
loss: tensor(266.1334) lr: 5e-05 time: 1401.81
pred_count_train 41644

Test...
loss: tensor(405.9707) lr: 5e-05 time: 1331.15
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.95 	r: 74.43 	f1: 78.9 	 4591 	 5469 	 6168
wo 	p: 92.14 	r: 87.57 	f1: 89.8 	 2967 	 3220 	 3388
ni 	p: 87.59 	r: 80.22 	f1: 83.74 	 1221 	 1394 	 1522

[32m iter_1[0m
ga 	p: 83.0 	r: 75.66 	f1: 79.16 	 4667 	 5623 	 6168
wo 	p: 92.04 	r: 87.66 	f1: 89.8 	 2970 	 3227 	 3388
ni 	p: 88.5 	r: 79.37 	f1: 83.69 	 1208 	 1365 	 1522

[32m iter_2[0m
ga 	p: 81.24 	r: 77.2 	f1: 79.17 	 4762 	 5862 	 6168
wo 	p: 92.07 	r: 87.75 	f1: 89.86 	 2973 	 3229 	 3388
ni 	p: 87.6 	r: 80.29 	f1: 83.78 	 1222 	 1395 	 1522
best_thres [[0.51, 0.32, 0.17], [0.48, 0.35, 0.19], [0.37, 0.35, 0.16]]
f [0.8297, 0.8303, 0.8304]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 8 	 [0.6, 0.53, 0.12] 	 lr: 0.0001 	 f: 83.15073196616413
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 14 [0m
Train...

[32m iter_0[0m
ga 	p: 82.0 	r: 75.21 	f1: 78.46 	 4639 	 5657 	 6168
wo 	p: 92.99 	r: 86.57 	f1: 89.67 	 2933 	 3154 	 3388
ni 	p: 84.88 	r: 74.51 	f1: 79.36 	 1134 	 1336 	 1522

[32m iter_1[0m
ga 	p: 81.89 	r: 75.6 	f1: 78.62 	 4663 	 5694 	 6168
wo 	p: 93.25 	r: 86.51 	f1: 89.76 	 2931 	 3143 	 3388
ni 	p: 83.88 	r: 75.89 	f1: 79.68 	 1155 	 1377 	 1522

[32m iter_2[0m
ga 	p: 81.82 	r: 75.6 	f1: 78.59 	 4663 	 5699 	 6168
wo 	p: 93.08 	r: 86.54 	f1: 89.69 	 2932 	 3150 	 3388
ni 	p: 83.08 	r: 76.48 	f1: 79.64 	 1164 	 1401 	 1522
best_thres [[0.38, 0.6, 0.11], [0.36, 0.82, 0.07], [0.35, 0.85, 0.06]]
f [0.8204, 0.8211, 0.8212]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.36, 0.71, 0.19] 	 lr: 0.0001 	 f: 82.8719723183391
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 16 [0m
Train...

[32m iter_0[0m
ga 	p: 83.17 	r: 74.61 	f1: 78.66 	 4602 	 5533 	 6168
wo 	p: 93.36 	r: 85.86 	f1: 89.45 	 2909 	 3116 	 3388
ni 	p: 87.07 	r: 77.0 	f1: 81.73 	 1172 	 1346 	 1522

[32m iter_1[0m
ga 	p: 81.65 	r: 75.84 	f1: 78.64 	 4678 	 5729 	 6168
wo 	p: 93.87 	r: 85.39 	f1: 89.43 	 2893 	 3082 	 3388
ni 	p: 85.52 	r: 77.99 	f1: 81.58 	 1187 	 1388 	 1522

[32m iter_2[0m
ga 	p: 81.67 	r: 75.83 	f1: 78.64 	 4677 	 5727 	 6168
wo 	p: 93.92 	r: 85.3 	f1: 89.4 	 2890 	 3077 	 3388
ni 	p: 87.52 	r: 76.48 	f1: 81.63 	 1164 	 1330 	 1522
best_thres [[0.56, 0.58, 0.2], [0.44, 0.64, 0.14], [0.44, 0.64, 0.2]]
f [0.8241, 0.8237, 0.8235]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 8 	 [0.41, 0.54, 0.2] 	 lr: 0.0001 	 f: 83.13630581153619
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 15 [0m
Train...
loss: tensor(416.2748) lr: 5e-05 time: 1386.77
pred_count_train 41644

Test...
loss: tensor(178.1799) lr: 5e-05 time: 1439.75
pred_count_train 41644

Test...
loss: tensor(257.6942) lr: 5e-05 time: 1359.91
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.22 	r: 74.08 	f1: 78.38 	 4569 	 5490 	 6168
wo 	p: 93.42 	r: 86.69 	f1: 89.93 	 2937 	 3144 	 3388
ni 	p: 88.23 	r: 76.35 	f1: 81.86 	 1162 	 1317 	 1522

[32m iter_1[0m
ga 	p: 82.54 	r: 74.72 	f1: 78.44 	 4609 	 5584 	 6168
wo 	p: 93.28 	r: 86.45 	f1: 89.74 	 2929 	 3140 	 3388
ni 	p: 86.76 	r: 77.92 	f1: 82.1 	 1186 	 1367 	 1522

[32m iter_2[0m
ga 	p: 81.72 	r: 75.45 	f1: 78.46 	 4654 	 5695 	 6168
wo 	p: 93.64 	r: 86.01 	f1: 89.66 	 2914 	 3112 	 3388
ni 	p: 87.49 	r: 77.2 	f1: 82.02 	 1175 	 1343 	 1522
best_thres [[0.57, 0.63, 0.29], [0.65, 0.75, 0.23], [0.6, 0.84, 0.27]]
f [0.8244, 0.8243, 0.8241]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 8 	 [0.6, 0.53, 0.12] 	 lr: 0.0001 	 f: 83.15073196616413
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 15 [0m
Train...

[32m iter_0[0m
ga 	p: 82.14 	r: 73.91 	f1: 77.81 	 4559 	 5550 	 6168
wo 	p: 91.65 	r: 86.84 	f1: 89.18 	 2942 	 3210 	 3388
ni 	p: 85.77 	r: 72.47 	f1: 78.56 	 1103 	 1286 	 1522

[32m iter_1[0m
ga 	p: 82.19 	r: 74.14 	f1: 77.96 	 4573 	 5564 	 6168
wo 	p: 91.99 	r: 86.75 	f1: 89.29 	 2939 	 3195 	 3388
ni 	p: 84.66 	r: 73.59 	f1: 78.73 	 1120 	 1323 	 1522

[32m iter_2[0m
ga 	p: 82.4 	r: 74.14 	f1: 78.05 	 4573 	 5550 	 6168
wo 	p: 91.98 	r: 86.72 	f1: 89.27 	 2938 	 3194 	 3388
ni 	p: 84.89 	r: 73.46 	f1: 78.76 	 1118 	 1317 	 1522
best_thres [[0.38, 0.32, 0.1], [0.39, 0.34, 0.06], [0.4, 0.34, 0.06]]
f [0.8146, 0.8152, 0.8156]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 8 	 [0.36, 0.71, 0.19] 	 lr: 0.0001 	 f: 82.8719723183391
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 17 [0m
Train...

[32m iter_0[0m
ga 	p: 82.63 	r: 74.71 	f1: 78.47 	 4608 	 5577 	 6168
wo 	p: 93.88 	r: 85.18 	f1: 89.32 	 2886 	 3074 	 3388
ni 	p: 87.49 	r: 76.28 	f1: 81.5 	 1161 	 1327 	 1522

[32m iter_1[0m
ga 	p: 82.74 	r: 75.15 	f1: 78.76 	 4635 	 5602 	 6168
wo 	p: 92.97 	r: 86.22 	f1: 89.46 	 2921 	 3142 	 3388
ni 	p: 87.81 	r: 76.22 	f1: 81.6 	 1160 	 1321 	 1522

[32m iter_2[0m
ga 	p: 82.13 	r: 75.63 	f1: 78.75 	 4665 	 5680 	 6168
wo 	p: 93.11 	r: 86.13 	f1: 89.48 	 2918 	 3134 	 3388
ni 	p: 87.8 	r: 76.15 	f1: 81.56 	 1159 	 1320 	 1522
best_thres [[0.52, 0.83, 0.19], [0.5, 0.7, 0.17], [0.45, 0.71, 0.17]]
f [0.8221, 0.8233, 0.8236]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 8 	 [0.41, 0.54, 0.2] 	 lr: 0.0001 	 f: 83.13630581153619
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 16 [0m
Train...
loss: tensor(272.3033) lr: 5e-05 time: 1391.24
pred_count_train 41644

Test...
loss: tensor(594.8286) lr: 2.5e-05 time: 1435.16
pred_count_train 41644

Test...
loss: tensor(168.9280) lr: 5e-05 time: 1378.97
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.19 	r: 74.08 	f1: 77.92 	 4569 	 5559 	 6168
wo 	p: 92.55 	r: 86.95 	f1: 89.67 	 2946 	 3183 	 3388
ni 	p: 85.05 	r: 76.61 	f1: 80.61 	 1166 	 1371 	 1522

[32m iter_1[0m
ga 	p: 81.09 	r: 75.18 	f1: 78.02 	 4637 	 5718 	 6168
wo 	p: 92.02 	r: 87.46 	f1: 89.68 	 2963 	 3220 	 3388
ni 	p: 85.86 	r: 77.0 	f1: 81.19 	 1172 	 1365 	 1522

[32m iter_2[0m
ga 	p: 81.6 	r: 74.79 	f1: 78.05 	 4613 	 5653 	 6168
wo 	p: 92.01 	r: 87.4 	f1: 89.65 	 2961 	 3218 	 3388
ni 	p: 86.04 	r: 76.94 	f1: 81.23 	 1171 	 1361 	 1522
best_thres [[0.36, 0.48, 0.08], [0.36, 0.47, 0.07], [0.47, 0.51, 0.07]]
f [0.8193, 0.8199, 0.8202]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 8 	 [0.6, 0.53, 0.12] 	 lr: 0.0001 	 f: 83.15073196616413
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 16 [0m
Train...

[32m iter_0[0m
ga 	p: 80.72 	r: 77.84 	f1: 79.25 	 4801 	 5948 	 6168
wo 	p: 93.3 	r: 86.66 	f1: 89.85 	 2936 	 3147 	 3388
ni 	p: 84.99 	r: 80.75 	f1: 82.82 	 1229 	 1446 	 1522

[32m iter_1[0m
ga 	p: 82.39 	r: 76.7 	f1: 79.45 	 4731 	 5742 	 6168
wo 	p: 92.68 	r: 87.1 	f1: 89.81 	 2951 	 3184 	 3388
ni 	p: 85.79 	r: 80.16 	f1: 82.88 	 1220 	 1422 	 1522

[32m iter_2[0m
ga 	p: 81.81 	r: 77.09 	f1: 79.38 	 4755 	 5812 	 6168
wo 	p: 92.73 	r: 86.95 	f1: 89.75 	 2946 	 3177 	 3388
ni 	p: 87.39 	r: 78.78 	f1: 82.86 	 1199 	 1372 	 1522
best_thres [[0.3, 0.57, 0.19], [0.39, 0.55, 0.2], [0.36, 0.6, 0.25]]
f [0.8295, 0.8302, 0.8302]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 17 	 [0.36, 0.6, 0.25] 	 lr: 2.5e-05 	 f: 83.02214502822406
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 18 [0m
Train...

[32m iter_0[0m
ga 	p: 82.54 	r: 74.51 	f1: 78.32 	 4596 	 5568 	 6168
wo 	p: 92.81 	r: 86.45 	f1: 89.52 	 2929 	 3156 	 3388
ni 	p: 86.04 	r: 75.3 	f1: 80.31 	 1146 	 1332 	 1522

[32m iter_1[0m
ga 	p: 83.4 	r: 73.95 	f1: 78.39 	 4561 	 5469 	 6168
wo 	p: 93.07 	r: 86.07 	f1: 89.43 	 2916 	 3133 	 3388
ni 	p: 87.59 	r: 73.72 	f1: 80.06 	 1122 	 1281 	 1522

[32m iter_2[0m
ga 	p: 83.83 	r: 73.61 	f1: 78.38 	 4540 	 5416 	 6168
wo 	p: 93.04 	r: 86.04 	f1: 89.4 	 2915 	 3133 	 3388
ni 	p: 82.35 	r: 77.86 	f1: 80.04 	 1185 	 1439 	 1522
best_thres [[0.48, 0.49, 0.16], [0.56, 0.58, 0.21], [0.6, 0.57, 0.07]]
f [0.8206, 0.8205, 0.8204]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 8 	 [0.41, 0.54, 0.2] 	 lr: 0.0001 	 f: 83.13630581153619
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 17 [0m
Train...
loss: tensor(186.8787) lr: 5e-05 time: 1398.48
pred_count_train 41644

Test...
loss: tensor(436.0885) lr: 2.5e-05 time: 1455.97
pred_count_train 41644

Test...
loss: tensor(595.2113) lr: 2.5e-05 time: 1403.64
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.42 	r: 75.03 	f1: 78.1 	 4628 	 5684 	 6168
wo 	p: 94.09 	r: 85.57 	f1: 89.63 	 2899 	 3081 	 3388
ni 	p: 84.68 	r: 74.84 	f1: 79.46 	 1139 	 1345 	 1522

[32m iter_1[0m
ga 	p: 82.02 	r: 74.98 	f1: 78.34 	 4625 	 5639 	 6168
wo 	p: 93.72 	r: 85.89 	f1: 89.63 	 2910 	 3105 	 3388
ni 	p: 84.38 	r: 76.68 	f1: 80.34 	 1167 	 1383 	 1522

[32m iter_2[0m
ga 	p: 81.78 	r: 75.34 	f1: 78.43 	 4647 	 5682 	 6168
wo 	p: 93.37 	r: 86.07 	f1: 89.57 	 2916 	 3123 	 3388
ni 	p: 84.85 	r: 76.54 	f1: 80.48 	 1165 	 1373 	 1522
best_thres [[0.34, 0.57, 0.11], [0.55, 0.72, 0.07], [0.6, 0.75, 0.07]]
f [0.818, 0.8194, 0.82]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 8 	 [0.6, 0.53, 0.12] 	 lr: 0.0001 	 f: 83.15073196616413
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 17 [0m
Train...

[32m iter_0[0m
ga 	p: 82.76 	r: 75.41 	f1: 78.91 	 4651 	 5620 	 6168
wo 	p: 93.72 	r: 86.3 	f1: 89.86 	 2924 	 3120 	 3388
ni 	p: 83.63 	r: 79.57 	f1: 81.55 	 1211 	 1448 	 1522

[32m iter_1[0m
ga 	p: 82.77 	r: 75.54 	f1: 78.99 	 4659 	 5629 	 6168
wo 	p: 93.72 	r: 86.28 	f1: 89.84 	 2923 	 3119 	 3388
ni 	p: 84.36 	r: 79.04 	f1: 81.61 	 1203 	 1426 	 1522

[32m iter_2[0m
ga 	p: 81.85 	r: 76.33 	f1: 78.99 	 4708 	 5752 	 6168
wo 	p: 93.6 	r: 86.3 	f1: 89.8 	 2924 	 3124 	 3388
ni 	p: 84.58 	r: 78.91 	f1: 81.65 	 1201 	 1420 	 1522
best_thres [[0.44, 0.55, 0.09], [0.45, 0.65, 0.09], [0.37, 0.67, 0.09]]
f [0.8263, 0.8265, 0.8265]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 17 	 [0.36, 0.6, 0.25] 	 lr: 2.5e-05 	 f: 83.02214502822406
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 19 [0m
Train...

[32m iter_0[0m
ga 	p: 80.52 	r: 78.16 	f1: 79.33 	 4821 	 5987 	 6168
wo 	p: 92.35 	r: 87.66 	f1: 89.95 	 2970 	 3216 	 3388
ni 	p: 86.3 	r: 80.29 	f1: 83.19 	 1222 	 1416 	 1522

[32m iter_1[0m
ga 	p: 80.62 	r: 78.32 	f1: 79.46 	 4831 	 5992 	 6168
wo 	p: 92.28 	r: 87.54 	f1: 89.85 	 2966 	 3214 	 3388
ni 	p: 86.5 	r: 79.96 	f1: 83.1 	 1217 	 1407 	 1522

[32m iter_2[0m
ga 	p: 80.29 	r: 78.71 	f1: 79.49 	 4855 	 6047 	 6168
wo 	p: 92.28 	r: 87.51 	f1: 89.83 	 2965 	 3213 	 3388
ni 	p: 86.56 	r: 79.96 	f1: 83.13 	 1217 	 1406 	 1522
best_thres [[0.32, 0.47, 0.26], [0.31, 0.46, 0.26], [0.29, 0.46, 0.26]]
f [0.8308, 0.831, 0.8311]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 8 	 [0.41, 0.54, 0.2] 	 lr: 0.0001 	 f: 83.13630581153619
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 18 [0m
Train...
loss: tensor(603.8151) lr: 2.5e-05 time: 1360.7
pred_count_train 41644

Test...
loss: tensor(315.5048) lr: 2.5e-05 time: 1425.36
pred_count_train 41644

Test...
loss: tensor(438.6139) lr: 2.5e-05 time: 1407.99
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.1 	r: 75.47 	f1: 79.1 	 4655 	 5602 	 6168
wo 	p: 91.78 	r: 87.99 	f1: 89.84 	 2981 	 3248 	 3388
ni 	p: 85.24 	r: 81.93 	f1: 83.55 	 1247 	 1463 	 1522

[32m iter_1[0m
ga 	p: 82.03 	r: 76.46 	f1: 79.15 	 4716 	 5749 	 6168
wo 	p: 93.99 	r: 86.28 	f1: 89.97 	 2923 	 3110 	 3388
ni 	p: 86.41 	r: 81.47 	f1: 83.87 	 1240 	 1435 	 1522

[32m iter_2[0m
ga 	p: 82.67 	r: 76.1 	f1: 79.25 	 4694 	 5678 	 6168
wo 	p: 93.57 	r: 86.75 	f1: 90.03 	 2939 	 3141 	 3388
ni 	p: 86.44 	r: 81.67 	f1: 83.99 	 1243 	 1438 	 1522
best_thres [[0.45, 0.43, 0.19], [0.41, 0.75, 0.19], [0.45, 0.72, 0.19]]
f [0.8305, 0.8307, 0.8312]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 8 	 [0.6, 0.53, 0.12] 	 lr: 0.0001 	 f: 83.15073196616413
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 18 [0m
Train...

[32m iter_0[0m
ga 	p: 80.65 	r: 76.3 	f1: 78.41 	 4706 	 5835 	 6168
wo 	p: 93.45 	r: 85.86 	f1: 89.49 	 2909 	 3113 	 3388
ni 	p: 85.19 	r: 75.56 	f1: 80.08 	 1150 	 1350 	 1522

[32m iter_1[0m
ga 	p: 83.52 	r: 74.03 	f1: 78.49 	 4566 	 5467 	 6168
wo 	p: 93.75 	r: 85.48 	f1: 89.42 	 2896 	 3089 	 3388
ni 	p: 81.43 	r: 78.65 	f1: 80.01 	 1197 	 1470 	 1522

[32m iter_2[0m
ga 	p: 83.5 	r: 73.99 	f1: 78.46 	 4564 	 5466 	 6168
wo 	p: 92.89 	r: 86.33 	f1: 89.49 	 2925 	 3149 	 3388
ni 	p: 81.58 	r: 78.58 	f1: 80.05 	 1196 	 1466 	 1522
best_thres [[0.25, 0.66, 0.15], [0.43, 0.84, 0.08], [0.43, 0.75, 0.08]]
f [0.8201, 0.8203, 0.8205]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 17 	 [0.36, 0.6, 0.25] 	 lr: 2.5e-05 	 f: 83.02214502822406
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 20 [0m
Train...

[32m iter_0[0m
ga 	p: 81.23 	r: 77.17 	f1: 79.15 	 4760 	 5860 	 6168
wo 	p: 92.2 	r: 87.54 	f1: 89.81 	 2966 	 3217 	 3388
ni 	p: 89.16 	r: 76.74 	f1: 82.49 	 1168 	 1310 	 1522

[32m iter_1[0m
ga 	p: 81.94 	r: 76.51 	f1: 79.13 	 4719 	 5759 	 6168
wo 	p: 94.2 	r: 85.74 	f1: 89.77 	 2905 	 3084 	 3388
ni 	p: 87.43 	r: 78.12 	f1: 82.51 	 1189 	 1360 	 1522

[32m iter_2[0m
ga 	p: 82.3 	r: 76.3 	f1: 79.19 	 4706 	 5718 	 6168
wo 	p: 93.48 	r: 86.3 	f1: 89.75 	 2924 	 3128 	 3388
ni 	p: 87.21 	r: 78.38 	f1: 82.56 	 1193 	 1368 	 1522
best_thres [[0.35, 0.4, 0.22], [0.4, 0.68, 0.16], [0.42, 0.58, 0.15]]
f [0.8287, 0.8285, 0.8286]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 8 	 [0.41, 0.54, 0.2] 	 lr: 0.0001 	 f: 83.13630581153619
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 19 [0m
Train...
loss: tensor(444.8828) lr: 2.5e-05 time: 1379.46
pred_count_train 41644

Test...
loss: tensor(219.4429) lr: 2.5e-05 time: 1435.66
pred_count_train 41644

Test...
loss: tensor(314.9213) lr: 2.5e-05 time: 1409.34
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.69 	r: 74.97 	f1: 78.64 	 4624 	 5592 	 6168
wo 	p: 93.47 	r: 86.6 	f1: 89.9 	 2934 	 3139 	 3388
ni 	p: 85.03 	r: 79.5 	f1: 82.17 	 1210 	 1423 	 1522

[32m iter_1[0m
ga 	p: 80.53 	r: 76.9 	f1: 78.67 	 4743 	 5890 	 6168
wo 	p: 93.58 	r: 86.51 	f1: 89.91 	 2931 	 3132 	 3388
ni 	p: 85.79 	r: 79.7 	f1: 82.63 	 1213 	 1414 	 1522

[32m iter_2[0m
ga 	p: 80.83 	r: 76.62 	f1: 78.67 	 4726 	 5847 	 6168
wo 	p: 93.27 	r: 86.78 	f1: 89.91 	 2940 	 3152 	 3388
ni 	p: 85.74 	r: 79.83 	f1: 82.68 	 1215 	 1417 	 1522
best_thres [[0.44, 0.58, 0.13], [0.35, 0.7, 0.11], [0.39, 0.67, 0.11]]
f [0.8259, 0.826, 0.8262]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 8 	 [0.6, 0.53, 0.12] 	 lr: 0.0001 	 f: 83.15073196616413
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 19 [0m
Train...

[32m iter_0[0m
ga 	p: 79.73 	r: 76.8 	f1: 78.24 	 4737 	 5941 	 6168
wo 	p: 93.89 	r: 85.74 	f1: 89.63 	 2905 	 3094 	 3388
ni 	p: 86.24 	r: 70.83 	f1: 77.78 	 1078 	 1250 	 1522

[32m iter_1[0m
ga 	p: 79.29 	r: 77.55 	f1: 78.41 	 4783 	 6032 	 6168
wo 	p: 93.92 	r: 85.71 	f1: 89.63 	 2904 	 3092 	 3388
ni 	p: 86.5 	r: 71.55 	f1: 78.32 	 1089 	 1259 	 1522

[32m iter_2[0m
ga 	p: 79.33 	r: 77.55 	f1: 78.43 	 4783 	 6029 	 6168
wo 	p: 92.99 	r: 86.54 	f1: 89.65 	 2932 	 3153 	 3388
ni 	p: 85.46 	r: 72.21 	f1: 78.28 	 1099 	 1286 	 1522
best_thres [[0.23, 0.67, 0.18], [0.18, 0.83, 0.15], [0.18, 0.66, 0.12]]
f [0.8164, 0.8171, 0.8175]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 17 	 [0.36, 0.6, 0.25] 	 lr: 2.5e-05 	 f: 83.02214502822406
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 21 [0m
Train...

[32m iter_0[0m
ga 	p: 81.97 	r: 76.09 	f1: 78.92 	 4693 	 5725 	 6168
wo 	p: 92.85 	r: 86.63 	f1: 89.63 	 2935 	 3161 	 3388
ni 	p: 87.49 	r: 75.36 	f1: 80.97 	 1147 	 1311 	 1522

[32m iter_1[0m
ga 	p: 82.69 	r: 75.36 	f1: 78.85 	 4648 	 5621 	 6168
wo 	p: 93.24 	r: 86.28 	f1: 89.62 	 2923 	 3135 	 3388
ni 	p: 84.68 	r: 78.06 	f1: 81.23 	 1188 	 1403 	 1522

[32m iter_2[0m
ga 	p: 82.69 	r: 75.37 	f1: 78.86 	 4649 	 5622 	 6168
wo 	p: 93.33 	r: 86.28 	f1: 89.66 	 2923 	 3132 	 3388
ni 	p: 84.66 	r: 77.99 	f1: 81.19 	 1187 	 1402 	 1522
best_thres [[0.38, 0.52, 0.21], [0.43, 0.57, 0.11], [0.43, 0.57, 0.11]]
f [0.8249, 0.8249, 0.8249]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 8 	 [0.41, 0.54, 0.2] 	 lr: 0.0001 	 f: 83.13630581153619
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 20 [0m
Train...
loss: tensor(323.5650) lr: 2.5e-05 time: 1331.44
pred_count_train 41644

Test...
loss: tensor(150.0544) lr: 2.5e-05 time: 1415.13
pred_count_train 41644

Test...
loss: tensor(215.8694) lr: 2.5e-05 time: 1406.8
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.41 	r: 74.51 	f1: 78.26 	 4596 	 5577 	 6168
wo 	p: 92.54 	r: 87.1 	f1: 89.74 	 2951 	 3189 	 3388
ni 	p: 83.47 	r: 79.3 	f1: 81.33 	 1207 	 1446 	 1522

[32m iter_1[0m
ga 	p: 82.31 	r: 74.84 	f1: 78.4 	 4616 	 5608 	 6168
wo 	p: 92.1 	r: 87.43 	f1: 89.7 	 2962 	 3216 	 3388
ni 	p: 86.09 	r: 77.27 	f1: 81.44 	 1176 	 1366 	 1522

[32m iter_2[0m
ga 	p: 83.09 	r: 74.22 	f1: 78.4 	 4578 	 5510 	 6168
wo 	p: 92.4 	r: 87.25 	f1: 89.75 	 2956 	 3199 	 3388
ni 	p: 83.91 	r: 79.17 	f1: 81.47 	 1205 	 1436 	 1522
best_thres [[0.38, 0.41, 0.11], [0.47, 0.4, 0.16], [0.61, 0.52, 0.1]]
f [0.8224, 0.8228, 0.823]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 8 	 [0.6, 0.53, 0.12] 	 lr: 0.0001 	 f: 83.15073196616413
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 20 [0m
Train...

[32m iter_0[0m
ga 	p: 80.89 	r: 75.37 	f1: 78.04 	 4649 	 5747 	 6168
wo 	p: 92.96 	r: 86.13 	f1: 89.41 	 2918 	 3139 	 3388
ni 	p: 85.12 	r: 71.42 	f1: 77.67 	 1087 	 1277 	 1522

[32m iter_1[0m
ga 	p: 80.39 	r: 75.76 	f1: 78.01 	 4673 	 5813 	 6168
wo 	p: 92.94 	r: 86.25 	f1: 89.47 	 2922 	 3144 	 3388
ni 	p: 82.82 	r: 73.78 	f1: 78.04 	 1123 	 1356 	 1522

[32m iter_2[0m
ga 	p: 80.44 	r: 75.81 	f1: 78.06 	 4676 	 5813 	 6168
wo 	p: 92.34 	r: 86.45 	f1: 89.3 	 2929 	 3172 	 3388
ni 	p: 88.17 	r: 70.04 	f1: 78.07 	 1066 	 1209 	 1522
best_thres [[0.32, 0.61, 0.15], [0.27, 0.83, 0.08], [0.27, 0.82, 0.21]]
f [0.8148, 0.815, 0.8151]
load model: epoch17
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 17 	 [0.36, 0.6, 0.25] 	 lr: 2.5e-05 	 f: 83.02214502822406
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 22 [0m
Train...

[32m iter_0[0m
ga 	p: 81.68 	r: 75.23 	f1: 78.32 	 4640 	 5681 	 6168
wo 	p: 92.36 	r: 87.1 	f1: 89.66 	 2951 	 3195 	 3388
ni 	p: 85.65 	r: 72.54 	f1: 78.55 	 1104 	 1289 	 1522

[32m iter_1[0m
ga 	p: 80.27 	r: 76.59 	f1: 78.39 	 4724 	 5885 	 6168
wo 	p: 92.69 	r: 86.87 	f1: 89.68 	 2943 	 3175 	 3388
ni 	p: 85.93 	r: 72.21 	f1: 78.47 	 1099 	 1279 	 1522

[32m iter_2[0m
ga 	p: 80.4 	r: 76.52 	f1: 78.41 	 4720 	 5871 	 6168
wo 	p: 92.75 	r: 86.87 	f1: 89.71 	 2943 	 3173 	 3388
ni 	p: 85.99 	r: 72.21 	f1: 78.5 	 1099 	 1278 	 1522
best_thres [[0.41, 0.38, 0.22], [0.27, 0.44, 0.21], [0.28, 0.44, 0.21]]
f [0.8186, 0.8186, 0.8187]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 8 	 [0.41, 0.54, 0.2] 	 lr: 0.0001 	 f: 83.13630581153619
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 21 [0m
Train...
loss: tensor(228.8785) lr: 2.5e-05 time: 1311.15
pred_count_train 41644

Test...
loss: tensor(420.6849) lr: 1.25e-05 time: 1400.23
pred_count_train 41644

Test...
loss: tensor(595.8080) lr: 1.25e-05 time: 1385.81
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.1 	r: 75.83 	f1: 77.9 	 4677 	 5839 	 6168
wo 	p: 91.9 	r: 87.72 	f1: 89.76 	 2972 	 3234 	 3388
ni 	p: 83.58 	r: 73.92 	f1: 78.45 	 1125 	 1346 	 1522

[32m iter_1[0m
ga 	p: 80.43 	r: 76.02 	f1: 78.16 	 4689 	 5830 	 6168
wo 	p: 92.49 	r: 87.25 	f1: 89.79 	 2956 	 3196 	 3388
ni 	p: 84.86 	r: 73.65 	f1: 78.86 	 1121 	 1321 	 1522

[32m iter_2[0m
ga 	p: 80.31 	r: 76.18 	f1: 78.19 	 4699 	 5851 	 6168
wo 	p: 92.07 	r: 87.43 	f1: 89.69 	 2962 	 3217 	 3388
ni 	p: 86.51 	r: 72.47 	f1: 78.87 	 1103 	 1275 	 1522
best_thres [[0.25, 0.36, 0.13], [0.33, 0.55, 0.13], [0.35, 0.51, 0.16]]
f [0.8163, 0.8173, 0.8176]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 8 	 [0.6, 0.53, 0.12] 	 lr: 0.0001 	 f: 83.15073196616413
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 21 [0m
Train...

[32m iter_0[0m
ga 	p: 82.98 	r: 75.41 	f1: 79.01 	 4651 	 5605 	 6168
wo 	p: 92.71 	r: 87.07 	f1: 89.8 	 2950 	 3182 	 3388
ni 	p: 83.97 	r: 78.84 	f1: 81.33 	 1200 	 1429 	 1522

[32m iter_1[0m
ga 	p: 81.23 	r: 76.98 	f1: 79.05 	 4748 	 5845 	 6168
wo 	p: 92.71 	r: 87.07 	f1: 89.8 	 2950 	 3182 	 3388
ni 	p: 85.33 	r: 77.99 	f1: 81.5 	 1187 	 1391 	 1522

[32m iter_2[0m
ga 	p: 81.89 	r: 76.46 	f1: 79.08 	 4716 	 5759 	 6168
wo 	p: 92.96 	r: 86.87 	f1: 89.81 	 2943 	 3166 	 3388
ni 	p: 85.39 	r: 77.92 	f1: 81.48 	 1186 	 1389 	 1522
best_thres [[0.52, 0.51, 0.13], [0.4, 0.51, 0.15], [0.45, 0.62, 0.15]]
f [0.8266, 0.8266, 0.8267]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 17 	 [0.36, 0.6, 0.25] 	 lr: 2.5e-05 	 f: 83.02214502822406
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 23 [0m
Train...

[32m iter_0[0m
ga 	p: 81.93 	r: 76.91 	f1: 79.34 	 4744 	 5790 	 6168
wo 	p: 93.46 	r: 86.42 	f1: 89.8 	 2928 	 3133 	 3388
ni 	p: 89.04 	r: 77.92 	f1: 83.11 	 1186 	 1332 	 1522

[32m iter_1[0m
ga 	p: 82.98 	r: 76.13 	f1: 79.41 	 4696 	 5659 	 6168
wo 	p: 93.36 	r: 86.36 	f1: 89.73 	 2926 	 3134 	 3388
ni 	p: 88.37 	r: 78.38 	f1: 83.08 	 1193 	 1350 	 1522

[32m iter_2[0m
ga 	p: 83.21 	r: 76.01 	f1: 79.44 	 4688 	 5634 	 6168
wo 	p: 93.54 	r: 86.33 	f1: 89.79 	 2925 	 3127 	 3388
ni 	p: 88.43 	r: 78.32 	f1: 83.07 	 1192 	 1348 	 1522
best_thres [[0.42, 0.58, 0.24], [0.47, 0.58, 0.21], [0.48, 0.6, 0.21]]
f [0.8305, 0.8306, 0.8308]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 8 	 [0.41, 0.54, 0.2] 	 lr: 0.0001 	 f: 83.13630581153619
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 22 [0m
Train...
loss: tensor(602.6010) lr: 1.25e-05 time: 1280.56
pred_count_train 41644

Test...
loss: tensor(344.9712) lr: 1.25e-05 time: 1404.99
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.32 	r: 77.19 	f1: 79.2 	 4761 	 5855 	 6168
wo 	p: 92.83 	r: 87.49 	f1: 90.08 	 2964 	 3193 	 3388
ni 	p: 87.52 	r: 80.62 	f1: 83.93 	 1227 	 1402 	 1522

[32m iter_1[0m
ga 	p: 81.52 	r: 77.45 	f1: 79.43 	 4777 	 5860 	 6168
wo 	p: 92.45 	r: 87.78 	f1: 90.05 	 2974 	 3217 	 3388
ni 	p: 85.87 	r: 82.26 	f1: 84.03 	 1252 	 1458 	 1522

[32m iter_2[0m
ga 	p: 81.56 	r: 77.5 	f1: 79.47 	 4780 	 5861 	 6168
wo 	p: 92.57 	r: 87.54 	f1: 89.99 	 2966 	 3204 	 3388
ni 	p: 86.48 	r: 81.93 	f1: 84.14 	 1247 	 1442 	 1522
best_thres [[0.38, 0.48, 0.17], [0.4, 0.48, 0.12], [0.4, 0.51, 0.13]]
f [0.8317, 0.8324, 0.8327]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 21 	 [0.4, 0.51, 0.13] 	 lr: 1.25e-05 	 f: 83.2679294255786
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 22 [0m
Train...
loss: tensor(485.7905) lr: 1.25e-05 time: 1401.8
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.89 	r: 77.24 	f1: 78.54 	 4764 	 5963 	 6168
wo 	p: 92.91 	r: 86.69 	f1: 89.69 	 2937 	 3161 	 3388
ni 	p: 84.45 	r: 78.84 	f1: 81.55 	 1200 	 1421 	 1522

[32m iter_1[0m
ga 	p: 80.15 	r: 77.58 	f1: 78.84 	 4785 	 5970 	 6168
wo 	p: 93.54 	r: 85.92 	f1: 89.57 	 2911 	 3112 	 3388
ni 	p: 86.17 	r: 77.4 	f1: 81.55 	 1178 	 1367 	 1522

[32m iter_2[0m
ga 	p: 80.18 	r: 77.53 	f1: 78.83 	 4782 	 5964 	 6168
wo 	p: 93.61 	r: 85.98 	f1: 89.63 	 2913 	 3112 	 3388
ni 	p: 83.95 	r: 79.37 	f1: 81.59 	 1208 	 1439 	 1522
best_thres [[0.3, 0.57, 0.12], [0.29, 0.78, 0.15], [0.29, 0.8, 0.1]]
f [0.8233, 0.8239, 0.8241]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 17 	 [0.36, 0.6, 0.25] 	 lr: 2.5e-05 	 f: 83.02214502822406
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 24 [0m
Train...

[32m iter_0[0m
ga 	p: 82.18 	r: 76.69 	f1: 79.34 	 4730 	 5756 	 6168
wo 	p: 92.54 	r: 87.49 	f1: 89.94 	 2964 	 3203 	 3388
ni 	p: 88.31 	r: 77.4 	f1: 82.49 	 1178 	 1334 	 1522

[32m iter_1[0m
ga 	p: 82.59 	r: 76.35 	f1: 79.34 	 4709 	 5702 	 6168
wo 	p: 92.63 	r: 87.22 	f1: 89.84 	 2955 	 3190 	 3388
ni 	p: 87.5 	r: 78.19 	f1: 82.58 	 1190 	 1360 	 1522

[32m iter_2[0m
ga 	p: 81.71 	r: 77.08 	f1: 79.33 	 4754 	 5818 	 6168
wo 	p: 92.44 	r: 87.4 	f1: 89.85 	 2961 	 3203 	 3388
ni 	p: 86.93 	r: 78.65 	f1: 82.58 	 1197 	 1377 	 1522
best_thres [[0.48, 0.45, 0.25], [0.5, 0.46, 0.21], [0.44, 0.44, 0.19]]
f [0.8303, 0.8302, 0.8301]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 8 	 [0.41, 0.54, 0.2] 	 lr: 0.0001 	 f: 83.13630581153619
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 23 [0m
Train...
loss: tensor(494.0039) lr: 1.25e-05 time: 1249.08
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.4 	r: 74.68 	f1: 78.8 	 4606 	 5523 	 6168
wo 	p: 93.17 	r: 86.95 	f1: 89.95 	 2946 	 3162 	 3388
ni 	p: 86.57 	r: 78.78 	f1: 82.49 	 1199 	 1385 	 1522

[32m iter_1[0m
ga 	p: 80.69 	r: 77.5 	f1: 79.06 	 4780 	 5924 	 6168
wo 	p: 93.62 	r: 86.6 	f1: 89.97 	 2934 	 3134 	 3388
ni 	p: 86.08 	r: 79.63 	f1: 82.73 	 1212 	 1408 	 1522

[32m iter_2[0m
ga 	p: 80.76 	r: 77.51 	f1: 79.1 	 4781 	 5920 	 6168
wo 	p: 93.03 	r: 87.07 	f1: 89.95 	 2950 	 3171 	 3388
ni 	p: 88.08 	r: 78.19 	f1: 82.84 	 1190 	 1351 	 1522
best_thres [[0.53, 0.54, 0.19], [0.37, 0.66, 0.16], [0.38, 0.61, 0.2]]
f [0.8276, 0.8281, 0.8284]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 21 	 [0.4, 0.51, 0.13] 	 lr: 1.25e-05 	 f: 83.2679294255786
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 23 [0m
Train...
loss: tensor(278.4333) lr: 1.25e-05 time: 1384.43
pred_count_train 41644

Test...
loss: tensor(404.9036) lr: 1.25e-05 time: 1385.39
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.13 	r: 76.04 	f1: 78.5 	 4690 	 5781 	 6168
wo 	p: 93.07 	r: 86.36 	f1: 89.59 	 2926 	 3144 	 3388
ni 	p: 85.0 	r: 76.68 	f1: 80.62 	 1167 	 1373 	 1522

[32m iter_1[0m
ga 	p: 82.2 	r: 75.41 	f1: 78.66 	 4651 	 5658 	 6168
wo 	p: 92.73 	r: 86.6 	f1: 89.56 	 2934 	 3164 	 3388
ni 	p: 85.39 	r: 76.41 	f1: 80.65 	 1163 	 1362 	 1522

[32m iter_2[0m
ga 	p: 82.22 	r: 75.34 	f1: 78.63 	 4647 	 5652 	 6168
wo 	p: 93.71 	r: 85.74 	f1: 89.55 	 2905 	 3100 	 3388
ni 	p: 84.63 	r: 77.07 	f1: 80.67 	 1173 	 1386 	 1522
best_thres [[0.31, 0.58, 0.11], [0.38, 0.61, 0.1], [0.38, 0.84, 0.09]]
f [0.8218, 0.8223, 0.8224]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 17 	 [0.36, 0.6, 0.25] 	 lr: 2.5e-05 	 f: 83.02214502822406
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 25 [0m
Train...

[32m iter_0[0m
ga 	p: 79.09 	r: 78.86 	f1: 78.97 	 4864 	 6150 	 6168
wo 	p: 92.52 	r: 87.28 	f1: 89.82 	 2957 	 3196 	 3388
ni 	p: 88.25 	r: 77.0 	f1: 82.25 	 1172 	 1328 	 1522

[32m iter_1[0m
ga 	p: 80.7 	r: 77.37 	f1: 79.0 	 4772 	 5913 	 6168
wo 	p: 92.77 	r: 87.13 	f1: 89.86 	 2952 	 3182 	 3388
ni 	p: 87.3 	r: 77.66 	f1: 82.2 	 1182 	 1354 	 1522

[32m iter_2[0m
ga 	p: 80.72 	r: 77.38 	f1: 79.02 	 4773 	 5913 	 6168
wo 	p: 92.69 	r: 87.25 	f1: 89.89 	 2956 	 3189 	 3388
ni 	p: 87.97 	r: 77.33 	f1: 82.31 	 1177 	 1338 	 1522
best_thres [[0.24, 0.46, 0.24], [0.34, 0.47, 0.2], [0.34, 0.46, 0.22]]
f [0.8269, 0.8271, 0.8274]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 8 	 [0.41, 0.54, 0.2] 	 lr: 0.0001 	 f: 83.13630581153619
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 24 [0m
Train...
loss: tensor(411.5811) lr: 1.25e-05 time: 1244.34
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.81 	r: 77.87 	f1: 78.83 	 4803 	 6018 	 6168
wo 	p: 93.57 	r: 86.39 	f1: 89.84 	 2927 	 3128 	 3388
ni 	p: 86.54 	r: 78.58 	f1: 82.37 	 1196 	 1382 	 1522

[32m iter_1[0m
ga 	p: 83.05 	r: 75.41 	f1: 79.04 	 4651 	 5600 	 6168
wo 	p: 93.3 	r: 86.75 	f1: 89.91 	 2939 	 3150 	 3388
ni 	p: 86.72 	r: 79.37 	f1: 82.88 	 1208 	 1393 	 1522

[32m iter_2[0m
ga 	p: 83.53 	r: 75.13 	f1: 79.11 	 4634 	 5548 	 6168
wo 	p: 93.17 	r: 86.95 	f1: 89.95 	 2946 	 3162 	 3388
ni 	p: 86.92 	r: 79.43 	f1: 83.01 	 1209 	 1391 	 1522
best_thres [[0.29, 0.62, 0.18], [0.56, 0.65, 0.15], [0.62, 0.65, 0.15]]
f [0.8263, 0.8277, 0.8285]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 21 	 [0.4, 0.51, 0.13] 	 lr: 1.25e-05 	 f: 83.2679294255786
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 24 [0m
Train...
loss: tensor(221.4166) lr: 1.25e-05 time: 1408.58
pred_count_train 41644

Test...
loss: tensor(332.5818) lr: 1.25e-05 time: 1409.67
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.73 	r: 75.03 	f1: 78.69 	 4628 	 5594 	 6168
wo 	p: 92.73 	r: 87.31 	f1: 89.94 	 2958 	 3190 	 3388
ni 	p: 87.84 	r: 75.49 	f1: 81.2 	 1149 	 1308 	 1522

[32m iter_1[0m
ga 	p: 79.46 	r: 77.79 	f1: 78.62 	 4798 	 6038 	 6168
wo 	p: 92.26 	r: 87.54 	f1: 89.84 	 2966 	 3215 	 3388
ni 	p: 89.32 	r: 74.7 	f1: 81.36 	 1137 	 1273 	 1522

[32m iter_2[0m
ga 	p: 81.84 	r: 75.71 	f1: 78.66 	 4670 	 5706 	 6168
wo 	p: 92.23 	r: 87.63 	f1: 89.87 	 2969 	 3219 	 3388
ni 	p: 88.37 	r: 75.36 	f1: 81.35 	 1147 	 1298 	 1522
best_thres [[0.47, 0.42, 0.19], [0.25, 0.38, 0.22], [0.4, 0.37, 0.19]]
f [0.8252, 0.8246, 0.8247]
load model: epoch8
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 8 	 [0.41, 0.54, 0.2] 	 lr: 0.0001 	 f: 83.13630581153619
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 25 [0m
Train...

[32m iter_0[0m
ga 	p: 79.42 	r: 77.29 	f1: 78.34 	 4767 	 6002 	 6168
wo 	p: 93.2 	r: 86.22 	f1: 89.57 	 2921 	 3134 	 3388
ni 	p: 83.16 	r: 75.3 	f1: 79.03 	 1146 	 1378 	 1522

[32m iter_1[0m
ga 	p: 80.33 	r: 76.86 	f1: 78.56 	 4741 	 5902 	 6168
wo 	p: 92.98 	r: 86.33 	f1: 89.53 	 2925 	 3146 	 3388
ni 	p: 86.23 	r: 73.26 	f1: 79.22 	 1115 	 1293 	 1522

[32m iter_2[0m
ga 	p: 80.44 	r: 76.88 	f1: 78.62 	 4742 	 5895 	 6168
wo 	p: 92.92 	r: 86.36 	f1: 89.52 	 2926 	 3149 	 3388
ni 	p: 82.53 	r: 76.35 	f1: 79.32 	 1162 	 1408 	 1522
best_thres [[0.21, 0.58, 0.11], [0.26, 0.7, 0.15], [0.26, 0.74, 0.08]]
f [0.8183, 0.8191, 0.8195]
load model: epoch17
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 17 	 [0.36, 0.6, 0.25] 	 lr: 2.5e-05 	 f: 83.02214502822406
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 26 [0m
Train...
loss: tensor(339.2007) lr: 1.25e-05 time: 1236.24
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.23 	r: 74.94 	f1: 78.41 	 4622 	 5621 	 6168
wo 	p: 93.7 	r: 86.1 	f1: 89.74 	 2917 	 3113 	 3388
ni 	p: 83.25 	r: 79.7 	f1: 81.44 	 1213 	 1457 	 1522

[32m iter_1[0m
ga 	p: 82.91 	r: 74.82 	f1: 78.66 	 4615 	 5566 	 6168
wo 	p: 94.0 	r: 85.95 	f1: 89.79 	 2912 	 3098 	 3388
ni 	p: 84.89 	r: 78.65 	f1: 81.65 	 1197 	 1410 	 1522

[32m iter_2[0m
ga 	p: 82.86 	r: 74.87 	f1: 78.66 	 4618 	 5573 	 6168
wo 	p: 94.05 	r: 85.77 	f1: 89.72 	 2906 	 3090 	 3388
ni 	p: 84.5 	r: 79.17 	f1: 81.75 	 1205 	 1426 	 1522
best_thres [[0.42, 0.61, 0.09], [0.56, 0.75, 0.1], [0.59, 0.79, 0.09]]
f [0.823, 0.8239, 0.8242]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 21 	 [0.4, 0.51, 0.13] 	 lr: 1.25e-05 	 f: 83.2679294255786
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 25 [0m
Train...
loss: tensor(599.1056) lr: 1e-05 time: 1386.29
pred_count_train 41644

Test...
loss: tensor(417.8584) lr: 1e-05 time: 1382.27
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.74 	r: 78.44 	f1: 79.57 	 4838 	 5992 	 6168
wo 	p: 93.49 	r: 86.45 	f1: 89.83 	 2929 	 3133 	 3388
ni 	p: 87.47 	r: 79.37 	f1: 83.22 	 1208 	 1381 	 1522

[32m iter_1[0m
ga 	p: 82.8 	r: 76.41 	f1: 79.48 	 4713 	 5692 	 6168
wo 	p: 92.39 	r: 87.49 	f1: 89.87 	 2964 	 3208 	 3388
ni 	p: 86.31 	r: 80.35 	f1: 83.23 	 1223 	 1417 	 1522

[32m iter_2[0m
ga 	p: 82.82 	r: 76.43 	f1: 79.49 	 4714 	 5692 	 6168
wo 	p: 92.48 	r: 87.43 	f1: 89.88 	 2962 	 3203 	 3388
ni 	p: 87.47 	r: 79.37 	f1: 83.22 	 1208 	 1381 	 1522
best_thres [[0.35, 0.54, 0.22], [0.48, 0.39, 0.18], [0.48, 0.4, 0.21]]
f [0.8316, 0.8318, 0.8319]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 25 	 [0.48, 0.4, 0.21] 	 lr: 1e-05 	 f: 83.18903206752367
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 26 [0m
Train...

[32m iter_0[0m
ga 	p: 82.49 	r: 75.63 	f1: 78.91 	 4665 	 5655 	 6168
wo 	p: 91.25 	r: 88.34 	f1: 89.77 	 2993 	 3280 	 3388
ni 	p: 86.74 	r: 75.62 	f1: 80.8 	 1151 	 1327 	 1522

[32m iter_1[0m
ga 	p: 80.52 	r: 77.61 	f1: 79.04 	 4787 	 5945 	 6168
wo 	p: 92.65 	r: 87.1 	f1: 89.79 	 2951 	 3185 	 3388
ni 	p: 86.86 	r: 75.56 	f1: 80.82 	 1150 	 1324 	 1522

[32m iter_2[0m
ga 	p: 81.12 	r: 77.12 	f1: 79.07 	 4757 	 5864 	 6168
wo 	p: 92.76 	r: 86.98 	f1: 89.78 	 2947 	 3177 	 3388
ni 	p: 87.06 	r: 75.56 	f1: 80.9 	 1150 	 1321 	 1522
best_thres [[0.45, 0.32, 0.16], [0.3, 0.51, 0.15], [0.34, 0.59, 0.15]]
f [0.8256, 0.8256, 0.8257]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 17 	 [0.36, 0.6, 0.25] 	 lr: 2.5e-05 	 f: 83.02214502822406
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 27 [0m
Train...
loss: tensor(275.3592) lr: 1.25e-05 time: 1243.05
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.66 	r: 76.75 	f1: 78.18 	 4734 	 5943 	 6168
wo 	p: 93.48 	r: 86.3 	f1: 89.75 	 2924 	 3128 	 3388
ni 	p: 84.63 	r: 75.23 	f1: 79.65 	 1145 	 1353 	 1522

[32m iter_1[0m
ga 	p: 82.35 	r: 74.72 	f1: 78.35 	 4609 	 5597 	 6168
wo 	p: 92.3 	r: 87.37 	f1: 89.76 	 2960 	 3207 	 3388
ni 	p: 84.31 	r: 76.61 	f1: 80.28 	 1166 	 1383 	 1522

[32m iter_2[0m
ga 	p: 81.62 	r: 75.39 	f1: 78.38 	 4650 	 5697 	 6168
wo 	p: 93.09 	r: 86.69 	f1: 89.78 	 2937 	 3155 	 3388
ni 	p: 84.67 	r: 76.54 	f1: 80.4 	 1165 	 1376 	 1522
best_thres [[0.25, 0.63, 0.17], [0.58, 0.48, 0.13], [0.54, 0.71, 0.13]]
f [0.8188, 0.8202, 0.8206]
load model: epoch21
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 21 	 [0.4, 0.51, 0.13] 	 lr: 1.25e-05 	 f: 83.2679294255786
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 26 [0m
Train...
loss: tensor(500.6074) lr: 1e-05 time: 1397.59
pred_count_train 41644

Test...
loss: tensor(354.1628) lr: 1e-05 time: 1389.12
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.86 	r: 77.63 	f1: 79.21 	 4788 	 5921 	 6168
wo 	p: 92.77 	r: 87.16 	f1: 89.88 	 2953 	 3183 	 3388
ni 	p: 86.17 	r: 78.19 	f1: 81.98 	 1190 	 1381 	 1522

[32m iter_1[0m
ga 	p: 82.28 	r: 76.48 	f1: 79.27 	 4717 	 5733 	 6168
wo 	p: 92.32 	r: 87.63 	f1: 89.92 	 2969 	 3216 	 3388
ni 	p: 87.05 	r: 77.73 	f1: 82.12 	 1183 	 1359 	 1522

[32m iter_2[0m
ga 	p: 83.34 	r: 75.65 	f1: 79.31 	 4666 	 5599 	 6168
wo 	p: 92.38 	r: 87.66 	f1: 89.96 	 2970 	 3215 	 3388
ni 	p: 87.81 	r: 77.14 	f1: 82.13 	 1174 	 1337 	 1522
best_thres [[0.35, 0.45, 0.17], [0.43, 0.39, 0.18], [0.49, 0.39, 0.2]]
f [0.8284, 0.8289, 0.8293]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 25 	 [0.48, 0.4, 0.21] 	 lr: 1e-05 	 f: 83.18903206752367
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 27 [0m
Train...
loss: tensor(491.4955) lr: 1e-05 time: 1245.78
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.6 	r: 74.98 	f1: 78.61 	 4625 	 5599 	 6168
wo 	p: 93.73 	r: 86.07 	f1: 89.74 	 2916 	 3111 	 3388
ni 	p: 86.32 	r: 76.28 	f1: 80.99 	 1161 	 1345 	 1522

[32m iter_1[0m
ga 	p: 81.15 	r: 76.59 	f1: 78.81 	 4724 	 5821 	 6168
wo 	p: 93.78 	r: 85.83 	f1: 89.63 	 2908 	 3101 	 3388
ni 	p: 87.68 	r: 75.3 	f1: 81.02 	 1146 	 1307 	 1522

[32m iter_2[0m
ga 	p: 81.42 	r: 76.46 	f1: 78.86 	 4716 	 5792 	 6168
wo 	p: 93.38 	r: 86.13 	f1: 89.61 	 2918 	 3125 	 3388
ni 	p: 87.86 	r: 75.16 	f1: 81.02 	 1144 	 1302 	 1522
best_thres [[0.46, 0.65, 0.18], [0.36, 0.77, 0.2], [0.37, 0.74, 0.2]]
f [0.8235, 0.8238, 0.8239]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 17 	 [0.36, 0.6, 0.25] 	 lr: 2.5e-05 	 f: 83.02214502822406
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 28 [0m
Train...

[32m iter_0[0m
ga 	p: 80.8 	r: 77.32 	f1: 79.02 	 4769 	 5902 	 6168
wo 	p: 94.44 	r: 85.74 	f1: 89.88 	 2905 	 3076 	 3388
ni 	p: 84.75 	r: 80.68 	f1: 82.67 	 1228 	 1449 	 1522

[32m iter_1[0m
ga 	p: 81.65 	r: 76.73 	f1: 79.11 	 4733 	 5797 	 6168
wo 	p: 94.65 	r: 85.68 	f1: 89.95 	 2903 	 3067 	 3388
ni 	p: 85.61 	r: 80.16 	f1: 82.8 	 1220 	 1425 	 1522

[32m iter_2[0m
ga 	p: 82.0 	r: 76.46 	f1: 79.13 	 4716 	 5751 	 6168
wo 	p: 94.48 	r: 85.83 	f1: 89.95 	 2908 	 3078 	 3388
ni 	p: 86.83 	r: 79.3 	f1: 82.9 	 1207 	 1390 	 1522
best_thres [[0.33, 0.67, 0.13], [0.42, 0.76, 0.13], [0.46, 0.76, 0.15]]
f [0.8279, 0.8284, 0.8287]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 21 	 [0.4, 0.51, 0.13] 	 lr: 1.25e-05 	 f: 83.2679294255786
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 27 [0m
Train...
loss: tensor(429.9619) lr: 1e-05 time: 1379.83
pred_count_train 41644

Test...
loss: tensor(298.3114) lr: 1e-05 time: 1385.85
pred_count_train 41644

Test...
loss: tensor(422.0990) lr: 1e-05 time: 1281.06
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.01 	r: 78.13 	f1: 79.06 	 4819 	 6023 	 6168
wo 	p: 93.03 	r: 87.04 	f1: 89.94 	 2949 	 3170 	 3388
ni 	p: 87.57 	r: 77.79 	f1: 82.39 	 1184 	 1352 	 1522

[32m iter_1[0m
ga 	p: 79.84 	r: 78.45 	f1: 79.14 	 4839 	 6061 	 6168
wo 	p: 92.67 	r: 87.34 	f1: 89.93 	 2959 	 3193 	 3388
ni 	p: 88.13 	r: 77.6 	f1: 82.53 	 1181 	 1340 	 1522

[32m iter_2[0m
ga 	p: 79.8 	r: 78.4 	f1: 79.1 	 4836 	 6060 	 6168
wo 	p: 93.23 	r: 86.92 	f1: 89.96 	 2945 	 3159 	 3388
ni 	p: 88.14 	r: 77.66 	f1: 82.57 	 1182 	 1341 	 1522
best_thres [[0.29, 0.48, 0.21], [0.27, 0.44, 0.22], [0.27, 0.49, 0.22]]
f [0.828, 0.8283, 0.8284]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 25 	 [0.48, 0.4, 0.21] 	 lr: 1e-05 	 f: 83.18903206752367
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 28 [0m
Train...

[32m iter_0[0m
ga 	p: 81.69 	r: 75.58 	f1: 78.52 	 4662 	 5707 	 6168
wo 	p: 92.98 	r: 86.45 	f1: 89.6 	 2929 	 3150 	 3388
ni 	p: 85.49 	r: 76.28 	f1: 80.62 	 1161 	 1358 	 1522

[32m iter_1[0m
ga 	p: 81.33 	r: 76.15 	f1: 78.66 	 4697 	 5775 	 6168
wo 	p: 93.27 	r: 86.25 	f1: 89.62 	 2922 	 3133 	 3388
ni 	p: 85.54 	r: 76.15 	f1: 80.57 	 1159 	 1355 	 1522

[32m iter_2[0m
ga 	p: 80.62 	r: 76.75 	f1: 78.64 	 4734 	 5872 	 6168
wo 	p: 92.9 	r: 86.48 	f1: 89.58 	 2930 	 3154 	 3388
ni 	p: 85.09 	r: 76.48 	f1: 80.55 	 1164 	 1368 	 1522
best_thres [[0.39, 0.6, 0.15], [0.36, 0.77, 0.13], [0.31, 0.75, 0.12]]
f [0.8221, 0.8223, 0.8223]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 17 	 [0.36, 0.6, 0.25] 	 lr: 2.5e-05 	 f: 83.02214502822406
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 29 [0m
Train...

[32m iter_0[0m
ga 	p: 82.77 	r: 74.85 	f1: 78.61 	 4617 	 5578 	 6168
wo 	p: 94.01 	r: 86.22 	f1: 89.95 	 2921 	 3107 	 3388
ni 	p: 86.36 	r: 79.04 	f1: 82.54 	 1203 	 1393 	 1522

[32m iter_1[0m
ga 	p: 79.61 	r: 78.36 	f1: 78.98 	 4833 	 6071 	 6168
wo 	p: 94.21 	r: 85.95 	f1: 89.89 	 2912 	 3091 	 3388
ni 	p: 86.56 	r: 79.57 	f1: 82.92 	 1211 	 1399 	 1522

[32m iter_2[0m
ga 	p: 79.78 	r: 78.24 	f1: 79.0 	 4826 	 6049 	 6168
wo 	p: 93.78 	r: 86.39 	f1: 89.94 	 2927 	 3121 	 3388
ni 	p: 86.67 	r: 79.43 	f1: 82.89 	 1209 	 1395 	 1522
best_thres [[0.48, 0.6, 0.16], [0.27, 0.71, 0.14], [0.29, 0.67, 0.14]]
f [0.8263, 0.8271, 0.8274]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 21 	 [0.4, 0.51, 0.13] 	 lr: 1.25e-05 	 f: 83.2679294255786
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 28 [0m
Train...
loss: tensor(368.9289) lr: 1e-05 time: 1391.05
pred_count_train 41644

Test...
loss: tensor(246.3916) lr: 1e-05 time: 1401.9
pred_count_train 41644

Test...
loss: tensor(360.7538) lr: 1e-05 time: 1301.53
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.54 	r: 75.47 	f1: 78.84 	 4655 	 5640 	 6168
wo 	p: 93.38 	r: 86.66 	f1: 89.9 	 2936 	 3144 	 3388
ni 	p: 84.78 	r: 78.71 	f1: 81.64 	 1198 	 1413 	 1522

[32m iter_1[0m
ga 	p: 81.49 	r: 76.18 	f1: 78.75 	 4699 	 5766 	 6168
wo 	p: 92.42 	r: 87.4 	f1: 89.84 	 2961 	 3204 	 3388
ni 	p: 86.57 	r: 77.92 	f1: 82.02 	 1186 	 1370 	 1522

[32m iter_2[0m
ga 	p: 82.76 	r: 75.16 	f1: 78.78 	 4636 	 5602 	 6168
wo 	p: 92.47 	r: 87.4 	f1: 89.86 	 2961 	 3202 	 3388
ni 	p: 86.4 	r: 78.06 	f1: 82.02 	 1188 	 1375 	 1522
best_thres [[0.45, 0.56, 0.15], [0.37, 0.43, 0.17], [0.46, 0.43, 0.17]]
f [0.8262, 0.8261, 0.8263]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 25 	 [0.48, 0.4, 0.21] 	 lr: 1e-05 	 f: 83.18903206752367
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 29 [0m
Train...

[32m iter_0[0m
ga 	p: 80.86 	r: 76.15 	f1: 78.43 	 4697 	 5809 	 6168
wo 	p: 93.11 	r: 86.22 	f1: 89.53 	 2921 	 3137 	 3388
ni 	p: 84.75 	r: 75.23 	f1: 79.71 	 1145 	 1351 	 1522

[32m iter_1[0m
ga 	p: 80.43 	r: 77.03 	f1: 78.69 	 4751 	 5907 	 6168
wo 	p: 92.53 	r: 86.6 	f1: 89.46 	 2934 	 3171 	 3388
ni 	p: 86.39 	r: 74.24 	f1: 79.86 	 1130 	 1308 	 1522

[32m iter_2[0m
ga 	p: 80.45 	r: 76.98 	f1: 78.67 	 4748 	 5902 	 6168
wo 	p: 93.74 	r: 85.68 	f1: 89.53 	 2903 	 3097 	 3388
ni 	p: 85.87 	r: 74.64 	f1: 79.86 	 1136 	 1323 	 1522
best_thres [[0.33, 0.58, 0.14], [0.28, 0.52, 0.15], [0.28, 0.85, 0.13]]
f [0.8199, 0.8207, 0.8208]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	best in epoch 17 	 [0.36, 0.6, 0.25] 	 lr: 2.5e-05 	 f: 83.02214502822406
2016 {'GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse': {'best_epoch': 27, 'best_thres': [0.69, 0.85, 0.09], 'best_lr': 1e-05, 'best_performance': 82.66645654400756}}
2018 {'GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse': {'best_epoch': 22, 'best_thres': [0.42, 0.44, 0.14], 'best_lr': 1.25e-05, 'best_performance': 82.95662797800856}}
2020 {'GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse': {'best_epoch': 17, 'best_thres': [0.36, 0.6, 0.25], 'best_lr': 2.5e-05, 'best_performance': 83.02214502822406}}
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...

[32m iter_0[0m
ga 	p: 81.97 	r: 75.39 	f1: 78.54 	 4650 	 5673 	 6168
wo 	p: 94.05 	r: 85.86 	f1: 89.77 	 2909 	 3093 	 3388
ni 	p: 85.59 	r: 77.66 	f1: 81.43 	 1182 	 1381 	 1522

[32m iter_1[0m
ga 	p: 81.41 	r: 76.18 	f1: 78.71 	 4699 	 5772 	 6168
wo 	p: 93.54 	r: 86.33 	f1: 89.79 	 2925 	 3127 	 3388
ni 	p: 84.58 	r: 79.3 	f1: 81.86 	 1207 	 1427 	 1522

[32m iter_2[0m
ga 	p: 81.25 	r: 76.44 	f1: 78.77 	 4715 	 5803 	 6168
wo 	p: 93.54 	r: 86.3 	f1: 89.78 	 2924 	 3126 	 3388
ni 	p: 84.89 	r: 79.37 	f1: 82.04 	 1208 	 1423 	 1522
best_thres [[0.4, 0.68, 0.17], [0.41, 0.7, 0.11], [0.4, 0.73, 0.11]]
f [0.8237, 0.8244, 0.8248]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 21 	 [0.4, 0.51, 0.13] 	 lr: 1.25e-05 	 f: 83.2679294255786
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 29 [0m
Train...
loss: tensor(311.5066) lr: 1e-05 time: 1241.35
pred_count_train 41644

Test...
loss: tensor(3608.4158) lr: 0.0001 time: 1230.16
pred_count_train 41644

Test...
loss: tensor(305.8120) lr: 1e-05 time: 1172.81
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.24 	r: 76.25 	f1: 78.67 	 4703 	 5789 	 6168
wo 	p: 92.91 	r: 86.66 	f1: 89.68 	 2936 	 3160 	 3388
ni 	p: 89.18 	r: 74.77 	f1: 81.34 	 1138 	 1276 	 1522

[32m iter_1[0m
ga 	p: 79.71 	r: 77.64 	f1: 78.66 	 4789 	 6008 	 6168
wo 	p: 93.34 	r: 86.39 	f1: 89.73 	 2927 	 3136 	 3388
ni 	p: 88.67 	r: 75.1 	f1: 81.32 	 1143 	 1289 	 1522

[32m iter_2[0m
ga 	p: 79.56 	r: 77.76 	f1: 78.65 	 4796 	 6028 	 6168
wo 	p: 93.33 	r: 86.33 	f1: 89.7 	 2925 	 3134 	 3388
ni 	p: 88.67 	r: 75.1 	f1: 81.32 	 1143 	 1289 	 1522
best_thres [[0.39, 0.5, 0.28], [0.27, 0.54, 0.24], [0.26, 0.54, 0.24]]
f [0.824, 0.8238, 0.8237]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	best in epoch 25 	 [0.48, 0.4, 0.21] 	 lr: 1e-05 	 f: 83.18903206752367
2016 {'GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse': {'best_epoch': 23, 'best_thres': [0.28, 0.74, 0.08], 'best_lr': 2.5e-05, 'best_performance': 82.9197354224328}}
2018 {'GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse': {'best_epoch': 16, 'best_thres': [0.41, 0.48, 0.17], 'best_lr': 2.5e-05, 'best_performance': 83.4139944421894}}
2020 {'GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse': {'best_epoch': 25, 'best_thres': [0.48, 0.4, 0.21], 'best_lr': 1e-05, 'best_performance': 83.18903206752367}}
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...

[32m iter_0[0m
ga 	p: 74.97 	r: 70.67 	f1: 72.76 	 4359 	 5814 	 6168
wo 	p: 90.8 	r: 82.41 	f1: 86.4 	 2792 	 3075 	 3388
ni 	p: 80.72 	r: 77.86 	f1: 79.26 	 1185 	 1468 	 1522

[32m iter_1[0m
ga 	p: 75.46 	r: 70.3 	f1: 72.79 	 4336 	 5746 	 6168
wo 	p: 91.86 	r: 81.55 	f1: 86.4 	 2763 	 3008 	 3388
ni 	p: 80.69 	r: 77.99 	f1: 79.32 	 1187 	 1471 	 1522

[32m iter_2[0m
ga 	p: 75.5 	r: 70.3 	f1: 72.81 	 4336 	 5743 	 6168
wo 	p: 91.86 	r: 81.55 	f1: 86.4 	 2763 	 3008 	 3388
ni 	p: 80.64 	r: 77.99 	f1: 79.29 	 1187 	 1472 	 1522
best_thres [[0.43, 0.2, 0.17], [0.45, 0.24, 0.17], [0.45, 0.24, 0.17]]
f [0.7778, 0.7779, 0.7779]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 1 	 [0.45, 0.24, 0.17] 	 lr: 0.0001 	 f: 77.7900966598479
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 2 [0m
Train...

[32m iter_0[0m
ga 	p: 82.03 	r: 75.06 	f1: 78.39 	 4630 	 5644 	 6168
wo 	p: 92.98 	r: 86.84 	f1: 89.8 	 2942 	 3164 	 3388
ni 	p: 84.11 	r: 77.53 	f1: 80.68 	 1180 	 1403 	 1522

[32m iter_1[0m
ga 	p: 82.04 	r: 75.47 	f1: 78.62 	 4655 	 5674 	 6168
wo 	p: 93.3 	r: 86.69 	f1: 89.87 	 2937 	 3148 	 3388
ni 	p: 82.6 	r: 79.24 	f1: 80.89 	 1206 	 1460 	 1522

[32m iter_2[0m
ga 	p: 81.31 	r: 76.2 	f1: 78.67 	 4700 	 5780 	 6168
wo 	p: 93.04 	r: 86.78 	f1: 89.8 	 2940 	 3160 	 3388
ni 	p: 83.57 	r: 78.52 	f1: 80.96 	 1195 	 1430 	 1522
best_thres [[0.42, 0.51, 0.13], [0.51, 0.66, 0.09], [0.47, 0.65, 0.1]]
f [0.8222, 0.823, 0.8233]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	best in epoch 21 	 [0.4, 0.51, 0.13] 	 lr: 1.25e-05 	 f: 83.2679294255786
2016 {'GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse': {'best_epoch': 24, 'best_thres': [0.5, 0.37, 0.21], 'best_lr': 1.25e-05, 'best_performance': 82.52022323948077}}
2018 {'GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse': {'best_epoch': 18, 'best_thres': [0.57, 0.85, 0.12], 'best_lr': 2.5e-05, 'best_performance': 82.91229066136815}}
2020 {'GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0002_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse': {'best_epoch': 21, 'best_thres': [0.4, 0.51, 0.13], 'best_lr': 1.25e-05, 'best_performance': 83.2679294255786}}
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(3608.4424) lr: 0.0001 time: 1217.98
pred_count_train 41644

Test...
loss: tensor(2217.7524) lr: 0.0001 time: 1225.44
pred_count_train 41644

Test...
loss: tensor(3608.8049) lr: 0.0001 time: 1153.71
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 74.88 	r: 70.64 	f1: 72.7 	 4357 	 5819 	 6168
wo 	p: 90.91 	r: 82.02 	f1: 86.24 	 2779 	 3057 	 3388
ni 	p: 82.94 	r: 76.02 	f1: 79.33 	 1157 	 1395 	 1522

[32m iter_1[0m
ga 	p: 74.92 	r: 70.61 	f1: 72.7 	 4355 	 5813 	 6168
wo 	p: 91.24 	r: 81.82 	f1: 86.27 	 2772 	 3038 	 3388
ni 	p: 83.6 	r: 75.36 	f1: 79.27 	 1147 	 1372 	 1522

[32m iter_2[0m
ga 	p: 74.96 	r: 70.65 	f1: 72.74 	 4358 	 5814 	 6168
wo 	p: 90.72 	r: 82.23 	f1: 86.27 	 2786 	 3071 	 3388
ni 	p: 83.22 	r: 75.62 	f1: 79.24 	 1151 	 1383 	 1522
best_thres [[0.44, 0.21, 0.21], [0.45, 0.22, 0.23], [0.45, 0.2, 0.22]]
f [0.7769, 0.7769, 0.777]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 1 	 [0.45, 0.2, 0.22] 	 lr: 0.0001 	 f: 77.69860616288517
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 2 [0m
Train...

[32m iter_0[0m
ga 	p: 79.86 	r: 73.59 	f1: 76.59 	 4539 	 5684 	 6168
wo 	p: 93.55 	r: 83.5 	f1: 88.24 	 2829 	 3024 	 3388
ni 	p: 83.18 	r: 80.55 	f1: 81.84 	 1226 	 1474 	 1522

[32m iter_1[0m
ga 	p: 80.07 	r: 73.39 	f1: 76.59 	 4527 	 5654 	 6168
wo 	p: 91.8 	r: 84.98 	f1: 88.26 	 2879 	 3136 	 3388
ni 	p: 83.84 	r: 80.42 	f1: 82.09 	 1224 	 1460 	 1522

[32m iter_2[0m
ga 	p: 79.29 	r: 74.11 	f1: 76.61 	 4571 	 5765 	 6168
wo 	p: 91.83 	r: 84.98 	f1: 88.27 	 2879 	 3135 	 3388
ni 	p: 83.9 	r: 80.49 	f1: 82.16 	 1225 	 1460 	 1522
best_thres [[0.46, 0.31, 0.22], [0.47, 0.2, 0.23], [0.44, 0.2, 0.23]]
f [0.8085, 0.8089, 0.809]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 2 	 [0.44, 0.2, 0.23] 	 lr: 0.0001 	 f: 80.9015087620654
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 3 [0m
Train...

[32m iter_0[0m
ga 	p: 74.91 	r: 70.57 	f1: 72.68 	 4353 	 5811 	 6168
wo 	p: 90.94 	r: 82.11 	f1: 86.3 	 2782 	 3059 	 3388
ni 	p: 84.08 	r: 74.97 	f1: 79.26 	 1141 	 1357 	 1522

[32m iter_1[0m
ga 	p: 74.15 	r: 71.19 	f1: 72.64 	 4391 	 5922 	 6168
wo 	p: 90.94 	r: 82.08 	f1: 86.29 	 2781 	 3058 	 3388
ni 	p: 83.05 	r: 75.95 	f1: 79.34 	 1156 	 1392 	 1522

[32m iter_2[0m
ga 	p: 74.07 	r: 71.17 	f1: 72.59 	 4390 	 5927 	 6168
wo 	p: 90.91 	r: 82.05 	f1: 86.26 	 2780 	 3058 	 3388
ni 	p: 81.16 	r: 77.53 	f1: 79.3 	 1180 	 1454 	 1522
best_thres [[0.42, 0.2, 0.24], [0.42, 0.2, 0.21], [0.42, 0.2, 0.18]]
f [0.7769, 0.7767, 0.7765]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 1 	 [0.42, 0.2, 0.18] 	 lr: 0.0001 	 f: 77.65123226288276
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 2 [0m
Train...
loss: tensor(2214.7158) lr: 0.0001 time: 1260.68
pred_count_train 41644

Test...
loss: tensor(1832.6874) lr: 0.0001 time: 1261.48
pred_count_train 41644

Test...
loss: tensor(2218.3335) lr: 0.0001 time: 1195.22
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.84 	r: 74.4 	f1: 77.02 	 4589 	 5748 	 6168
wo 	p: 92.66 	r: 84.95 	f1: 88.64 	 2878 	 3106 	 3388
ni 	p: 86.65 	r: 77.6 	f1: 81.87 	 1181 	 1363 	 1522

[32m iter_1[0m
ga 	p: 80.72 	r: 74.14 	f1: 77.29 	 4573 	 5665 	 6168
wo 	p: 92.69 	r: 84.95 	f1: 88.65 	 2878 	 3105 	 3388
ni 	p: 87.01 	r: 77.46 	f1: 81.96 	 1179 	 1355 	 1522

[32m iter_2[0m
ga 	p: 80.71 	r: 74.12 	f1: 77.28 	 4572 	 5665 	 6168
wo 	p: 92.91 	r: 84.77 	f1: 88.66 	 2872 	 3091 	 3388
ni 	p: 87.08 	r: 77.53 	f1: 82.03 	 1180 	 1355 	 1522
best_thres [[0.41, 0.5, 0.2], [0.43, 0.53, 0.2], [0.43, 0.58, 0.2]]
f [0.8122, 0.8131, 0.8134]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 3 	 [0.43, 0.58, 0.2] 	 lr: 0.0001 	 f: 81.34156107211832
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 4 [0m
Train...

[32m iter_0[0m
ga 	p: 79.59 	r: 73.77 	f1: 76.57 	 4550 	 5717 	 6168
wo 	p: 93.47 	r: 83.68 	f1: 88.3 	 2835 	 3033 	 3388
ni 	p: 81.8 	r: 81.8 	f1: 81.8 	 1245 	 1522 	 1522

[32m iter_1[0m
ga 	p: 81.27 	r: 72.45 	f1: 76.61 	 4469 	 5499 	 6168
wo 	p: 93.09 	r: 83.88 	f1: 88.25 	 2842 	 3053 	 3388
ni 	p: 82.28 	r: 81.47 	f1: 81.88 	 1240 	 1507 	 1522

[32m iter_2[0m
ga 	p: 81.27 	r: 72.45 	f1: 76.61 	 4469 	 5499 	 6168
wo 	p: 91.59 	r: 85.15 	f1: 88.25 	 2885 	 3150 	 3388
ni 	p: 83.38 	r: 80.42 	f1: 81.87 	 1224 	 1468 	 1522
best_thres [[0.45, 0.31, 0.16], [0.52, 0.28, 0.17], [0.52, 0.2, 0.2]]
f [0.8084, 0.8088, 0.809]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 2 	 [0.52, 0.2, 0.2] 	 lr: 0.0001 	 f: 80.89884111679909
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 3 [0m
Train...

[32m iter_0[0m
ga 	p: 79.49 	r: 74.03 	f1: 76.66 	 4566 	 5744 	 6168
wo 	p: 94.02 	r: 83.09 	f1: 88.22 	 2815 	 2994 	 3388
ni 	p: 82.23 	r: 81.47 	f1: 81.85 	 1240 	 1508 	 1522

[32m iter_1[0m
ga 	p: 78.75 	r: 74.81 	f1: 76.73 	 4614 	 5859 	 6168
wo 	p: 92.3 	r: 84.5 	f1: 88.23 	 2863 	 3102 	 3388
ni 	p: 83.04 	r: 81.08 	f1: 82.05 	 1234 	 1486 	 1522

[32m iter_2[0m
ga 	p: 78.72 	r: 74.85 	f1: 76.74 	 4617 	 5865 	 6168
wo 	p: 92.3 	r: 84.5 	f1: 88.23 	 2863 	 3102 	 3388
ni 	p: 82.42 	r: 81.6 	f1: 82.01 	 1242 	 1507 	 1522
best_thres [[0.43, 0.34, 0.19], [0.42, 0.23, 0.2], [0.42, 0.23, 0.19]]
f [0.8086, 0.809, 0.8091]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 2 	 [0.42, 0.23, 0.19] 	 lr: 0.0001 	 f: 80.9117870840515
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 3 [0m
Train...
loss: tensor(1536.5531) lr: 0.0001 time: 1263.83
pred_count_train 41644

Test...
loss: tensor(1827.7863) lr: 0.0001 time: 1275.45
pred_count_train 41644

Test...
loss: tensor(1835.1554) lr: 0.0001 time: 1225.35
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.15 	r: 74.9 	f1: 77.9 	 4620 	 5693 	 6168
wo 	p: 92.4 	r: 85.74 	f1: 88.95 	 2905 	 3144 	 3388
ni 	p: 87.88 	r: 79.11 	f1: 83.26 	 1204 	 1370 	 1522

[32m iter_1[0m
ga 	p: 80.45 	r: 75.57 	f1: 77.93 	 4661 	 5794 	 6168
wo 	p: 91.6 	r: 86.22 	f1: 88.82 	 2921 	 3189 	 3388
ni 	p: 84.18 	r: 82.85 	f1: 83.51 	 1261 	 1498 	 1522

[32m iter_2[0m
ga 	p: 80.51 	r: 75.54 	f1: 77.94 	 4659 	 5787 	 6168
wo 	p: 91.6 	r: 86.28 	f1: 88.86 	 2923 	 3191 	 3388
ni 	p: 84.25 	r: 82.92 	f1: 83.58 	 1262 	 1498 	 1522
best_thres [[0.23, 0.48, 0.36], [0.21, 0.42, 0.21], [0.21, 0.42, 0.21]]
f [0.8202, 0.8203, 0.8204]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 4 	 [0.21, 0.42, 0.21] 	 lr: 0.0001 	 f: 82.03981490108389
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 5 [0m
Train...

[32m iter_0[0m
ga 	p: 78.79 	r: 76.13 	f1: 77.44 	 4696 	 5960 	 6168
wo 	p: 93.54 	r: 84.24 	f1: 88.65 	 2854 	 3051 	 3388
ni 	p: 84.57 	r: 79.57 	f1: 81.99 	 1211 	 1432 	 1522

[32m iter_1[0m
ga 	p: 82.13 	r: 73.23 	f1: 77.43 	 4517 	 5500 	 6168
wo 	p: 93.14 	r: 84.53 	f1: 88.63 	 2864 	 3075 	 3388
ni 	p: 85.83 	r: 78.38 	f1: 81.94 	 1193 	 1390 	 1522

[32m iter_2[0m
ga 	p: 82.1 	r: 73.27 	f1: 77.43 	 4519 	 5504 	 6168
wo 	p: 93.23 	r: 84.5 	f1: 88.65 	 2863 	 3071 	 3388
ni 	p: 85.77 	r: 78.38 	f1: 81.91 	 1193 	 1391 	 1522
best_thres [[0.36, 0.54, 0.14], [0.49, 0.52, 0.16], [0.49, 0.52, 0.16]]
f [0.8142, 0.8145, 0.8147]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 3 	 [0.49, 0.52, 0.16] 	 lr: 0.0001 	 f: 81.46773990692995
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 4 [0m
Train...

[32m iter_0[0m
ga 	p: 81.48 	r: 73.05 	f1: 77.04 	 4506 	 5530 	 6168
wo 	p: 92.89 	r: 84.8 	f1: 88.66 	 2873 	 3093 	 3388
ni 	p: 86.38 	r: 78.32 	f1: 82.15 	 1192 	 1380 	 1522

[32m iter_1[0m
ga 	p: 81.27 	r: 73.51 	f1: 77.19 	 4534 	 5579 	 6168
wo 	p: 93.18 	r: 84.74 	f1: 88.76 	 2871 	 3081 	 3388
ni 	p: 87.43 	r: 77.66 	f1: 82.25 	 1182 	 1352 	 1522

[32m iter_2[0m
ga 	p: 82.4 	r: 72.73 	f1: 77.26 	 4486 	 5444 	 6168
wo 	p: 93.41 	r: 84.53 	f1: 88.75 	 2864 	 3066 	 3388
ni 	p: 87.36 	r: 77.66 	f1: 82.23 	 1182 	 1353 	 1522
best_thres [[0.46, 0.5, 0.2], [0.48, 0.56, 0.21], [0.53, 0.59, 0.21]]
f [0.8131, 0.8137, 0.8141]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 3 	 [0.53, 0.59, 0.21] 	 lr: 0.0001 	 f: 81.41082519964507
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 4 [0m
Train...
loss: tensor(1252.7246) lr: 0.0001 time: 1248.17
pred_count_train 41644

Test...
loss: tensor(1528.0774) lr: 0.0001 time: 1272.97
pred_count_train 41644

Test...
loss: tensor(1538.6978) lr: 0.0001 time: 1237.85
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 78.68 	r: 77.56 	f1: 78.12 	 4784 	 6080 	 6168
wo 	p: 92.88 	r: 85.8 	f1: 89.2 	 2907 	 3130 	 3388
ni 	p: 86.07 	r: 74.31 	f1: 79.76 	 1131 	 1314 	 1522

[32m iter_1[0m
ga 	p: 79.11 	r: 77.4 	f1: 78.24 	 4774 	 6035 	 6168
wo 	p: 93.35 	r: 85.36 	f1: 89.18 	 2892 	 3098 	 3388
ni 	p: 83.93 	r: 76.54 	f1: 80.07 	 1165 	 1388 	 1522

[32m iter_2[0m
ga 	p: 79.11 	r: 77.4 	f1: 78.24 	 4774 	 6035 	 6168
wo 	p: 93.41 	r: 85.33 	f1: 89.19 	 2891 	 3095 	 3388
ni 	p: 84.1 	r: 76.48 	f1: 80.11 	 1164 	 1384 	 1522
best_thres [[0.25, 0.44, 0.19], [0.26, 0.51, 0.13], [0.26, 0.57, 0.13]]
f [0.8168, 0.8172, 0.8174]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 4 	 [0.21, 0.42, 0.21] 	 lr: 0.0001 	 f: 82.03981490108389
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 6 [0m
Train...

[32m iter_0[0m
ga 	p: 80.98 	r: 74.32 	f1: 77.5 	 4584 	 5661 	 6168
wo 	p: 92.44 	r: 85.92 	f1: 89.06 	 2911 	 3149 	 3388
ni 	p: 85.99 	r: 80.22 	f1: 83.0 	 1221 	 1420 	 1522

[32m iter_1[0m
ga 	p: 78.85 	r: 76.36 	f1: 77.59 	 4710 	 5973 	 6168
wo 	p: 92.84 	r: 85.71 	f1: 89.13 	 2904 	 3128 	 3388
ni 	p: 86.31 	r: 79.96 	f1: 83.02 	 1217 	 1410 	 1522

[32m iter_2[0m
ga 	p: 78.85 	r: 76.35 	f1: 77.58 	 4709 	 5972 	 6168
wo 	p: 92.81 	r: 85.71 	f1: 89.12 	 2904 	 3129 	 3388
ni 	p: 85.78 	r: 80.49 	f1: 83.05 	 1225 	 1428 	 1522
best_thres [[0.25, 0.53, 0.3], [0.19, 0.55, 0.32], [0.19, 0.55, 0.29]]
f [0.8181, 0.8181, 0.8181]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 4 	 [0.19, 0.55, 0.29] 	 lr: 0.0001 	 f: 81.80888006945307
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 5 [0m
Train...

[32m iter_0[0m
ga 	p: 77.95 	r: 76.62 	f1: 77.28 	 4726 	 6063 	 6168
wo 	p: 92.1 	r: 85.71 	f1: 88.79 	 2904 	 3153 	 3388
ni 	p: 87.38 	r: 78.71 	f1: 82.82 	 1198 	 1371 	 1522

[32m iter_1[0m
ga 	p: 79.36 	r: 75.31 	f1: 77.28 	 4645 	 5853 	 6168
wo 	p: 93.65 	r: 84.5 	f1: 88.84 	 2863 	 3057 	 3388
ni 	p: 86.98 	r: 79.04 	f1: 82.82 	 1203 	 1383 	 1522

[32m iter_2[0m
ga 	p: 79.37 	r: 75.21 	f1: 77.23 	 4639 	 5845 	 6168
wo 	p: 93.37 	r: 84.8 	f1: 88.88 	 2873 	 3077 	 3388
ni 	p: 86.91 	r: 78.98 	f1: 82.75 	 1202 	 1383 	 1522
best_thres [[0.16, 0.46, 0.37], [0.19, 0.66, 0.34], [0.19, 0.65, 0.34]]
f [0.815, 0.8151, 0.8151]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 4 	 [0.19, 0.65, 0.34] 	 lr: 0.0001 	 f: 81.50700880175104
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 5 [0m
Train...
loss: tensor(991.5582) lr: 0.0001 time: 1232.75
pred_count_train 41644

Test...
loss: tensor(1245.4557) lr: 0.0001 time: 1269.93
pred_count_train 41644

Test...
loss: tensor(1260.1881) lr: 0.0001 time: 1246.06
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 78.78 	r: 76.31 	f1: 77.53 	 4707 	 5975 	 6168
wo 	p: 93.41 	r: 84.95 	f1: 88.98 	 2878 	 3081 	 3388
ni 	p: 85.25 	r: 75.16 	f1: 79.89 	 1144 	 1342 	 1522

[32m iter_1[0m
ga 	p: 79.49 	r: 76.04 	f1: 77.73 	 4690 	 5900 	 6168
wo 	p: 93.31 	r: 85.21 	f1: 89.08 	 2887 	 3094 	 3388
ni 	p: 83.99 	r: 76.54 	f1: 80.1 	 1165 	 1387 	 1522

[32m iter_2[0m
ga 	p: 79.57 	r: 76.04 	f1: 77.76 	 4690 	 5894 	 6168
wo 	p: 93.28 	r: 85.21 	f1: 89.06 	 2887 	 3095 	 3388
ni 	p: 84.28 	r: 76.41 	f1: 80.15 	 1163 	 1380 	 1522
best_thres [[0.34, 0.49, 0.11], [0.37, 0.46, 0.08], [0.37, 0.46, 0.08]]
f [0.8129, 0.8138, 0.8142]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 4 	 [0.21, 0.42, 0.21] 	 lr: 0.0001 	 f: 82.03981490108389
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 7 [0m
Train...

[32m iter_0[0m
ga 	p: 80.97 	r: 75.55 	f1: 78.17 	 4660 	 5755 	 6168
wo 	p: 93.46 	r: 85.24 	f1: 89.16 	 2888 	 3090 	 3388
ni 	p: 83.88 	r: 74.51 	f1: 78.91 	 1134 	 1352 	 1522

[32m iter_1[0m
ga 	p: 81.21 	r: 75.24 	f1: 78.11 	 4641 	 5715 	 6168
wo 	p: 93.18 	r: 85.48 	f1: 89.16 	 2896 	 3108 	 3388
ni 	p: 82.89 	r: 75.43 	f1: 78.98 	 1148 	 1385 	 1522

[32m iter_2[0m
ga 	p: 79.08 	r: 77.22 	f1: 78.14 	 4763 	 6023 	 6168
wo 	p: 93.21 	r: 85.48 	f1: 89.18 	 2896 	 3107 	 3388
ni 	p: 82.52 	r: 75.69 	f1: 78.96 	 1152 	 1396 	 1522
best_thres [[0.35, 0.51, 0.2], [0.36, 0.46, 0.17], [0.26, 0.46, 0.16]]
f [0.8162, 0.8161, 0.816]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 4 	 [0.19, 0.55, 0.29] 	 lr: 0.0001 	 f: 81.80888006945307
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 6 [0m
Train...

[32m iter_0[0m
ga 	p: 80.72 	r: 75.47 	f1: 78.01 	 4655 	 5767 	 6168
wo 	p: 92.94 	r: 85.86 	f1: 89.26 	 2909 	 3130 	 3388
ni 	p: 82.2 	r: 77.07 	f1: 79.55 	 1173 	 1427 	 1522

[32m iter_1[0m
ga 	p: 78.67 	r: 77.56 	f1: 78.11 	 4784 	 6081 	 6168
wo 	p: 92.9 	r: 86.16 	f1: 89.4 	 2919 	 3142 	 3388
ni 	p: 82.45 	r: 77.79 	f1: 80.05 	 1184 	 1436 	 1522

[32m iter_2[0m
ga 	p: 78.63 	r: 77.71 	f1: 78.16 	 4793 	 6096 	 6168
wo 	p: 92.68 	r: 86.28 	f1: 89.36 	 2923 	 3154 	 3388
ni 	p: 83.1 	r: 77.2 	f1: 80.04 	 1175 	 1414 	 1522
best_thres [[0.29, 0.38, 0.16], [0.23, 0.42, 0.15], [0.23, 0.42, 0.16]]
f [0.8165, 0.8171, 0.8173]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 5 	 [0.23, 0.42, 0.16] 	 lr: 0.0001 	 f: 81.73425193816371
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 6 [0m
Train...
loss: tensor(766.6984) lr: 0.0001 time: 1209.33
pred_count_train 41644

Test...
loss: tensor(980.5426) lr: 0.0001 time: 1263.37
pred_count_train 41644

Test...
loss: tensor(1007.3924) lr: 0.0001 time: 1251.74
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.26 	r: 75.96 	f1: 77.57 	 4685 	 5911 	 6168
wo 	p: 93.42 	r: 84.71 	f1: 88.85 	 2870 	 3072 	 3388
ni 	p: 84.1 	r: 71.22 	f1: 77.13 	 1084 	 1289 	 1522

[32m iter_1[0m
ga 	p: 79.47 	r: 75.88 	f1: 77.63 	 4680 	 5889 	 6168
wo 	p: 92.46 	r: 85.36 	f1: 88.77 	 2892 	 3128 	 3388
ni 	p: 86.22 	r: 70.3 	f1: 77.45 	 1070 	 1241 	 1522

[32m iter_2[0m
ga 	p: 80.5 	r: 75.02 	f1: 77.66 	 4627 	 5748 	 6168
wo 	p: 92.55 	r: 85.39 	f1: 88.82 	 2893 	 3126 	 3388
ni 	p: 86.42 	r: 70.24 	f1: 77.49 	 1069 	 1237 	 1522
best_thres [[0.46, 0.44, 0.1], [0.47, 0.32, 0.11], [0.68, 0.32, 0.11]]
f [0.8093, 0.8097, 0.81]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 4 	 [0.21, 0.42, 0.21] 	 lr: 0.0001 	 f: 82.03981490108389
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 8 [0m
Train...

[32m iter_0[0m
ga 	p: 80.83 	r: 75.0 	f1: 77.81 	 4626 	 5723 	 6168
wo 	p: 92.7 	r: 85.8 	f1: 89.12 	 2907 	 3136 	 3388
ni 	p: 84.79 	r: 77.27 	f1: 80.85 	 1176 	 1387 	 1522

[32m iter_1[0m
ga 	p: 81.82 	r: 74.5 	f1: 77.99 	 4595 	 5616 	 6168
wo 	p: 92.94 	r: 85.54 	f1: 89.09 	 2898 	 3118 	 3388
ni 	p: 84.56 	r: 78.06 	f1: 81.18 	 1188 	 1405 	 1522

[32m iter_2[0m
ga 	p: 82.08 	r: 74.34 	f1: 78.02 	 4585 	 5586 	 6168
wo 	p: 92.55 	r: 85.8 	f1: 89.05 	 2907 	 3141 	 3388
ni 	p: 84.8 	r: 78.06 	f1: 81.29 	 1188 	 1401 	 1522
best_thres [[0.42, 0.4, 0.12], [0.56, 0.53, 0.1], [0.61, 0.52, 0.1]]
f [0.8168, 0.8176, 0.8179]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 6 	 [0.61, 0.52, 0.1] 	 lr: 0.0001 	 f: 81.79208433338039
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 7 [0m
Train...

[32m iter_0[0m
ga 	p: 81.66 	r: 74.48 	f1: 77.9 	 4594 	 5626 	 6168
wo 	p: 92.7 	r: 85.51 	f1: 88.96 	 2897 	 3125 	 3388
ni 	p: 83.33 	r: 78.52 	f1: 80.85 	 1195 	 1434 	 1522

[32m iter_1[0m
ga 	p: 78.85 	r: 77.04 	f1: 77.93 	 4752 	 6027 	 6168
wo 	p: 93.1 	r: 85.21 	f1: 88.98 	 2887 	 3101 	 3388
ni 	p: 83.06 	r: 78.91 	f1: 80.93 	 1201 	 1446 	 1522

[32m iter_2[0m
ga 	p: 79.05 	r: 76.88 	f1: 77.95 	 4742 	 5999 	 6168
wo 	p: 93.04 	r: 85.27 	f1: 88.99 	 2889 	 3105 	 3388
ni 	p: 83.0 	r: 78.91 	f1: 80.9 	 1201 	 1447 	 1522
best_thres [[0.5, 0.42, 0.11], [0.35, 0.45, 0.1], [0.36, 0.44, 0.1]]
f [0.817, 0.8168, 0.8167]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 4 	 [0.19, 0.55, 0.29] 	 lr: 0.0001 	 f: 81.80888006945307
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 7 [0m
Train...
loss: tensor(597.6699) lr: 0.0001 time: 1184.0
pred_count_train 41644

Test...
loss: tensor(781.0677) lr: 0.0001 time: 1236.95
pred_count_train 41644

Test...
loss: tensor(740.1652) lr: 0.0001 time: 1238.31
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.37 	r: 72.71 	f1: 76.8 	 4485 	 5512 	 6168
wo 	p: 93.86 	r: 84.42 	f1: 88.89 	 2860 	 3047 	 3388
ni 	p: 85.41 	r: 70.76 	f1: 77.4 	 1077 	 1261 	 1522

[32m iter_1[0m
ga 	p: 80.04 	r: 73.83 	f1: 76.81 	 4554 	 5690 	 6168
wo 	p: 92.09 	r: 85.95 	f1: 88.92 	 2912 	 3162 	 3388
ni 	p: 85.99 	r: 70.96 	f1: 77.75 	 1080 	 1256 	 1522

[32m iter_2[0m
ga 	p: 79.7 	r: 73.96 	f1: 76.72 	 4562 	 5724 	 6168
wo 	p: 93.3 	r: 85.09 	f1: 89.01 	 2883 	 3090 	 3388
ni 	p: 86.11 	r: 70.89 	f1: 77.77 	 1079 	 1253 	 1522
best_thres [[0.63, 0.61, 0.15], [0.69, 0.36, 0.15], [0.69, 0.72, 0.15]]
f [0.806, 0.8064, 0.8063]
load model: epoch4
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 4 	 [0.21, 0.42, 0.21] 	 lr: 0.0001 	 f: 82.03981490108389
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 9 [0m
Train...

[32m iter_0[0m
ga 	p: 80.81 	r: 74.55 	f1: 77.55 	 4598 	 5690 	 6168
wo 	p: 93.73 	r: 84.68 	f1: 88.98 	 2869 	 3061 	 3388
ni 	p: 82.7 	r: 72.54 	f1: 77.28 	 1104 	 1335 	 1522

[32m iter_1[0m
ga 	p: 80.21 	r: 75.55 	f1: 77.81 	 4660 	 5810 	 6168
wo 	p: 93.2 	r: 85.42 	f1: 89.14 	 2894 	 3105 	 3388
ni 	p: 80.51 	r: 74.64 	f1: 77.46 	 1136 	 1411 	 1522

[32m iter_2[0m
ga 	p: 79.88 	r: 76.02 	f1: 77.9 	 4689 	 5870 	 6168
wo 	p: 93.61 	r: 85.18 	f1: 89.2 	 2886 	 3083 	 3388
ni 	p: 82.88 	r: 72.86 	f1: 77.55 	 1109 	 1338 	 1522
best_thres [[0.52, 0.47, 0.1], [0.6, 0.51, 0.06], [0.62, 0.62, 0.08]]
f [0.81, 0.811, 0.8116]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 6 	 [0.61, 0.52, 0.1] 	 lr: 0.0001 	 f: 81.79208433338039
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 8 [0m
Train...

[32m iter_0[0m
ga 	p: 80.57 	r: 76.02 	f1: 78.23 	 4689 	 5820 	 6168
wo 	p: 92.93 	r: 84.62 	f1: 88.58 	 2867 	 3085 	 3388
ni 	p: 83.87 	r: 71.42 	f1: 77.15 	 1087 	 1296 	 1522

[32m iter_1[0m
ga 	p: 79.4 	r: 77.17 	f1: 78.27 	 4760 	 5995 	 6168
wo 	p: 92.83 	r: 84.86 	f1: 88.67 	 2875 	 3097 	 3388
ni 	p: 82.79 	r: 73.0 	f1: 77.58 	 1111 	 1342 	 1522

[32m iter_2[0m
ga 	p: 79.56 	r: 77.06 	f1: 78.29 	 4753 	 5974 	 6168
wo 	p: 92.83 	r: 84.8 	f1: 88.63 	 2873 	 3095 	 3388
ni 	p: 82.77 	r: 72.93 	f1: 77.54 	 1110 	 1341 	 1522
best_thres [[0.61, 0.42, 0.11], [0.52, 0.4, 0.09], [0.53, 0.4, 0.09]]
f [0.8124, 0.8127, 0.8129]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 4 	 [0.19, 0.55, 0.29] 	 lr: 0.0001 	 f: 81.80888006945307
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 8 [0m
Train...
loss: tensor(1090.4066) lr: 5e-05 time: 1152.82
pred_count_train 41644

Test...
loss: tensor(610.4503) lr: 0.0001 time: 1215.87
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.64 	r: 77.01 	f1: 78.31 	 4750 	 5964 	 6168
wo 	p: 93.21 	r: 85.95 	f1: 89.43 	 2912 	 3124 	 3388
ni 	p: 88.04 	r: 73.52 	f1: 80.13 	 1119 	 1271 	 1522

[32m iter_1[0m
ga 	p: 79.76 	r: 76.75 	f1: 78.23 	 4734 	 5935 	 6168
wo 	p: 93.29 	r: 85.74 	f1: 89.36 	 2905 	 3114 	 3388
ni 	p: 88.38 	r: 73.46 	f1: 80.23 	 1118 	 1265 	 1522

[32m iter_2[0m
ga 	p: 79.23 	r: 77.24 	f1: 78.22 	 4764 	 6013 	 6168
wo 	p: 93.64 	r: 85.6 	f1: 89.44 	 2900 	 3097 	 3388
ni 	p: 88.37 	r: 73.39 	f1: 80.19 	 1117 	 1264 	 1522
best_thres [[0.23, 0.46, 0.2], [0.23, 0.47, 0.19], [0.21, 0.58, 0.19]]
f [0.8192, 0.819, 0.8189]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 4 	 [0.21, 0.42, 0.21] 	 lr: 0.0001 	 f: 82.03981490108389
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 10 [0m
Train...
loss: tensor(563.9560) lr: 0.0001 time: 1210.08
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.33 	r: 74.09 	f1: 76.62 	 4570 	 5761 	 6168
wo 	p: 91.45 	r: 86.22 	f1: 88.76 	 2921 	 3194 	 3388
ni 	p: 84.05 	r: 70.63 	f1: 76.76 	 1075 	 1279 	 1522

[32m iter_1[0m
ga 	p: 78.88 	r: 74.53 	f1: 76.64 	 4597 	 5828 	 6168
wo 	p: 91.06 	r: 86.87 	f1: 88.91 	 2943 	 3232 	 3388
ni 	p: 84.3 	r: 70.89 	f1: 77.02 	 1079 	 1280 	 1522

[32m iter_2[0m
ga 	p: 77.84 	r: 75.55 	f1: 76.68 	 4660 	 5987 	 6168
wo 	p: 91.01 	r: 86.98 	f1: 88.95 	 2947 	 3238 	 3388
ni 	p: 84.58 	r: 70.63 	f1: 76.98 	 1075 	 1271 	 1522
best_thres [[0.47, 0.4, 0.14], [0.6, 0.44, 0.13], [0.57, 0.49, 0.13]]
f [0.8039, 0.8044, 0.8045]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 6 	 [0.61, 0.52, 0.1] 	 lr: 0.0001 	 f: 81.79208433338039
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 9 [0m
Train...

[32m iter_0[0m
ga 	p: 82.12 	r: 72.13 	f1: 76.8 	 4449 	 5418 	 6168
wo 	p: 92.32 	r: 85.83 	f1: 88.96 	 2908 	 3150 	 3388
ni 	p: 83.18 	r: 72.47 	f1: 77.46 	 1103 	 1326 	 1522

[32m iter_1[0m
ga 	p: 82.13 	r: 72.34 	f1: 76.92 	 4462 	 5433 	 6168
wo 	p: 92.13 	r: 86.01 	f1: 88.96 	 2914 	 3163 	 3388
ni 	p: 84.28 	r: 71.48 	f1: 77.36 	 1088 	 1291 	 1522

[32m iter_2[0m
ga 	p: 81.98 	r: 72.42 	f1: 76.9 	 4467 	 5449 	 6168
wo 	p: 92.21 	r: 85.95 	f1: 88.97 	 2912 	 3158 	 3388
ni 	p: 84.28 	r: 71.48 	f1: 77.36 	 1088 	 1291 	 1522
best_thres [[0.67, 0.56, 0.14], [0.66, 0.53, 0.16], [0.65, 0.53, 0.16]]
f [0.8068, 0.8071, 0.8072]
load model: epoch4
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 4 	 [0.19, 0.55, 0.29] 	 lr: 0.0001 	 f: 81.80888006945307
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 9 [0m
Train...
loss: tensor(864.7068) lr: 5e-05 time: 1120.16
pred_count_train 41644

Test...
loss: tensor(483.9385) lr: 0.0001 time: 1187.0
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.61 	r: 75.1 	f1: 78.22 	 4632 	 5676 	 6168
wo 	p: 93.77 	r: 84.83 	f1: 89.07 	 2874 	 3065 	 3388
ni 	p: 86.58 	r: 73.32 	f1: 79.4 	 1116 	 1289 	 1522

[32m iter_1[0m
ga 	p: 81.62 	r: 75.11 	f1: 78.23 	 4633 	 5676 	 6168
wo 	p: 93.83 	r: 84.83 	f1: 89.1 	 2874 	 3063 	 3388
ni 	p: 85.9 	r: 74.44 	f1: 79.76 	 1133 	 1319 	 1522

[32m iter_2[0m
ga 	p: 81.49 	r: 75.24 	f1: 78.24 	 4641 	 5695 	 6168
wo 	p: 93.8 	r: 84.86 	f1: 89.11 	 2875 	 3065 	 3388
ni 	p: 86.09 	r: 74.44 	f1: 79.84 	 1133 	 1316 	 1522
best_thres [[0.4, 0.64, 0.27], [0.41, 0.82, 0.22], [0.4, 0.85, 0.22]]
f [0.8169, 0.8173, 0.8174]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 4 	 [0.21, 0.42, 0.21] 	 lr: 0.0001 	 f: 82.03981490108389
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 11 [0m
Train...
loss: tensor(1083.5366) lr: 5e-05 time: 1192.58
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.65 	r: 73.78 	f1: 77.06 	 4551 	 5643 	 6168
wo 	p: 91.96 	r: 85.77 	f1: 88.76 	 2906 	 3160 	 3388
ni 	p: 84.25 	r: 72.08 	f1: 77.69 	 1097 	 1302 	 1522

[32m iter_1[0m
ga 	p: 82.09 	r: 73.27 	f1: 77.43 	 4519 	 5505 	 6168
wo 	p: 92.67 	r: 85.51 	f1: 88.95 	 2897 	 3126 	 3388
ni 	p: 87.35 	r: 69.84 	f1: 77.62 	 1063 	 1217 	 1522

[32m iter_2[0m
ga 	p: 82.28 	r: 73.39 	f1: 77.58 	 4527 	 5502 	 6168
wo 	p: 92.48 	r: 85.66 	f1: 88.94 	 2902 	 3138 	 3388
ni 	p: 85.94 	r: 71.09 	f1: 77.81 	 1082 	 1259 	 1522
best_thres [[0.35, 0.53, 0.14], [0.6, 0.79, 0.22], [0.68, 0.85, 0.15]]
f [0.8076, 0.809, 0.8098]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 6 	 [0.61, 0.52, 0.1] 	 lr: 0.0001 	 f: 81.79208433338039
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 10 [0m
Train...

[32m iter_0[0m
ga 	p: 81.38 	r: 75.66 	f1: 78.42 	 4667 	 5735 	 6168
wo 	p: 93.36 	r: 85.95 	f1: 89.5 	 2912 	 3119 	 3388
ni 	p: 86.33 	r: 75.95 	f1: 80.81 	 1156 	 1339 	 1522

[32m iter_1[0m
ga 	p: 79.44 	r: 77.3 	f1: 78.36 	 4768 	 6002 	 6168
wo 	p: 93.46 	r: 86.1 	f1: 89.63 	 2917 	 3121 	 3388
ni 	p: 86.92 	r: 75.56 	f1: 80.84 	 1150 	 1323 	 1522

[32m iter_2[0m
ga 	p: 79.44 	r: 77.35 	f1: 78.38 	 4771 	 6006 	 6168
wo 	p: 93.55 	r: 85.98 	f1: 89.6 	 2913 	 3114 	 3388
ni 	p: 86.92 	r: 75.56 	f1: 80.84 	 1150 	 1323 	 1522
best_thres [[0.33, 0.52, 0.16], [0.24, 0.51, 0.17], [0.24, 0.52, 0.17]]
f [0.8213, 0.8211, 0.8211]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 9 	 [0.24, 0.52, 0.17] 	 lr: 5e-05 	 f: 82.10709621245103
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 10 [0m
Train...
loss: tensor(678.0065) lr: 5e-05 time: 1104.03
pred_count_train 41644

Test...
loss: tensor(388.9935) lr: 0.0001 time: 1160.0
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.06 	r: 73.87 	f1: 77.75 	 4556 	 5552 	 6168
wo 	p: 93.66 	r: 85.09 	f1: 89.17 	 2883 	 3078 	 3388
ni 	p: 84.28 	r: 72.93 	f1: 78.2 	 1110 	 1317 	 1522

[32m iter_1[0m
ga 	p: 82.08 	r: 73.98 	f1: 77.82 	 4563 	 5559 	 6168
wo 	p: 91.95 	r: 85.98 	f1: 88.87 	 2913 	 3168 	 3388
ni 	p: 83.27 	r: 74.24 	f1: 78.5 	 1130 	 1357 	 1522

[32m iter_2[0m
ga 	p: 82.1 	r: 73.98 	f1: 77.83 	 4563 	 5558 	 6168
wo 	p: 91.15 	r: 86.6 	f1: 88.81 	 2934 	 3219 	 3388
ni 	p: 85.96 	r: 72.4 	f1: 78.6 	 1102 	 1282 	 1522
best_thres [[0.46, 0.77, 0.19], [0.47, 0.85, 0.13], [0.47, 0.79, 0.19]]
f [0.8132, 0.8133, 0.8134]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 4 	 [0.21, 0.42, 0.21] 	 lr: 0.0001 	 f: 82.03981490108389
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 12 [0m
Train...
loss: tensor(855.1129) lr: 5e-05 time: 1169.45
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.57 	r: 74.95 	f1: 77.66 	 4623 	 5738 	 6168
wo 	p: 92.51 	r: 85.36 	f1: 88.79 	 2892 	 3126 	 3388
ni 	p: 81.34 	r: 73.32 	f1: 77.13 	 1116 	 1372 	 1522

[32m iter_1[0m
ga 	p: 80.13 	r: 75.41 	f1: 77.7 	 4651 	 5804 	 6168
wo 	p: 93.59 	r: 84.53 	f1: 88.83 	 2864 	 3060 	 3388
ni 	p: 82.15 	r: 73.46 	f1: 77.56 	 1118 	 1361 	 1522

[32m iter_2[0m
ga 	p: 80.22 	r: 75.21 	f1: 77.63 	 4639 	 5783 	 6168
wo 	p: 93.74 	r: 84.45 	f1: 88.85 	 2861 	 3052 	 3388
ni 	p: 82.67 	r: 73.32 	f1: 77.72 	 1116 	 1350 	 1522
best_thres [[0.43, 0.23, 0.06], [0.55, 0.48, 0.05], [0.65, 0.61, 0.05]]
f [0.8099, 0.8102, 0.8103]
load model: epoch6
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 6 	 [0.61, 0.52, 0.1] 	 lr: 0.0001 	 f: 81.79208433338039
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 11 [0m
Train...

[32m iter_0[0m
ga 	p: 80.53 	r: 76.57 	f1: 78.5 	 4723 	 5865 	 6168
wo 	p: 92.16 	r: 86.42 	f1: 89.2 	 2928 	 3177 	 3388
ni 	p: 86.25 	r: 75.03 	f1: 80.25 	 1142 	 1324 	 1522

[32m iter_1[0m
ga 	p: 80.87 	r: 76.3 	f1: 78.52 	 4706 	 5819 	 6168
wo 	p: 92.23 	r: 86.54 	f1: 89.29 	 2932 	 3179 	 3388
ni 	p: 85.94 	r: 75.49 	f1: 80.38 	 1149 	 1337 	 1522

[32m iter_2[0m
ga 	p: 80.83 	r: 76.3 	f1: 78.5 	 4706 	 5822 	 6168
wo 	p: 92.23 	r: 86.51 	f1: 89.28 	 2931 	 3178 	 3388
ni 	p: 83.63 	r: 77.2 	f1: 80.29 	 1175 	 1405 	 1522
best_thres [[0.37, 0.51, 0.28], [0.39, 0.49, 0.27], [0.39, 0.49, 0.2]]
f [0.8201, 0.8204, 0.8204]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 9 	 [0.24, 0.52, 0.17] 	 lr: 5e-05 	 f: 82.10709621245103
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 11 [0m
Train...
loss: tensor(522.7368) lr: 5e-05 time: 1089.89
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.72 	r: 75.28 	f1: 77.43 	 4643 	 5824 	 6168
wo 	p: 93.48 	r: 85.04 	f1: 89.06 	 2881 	 3082 	 3388
ni 	p: 85.06 	r: 68.86 	f1: 76.11 	 1048 	 1232 	 1522

[32m iter_1[0m
ga 	p: 80.09 	r: 75.02 	f1: 77.47 	 4627 	 5777 	 6168
wo 	p: 91.86 	r: 86.22 	f1: 88.95 	 2921 	 3180 	 3388
ni 	p: 81.02 	r: 72.34 	f1: 76.43 	 1101 	 1359 	 1522

[32m iter_2[0m
ga 	p: 80.26 	r: 74.97 	f1: 77.53 	 4624 	 5761 	 6168
wo 	p: 92.25 	r: 85.74 	f1: 88.88 	 2905 	 3149 	 3388
ni 	p: 82.22 	r: 71.68 	f1: 76.59 	 1091 	 1327 	 1522
best_thres [[0.29, 0.67, 0.17], [0.3, 0.57, 0.08], [0.31, 0.84, 0.09]]
f [0.8081, 0.8083, 0.8085]
load model: epoch4
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 4 	 [0.21, 0.42, 0.21] 	 lr: 0.0001 	 f: 82.03981490108389
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 13 [0m
Train...
loss: tensor(610.2892) lr: 5e-05 time: 1140.39
pred_count_train 41644

Test...
loss: tensor(663.1197) lr: 5e-05 time: 1148.54
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.16 	r: 75.65 	f1: 77.84 	 4666 	 5821 	 6168
wo 	p: 92.71 	r: 86.01 	f1: 89.24 	 2914 	 3143 	 3388
ni 	p: 84.56 	r: 74.11 	f1: 78.99 	 1128 	 1334 	 1522

[32m iter_1[0m
ga 	p: 82.61 	r: 73.77 	f1: 77.94 	 4550 	 5508 	 6168
wo 	p: 93.35 	r: 85.42 	f1: 89.21 	 2894 	 3100 	 3388
ni 	p: 84.36 	r: 74.44 	f1: 79.09 	 1133 	 1343 	 1522

[32m iter_2[0m
ga 	p: 81.98 	r: 74.37 	f1: 77.99 	 4587 	 5595 	 6168
wo 	p: 92.16 	r: 86.42 	f1: 89.2 	 2928 	 3177 	 3388
ni 	p: 86.43 	r: 73.26 	f1: 79.3 	 1115 	 1290 	 1522
best_thres [[0.34, 0.57, 0.15], [0.67, 0.84, 0.12], [0.68, 0.78, 0.14]]
f [0.8147, 0.8152, 0.8156]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 6 	 [0.61, 0.52, 0.1] 	 lr: 0.0001 	 f: 81.79208433338039
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 12 [0m
Train...
loss: tensor(1027.5238) lr: 2.5e-05 time: 1099.34
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.19 	r: 73.87 	f1: 77.81 	 4556 	 5543 	 6168
wo 	p: 93.58 	r: 85.6 	f1: 89.41 	 2900 	 3099 	 3388
ni 	p: 86.67 	r: 71.75 	f1: 78.5 	 1092 	 1260 	 1522

[32m iter_1[0m
ga 	p: 78.45 	r: 77.27 	f1: 77.86 	 4766 	 6075 	 6168
wo 	p: 93.67 	r: 85.57 	f1: 89.43 	 2899 	 3095 	 3388
ni 	p: 85.68 	r: 72.73 	f1: 78.68 	 1107 	 1292 	 1522

[32m iter_2[0m
ga 	p: 78.76 	r: 76.99 	f1: 77.87 	 4749 	 6030 	 6168
wo 	p: 93.66 	r: 85.51 	f1: 89.4 	 2897 	 3093 	 3388
ni 	p: 85.09 	r: 73.13 	f1: 78.66 	 1113 	 1308 	 1522
best_thres [[0.52, 0.76, 0.26], [0.25, 0.76, 0.21], [0.27, 0.76, 0.2]]
f [0.8149, 0.8147, 0.8146]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 9 	 [0.24, 0.52, 0.17] 	 lr: 5e-05 	 f: 82.10709621245103
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 12 [0m
Train...

[32m iter_0[0m
ga 	p: 83.08 	r: 74.69 	f1: 78.66 	 4607 	 5545 	 6168
wo 	p: 92.32 	r: 86.57 	f1: 89.35 	 2933 	 3177 	 3388
ni 	p: 86.64 	r: 76.28 	f1: 81.13 	 1161 	 1340 	 1522

[32m iter_1[0m
ga 	p: 82.62 	r: 75.28 	f1: 78.78 	 4643 	 5620 	 6168
wo 	p: 92.79 	r: 86.57 	f1: 89.57 	 2933 	 3161 	 3388
ni 	p: 86.26 	r: 77.14 	f1: 81.44 	 1174 	 1361 	 1522

[32m iter_2[0m
ga 	p: 83.32 	r: 74.72 	f1: 78.79 	 4609 	 5532 	 6168
wo 	p: 92.87 	r: 86.54 	f1: 89.6 	 2932 	 3157 	 3388
ni 	p: 86.32 	r: 77.14 	f1: 81.47 	 1174 	 1360 	 1522
best_thres [[0.4, 0.43, 0.18], [0.37, 0.45, 0.15], [0.4, 0.45, 0.15]]
f [0.8232, 0.8239, 0.8243]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 13 	 [0.4, 0.45, 0.15] 	 lr: 2.5e-05 	 f: 82.42947375053161
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 14 [0m
Train...
loss: tensor(428.4916) lr: 5e-05 time: 1124.8
pred_count_train 41644

Test...
loss: tensor(504.0673) lr: 5e-05 time: 1145.01
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.63 	r: 76.49 	f1: 78.03 	 4718 	 5925 	 6168
wo 	p: 93.63 	r: 85.01 	f1: 89.11 	 2880 	 3076 	 3388
ni 	p: 78.56 	r: 73.19 	f1: 75.78 	 1114 	 1418 	 1522

[32m iter_1[0m
ga 	p: 79.53 	r: 76.59 	f1: 78.03 	 4724 	 5940 	 6168
wo 	p: 93.45 	r: 85.45 	f1: 89.27 	 2895 	 3098 	 3388
ni 	p: 80.95 	r: 72.01 	f1: 76.22 	 1096 	 1354 	 1522

[32m iter_2[0m
ga 	p: 80.55 	r: 75.58 	f1: 77.99 	 4662 	 5788 	 6168
wo 	p: 93.82 	r: 85.06 	f1: 89.23 	 2882 	 3072 	 3388
ni 	p: 81.34 	r: 71.88 	f1: 76.32 	 1094 	 1345 	 1522
best_thres [[0.23, 0.49, 0.07], [0.29, 0.66, 0.07], [0.49, 0.81, 0.07]]
f [0.8105, 0.8112, 0.8114]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 6 	 [0.61, 0.52, 0.1] 	 lr: 0.0001 	 f: 81.79208433338039
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 13 [0m
Train...
loss: tensor(850.6833) lr: 2.5e-05 time: 1118.04
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.33 	r: 74.51 	f1: 77.77 	 4596 	 5651 	 6168
wo 	p: 92.82 	r: 85.86 	f1: 89.21 	 2909 	 3134 	 3388
ni 	p: 81.93 	r: 71.81 	f1: 76.54 	 1093 	 1334 	 1522

[32m iter_1[0m
ga 	p: 79.59 	r: 76.05 	f1: 77.78 	 4691 	 5894 	 6168
wo 	p: 93.09 	r: 85.48 	f1: 89.12 	 2896 	 3111 	 3388
ni 	p: 79.82 	r: 74.05 	f1: 76.82 	 1127 	 1412 	 1522

[32m iter_2[0m
ga 	p: 79.63 	r: 76.05 	f1: 77.8 	 4691 	 5891 	 6168
wo 	p: 92.76 	r: 85.8 	f1: 89.14 	 2907 	 3134 	 3388
ni 	p: 81.92 	r: 72.34 	f1: 76.83 	 1101 	 1344 	 1522
best_thres [[0.49, 0.61, 0.15], [0.36, 0.64, 0.1], [0.36, 0.59, 0.13]]
f [0.8112, 0.811, 0.8111]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 9 	 [0.24, 0.52, 0.17] 	 lr: 5e-05 	 f: 82.10709621245103
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 13 [0m
Train...

[32m iter_0[0m
ga 	p: 80.78 	r: 76.46 	f1: 78.56 	 4716 	 5838 	 6168
wo 	p: 93.94 	r: 85.09 	f1: 89.3 	 2883 	 3069 	 3388
ni 	p: 84.28 	r: 79.24 	f1: 81.68 	 1206 	 1431 	 1522

[32m iter_1[0m
ga 	p: 79.52 	r: 77.63 	f1: 78.56 	 4788 	 6021 	 6168
wo 	p: 91.87 	r: 87.01 	f1: 89.37 	 2948 	 3209 	 3388
ni 	p: 84.92 	r: 79.17 	f1: 81.94 	 1205 	 1419 	 1522

[32m iter_2[0m
ga 	p: 79.56 	r: 77.64 	f1: 78.59 	 4789 	 6019 	 6168
wo 	p: 93.46 	r: 85.68 	f1: 89.41 	 2903 	 3106 	 3388
ni 	p: 84.9 	r: 79.04 	f1: 81.86 	 1203 	 1417 	 1522
best_thres [[0.3, 0.64, 0.19], [0.24, 0.43, 0.18], [0.24, 0.77, 0.18]]
f [0.8223, 0.8227, 0.8227]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 13 	 [0.4, 0.45, 0.15] 	 lr: 2.5e-05 	 f: 82.42947375053161
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 15 [0m
Train...
loss: tensor(322.3051) lr: 5e-05 time: 1111.68
pred_count_train 41644

Test...
loss: tensor(384.1040) lr: 5e-05 time: 1143.73
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.14 	r: 73.64 	f1: 77.21 	 4542 	 5598 	 6168
wo 	p: 93.1 	r: 84.8 	f1: 88.76 	 2873 	 3086 	 3388
ni 	p: 84.65 	r: 69.19 	f1: 76.14 	 1053 	 1244 	 1522

[32m iter_1[0m
ga 	p: 80.31 	r: 74.59 	f1: 77.35 	 4601 	 5729 	 6168
wo 	p: 91.84 	r: 85.66 	f1: 88.64 	 2902 	 3160 	 3388
ni 	p: 81.15 	r: 72.4 	f1: 76.53 	 1102 	 1358 	 1522

[32m iter_2[0m
ga 	p: 80.63 	r: 74.59 	f1: 77.5 	 4601 	 5706 	 6168
wo 	p: 92.43 	r: 85.04 	f1: 88.58 	 2881 	 3117 	 3388
ni 	p: 80.86 	r: 73.0 	f1: 76.73 	 1111 	 1374 	 1522
best_thres [[0.3, 0.52, 0.15], [0.38, 0.58, 0.07], [0.47, 0.84, 0.06]]
f [0.8062, 0.8066, 0.807]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 6 	 [0.61, 0.52, 0.1] 	 lr: 0.0001 	 f: 81.79208433338039
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 14 [0m
Train...
loss: tensor(710.7679) lr: 2.5e-05 time: 1129.62
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.73 	r: 74.72 	f1: 77.14 	 4609 	 5781 	 6168
wo 	p: 92.81 	r: 85.3 	f1: 88.9 	 2890 	 3114 	 3388
ni 	p: 78.26 	r: 71.68 	f1: 74.83 	 1091 	 1394 	 1522

[32m iter_1[0m
ga 	p: 80.21 	r: 74.5 	f1: 77.25 	 4595 	 5729 	 6168
wo 	p: 93.02 	r: 85.36 	f1: 89.03 	 2892 	 3109 	 3388
ni 	p: 82.57 	r: 69.12 	f1: 75.25 	 1052 	 1274 	 1522

[32m iter_2[0m
ga 	p: 80.46 	r: 74.29 	f1: 77.25 	 4582 	 5695 	 6168
wo 	p: 92.03 	r: 86.19 	f1: 89.01 	 2920 	 3173 	 3388
ni 	p: 81.85 	r: 69.65 	f1: 75.26 	 1060 	 1295 	 1522
best_thres [[0.38, 0.68, 0.07], [0.39, 0.69, 0.1], [0.41, 0.54, 0.09]]
f [0.804, 0.805, 0.8054]
load model: epoch9
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 9 	 [0.24, 0.52, 0.17] 	 lr: 5e-05 	 f: 82.10709621245103
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 14 [0m
Train...

[32m iter_0[0m
ga 	p: 80.75 	r: 75.88 	f1: 78.23 	 4680 	 5796 	 6168
wo 	p: 93.4 	r: 85.6 	f1: 89.33 	 2900 	 3105 	 3388
ni 	p: 85.54 	r: 76.54 	f1: 80.79 	 1165 	 1362 	 1522

[32m iter_1[0m
ga 	p: 80.7 	r: 75.99 	f1: 78.27 	 4687 	 5808 	 6168
wo 	p: 92.49 	r: 86.13 	f1: 89.19 	 2918 	 3155 	 3388
ni 	p: 86.07 	r: 76.35 	f1: 80.92 	 1162 	 1350 	 1522

[32m iter_2[0m
ga 	p: 81.84 	r: 74.97 	f1: 78.25 	 4624 	 5650 	 6168
wo 	p: 92.75 	r: 86.07 	f1: 89.28 	 2916 	 3144 	 3388
ni 	p: 84.44 	r: 77.73 	f1: 80.94 	 1183 	 1401 	 1522
best_thres [[0.4, 0.57, 0.22], [0.39, 0.5, 0.22], [0.46, 0.65, 0.17]]
f [0.8195, 0.8196, 0.8198]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 13 	 [0.4, 0.45, 0.15] 	 lr: 2.5e-05 	 f: 82.42947375053161
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 16 [0m
Train...
loss: tensor(253.7434) lr: 5e-05 time: 1093.26
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.54 	r: 74.12 	f1: 77.66 	 4572 	 5607 	 6168
wo 	p: 93.66 	r: 84.65 	f1: 88.93 	 2868 	 3062 	 3388
ni 	p: 82.02 	r: 75.23 	f1: 78.48 	 1145 	 1396 	 1522

[32m iter_1[0m
ga 	p: 80.76 	r: 74.64 	f1: 77.58 	 4604 	 5701 	 6168
wo 	p: 92.34 	r: 85.77 	f1: 88.94 	 2906 	 3147 	 3388
ni 	p: 84.23 	r: 73.72 	f1: 78.63 	 1122 	 1332 	 1522

[32m iter_2[0m
ga 	p: 80.57 	r: 74.81 	f1: 77.58 	 4614 	 5727 	 6168
wo 	p: 92.14 	r: 85.77 	f1: 88.84 	 2906 	 3154 	 3388
ni 	p: 82.0 	r: 76.02 	f1: 78.9 	 1157 	 1411 	 1522
best_thres [[0.35, 0.64, 0.15], [0.5, 0.71, 0.22], [0.62, 0.83, 0.11]]
f [0.8121, 0.8121, 0.8121]
load model: epoch6
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 6 	 [0.61, 0.52, 0.1] 	 lr: 0.0001 	 f: 81.79208433338039
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 15 [0m
Train...
loss: tensor(766.1248) lr: 2.5e-05 time: 1146.36
pred_count_train 41644

Test...
loss: tensor(584.3339) lr: 2.5e-05 time: 1142.73
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.82 	r: 77.84 	f1: 78.81 	 4801 	 6015 	 6168
wo 	p: 93.17 	r: 86.19 	f1: 89.54 	 2920 	 3134 	 3388
ni 	p: 82.82 	r: 79.17 	f1: 80.95 	 1205 	 1455 	 1522

[32m iter_1[0m
ga 	p: 81.04 	r: 76.78 	f1: 78.85 	 4736 	 5844 	 6168
wo 	p: 93.09 	r: 86.3 	f1: 89.57 	 2924 	 3141 	 3388
ni 	p: 84.41 	r: 78.25 	f1: 81.21 	 1191 	 1411 	 1522

[32m iter_2[0m
ga 	p: 82.15 	r: 75.81 	f1: 78.85 	 4676 	 5692 	 6168
wo 	p: 93.06 	r: 86.28 	f1: 89.54 	 2923 	 3141 	 3388
ni 	p: 84.59 	r: 78.25 	f1: 81.3 	 1191 	 1408 	 1522
best_thres [[0.28, 0.58, 0.14], [0.35, 0.56, 0.16], [0.41, 0.56, 0.16]]
f [0.8234, 0.8238, 0.8241]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 14 	 [0.41, 0.56, 0.16] 	 lr: 2.5e-05 	 f: 82.41023652578518
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 15 [0m
Train...
loss: tensor(561.4452) lr: 2.5e-05 time: 1083.4
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.48 	r: 74.59 	f1: 77.88 	 4601 	 5647 	 6168
wo 	p: 93.23 	r: 85.33 	f1: 89.1 	 2891 	 3101 	 3388
ni 	p: 81.81 	r: 74.18 	f1: 77.81 	 1129 	 1380 	 1522

[32m iter_1[0m
ga 	p: 81.08 	r: 75.05 	f1: 77.95 	 4629 	 5709 	 6168
wo 	p: 93.43 	r: 85.21 	f1: 89.13 	 2887 	 3090 	 3388
ni 	p: 83.51 	r: 73.52 	f1: 78.2 	 1119 	 1340 	 1522

[32m iter_2[0m
ga 	p: 80.87 	r: 75.24 	f1: 77.95 	 4641 	 5739 	 6168
wo 	p: 93.67 	r: 85.12 	f1: 89.19 	 2884 	 3079 	 3388
ni 	p: 84.69 	r: 72.67 	f1: 78.22 	 1106 	 1306 	 1522
best_thres [[0.25, 0.48, 0.09], [0.22, 0.52, 0.09], [0.21, 0.76, 0.1]]
f [0.8131, 0.8135, 0.8137]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 13 	 [0.4, 0.45, 0.15] 	 lr: 2.5e-05 	 f: 82.42947375053161
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 17 [0m
Train...

[32m iter_0[0m
ga 	p: 81.98 	r: 74.04 	f1: 77.81 	 4567 	 5571 	 6168
wo 	p: 92.6 	r: 86.45 	f1: 89.42 	 2929 	 3163 	 3388
ni 	p: 84.29 	r: 77.92 	f1: 80.98 	 1186 	 1407 	 1522

[32m iter_1[0m
ga 	p: 80.75 	r: 75.5 	f1: 78.04 	 4657 	 5767 	 6168
wo 	p: 92.25 	r: 86.81 	f1: 89.45 	 2941 	 3188 	 3388
ni 	p: 84.9 	r: 78.32 	f1: 81.48 	 1192 	 1404 	 1522

[32m iter_2[0m
ga 	p: 80.7 	r: 75.91 	f1: 78.23 	 4682 	 5802 	 6168
wo 	p: 92.36 	r: 86.66 	f1: 89.42 	 2936 	 3179 	 3388
ni 	p: 85.29 	r: 78.12 	f1: 81.55 	 1189 	 1394 	 1522
best_thres [[0.45, 0.48, 0.2], [0.51, 0.57, 0.2], [0.54, 0.66, 0.21]]
f [0.8183, 0.8192, 0.8198]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 15 	 [0.54, 0.66, 0.21] 	 lr: 2.5e-05 	 f: 81.98224898220218
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 16 [0m
Train...
loss: tensor(626.0640) lr: 2.5e-05 time: 1162.66
pred_count_train 41644

Test...
loss: tensor(481.8213) lr: 2.5e-05 time: 1162.09
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.56 	r: 75.73 	f1: 78.54 	 4671 	 5727 	 6168
wo 	p: 94.08 	r: 85.39 	f1: 89.52 	 2893 	 3075 	 3388
ni 	p: 86.83 	r: 74.97 	f1: 80.47 	 1141 	 1314 	 1522

[32m iter_1[0m
ga 	p: 81.04 	r: 76.35 	f1: 78.62 	 4709 	 5811 	 6168
wo 	p: 94.28 	r: 85.21 	f1: 89.52 	 2887 	 3062 	 3388
ni 	p: 84.82 	r: 76.74 	f1: 80.58 	 1168 	 1377 	 1522

[32m iter_2[0m
ga 	p: 81.04 	r: 76.36 	f1: 78.63 	 4710 	 5812 	 6168
wo 	p: 94.13 	r: 85.24 	f1: 89.47 	 2888 	 3068 	 3388
ni 	p: 84.88 	r: 76.74 	f1: 80.61 	 1168 	 1376 	 1522
best_thres [[0.46, 0.71, 0.3], [0.42, 0.73, 0.23], [0.42, 0.71, 0.23]]
f [0.8215, 0.8216, 0.8217]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 14 	 [0.41, 0.56, 0.16] 	 lr: 2.5e-05 	 f: 82.41023652578518
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 16 [0m
Train...
loss: tensor(389.2740) lr: 2.5e-05 time: 1097.12
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.52 	r: 73.74 	f1: 77.43 	 4548 	 5579 	 6168
wo 	p: 93.7 	r: 84.33 	f1: 88.77 	 2857 	 3049 	 3388
ni 	p: 83.51 	r: 72.21 	f1: 77.45 	 1099 	 1316 	 1522

[32m iter_1[0m
ga 	p: 80.7 	r: 74.9 	f1: 77.69 	 4620 	 5725 	 6168
wo 	p: 92.24 	r: 85.57 	f1: 88.78 	 2899 	 3143 	 3388
ni 	p: 82.95 	r: 73.52 	f1: 77.95 	 1119 	 1349 	 1522

[32m iter_2[0m
ga 	p: 80.29 	r: 75.15 	f1: 77.63 	 4635 	 5773 	 6168
wo 	p: 92.94 	r: 85.06 	f1: 88.83 	 2882 	 3101 	 3388
ni 	p: 84.17 	r: 73.0 	f1: 78.18 	 1111 	 1320 	 1522
best_thres [[0.45, 0.65, 0.16], [0.38, 0.45, 0.12], [0.35, 0.85, 0.13]]
f [0.8091, 0.8102, 0.8105]
load model: epoch13
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 13 	 [0.4, 0.45, 0.15] 	 lr: 2.5e-05 	 f: 82.42947375053161
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 18 [0m
Train...

[32m iter_0[0m
ga 	p: 80.82 	r: 75.36 	f1: 77.99 	 4648 	 5751 	 6168
wo 	p: 93.92 	r: 84.8 	f1: 89.13 	 2873 	 3059 	 3388
ni 	p: 84.68 	r: 72.27 	f1: 77.99 	 1100 	 1299 	 1522

[32m iter_1[0m
ga 	p: 81.86 	r: 74.79 	f1: 78.17 	 4613 	 5635 	 6168
wo 	p: 93.7 	r: 85.09 	f1: 89.19 	 2883 	 3077 	 3388
ni 	p: 87.57 	r: 70.83 	f1: 78.31 	 1078 	 1231 	 1522

[32m iter_2[0m
ga 	p: 81.7 	r: 75.0 	f1: 78.21 	 4626 	 5662 	 6168
wo 	p: 93.95 	r: 84.77 	f1: 89.12 	 2872 	 3057 	 3388
ni 	p: 83.22 	r: 74.31 	f1: 78.51 	 1131 	 1359 	 1522
best_thres [[0.23, 0.49, 0.1], [0.39, 0.65, 0.12], [0.43, 0.83, 0.06]]
f [0.8138, 0.8148, 0.8151]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 15 	 [0.54, 0.66, 0.21] 	 lr: 2.5e-05 	 f: 81.98224898220218
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 17 [0m
Train...
loss: tensor(502.5884) lr: 2.5e-05 time: 1176.57
pred_count_train 41644

Test...
loss: tensor(810.9311) lr: 1.25e-05 time: 1184.33
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.14 	r: 74.79 	f1: 77.84 	 4613 	 5685 	 6168
wo 	p: 93.39 	r: 85.51 	f1: 89.28 	 2897 	 3102 	 3388
ni 	p: 82.35 	r: 73.26 	f1: 77.54 	 1115 	 1354 	 1522

[32m iter_1[0m
ga 	p: 80.61 	r: 75.54 	f1: 77.99 	 4659 	 5780 	 6168
wo 	p: 93.77 	r: 85.33 	f1: 89.35 	 2891 	 3083 	 3388
ni 	p: 82.02 	r: 74.31 	f1: 77.97 	 1131 	 1379 	 1522

[32m iter_2[0m
ga 	p: 80.63 	r: 75.57 	f1: 78.01 	 4661 	 5781 	 6168
wo 	p: 93.29 	r: 85.71 	f1: 89.34 	 2904 	 3113 	 3388
ni 	p: 82.06 	r: 74.24 	f1: 77.96 	 1130 	 1377 	 1522
best_thres [[0.28, 0.57, 0.1], [0.23, 0.59, 0.08], [0.23, 0.52, 0.08]]
f [0.813, 0.8137, 0.814]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 14 	 [0.41, 0.56, 0.16] 	 lr: 2.5e-05 	 f: 82.41023652578518
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 17 [0m
Train...
loss: tensor(289.3400) lr: 2.5e-05 time: 1106.99
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.1 	r: 76.59 	f1: 78.78 	 4724 	 5825 	 6168
wo 	p: 93.85 	r: 85.54 	f1: 89.5 	 2898 	 3088 	 3388
ni 	p: 88.71 	r: 76.94 	f1: 82.41 	 1171 	 1320 	 1522

[32m iter_1[0m
ga 	p: 81.13 	r: 76.83 	f1: 78.92 	 4739 	 5841 	 6168
wo 	p: 93.34 	r: 86.01 	f1: 89.52 	 2914 	 3122 	 3388
ni 	p: 88.23 	r: 77.79 	f1: 82.68 	 1184 	 1342 	 1522

[32m iter_2[0m
ga 	p: 80.87 	r: 76.98 	f1: 78.88 	 4748 	 5871 	 6168
wo 	p: 93.37 	r: 86.01 	f1: 89.54 	 2914 	 3121 	 3388
ni 	p: 88.22 	r: 77.73 	f1: 82.64 	 1183 	 1341 	 1522
best_thres [[0.33, 0.52, 0.18], [0.32, 0.46, 0.15], [0.31, 0.46, 0.15]]
f [0.8252, 0.8259, 0.826]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 18 	 [0.31, 0.46, 0.15] 	 lr: 1.25e-05 	 f: 82.59886124327275
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 19 [0m
Train...

[32m iter_0[0m
ga 	p: 82.13 	r: 72.94 	f1: 77.26 	 4499 	 5478 	 6168
wo 	p: 93.2 	r: 84.92 	f1: 88.86 	 2877 	 3087 	 3388
ni 	p: 83.94 	r: 71.42 	f1: 77.17 	 1087 	 1295 	 1522

[32m iter_1[0m
ga 	p: 83.18 	r: 72.57 	f1: 77.51 	 4476 	 5381 	 6168
wo 	p: 92.78 	r: 85.74 	f1: 89.12 	 2905 	 3131 	 3388
ni 	p: 84.75 	r: 72.27 	f1: 78.01 	 1100 	 1298 	 1522

[32m iter_2[0m
ga 	p: 81.16 	r: 74.09 	f1: 77.46 	 4570 	 5631 	 6168
wo 	p: 92.82 	r: 85.8 	f1: 89.17 	 2907 	 3132 	 3388
ni 	p: 83.25 	r: 73.13 	f1: 77.86 	 1113 	 1337 	 1522
best_thres [[0.35, 0.54, 0.11], [0.64, 0.68, 0.09], [0.54, 0.79, 0.07]]
f [0.8084, 0.8102, 0.8106]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 15 	 [0.54, 0.66, 0.21] 	 lr: 2.5e-05 	 f: 81.98224898220218
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 18 [0m
Train...
loss: tensor(406.0417) lr: 2.5e-05 time: 1193.07
pred_count_train 41644

Test...
loss: tensor(723.8834) lr: 1.25e-05 time: 1210.41
pred_count_train 41644

Test...
loss: tensor(220.2686) lr: 2.5e-05 time: 1136.26
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.55 	r: 75.32 	f1: 77.85 	 4646 	 5768 	 6168
wo 	p: 93.13 	r: 85.66 	f1: 89.24 	 2902 	 3116 	 3388
ni 	p: 83.68 	r: 74.44 	f1: 78.79 	 1133 	 1354 	 1522

[32m iter_1[0m
ga 	p: 81.2 	r: 74.92 	f1: 77.93 	 4621 	 5691 	 6168
wo 	p: 92.47 	r: 86.22 	f1: 89.23 	 2921 	 3159 	 3388
ni 	p: 81.93 	r: 76.28 	f1: 79.01 	 1161 	 1417 	 1522

[32m iter_2[0m
ga 	p: 81.19 	r: 74.87 	f1: 77.9 	 4618 	 5688 	 6168
wo 	p: 92.49 	r: 86.19 	f1: 89.23 	 2920 	 3157 	 3388
ni 	p: 82.04 	r: 76.22 	f1: 79.02 	 1160 	 1414 	 1522
best_thres [[0.43, 0.65, 0.14], [0.47, 0.54, 0.09], [0.47, 0.54, 0.09]]
f [0.8145, 0.815, 0.8151]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 14 	 [0.41, 0.56, 0.16] 	 lr: 2.5e-05 	 f: 82.41023652578518
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 18 [0m
Train...

[32m iter_0[0m
ga 	p: 79.74 	r: 77.27 	f1: 78.48 	 4766 	 5977 	 6168
wo 	p: 92.92 	r: 85.98 	f1: 89.31 	 2913 	 3135 	 3388
ni 	p: 86.27 	r: 77.2 	f1: 81.48 	 1175 	 1362 	 1522

[32m iter_1[0m
ga 	p: 79.88 	r: 77.19 	f1: 78.51 	 4761 	 5960 	 6168
wo 	p: 93.87 	r: 85.39 	f1: 89.43 	 2893 	 3082 	 3388
ni 	p: 86.27 	r: 77.2 	f1: 81.48 	 1175 	 1362 	 1522

[32m iter_2[0m
ga 	p: 79.63 	r: 77.5 	f1: 78.55 	 4780 	 6003 	 6168
wo 	p: 94.01 	r: 85.27 	f1: 89.43 	 2889 	 3073 	 3388
ni 	p: 85.66 	r: 77.73 	f1: 81.5 	 1183 	 1381 	 1522
best_thres [[0.3, 0.45, 0.19], [0.31, 0.66, 0.18], [0.29, 0.71, 0.16]]
f [0.8216, 0.8218, 0.8219]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 18 	 [0.31, 0.46, 0.15] 	 lr: 1.25e-05 	 f: 82.59886124327275
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 20 [0m
Train...

[32m iter_0[0m
ga 	p: 80.42 	r: 74.63 	f1: 77.41 	 4603 	 5724 	 6168
wo 	p: 92.94 	r: 85.12 	f1: 88.86 	 2884 	 3103 	 3388
ni 	p: 80.1 	r: 74.31 	f1: 77.1 	 1131 	 1412 	 1522

[32m iter_1[0m
ga 	p: 80.32 	r: 74.56 	f1: 77.33 	 4599 	 5726 	 6168
wo 	p: 92.95 	r: 85.21 	f1: 88.91 	 2887 	 3106 	 3388
ni 	p: 81.17 	r: 74.51 	f1: 77.7 	 1134 	 1397 	 1522

[32m iter_2[0m
ga 	p: 79.7 	r: 75.36 	f1: 77.47 	 4648 	 5832 	 6168
wo 	p: 92.59 	r: 85.51 	f1: 88.91 	 2897 	 3129 	 3388
ni 	p: 81.96 	r: 74.31 	f1: 77.95 	 1131 	 1380 	 1522
best_thres [[0.26, 0.47, 0.05], [0.45, 0.73, 0.04], [0.38, 0.81, 0.04]]
f [0.8086, 0.8088, 0.8093]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 15 	 [0.54, 0.66, 0.21] 	 lr: 2.5e-05 	 f: 81.98224898220218
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 19 [0m
Train...
loss: tensor(311.5380) lr: 2.5e-05 time: 1236.24
pred_count_train 41644

Test...
loss: tensor(642.9720) lr: 1.25e-05 time: 1245.76
pred_count_train 41644

Test...
loss: tensor(170.7192) lr: 2.5e-05 time: 1180.96
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.8 	r: 74.79 	f1: 77.68 	 4613 	 5709 	 6168
wo 	p: 93.14 	r: 85.01 	f1: 88.89 	 2880 	 3092 	 3388
ni 	p: 85.67 	r: 70.3 	f1: 77.23 	 1070 	 1249 	 1522

[32m iter_1[0m
ga 	p: 81.14 	r: 74.48 	f1: 77.67 	 4594 	 5662 	 6168
wo 	p: 93.24 	r: 84.71 	f1: 88.77 	 2870 	 3078 	 3388
ni 	p: 84.84 	r: 71.35 	f1: 77.52 	 1086 	 1280 	 1522

[32m iter_2[0m
ga 	p: 80.99 	r: 74.53 	f1: 77.63 	 4597 	 5676 	 6168
wo 	p: 93.21 	r: 84.71 	f1: 88.76 	 2870 	 3079 	 3388
ni 	p: 84.98 	r: 71.35 	f1: 77.57 	 1086 	 1278 	 1522
best_thres [[0.51, 0.58, 0.13], [0.53, 0.59, 0.09], [0.52, 0.58, 0.09]]
f [0.8106, 0.8105, 0.8105]
load model: epoch14
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 14 	 [0.41, 0.56, 0.16] 	 lr: 2.5e-05 	 f: 82.41023652578518
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 19 [0m
Train...

[32m iter_0[0m
ga 	p: 82.3 	r: 74.19 	f1: 78.04 	 4576 	 5560 	 6168
wo 	p: 91.8 	r: 86.54 	f1: 89.09 	 2932 	 3194 	 3388
ni 	p: 85.81 	r: 74.7 	f1: 79.87 	 1137 	 1325 	 1522

[32m iter_1[0m
ga 	p: 81.97 	r: 74.74 	f1: 78.19 	 4610 	 5624 	 6168
wo 	p: 92.11 	r: 86.45 	f1: 89.19 	 2929 	 3180 	 3388
ni 	p: 86.81 	r: 74.38 	f1: 80.11 	 1132 	 1304 	 1522

[32m iter_2[0m
ga 	p: 82.08 	r: 74.55 	f1: 78.13 	 4598 	 5602 	 6168
wo 	p: 92.02 	r: 86.48 	f1: 89.17 	 2930 	 3184 	 3388
ni 	p: 87.1 	r: 74.11 	f1: 80.09 	 1128 	 1295 	 1522
best_thres [[0.42, 0.44, 0.18], [0.39, 0.45, 0.18], [0.4, 0.44, 0.18]]
f [0.8172, 0.8179, 0.818]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 18 	 [0.31, 0.46, 0.15] 	 lr: 1.25e-05 	 f: 82.59886124327275
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 21 [0m
Train...

[32m iter_0[0m
ga 	p: 77.79 	r: 75.39 	f1: 76.57 	 4650 	 5978 	 6168
wo 	p: 92.65 	r: 85.21 	f1: 88.78 	 2887 	 3116 	 3388
ni 	p: 85.79 	r: 70.63 	f1: 77.48 	 1075 	 1253 	 1522

[32m iter_1[0m
ga 	p: 78.57 	r: 74.68 	f1: 76.58 	 4606 	 5862 	 6168
wo 	p: 92.74 	r: 85.27 	f1: 88.85 	 2889 	 3115 	 3388
ni 	p: 85.42 	r: 70.83 	f1: 77.44 	 1078 	 1262 	 1522

[32m iter_2[0m
ga 	p: 80.59 	r: 73.05 	f1: 76.64 	 4506 	 5591 	 6168
wo 	p: 91.49 	r: 86.36 	f1: 88.86 	 2926 	 3198 	 3388
ni 	p: 86.62 	r: 70.63 	f1: 77.81 	 1075 	 1241 	 1522
best_thres [[0.12, 0.46, 0.1], [0.14, 0.8, 0.07], [0.62, 0.63, 0.07]]
f [0.8039, 0.8041, 0.8048]
load model: epoch15
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 15 	 [0.54, 0.66, 0.21] 	 lr: 2.5e-05 	 f: 81.98224898220218
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 20 [0m
Train...
loss: tensor(586.5964) lr: 1.25e-05 time: 1272.44
pred_count_train 41644

Test...
loss: tensor(569.9667) lr: 1.25e-05 time: 1272.35
pred_count_train 41644

Test...
loss: tensor(362.2225) lr: 1.25e-05 time: 1214.48
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 78.37 	r: 76.62 	f1: 77.49 	 4726 	 6030 	 6168
wo 	p: 90.36 	r: 87.66 	f1: 88.99 	 2970 	 3287 	 3388
ni 	p: 81.06 	r: 73.39 	f1: 77.03 	 1117 	 1378 	 1522

[32m iter_1[0m
ga 	p: 79.61 	r: 75.66 	f1: 77.59 	 4667 	 5862 	 6168
wo 	p: 91.62 	r: 86.51 	f1: 88.99 	 2931 	 3199 	 3388
ni 	p: 86.76 	r: 70.63 	f1: 77.87 	 1075 	 1239 	 1522

[32m iter_2[0m
ga 	p: 79.6 	r: 75.62 	f1: 77.56 	 4664 	 5859 	 6168
wo 	p: 91.68 	r: 86.57 	f1: 89.05 	 2933 	 3199 	 3388
ni 	p: 85.61 	r: 71.55 	f1: 77.95 	 1089 	 1272 	 1522
best_thres [[0.2, 0.25, 0.09], [0.24, 0.37, 0.13], [0.24, 0.37, 0.11]]
f [0.8095, 0.8105, 0.8108]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 18 	 [0.31, 0.46, 0.15] 	 lr: 1.25e-05 	 f: 82.59886124327275
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 22 [0m
Train...

[32m iter_0[0m
ga 	p: 81.0 	r: 75.97 	f1: 78.41 	 4686 	 5785 	 6168
wo 	p: 93.17 	r: 86.1 	f1: 89.49 	 2917 	 3131 	 3388
ni 	p: 83.51 	r: 77.53 	f1: 80.41 	 1180 	 1413 	 1522

[32m iter_1[0m
ga 	p: 80.33 	r: 76.86 	f1: 78.56 	 4741 	 5902 	 6168
wo 	p: 93.52 	r: 86.01 	f1: 89.61 	 2914 	 3116 	 3388
ni 	p: 85.62 	r: 75.89 	f1: 80.46 	 1155 	 1349 	 1522

[32m iter_2[0m
ga 	p: 81.18 	r: 76.15 	f1: 78.58 	 4697 	 5786 	 6168
wo 	p: 93.45 	r: 85.95 	f1: 89.54 	 2912 	 3116 	 3388
ni 	p: 85.61 	r: 75.82 	f1: 80.42 	 1154 	 1348 	 1522
best_thres [[0.44, 0.51, 0.14], [0.38, 0.52, 0.18], [0.44, 0.52, 0.18]]
f [0.8206, 0.8211, 0.8213]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 14 	 [0.41, 0.56, 0.16] 	 lr: 2.5e-05 	 f: 82.41023652578518
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 20 [0m
Train...

[32m iter_0[0m
ga 	p: 80.43 	r: 75.65 	f1: 77.97 	 4666 	 5801 	 6168
wo 	p: 92.7 	r: 86.25 	f1: 89.36 	 2922 	 3152 	 3388
ni 	p: 87.2 	r: 71.62 	f1: 78.64 	 1090 	 1250 	 1522

[32m iter_1[0m
ga 	p: 81.81 	r: 74.64 	f1: 78.06 	 4604 	 5628 	 6168
wo 	p: 92.59 	r: 86.28 	f1: 89.32 	 2923 	 3157 	 3388
ni 	p: 85.82 	r: 73.59 	f1: 79.24 	 1120 	 1305 	 1522

[32m iter_2[0m
ga 	p: 81.38 	r: 75.18 	f1: 78.16 	 4637 	 5698 	 6168
wo 	p: 92.45 	r: 86.42 	f1: 89.34 	 2928 	 3167 	 3388
ni 	p: 86.2 	r: 73.06 	f1: 79.09 	 1112 	 1290 	 1522
best_thres [[0.26, 0.46, 0.24], [0.48, 0.63, 0.17], [0.51, 0.68, 0.17]]
f [0.8156, 0.8163, 0.8166]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 15 	 [0.54, 0.66, 0.21] 	 lr: 2.5e-05 	 f: 81.98224898220218
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 21 [0m
Train...
loss: tensor(497.5717) lr: 1.25e-05 time: 1275.3
pred_count_train 41644

Test...
loss: tensor(508.7731) lr: 1.25e-05 time: 1284.34
pred_count_train 41644

Test...
loss: tensor(290.4579) lr: 1.25e-05 time: 1237.71
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.74 	r: 74.68 	f1: 77.59 	 4606 	 5705 	 6168
wo 	p: 92.4 	r: 85.8 	f1: 88.98 	 2907 	 3146 	 3388
ni 	p: 82.31 	r: 74.31 	f1: 78.11 	 1131 	 1374 	 1522

[32m iter_1[0m
ga 	p: 81.38 	r: 74.24 	f1: 77.64 	 4579 	 5627 	 6168
wo 	p: 91.39 	r: 86.75 	f1: 89.01 	 2939 	 3216 	 3388
ni 	p: 84.58 	r: 73.52 	f1: 78.66 	 1119 	 1323 	 1522

[32m iter_2[0m
ga 	p: 81.26 	r: 74.16 	f1: 77.55 	 4574 	 5629 	 6168
wo 	p: 91.23 	r: 86.92 	f1: 89.03 	 2945 	 3228 	 3388
ni 	p: 84.76 	r: 73.46 	f1: 78.7 	 1118 	 1319 	 1522
best_thres [[0.29, 0.44, 0.1], [0.31, 0.25, 0.1], [0.31, 0.23, 0.1]]
f [0.8115, 0.8123, 0.8125]
load model: epoch18
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 18 	 [0.31, 0.46, 0.15] 	 lr: 1.25e-05 	 f: 82.59886124327275
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 23 [0m
Train...

[32m iter_0[0m
ga 	p: 80.3 	r: 76.12 	f1: 78.15 	 4695 	 5847 	 6168
wo 	p: 92.92 	r: 85.95 	f1: 89.3 	 2912 	 3134 	 3388
ni 	p: 83.6 	r: 75.36 	f1: 79.27 	 1147 	 1372 	 1522

[32m iter_1[0m
ga 	p: 79.83 	r: 76.62 	f1: 78.19 	 4726 	 5920 	 6168
wo 	p: 91.98 	r: 86.66 	f1: 89.24 	 2936 	 3192 	 3388
ni 	p: 84.46 	r: 74.64 	f1: 79.25 	 1136 	 1345 	 1522

[32m iter_2[0m
ga 	p: 80.42 	r: 76.12 	f1: 78.21 	 4695 	 5838 	 6168
wo 	p: 92.01 	r: 86.66 	f1: 89.25 	 2936 	 3191 	 3388
ni 	p: 84.4 	r: 74.64 	f1: 79.22 	 1136 	 1346 	 1522
best_thres [[0.34, 0.57, 0.15], [0.31, 0.45, 0.15], [0.34, 0.45, 0.15]]
f [0.8169, 0.817, 0.8171]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 14 	 [0.41, 0.56, 0.16] 	 lr: 2.5e-05 	 f: 82.41023652578518
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 21 [0m
Train...

[32m iter_0[0m
ga 	p: 80.88 	r: 74.22 	f1: 77.41 	 4578 	 5660 	 6168
wo 	p: 93.75 	r: 84.98 	f1: 89.15 	 2879 	 3071 	 3388
ni 	p: 84.62 	r: 72.27 	f1: 77.96 	 1100 	 1300 	 1522

[32m iter_1[0m
ga 	p: 81.32 	r: 74.17 	f1: 77.58 	 4575 	 5626 	 6168
wo 	p: 93.17 	r: 85.42 	f1: 89.13 	 2894 	 3106 	 3388
ni 	p: 84.87 	r: 71.88 	f1: 77.84 	 1094 	 1289 	 1522

[32m iter_2[0m
ga 	p: 80.79 	r: 74.85 	f1: 77.71 	 4617 	 5715 	 6168
wo 	p: 92.68 	r: 85.98 	f1: 89.21 	 2913 	 3143 	 3388
ni 	p: 84.65 	r: 72.47 	f1: 78.09 	 1103 	 1303 	 1522
best_thres [[0.3, 0.58, 0.12], [0.5, 0.72, 0.11], [0.53, 0.7, 0.09]]
f [0.8107, 0.8112, 0.8118]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 15 	 [0.54, 0.66, 0.21] 	 lr: 2.5e-05 	 f: 81.98224898220218
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 22 [0m
Train...
loss: tensor(703.5878) lr: 6.25e-06 time: 1268.95
pred_count_train 41644

Test...
loss: tensor(438.9903) lr: 1.25e-05 time: 1289.02
pred_count_train 41644

Test...
loss: tensor(233.9153) lr: 1.25e-05 time: 1255.89
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.33 	r: 76.62 	f1: 78.43 	 4726 	 5883 	 6168
wo 	p: 92.86 	r: 86.04 	f1: 89.32 	 2915 	 3139 	 3388
ni 	p: 86.19 	r: 77.07 	f1: 81.37 	 1173 	 1361 	 1522

[32m iter_1[0m
ga 	p: 80.54 	r: 76.64 	f1: 78.54 	 4727 	 5869 	 6168
wo 	p: 93.11 	r: 85.74 	f1: 89.27 	 2905 	 3120 	 3388
ni 	p: 84.89 	r: 78.25 	f1: 81.44 	 1191 	 1403 	 1522

[32m iter_2[0m
ga 	p: 80.57 	r: 76.69 	f1: 78.58 	 4730 	 5871 	 6168
wo 	p: 92.72 	r: 86.13 	f1: 89.3 	 2918 	 3147 	 3388
ni 	p: 85.12 	r: 78.19 	f1: 81.51 	 1190 	 1398 	 1522
best_thres [[0.33, 0.53, 0.19], [0.33, 0.7, 0.14], [0.33, 0.65, 0.14]]
f [0.8214, 0.8216, 0.8219]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 18 	 [0.31, 0.46, 0.15] 	 lr: 1.25e-05 	 f: 82.59886124327275
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 24 [0m
Train...

[32m iter_0[0m
ga 	p: 81.71 	r: 74.61 	f1: 78.0 	 4602 	 5632 	 6168
wo 	p: 92.34 	r: 86.45 	f1: 89.3 	 2929 	 3172 	 3388
ni 	p: 82.53 	r: 72.93 	f1: 77.43 	 1110 	 1345 	 1522

[32m iter_1[0m
ga 	p: 80.88 	r: 75.49 	f1: 78.09 	 4656 	 5757 	 6168
wo 	p: 91.63 	r: 87.19 	f1: 89.35 	 2954 	 3224 	 3388
ni 	p: 83.5 	r: 72.47 	f1: 77.59 	 1103 	 1321 	 1522

[32m iter_2[0m
ga 	p: 81.05 	r: 75.32 	f1: 78.08 	 4646 	 5732 	 6168
wo 	p: 91.49 	r: 87.25 	f1: 89.32 	 2956 	 3231 	 3388
ni 	p: 82.37 	r: 73.39 	f1: 77.62 	 1117 	 1356 	 1522
best_thres [[0.42, 0.51, 0.11], [0.35, 0.39, 0.11], [0.36, 0.38, 0.09]]
f [0.8142, 0.8146, 0.8147]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 14 	 [0.41, 0.56, 0.16] 	 lr: 2.5e-05 	 f: 82.41023652578518
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 22 [0m
Train...

[32m iter_0[0m
ga 	p: 80.53 	r: 74.56 	f1: 77.43 	 4599 	 5711 	 6168
wo 	p: 92.32 	r: 85.89 	f1: 88.99 	 2910 	 3152 	 3388
ni 	p: 86.04 	r: 71.29 	f1: 77.97 	 1085 	 1261 	 1522

[32m iter_1[0m
ga 	p: 82.26 	r: 73.38 	f1: 77.57 	 4526 	 5502 	 6168
wo 	p: 93.13 	r: 85.57 	f1: 89.19 	 2899 	 3113 	 3388
ni 	p: 87.43 	r: 70.83 	f1: 78.26 	 1078 	 1233 	 1522

[32m iter_2[0m
ga 	p: 80.09 	r: 75.32 	f1: 77.63 	 4646 	 5801 	 6168
wo 	p: 92.67 	r: 85.83 	f1: 89.12 	 2908 	 3138 	 3388
ni 	p: 87.92 	r: 70.76 	f1: 78.41 	 1077 	 1225 	 1522
best_thres [[0.29, 0.4, 0.21], [0.68, 0.71, 0.28], [0.53, 0.78, 0.31]]
f [0.8107, 0.8117, 0.812]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 15 	 [0.54, 0.66, 0.21] 	 lr: 2.5e-05 	 f: 81.98224898220218
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 23 [0m
Train...
loss: tensor(657.8482) lr: 6.25e-06 time: 1256.07
pred_count_train 41644

Test...
loss: tensor(374.9510) lr: 1.25e-05 time: 1288.75
pred_count_train 41644

Test...
loss: tensor(188.8287) lr: 1.25e-05 time: 1268.25
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.69 	r: 76.07 	f1: 78.31 	 4692 	 5815 	 6168
wo 	p: 91.63 	r: 86.95 	f1: 89.23 	 2946 	 3215 	 3388
ni 	p: 87.66 	r: 74.18 	f1: 80.36 	 1129 	 1288 	 1522

[32m iter_1[0m
ga 	p: 80.8 	r: 76.12 	f1: 78.39 	 4695 	 5811 	 6168
wo 	p: 93.17 	r: 85.74 	f1: 89.3 	 2905 	 3118 	 3388
ni 	p: 83.88 	r: 77.27 	f1: 80.44 	 1176 	 1402 	 1522

[32m iter_2[0m
ga 	p: 79.67 	r: 77.09 	f1: 78.36 	 4755 	 5968 	 6168
wo 	p: 92.84 	r: 86.07 	f1: 89.32 	 2916 	 3141 	 3388
ni 	p: 84.07 	r: 77.33 	f1: 80.56 	 1177 	 1400 	 1522
best_thres [[0.35, 0.42, 0.23], [0.35, 0.73, 0.13], [0.29, 0.71, 0.13]]
f [0.8195, 0.8197, 0.8197]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 18 	 [0.31, 0.46, 0.15] 	 lr: 1.25e-05 	 f: 82.59886124327275
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 25 [0m
Train...

[32m iter_0[0m
ga 	p: 81.52 	r: 74.64 	f1: 77.93 	 4604 	 5648 	 6168
wo 	p: 92.68 	r: 85.98 	f1: 89.21 	 2913 	 3143 	 3388
ni 	p: 84.44 	r: 72.01 	f1: 77.73 	 1096 	 1298 	 1522

[32m iter_1[0m
ga 	p: 80.75 	r: 75.21 	f1: 77.88 	 4639 	 5745 	 6168
wo 	p: 94.45 	r: 84.45 	f1: 89.17 	 2861 	 3029 	 3388
ni 	p: 82.94 	r: 73.46 	f1: 77.91 	 1118 	 1348 	 1522

[32m iter_2[0m
ga 	p: 81.15 	r: 74.81 	f1: 77.85 	 4614 	 5686 	 6168
wo 	p: 93.12 	r: 85.51 	f1: 89.15 	 2897 	 3111 	 3388
ni 	p: 85.14 	r: 71.88 	f1: 77.95 	 1094 	 1285 	 1522
best_thres [[0.37, 0.47, 0.16], [0.32, 0.67, 0.12], [0.35, 0.49, 0.16]]
f [0.8138, 0.8134, 0.8134]
load model: epoch14
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 14 	 [0.41, 0.56, 0.16] 	 lr: 2.5e-05 	 f: 82.41023652578518
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 23 [0m
Train...

[32m iter_0[0m
ga 	p: 79.19 	r: 75.47 	f1: 77.29 	 4655 	 5878 	 6168
wo 	p: 91.5 	r: 86.16 	f1: 88.75 	 2919 	 3190 	 3388
ni 	p: 83.31 	r: 71.81 	f1: 77.13 	 1093 	 1312 	 1522

[32m iter_1[0m
ga 	p: 78.99 	r: 75.76 	f1: 77.34 	 4673 	 5916 	 6168
wo 	p: 92.2 	r: 85.83 	f1: 88.9 	 2908 	 3154 	 3388
ni 	p: 86.62 	r: 70.63 	f1: 77.81 	 1075 	 1241 	 1522

[32m iter_2[0m
ga 	p: 80.01 	r: 74.84 	f1: 77.34 	 4616 	 5769 	 6168
wo 	p: 91.67 	r: 86.04 	f1: 88.76 	 2915 	 3180 	 3388
ni 	p: 86.73 	r: 70.83 	f1: 77.97 	 1078 	 1243 	 1522
best_thres [[0.18, 0.44, 0.17], [0.16, 0.78, 0.31], [0.41, 0.84, 0.33]]
f [0.8078, 0.8086, 0.8089]
load model: epoch15
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 15 	 [0.54, 0.66, 0.21] 	 lr: 2.5e-05 	 f: 81.98224898220218
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 24 [0m
Train...
loss: tensor(613.8875) lr: 6.25e-06 time: 1234.56
pred_count_train 41644

Test...
loss: tensor(570.3878) lr: 6.25e-06 time: 1284.64
pred_count_train 41644

Test...
loss: tensor(353.2401) lr: 6.25e-06 time: 1272.76
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.93 	r: 76.25 	f1: 78.05 	 4703 	 5884 	 6168
wo 	p: 93.19 	r: 85.57 	f1: 89.21 	 2899 	 3111 	 3388
ni 	p: 85.57 	r: 74.05 	f1: 79.39 	 1127 	 1317 	 1522

[32m iter_1[0m
ga 	p: 80.13 	r: 76.31 	f1: 78.18 	 4707 	 5874 	 6168
wo 	p: 92.39 	r: 86.36 	f1: 89.28 	 2926 	 3167 	 3388
ni 	p: 86.33 	r: 74.24 	f1: 79.83 	 1130 	 1309 	 1522

[32m iter_2[0m
ga 	p: 80.2 	r: 76.33 	f1: 78.22 	 4708 	 5870 	 6168
wo 	p: 92.45 	r: 86.39 	f1: 89.32 	 2927 	 3166 	 3388
ni 	p: 85.84 	r: 74.51 	f1: 79.77 	 1134 	 1321 	 1522
best_thres [[0.33, 0.56, 0.16], [0.33, 0.49, 0.15], [0.33, 0.49, 0.14]]
f [0.8162, 0.817, 0.8174]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 18 	 [0.31, 0.46, 0.15] 	 lr: 1.25e-05 	 f: 82.59886124327275
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 26 [0m
Train...

[32m iter_0[0m
ga 	p: 80.83 	r: 74.97 	f1: 77.79 	 4624 	 5721 	 6168
wo 	p: 92.24 	r: 86.66 	f1: 89.36 	 2936 	 3183 	 3388
ni 	p: 86.69 	r: 72.73 	f1: 79.1 	 1107 	 1277 	 1522

[32m iter_1[0m
ga 	p: 81.74 	r: 74.46 	f1: 77.93 	 4593 	 5619 	 6168
wo 	p: 92.49 	r: 86.48 	f1: 89.38 	 2930 	 3168 	 3388
ni 	p: 87.43 	r: 73.13 	f1: 79.64 	 1113 	 1273 	 1522

[32m iter_2[0m
ga 	p: 81.48 	r: 74.89 	f1: 78.04 	 4619 	 5669 	 6168
wo 	p: 92.65 	r: 86.36 	f1: 89.4 	 2926 	 3158 	 3388
ni 	p: 86.61 	r: 73.92 	f1: 79.76 	 1125 	 1299 	 1522
best_thres [[0.34, 0.44, 0.19], [0.55, 0.62, 0.17], [0.6, 0.73, 0.14]]
f [0.8154, 0.8162, 0.8167]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 15 	 [0.54, 0.66, 0.21] 	 lr: 2.5e-05 	 f: 81.98224898220218
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 25 [0m
Train...

[32m iter_0[0m
ga 	p: 81.82 	r: 75.68 	f1: 78.63 	 4668 	 5705 	 6168
wo 	p: 94.26 	r: 85.27 	f1: 89.54 	 2889 	 3065 	 3388
ni 	p: 85.66 	r: 76.15 	f1: 80.63 	 1159 	 1353 	 1522

[32m iter_1[0m
ga 	p: 82.16 	r: 75.62 	f1: 78.75 	 4664 	 5677 	 6168
wo 	p: 93.68 	r: 85.8 	f1: 89.57 	 2907 	 3103 	 3388
ni 	p: 87.18 	r: 75.49 	f1: 80.92 	 1149 	 1318 	 1522

[32m iter_2[0m
ga 	p: 82.16 	r: 75.63 	f1: 78.76 	 4665 	 5678 	 6168
wo 	p: 93.76 	r: 85.63 	f1: 89.51 	 2901 	 3094 	 3388
ni 	p: 87.16 	r: 75.36 	f1: 80.83 	 1147 	 1316 	 1522
best_thres [[0.46, 0.68, 0.2], [0.47, 0.61, 0.24], [0.47, 0.62, 0.24]]
f [0.8222, 0.8229, 0.823]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 14 	 [0.41, 0.56, 0.16] 	 lr: 2.5e-05 	 f: 82.41023652578518
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 24 [0m
Train...
loss: tensor(570.9398) lr: 6.25e-06 time: 1203.02
pred_count_train 41644

Test...
loss: tensor(305.2961) lr: 6.25e-06 time: 1252.94
pred_count_train 41644

Test...
loss: tensor(523.8134) lr: 6.25e-06 time: 1253.11
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.12 	r: 74.53 	f1: 78.14 	 4597 	 5598 	 6168
wo 	p: 92.46 	r: 86.45 	f1: 89.35 	 2929 	 3168 	 3388
ni 	p: 83.68 	r: 76.48 	f1: 79.92 	 1164 	 1391 	 1522

[32m iter_1[0m
ga 	p: 81.61 	r: 75.18 	f1: 78.26 	 4637 	 5682 	 6168
wo 	p: 93.18 	r: 85.86 	f1: 89.37 	 2909 	 3122 	 3388
ni 	p: 84.53 	r: 76.48 	f1: 80.3 	 1164 	 1377 	 1522

[32m iter_2[0m
ga 	p: 81.57 	r: 75.29 	f1: 78.31 	 4644 	 5693 	 6168
wo 	p: 93.01 	r: 85.98 	f1: 89.36 	 2913 	 3132 	 3388
ni 	p: 84.77 	r: 76.41 	f1: 80.37 	 1163 	 1372 	 1522
best_thres [[0.42, 0.44, 0.13], [0.38, 0.64, 0.12], [0.37, 0.5, 0.12]]
f [0.8185, 0.8189, 0.8192]
load model: epoch18
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 18 	 [0.31, 0.46, 0.15] 	 lr: 1.25e-05 	 f: 82.59886124327275
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 27 [0m
Train...

[32m iter_0[0m
ga 	p: 80.17 	r: 74.87 	f1: 77.43 	 4618 	 5760 	 6168
wo 	p: 92.11 	r: 86.51 	f1: 89.22 	 2931 	 3182 	 3388
ni 	p: 87.6 	r: 71.02 	f1: 78.45 	 1081 	 1234 	 1522

[32m iter_1[0m
ga 	p: 79.79 	r: 75.54 	f1: 77.6 	 4659 	 5839 	 6168
wo 	p: 93.34 	r: 85.57 	f1: 89.28 	 2899 	 3106 	 3388
ni 	p: 83.6 	r: 74.38 	f1: 78.72 	 1132 	 1354 	 1522

[32m iter_2[0m
ga 	p: 80.08 	r: 75.49 	f1: 77.72 	 4656 	 5814 	 6168
wo 	p: 93.05 	r: 85.71 	f1: 89.23 	 2904 	 3121 	 3388
ni 	p: 84.38 	r: 74.18 	f1: 78.95 	 1129 	 1338 	 1522
best_thres [[0.33, 0.41, 0.22], [0.43, 0.77, 0.11], [0.53, 0.82, 0.11]]
f [0.8121, 0.8126, 0.813]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 15 	 [0.54, 0.66, 0.21] 	 lr: 2.5e-05 	 f: 81.98224898220218
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 26 [0m
Train...

[32m iter_0[0m
ga 	p: 82.71 	r: 74.53 	f1: 78.41 	 4597 	 5558 	 6168
wo 	p: 94.55 	r: 84.95 	f1: 89.49 	 2878 	 3044 	 3388
ni 	p: 85.19 	r: 75.23 	f1: 79.9 	 1145 	 1344 	 1522

[32m iter_1[0m
ga 	p: 82.36 	r: 75.03 	f1: 78.53 	 4628 	 5619 	 6168
wo 	p: 94.4 	r: 85.09 	f1: 89.51 	 2883 	 3054 	 3388
ni 	p: 83.48 	r: 76.68 	f1: 79.93 	 1167 	 1398 	 1522

[32m iter_2[0m
ga 	p: 82.04 	r: 75.31 	f1: 78.53 	 4645 	 5662 	 6168
wo 	p: 94.31 	r: 85.15 	f1: 89.5 	 2885 	 3059 	 3388
ni 	p: 83.48 	r: 76.68 	f1: 79.93 	 1167 	 1398 	 1522
best_thres [[0.5, 0.72, 0.19], [0.47, 0.71, 0.14], [0.45, 0.7, 0.14]]
f [0.82, 0.8203, 0.8204]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 14 	 [0.41, 0.56, 0.16] 	 lr: 2.5e-05 	 f: 82.41023652578518
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 25 [0m
Train...
loss: tensor(701.0398) lr: 5e-06 time: 1171.77
pred_count_train 41644

Test...
loss: tensor(266.5358) lr: 6.25e-06 time: 1224.82
pred_count_train 41644

Test...
loss: tensor(482.0147) lr: 6.25e-06 time: 1217.79
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.23 	r: 76.12 	f1: 78.59 	 4695 	 5780 	 6168
wo 	p: 92.7 	r: 86.25 	f1: 89.36 	 2922 	 3152 	 3388
ni 	p: 87.03 	r: 75.82 	f1: 81.04 	 1154 	 1326 	 1522

[32m iter_1[0m
ga 	p: 82.26 	r: 75.49 	f1: 78.73 	 4656 	 5660 	 6168
wo 	p: 92.8 	r: 86.3 	f1: 89.43 	 2924 	 3151 	 3388
ni 	p: 83.93 	r: 78.91 	f1: 81.34 	 1201 	 1431 	 1522

[32m iter_2[0m
ga 	p: 80.83 	r: 76.82 	f1: 78.77 	 4738 	 5862 	 6168
wo 	p: 93.1 	r: 85.98 	f1: 89.4 	 2913 	 3129 	 3388
ni 	p: 84.08 	r: 78.78 	f1: 81.34 	 1199 	 1426 	 1522
best_thres [[0.36, 0.46, 0.19], [0.4, 0.46, 0.12], [0.32, 0.6, 0.12]]
f [0.8222, 0.823, 0.8231]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 18 	 [0.31, 0.46, 0.15] 	 lr: 1.25e-05 	 f: 82.59886124327275
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 28 [0m
Train...

[32m iter_0[0m
ga 	p: 80.76 	r: 74.68 	f1: 77.6 	 4606 	 5703 	 6168
wo 	p: 92.99 	r: 85.42 	f1: 89.05 	 2894 	 3112 	 3388
ni 	p: 86.93 	r: 71.68 	f1: 78.57 	 1091 	 1255 	 1522

[32m iter_1[0m
ga 	p: 81.28 	r: 74.68 	f1: 77.84 	 4606 	 5667 	 6168
wo 	p: 92.93 	r: 85.68 	f1: 89.16 	 2903 	 3124 	 3388
ni 	p: 87.04 	r: 72.34 	f1: 79.01 	 1101 	 1265 	 1522

[32m iter_2[0m
ga 	p: 81.13 	r: 74.79 	f1: 77.83 	 4613 	 5686 	 6168
wo 	p: 93.02 	r: 85.68 	f1: 89.2 	 2903 	 3121 	 3388
ni 	p: 87.37 	r: 72.27 	f1: 79.11 	 1100 	 1259 	 1522
best_thres [[0.29, 0.54, 0.22], [0.49, 0.73, 0.17], [0.58, 0.84, 0.17]]
f [0.8125, 0.8136, 0.8141]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 15 	 [0.54, 0.66, 0.21] 	 lr: 2.5e-05 	 f: 81.98224898220218
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 27 [0m
Train...

[32m iter_0[0m
ga 	p: 82.33 	r: 74.17 	f1: 78.04 	 4575 	 5557 	 6168
wo 	p: 93.25 	r: 85.66 	f1: 89.29 	 2902 	 3112 	 3388
ni 	p: 85.49 	r: 73.52 	f1: 79.05 	 1119 	 1309 	 1522

[32m iter_1[0m
ga 	p: 81.0 	r: 75.66 	f1: 78.24 	 4667 	 5762 	 6168
wo 	p: 93.45 	r: 85.54 	f1: 89.32 	 2898 	 3101 	 3388
ni 	p: 82.53 	r: 75.76 	f1: 79.0 	 1153 	 1397 	 1522

[32m iter_2[0m
ga 	p: 80.06 	r: 76.51 	f1: 78.25 	 4719 	 5894 	 6168
wo 	p: 92.1 	r: 86.69 	f1: 89.31 	 2937 	 3189 	 3388
ni 	p: 82.11 	r: 76.28 	f1: 79.09 	 1161 	 1414 	 1522
best_thres [[0.54, 0.62, 0.17], [0.44, 0.64, 0.11], [0.37, 0.47, 0.1]]
f [0.8165, 0.8168, 0.817]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 14 	 [0.41, 0.56, 0.16] 	 lr: 2.5e-05 	 f: 82.41023652578518
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 26 [0m
Train...
loss: tensor(661.5409) lr: 5e-06 time: 1140.98
pred_count_train 41644

Test...
loss: tensor(233.3052) lr: 6.25e-06 time: 1200.59
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.7 	r: 76.22 	f1: 78.4 	 4701 	 5825 	 6168
wo 	p: 92.8 	r: 86.01 	f1: 89.28 	 2914 	 3140 	 3388
ni 	p: 88.01 	r: 73.78 	f1: 80.27 	 1123 	 1276 	 1522

[32m iter_1[0m
ga 	p: 79.13 	r: 77.8 	f1: 78.46 	 4799 	 6065 	 6168
wo 	p: 93.12 	r: 85.95 	f1: 89.39 	 2912 	 3127 	 3388
ni 	p: 84.28 	r: 77.14 	f1: 80.55 	 1174 	 1393 	 1522

[32m iter_2[0m
ga 	p: 79.23 	r: 77.79 	f1: 78.5 	 4798 	 6056 	 6168
wo 	p: 93.33 	r: 85.89 	f1: 89.46 	 2910 	 3118 	 3388
ni 	p: 84.46 	r: 77.14 	f1: 80.63 	 1174 	 1390 	 1522
best_thres [[0.32, 0.47, 0.21], [0.23, 0.5, 0.12], [0.23, 0.63, 0.12]]
f [0.8197, 0.82, 0.8203]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 18 	 [0.31, 0.46, 0.15] 	 lr: 1.25e-05 	 f: 82.59886124327275
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 29 [0m
Train...
loss: tensor(443.6099) lr: 6.25e-06 time: 1195.17
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.24 	r: 73.43 	f1: 77.14 	 4529 	 5575 	 6168
wo 	p: 92.91 	r: 85.51 	f1: 89.06 	 2897 	 3118 	 3388
ni 	p: 85.31 	r: 70.96 	f1: 77.47 	 1080 	 1266 	 1522

[32m iter_1[0m
ga 	p: 80.04 	r: 74.84 	f1: 77.35 	 4616 	 5767 	 6168
wo 	p: 92.9 	r: 85.71 	f1: 89.16 	 2904 	 3126 	 3388
ni 	p: 85.89 	r: 71.62 	f1: 78.11 	 1090 	 1269 	 1522

[32m iter_2[0m
ga 	p: 80.31 	r: 74.85 	f1: 77.49 	 4617 	 5749 	 6168
wo 	p: 92.88 	r: 85.92 	f1: 89.27 	 2911 	 3134 	 3388
ni 	p: 86.39 	r: 71.29 	f1: 78.11 	 1085 	 1256 	 1522
best_thres [[0.34, 0.48, 0.17], [0.41, 0.67, 0.14], [0.53, 0.76, 0.14]]
f [0.8087, 0.8097, 0.8104]
load model: epoch15
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 15 	 [0.54, 0.66, 0.21] 	 lr: 2.5e-05 	 f: 81.98224898220218
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 28 [0m
Train...

[32m iter_0[0m
ga 	p: 81.84 	r: 74.9 	f1: 78.22 	 4620 	 5645 	 6168
wo 	p: 93.69 	r: 85.42 	f1: 89.36 	 2894 	 3089 	 3388
ni 	p: 84.32 	r: 74.9 	f1: 79.33 	 1140 	 1352 	 1522

[32m iter_1[0m
ga 	p: 81.31 	r: 75.66 	f1: 78.38 	 4667 	 5740 	 6168
wo 	p: 93.98 	r: 85.3 	f1: 89.43 	 2890 	 3075 	 3388
ni 	p: 83.3 	r: 75.69 	f1: 79.31 	 1152 	 1383 	 1522

[32m iter_2[0m
ga 	p: 81.36 	r: 75.73 	f1: 78.44 	 4671 	 5741 	 6168
wo 	p: 93.98 	r: 85.27 	f1: 89.42 	 2889 	 3074 	 3388
ni 	p: 83.99 	r: 75.16 	f1: 79.33 	 1144 	 1362 	 1522
best_thres [[0.46, 0.65, 0.15], [0.41, 0.66, 0.12], [0.41, 0.66, 0.13]]
f [0.8178, 0.8182, 0.8185]
load model: epoch14
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 14 	 [0.41, 0.56, 0.16] 	 lr: 2.5e-05 	 f: 82.41023652578518
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 27 [0m
Train...
loss: tensor(625.5500) lr: 5e-06 time: 1120.67
pred_count_train 41644

Test...
loss: tensor(350.6187) lr: 5e-06 time: 1175.56
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.53 	r: 76.04 	f1: 78.22 	 4690 	 5824 	 6168
wo 	p: 93.17 	r: 85.74 	f1: 89.3 	 2905 	 3118 	 3388
ni 	p: 84.71 	r: 74.97 	f1: 79.54 	 1141 	 1347 	 1522

[32m iter_1[0m
ga 	p: 79.24 	r: 77.5 	f1: 78.36 	 4780 	 6032 	 6168
wo 	p: 93.19 	r: 85.66 	f1: 89.26 	 2902 	 3114 	 3388
ni 	p: 84.66 	r: 75.43 	f1: 79.78 	 1148 	 1356 	 1522

[32m iter_2[0m
ga 	p: 79.3 	r: 77.5 	f1: 78.39 	 4780 	 6028 	 6168
wo 	p: 93.56 	r: 85.39 	f1: 89.29 	 2893 	 3092 	 3388
ni 	p: 85.4 	r: 74.97 	f1: 79.85 	 1141 	 1336 	 1522
best_thres [[0.31, 0.48, 0.15], [0.23, 0.49, 0.13], [0.23, 0.69, 0.14]]
f [0.8177, 0.818, 0.8182]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	current best epoch 18 	 [0.31, 0.46, 0.15] 	 lr: 1.25e-05 	 f: 82.59886124327275
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse[0m [33m epoch 30 [0m
Train...
loss: tensor(569.0450) lr: 5e-06 time: 1176.37
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.51 	r: 75.1 	f1: 77.71 	 4632 	 5753 	 6168
wo 	p: 93.41 	r: 85.77 	f1: 89.43 	 2906 	 3111 	 3388
ni 	p: 81.8 	r: 76.48 	f1: 79.05 	 1164 	 1423 	 1522

[32m iter_1[0m
ga 	p: 81.62 	r: 74.64 	f1: 77.97 	 4604 	 5641 	 6168
wo 	p: 93.61 	r: 85.63 	f1: 89.44 	 2901 	 3099 	 3388
ni 	p: 84.53 	r: 75.03 	f1: 79.5 	 1142 	 1351 	 1522

[32m iter_2[0m
ga 	p: 82.68 	r: 73.77 	f1: 77.97 	 4550 	 5503 	 6168
wo 	p: 92.71 	r: 86.33 	f1: 89.41 	 2925 	 3155 	 3388
ni 	p: 85.1 	r: 74.7 	f1: 79.57 	 1137 	 1336 	 1522
best_thres [[0.3, 0.5, 0.11], [0.5, 0.74, 0.12], [0.68, 0.63, 0.12]]
f [0.8146, 0.8158, 0.8163]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 15 	 [0.54, 0.66, 0.21] 	 lr: 2.5e-05 	 f: 81.98224898220218
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 29 [0m
Train...

[32m iter_0[0m
ga 	p: 82.11 	r: 75.55 	f1: 78.7 	 4660 	 5675 	 6168
wo 	p: 93.74 	r: 85.8 	f1: 89.6 	 2907 	 3101 	 3388
ni 	p: 85.75 	r: 75.89 	f1: 80.52 	 1155 	 1347 	 1522

[32m iter_1[0m
ga 	p: 82.1 	r: 75.68 	f1: 78.76 	 4668 	 5686 	 6168
wo 	p: 93.38 	r: 86.16 	f1: 89.62 	 2919 	 3126 	 3388
ni 	p: 87.2 	r: 74.77 	f1: 80.51 	 1138 	 1305 	 1522

[32m iter_2[0m
ga 	p: 82.09 	r: 75.66 	f1: 78.75 	 4667 	 5685 	 6168
wo 	p: 93.38 	r: 86.13 	f1: 89.61 	 2918 	 3125 	 3388
ni 	p: 87.2 	r: 74.77 	f1: 80.51 	 1138 	 1305 	 1522
best_thres [[0.43, 0.58, 0.18], [0.43, 0.53, 0.21], [0.43, 0.53, 0.21]]
f [0.8228, 0.823, 0.8231]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 14 	 [0.41, 0.56, 0.16] 	 lr: 2.5e-05 	 f: 82.41023652578518
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 28 [0m
Train...
loss: tensor(591.5579) lr: 5e-06 time: 1104.87
pred_count_train 41644

Test...
loss: tensor(310.6142) lr: 5e-06 time: 1150.48
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.31 	r: 76.36 	f1: 77.81 	 4710 	 5939 	 6168
wo 	p: 93.19 	r: 85.57 	f1: 89.21 	 2899 	 3111 	 3388
ni 	p: 85.47 	r: 74.57 	f1: 79.65 	 1135 	 1328 	 1522

[32m iter_1[0m
ga 	p: 80.85 	r: 75.21 	f1: 77.93 	 4639 	 5738 	 6168
wo 	p: 93.09 	r: 85.86 	f1: 89.33 	 2909 	 3125 	 3388
ni 	p: 85.52 	r: 75.3 	f1: 80.08 	 1146 	 1340 	 1522

[32m iter_2[0m
ga 	p: 80.86 	r: 75.21 	f1: 77.93 	 4639 	 5737 	 6168
wo 	p: 93.12 	r: 85.89 	f1: 89.36 	 2910 	 3125 	 3388
ni 	p: 85.71 	r: 75.3 	f1: 80.17 	 1146 	 1337 	 1522
best_thres [[0.25, 0.53, 0.2], [0.31, 0.63, 0.17], [0.31, 0.67, 0.17]]
f [0.8151, 0.8161, 0.8165]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse 	best in epoch 18 	 [0.31, 0.46, 0.15] 	 lr: 1.25e-05 	 f: 82.59886124327275
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(527.4280) lr: 5e-06 time: 1156.5
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.13 	r: 73.67 	f1: 77.67 	 4544 	 5533 	 6168
wo 	p: 94.71 	r: 84.5 	f1: 89.32 	 2863 	 3023 	 3388
ni 	p: 87.7 	r: 70.76 	f1: 78.33 	 1077 	 1228 	 1522

[32m iter_1[0m
ga 	p: 80.59 	r: 75.41 	f1: 77.91 	 4651 	 5771 	 6168
wo 	p: 92.1 	r: 86.72 	f1: 89.33 	 2938 	 3190 	 3388
ni 	p: 83.33 	r: 74.57 	f1: 78.71 	 1135 	 1362 	 1522

[32m iter_2[0m
ga 	p: 81.13 	r: 74.94 	f1: 77.91 	 4622 	 5697 	 6168
wo 	p: 92.66 	r: 86.42 	f1: 89.43 	 2928 	 3160 	 3388
ni 	p: 84.12 	r: 74.51 	f1: 79.02 	 1134 	 1348 	 1522
best_thres [[0.35, 0.64, 0.22], [0.37, 0.42, 0.1], [0.49, 0.58, 0.1]]
f [0.8133, 0.8143, 0.8149]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 15 	 [0.54, 0.66, 0.21] 	 lr: 2.5e-05 	 f: 81.98224898220218
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 30 [0m
Train...

[32m iter_0[0m
ga 	p: 80.92 	r: 76.2 	f1: 78.49 	 4700 	 5808 	 6168
wo 	p: 93.31 	r: 86.07 	f1: 89.54 	 2916 	 3125 	 3388
ni 	p: 85.78 	r: 74.9 	f1: 79.97 	 1140 	 1329 	 1522

[32m iter_1[0m
ga 	p: 81.4 	r: 75.97 	f1: 78.59 	 4686 	 5757 	 6168
wo 	p: 93.48 	r: 85.92 	f1: 89.54 	 2911 	 3114 	 3388
ni 	p: 84.12 	r: 76.22 	f1: 79.97 	 1160 	 1379 	 1522

[32m iter_2[0m
ga 	p: 81.93 	r: 75.57 	f1: 78.62 	 4661 	 5689 	 6168
wo 	p: 93.48 	r: 85.89 	f1: 89.52 	 2910 	 3113 	 3388
ni 	p: 84.43 	r: 76.22 	f1: 80.11 	 1160 	 1374 	 1522
best_thres [[0.36, 0.53, 0.18], [0.38, 0.55, 0.14], [0.41, 0.55, 0.14]]
f [0.8206, 0.8209, 0.8211]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 14 	 [0.41, 0.56, 0.16] 	 lr: 2.5e-05 	 f: 82.41023652578518
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 29 [0m
Train...
loss: tensor(3602.6238) lr: 0.0001 time: 1116.76
pred_count_train 41644

Test...
loss: tensor(277.5463) lr: 5e-06 time: 1129.09
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 76.86 	r: 71.47 	f1: 74.07 	 4408 	 5735 	 6168
wo 	p: 91.24 	r: 83.0 	f1: 86.92 	 2812 	 3082 	 3388
ni 	p: 79.13 	r: 78.71 	f1: 78.92 	 1198 	 1514 	 1522

[32m iter_1[0m
ga 	p: 76.5 	r: 71.5 	f1: 73.91 	 4410 	 5765 	 6168
wo 	p: 91.4 	r: 82.82 	f1: 86.9 	 2806 	 3070 	 3388
ni 	p: 79.52 	r: 78.32 	f1: 78.91 	 1192 	 1499 	 1522

[32m iter_2[0m
ga 	p: 76.53 	r: 71.53 	f1: 73.95 	 4412 	 5765 	 6168
wo 	p: 91.43 	r: 82.79 	f1: 86.9 	 2805 	 3068 	 3388
ni 	p: 79.52 	r: 78.32 	f1: 78.91 	 1192 	 1499 	 1522
best_thres [[0.41, 0.32, 0.2], [0.4, 0.33, 0.2], [0.4, 0.33, 0.2]]
f [0.7864, 0.7859, 0.7858]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 1 	 [0.4, 0.33, 0.2] 	 lr: 0.0001 	 f: 78.57576559605175
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 2 [0m
Train...
loss: tensor(494.2680) lr: 5e-06 time: 1131.4
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 78.91 	r: 75.99 	f1: 77.42 	 4687 	 5940 	 6168
wo 	p: 93.24 	r: 85.48 	f1: 89.19 	 2896 	 3106 	 3388
ni 	p: 86.32 	r: 71.75 	f1: 78.36 	 1092 	 1265 	 1522

[32m iter_1[0m
ga 	p: 80.85 	r: 74.45 	f1: 77.52 	 4592 	 5680 	 6168
wo 	p: 92.42 	r: 86.36 	f1: 89.29 	 2926 	 3166 	 3388
ni 	p: 87.25 	r: 71.94 	f1: 78.86 	 1095 	 1255 	 1522

[32m iter_2[0m
ga 	p: 79.94 	r: 75.65 	f1: 77.73 	 4666 	 5837 	 6168
wo 	p: 92.23 	r: 86.54 	f1: 89.29 	 2932 	 3179 	 3388
ni 	p: 85.87 	r: 73.06 	f1: 78.95 	 1112 	 1295 	 1522
best_thres [[0.24, 0.53, 0.19], [0.53, 0.58, 0.18], [0.48, 0.62, 0.13]]
f [0.8112, 0.8123, 0.813]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	current best epoch 15 	 [0.54, 0.66, 0.21] 	 lr: 2.5e-05 	 f: 81.98224898220218
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse[0m [33m epoch 31 [0m
Train...
loss: tensor(2225.7732) lr: 0.0001 time: 1119.76
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.89 	r: 76.04 	f1: 78.39 	 4690 	 5798 	 6168
wo 	p: 93.72 	r: 85.45 	f1: 89.39 	 2895 	 3089 	 3388
ni 	p: 83.98 	r: 75.43 	f1: 79.47 	 1148 	 1367 	 1522

[32m iter_1[0m
ga 	p: 80.98 	r: 76.12 	f1: 78.47 	 4695 	 5798 	 6168
wo 	p: 92.49 	r: 86.54 	f1: 89.42 	 2932 	 3170 	 3388
ni 	p: 82.61 	r: 76.15 	f1: 79.25 	 1159 	 1403 	 1522

[32m iter_2[0m
ga 	p: 80.83 	r: 76.23 	f1: 78.46 	 4702 	 5817 	 6168
wo 	p: 92.52 	r: 86.54 	f1: 89.43 	 2932 	 3169 	 3388
ni 	p: 82.93 	r: 76.28 	f1: 79.47 	 1161 	 1400 	 1522
best_thres [[0.35, 0.6, 0.14], [0.35, 0.44, 0.11], [0.34, 0.44, 0.11]]
f [0.8188, 0.819, 0.8192]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	current best epoch 14 	 [0.41, 0.56, 0.16] 	 lr: 2.5e-05 	 f: 82.41023652578518
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse[0m [33m epoch 30 [0m
Train...
loss: tensor(247.7363) lr: 5e-06 time: 1117.77
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.83 	r: 74.76 	f1: 77.21 	 4611 	 5776 	 6168
wo 	p: 91.72 	r: 85.98 	f1: 88.76 	 2913 	 3176 	 3388
ni 	p: 85.71 	r: 78.45 	f1: 81.92 	 1194 	 1393 	 1522

[32m iter_1[0m
ga 	p: 79.94 	r: 74.87 	f1: 77.32 	 4618 	 5777 	 6168
wo 	p: 91.84 	r: 86.01 	f1: 88.83 	 2914 	 3173 	 3388
ni 	p: 85.65 	r: 78.45 	f1: 81.89 	 1194 	 1394 	 1522

[32m iter_2[0m
ga 	p: 79.97 	r: 74.87 	f1: 77.33 	 4618 	 5775 	 6168
wo 	p: 91.84 	r: 86.01 	f1: 88.83 	 2914 	 3173 	 3388
ni 	p: 85.65 	r: 78.45 	f1: 81.89 	 1194 	 1394 	 1522
best_thres [[0.29, 0.29, 0.17], [0.29, 0.29, 0.17], [0.29, 0.29, 0.17]]
f [0.8139, 0.8143, 0.8144]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 2 	 [0.29, 0.29, 0.17] 	 lr: 0.0001 	 f: 81.44402085116316
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 3 [0m
Train...
loss: tensor(462.5059) lr: 5e-06 time: 1124.72
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.78 	r: 73.28 	f1: 77.3 	 4520 	 5527 	 6168
wo 	p: 92.68 	r: 85.63 	f1: 89.02 	 2901 	 3130 	 3388
ni 	p: 85.59 	r: 71.81 	f1: 78.1 	 1093 	 1277 	 1522

[32m iter_1[0m
ga 	p: 81.69 	r: 73.28 	f1: 77.26 	 4520 	 5533 	 6168
wo 	p: 92.99 	r: 85.68 	f1: 89.19 	 2903 	 3122 	 3388
ni 	p: 87.59 	r: 71.42 	f1: 78.68 	 1087 	 1241 	 1522

[32m iter_2[0m
ga 	p: 80.07 	r: 74.69 	f1: 77.29 	 4607 	 5754 	 6168
wo 	p: 93.01 	r: 85.63 	f1: 89.17 	 2901 	 3119 	 3388
ni 	p: 87.55 	r: 71.62 	f1: 78.79 	 1090 	 1245 	 1522
best_thres [[0.35, 0.45, 0.17], [0.58, 0.68, 0.18], [0.49, 0.79, 0.16]]
f [0.8104, 0.8109, 0.8111]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse 	best in epoch 15 	 [0.54, 0.66, 0.21] 	 lr: 2.5e-05 	 f: 81.98224898220218
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(1832.7944) lr: 0.0001 time: 1129.22
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.52 	r: 76.12 	f1: 78.26 	 4695 	 5831 	 6168
wo 	p: 93.95 	r: 85.3 	f1: 89.42 	 2890 	 3076 	 3388
ni 	p: 83.81 	r: 75.82 	f1: 79.61 	 1154 	 1377 	 1522

[32m iter_1[0m
ga 	p: 80.51 	r: 76.23 	f1: 78.31 	 4702 	 5840 	 6168
wo 	p: 93.63 	r: 85.51 	f1: 89.39 	 2897 	 3094 	 3388
ni 	p: 83.26 	r: 76.15 	f1: 79.55 	 1159 	 1392 	 1522

[32m iter_2[0m
ga 	p: 80.18 	r: 76.54 	f1: 78.32 	 4721 	 5888 	 6168
wo 	p: 92.11 	r: 86.81 	f1: 89.38 	 2941 	 3193 	 3388
ni 	p: 83.32 	r: 76.15 	f1: 79.57 	 1159 	 1391 	 1522
best_thres [[0.34, 0.67, 0.15], [0.33, 0.64, 0.13], [0.31, 0.42, 0.13]]
f [0.8182, 0.8183, 0.8184]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse 	best in epoch 14 	 [0.41, 0.56, 0.16] 	 lr: 2.5e-05 	 f: 82.41023652578518
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(3602.5713) lr: 0.0001 time: 1130.21
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.47 	r: 74.82 	f1: 78.0 	 4615 	 5665 	 6168
wo 	p: 92.74 	r: 86.01 	f1: 89.25 	 2914 	 3142 	 3388
ni 	p: 85.46 	r: 80.35 	f1: 82.83 	 1223 	 1431 	 1522

[32m iter_1[0m
ga 	p: 81.49 	r: 74.72 	f1: 77.96 	 4609 	 5656 	 6168
wo 	p: 92.42 	r: 86.42 	f1: 89.32 	 2928 	 3168 	 3388
ni 	p: 85.94 	r: 79.89 	f1: 82.81 	 1216 	 1415 	 1522

[32m iter_2[0m
ga 	p: 80.91 	r: 75.23 	f1: 77.96 	 4640 	 5735 	 6168
wo 	p: 92.76 	r: 86.16 	f1: 89.33 	 2919 	 3147 	 3388
ni 	p: 85.94 	r: 79.89 	f1: 82.81 	 1216 	 1415 	 1522
best_thres [[0.32, 0.53, 0.15], [0.32, 0.52, 0.16], [0.3, 0.59, 0.16]]
f [0.8212, 0.8212, 0.8211]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 3 	 [0.3, 0.59, 0.16] 	 lr: 0.0001 	 f: 82.11473565804273
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 4 [0m
Train...
loss: tensor(3602.2744) lr: 0.0001 time: 1142.57
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 76.35 	r: 71.92 	f1: 74.07 	 4436 	 5810 	 6168
wo 	p: 91.49 	r: 82.82 	f1: 86.94 	 2806 	 3067 	 3388
ni 	p: 78.66 	r: 79.7 	f1: 79.18 	 1213 	 1542 	 1522

[32m iter_1[0m
ga 	p: 76.1 	r: 72.07 	f1: 74.03 	 4445 	 5841 	 6168
wo 	p: 91.37 	r: 82.85 	f1: 86.9 	 2807 	 3072 	 3388
ni 	p: 79.87 	r: 78.45 	f1: 79.15 	 1194 	 1495 	 1522

[32m iter_2[0m
ga 	p: 76.1 	r: 72.11 	f1: 74.05 	 4448 	 5845 	 6168
wo 	p: 91.37 	r: 82.85 	f1: 86.9 	 2807 	 3072 	 3388
ni 	p: 79.02 	r: 79.17 	f1: 79.09 	 1205 	 1525 	 1522
best_thres [[0.38, 0.33, 0.19], [0.38, 0.33, 0.2], [0.38, 0.33, 0.19]]
f [0.7866, 0.7864, 0.7864]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 1 	 [0.38, 0.33, 0.19] 	 lr: 0.0001 	 f: 78.63510224330652
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 2 [0m
Train...
loss: tensor(1534.0389) lr: 0.0001 time: 1138.27
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 76.61 	r: 71.58 	f1: 74.01 	 4415 	 5763 	 6168
wo 	p: 91.21 	r: 82.97 	f1: 86.89 	 2811 	 3082 	 3388
ni 	p: 79.97 	r: 77.66 	f1: 78.8 	 1182 	 1478 	 1522

[32m iter_1[0m
ga 	p: 75.8 	r: 72.16 	f1: 73.94 	 4451 	 5872 	 6168
wo 	p: 91.17 	r: 82.91 	f1: 86.84 	 2809 	 3081 	 3388
ni 	p: 79.56 	r: 78.25 	f1: 78.9 	 1191 	 1497 	 1522

[32m iter_2[0m
ga 	p: 75.8 	r: 72.16 	f1: 73.94 	 4451 	 5872 	 6168
wo 	p: 91.14 	r: 82.88 	f1: 86.81 	 2808 	 3081 	 3388
ni 	p: 79.56 	r: 78.25 	f1: 78.9 	 1191 	 1497 	 1522
best_thres [[0.41, 0.33, 0.21], [0.38, 0.33, 0.2], [0.38, 0.33, 0.2]]
f [0.7858, 0.7854, 0.7853]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 1 	 [0.38, 0.33, 0.2] 	 lr: 0.0001 	 f: 78.5298726282638
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 2 [0m
Train...
loss: tensor(2225.4011) lr: 0.0001 time: 1118.97
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.23 	r: 75.16 	f1: 78.54 	 4636 	 5638 	 6168
wo 	p: 94.35 	r: 84.83 	f1: 89.34 	 2874 	 3046 	 3388
ni 	p: 85.97 	r: 80.55 	f1: 83.18 	 1226 	 1426 	 1522

[32m iter_1[0m
ga 	p: 82.95 	r: 74.94 	f1: 78.74 	 4622 	 5572 	 6168
wo 	p: 93.42 	r: 85.54 	f1: 89.31 	 2898 	 3102 	 3388
ni 	p: 86.32 	r: 80.03 	f1: 83.05 	 1218 	 1411 	 1522

[32m iter_2[0m
ga 	p: 82.98 	r: 74.94 	f1: 78.75 	 4622 	 5570 	 6168
wo 	p: 92.97 	r: 85.92 	f1: 89.31 	 2911 	 3131 	 3388
ni 	p: 85.95 	r: 80.35 	f1: 83.06 	 1223 	 1423 	 1522
best_thres [[0.38, 0.42, 0.22], [0.4, 0.33, 0.23], [0.4, 0.3, 0.22]]
f [0.8246, 0.8252, 0.8255]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 4 	 [0.4, 0.3, 0.22] 	 lr: 0.0001 	 f: 82.54527717023585
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 5 [0m
Train...

[32m iter_0[0m
ga 	p: 78.37 	r: 75.94 	f1: 77.13 	 4684 	 5977 	 6168
wo 	p: 92.0 	r: 85.89 	f1: 88.84 	 2910 	 3163 	 3388
ni 	p: 84.41 	r: 79.7 	f1: 81.99 	 1213 	 1437 	 1522

[32m iter_1[0m
ga 	p: 78.54 	r: 75.99 	f1: 77.24 	 4687 	 5968 	 6168
wo 	p: 92.27 	r: 85.63 	f1: 88.82 	 2901 	 3144 	 3388
ni 	p: 84.72 	r: 79.76 	f1: 82.17 	 1214 	 1433 	 1522

[32m iter_2[0m
ga 	p: 78.55 	r: 76.05 	f1: 77.28 	 4691 	 5972 	 6168
wo 	p: 91.65 	r: 86.13 	f1: 88.8 	 2918 	 3184 	 3388
ni 	p: 84.61 	r: 79.83 	f1: 82.15 	 1215 	 1436 	 1522
best_thres [[0.23, 0.29, 0.16], [0.24, 0.32, 0.16], [0.24, 0.29, 0.16]]
f [0.8134, 0.8138, 0.814]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 2 	 [0.24, 0.29, 0.16] 	 lr: 0.0001 	 f: 81.39742563281395
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 3 [0m
Train...
loss: tensor(2222.0396) lr: 0.0001 time: 1140.92
pred_count_train 41644

Test...
loss: tensor(1258.8893) lr: 0.0001 time: 1148.25
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 78.67 	r: 75.76 	f1: 77.19 	 4673 	 5940 	 6168
wo 	p: 92.72 	r: 85.33 	f1: 88.87 	 2891 	 3118 	 3388
ni 	p: 85.02 	r: 79.04 	f1: 81.92 	 1203 	 1415 	 1522

[32m iter_1[0m
ga 	p: 80.24 	r: 74.64 	f1: 77.34 	 4604 	 5738 	 6168
wo 	p: 92.6 	r: 85.33 	f1: 88.82 	 2891 	 3122 	 3388
ni 	p: 85.07 	r: 78.98 	f1: 81.91 	 1202 	 1413 	 1522

[32m iter_2[0m
ga 	p: 80.23 	r: 74.66 	f1: 77.34 	 4605 	 5740 	 6168
wo 	p: 92.6 	r: 85.33 	f1: 88.82 	 2891 	 3122 	 3388
ni 	p: 85.01 	r: 78.98 	f1: 81.88 	 1202 	 1414 	 1522
best_thres [[0.25, 0.35, 0.17], [0.3, 0.35, 0.17], [0.3, 0.35, 0.17]]
f [0.8136, 0.8141, 0.8143]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 2 	 [0.3, 0.35, 0.17] 	 lr: 0.0001 	 f: 81.4305278884462
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 3 [0m
Train...
loss: tensor(1833.9253) lr: 0.0001 time: 1110.55
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.27 	r: 76.48 	f1: 78.8 	 4717 	 5804 	 6168
wo 	p: 93.24 	r: 86.75 	f1: 89.88 	 2939 	 3152 	 3388
ni 	p: 86.56 	r: 78.25 	f1: 82.19 	 1191 	 1376 	 1522

[32m iter_1[0m
ga 	p: 81.5 	r: 76.3 	f1: 78.81 	 4706 	 5774 	 6168
wo 	p: 93.63 	r: 86.39 	f1: 89.87 	 2927 	 3126 	 3388
ni 	p: 83.79 	r: 80.81 	f1: 82.27 	 1230 	 1468 	 1522

[32m iter_2[0m
ga 	p: 81.51 	r: 76.25 	f1: 78.79 	 4703 	 5770 	 6168
wo 	p: 93.6 	r: 86.36 	f1: 89.84 	 2926 	 3126 	 3388
ni 	p: 83.82 	r: 80.68 	f1: 82.22 	 1228 	 1465 	 1522
best_thres [[0.35, 0.4, 0.36], [0.36, 0.44, 0.26], [0.36, 0.44, 0.26]]
f [0.8264, 0.8265, 0.8264]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 5 	 [0.36, 0.44, 0.26] 	 lr: 0.0001 	 f: 82.64095186250874
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 6 [0m
Train...

[32m iter_0[0m
ga 	p: 80.47 	r: 75.5 	f1: 77.91 	 4657 	 5787 	 6168
wo 	p: 93.08 	r: 85.74 	f1: 89.26 	 2905 	 3121 	 3388
ni 	p: 83.77 	r: 81.73 	f1: 82.74 	 1244 	 1485 	 1522

[32m iter_1[0m
ga 	p: 79.79 	r: 76.05 	f1: 77.88 	 4691 	 5879 	 6168
wo 	p: 93.14 	r: 85.8 	f1: 89.32 	 2907 	 3121 	 3388
ni 	p: 84.37 	r: 80.88 	f1: 82.59 	 1231 	 1459 	 1522

[32m iter_2[0m
ga 	p: 79.8 	r: 76.09 	f1: 77.9 	 4693 	 5881 	 6168
wo 	p: 93.45 	r: 85.51 	f1: 89.3 	 2897 	 3100 	 3388
ni 	p: 84.43 	r: 80.88 	f1: 82.62 	 1231 	 1458 	 1522
best_thres [[0.28, 0.56, 0.14], [0.27, 0.61, 0.15], [0.27, 0.64, 0.15]]
f [0.8203, 0.8201, 0.82]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 3 	 [0.27, 0.64, 0.15] 	 lr: 0.0001 	 f: 82.00232468035644
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 4 [0m
Train...
loss: tensor(1828.0558) lr: 0.0001 time: 1155.36
pred_count_train 41644

Test...
loss: tensor(996.6819) lr: 0.0001 time: 1170.37
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.31 	r: 75.7 	f1: 77.93 	 4669 	 5814 	 6168
wo 	p: 92.55 	r: 86.19 	f1: 89.26 	 2920 	 3155 	 3388
ni 	p: 84.48 	r: 81.21 	f1: 82.81 	 1236 	 1463 	 1522

[32m iter_1[0m
ga 	p: 79.99 	r: 75.81 	f1: 77.84 	 4676 	 5846 	 6168
wo 	p: 92.88 	r: 86.19 	f1: 89.41 	 2920 	 3144 	 3388
ni 	p: 85.35 	r: 80.03 	f1: 82.6 	 1218 	 1427 	 1522

[32m iter_2[0m
ga 	p: 79.99 	r: 75.83 	f1: 77.85 	 4677 	 5847 	 6168
wo 	p: 92.88 	r: 86.19 	f1: 89.41 	 2920 	 3144 	 3388
ni 	p: 85.35 	r: 80.03 	f1: 82.6 	 1218 	 1427 	 1522
best_thres [[0.28, 0.53, 0.13], [0.27, 0.54, 0.15], [0.27, 0.54, 0.15]]
f [0.8205, 0.8203, 0.8203]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 3 	 [0.27, 0.54, 0.15] 	 lr: 0.0001 	 f: 82.02663524596517
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 4 [0m
Train...
loss: tensor(1535.0967) lr: 0.0001 time: 1121.78
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.5 	r: 73.44 	f1: 78.15 	 4530 	 5425 	 6168
wo 	p: 93.28 	r: 85.63 	f1: 89.29 	 2901 	 3110 	 3388
ni 	p: 87.9 	r: 75.43 	f1: 81.19 	 1148 	 1306 	 1522

[32m iter_1[0m
ga 	p: 81.85 	r: 74.81 	f1: 78.17 	 4614 	 5637 	 6168
wo 	p: 93.6 	r: 85.54 	f1: 89.39 	 2898 	 3096 	 3388
ni 	p: 85.39 	r: 77.92 	f1: 81.48 	 1186 	 1389 	 1522

[32m iter_2[0m
ga 	p: 81.92 	r: 74.87 	f1: 78.24 	 4618 	 5637 	 6168
wo 	p: 93.46 	r: 85.57 	f1: 89.34 	 2899 	 3102 	 3388
ni 	p: 87.24 	r: 76.35 	f1: 81.43 	 1162 	 1332 	 1522
best_thres [[0.61, 0.49, 0.16], [0.63, 0.6, 0.1], [0.65, 0.6, 0.13]]
f [0.8202, 0.8204, 0.8205]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 5 	 [0.36, 0.44, 0.26] 	 lr: 0.0001 	 f: 82.64095186250874
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 7 [0m
Train...

[32m iter_0[0m
ga 	p: 82.44 	r: 75.03 	f1: 78.56 	 4628 	 5614 	 6168
wo 	p: 93.62 	r: 85.8 	f1: 89.54 	 2907 	 3105 	 3388
ni 	p: 85.49 	r: 80.88 	f1: 83.12 	 1231 	 1440 	 1522

[32m iter_1[0m
ga 	p: 82.22 	r: 75.15 	f1: 78.53 	 4635 	 5637 	 6168
wo 	p: 93.6 	r: 85.51 	f1: 89.37 	 2897 	 3095 	 3388
ni 	p: 85.35 	r: 81.14 	f1: 83.19 	 1235 	 1447 	 1522

[32m iter_2[0m
ga 	p: 81.31 	r: 75.94 	f1: 78.53 	 4684 	 5761 	 6168
wo 	p: 93.66 	r: 85.42 	f1: 89.35 	 2894 	 3090 	 3388
ni 	p: 85.35 	r: 81.14 	f1: 83.19 	 1235 	 1447 	 1522
best_thres [[0.39, 0.34, 0.22], [0.43, 0.38, 0.21], [0.4, 0.39, 0.21]]
f [0.8255, 0.8252, 0.825]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 4 	 [0.4, 0.39, 0.21] 	 lr: 0.0001 	 f: 82.49882573978394
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 5 [0m
Train...
loss: tensor(1525.3915) lr: 0.0001 time: 1172.69
pred_count_train 41644

Test...
loss: tensor(769.4913) lr: 0.0001 time: 1189.24
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.28 	r: 74.69 	f1: 78.75 	 4607 	 5532 	 6168
wo 	p: 93.38 	r: 86.22 	f1: 89.66 	 2921 	 3128 	 3388
ni 	p: 87.64 	r: 79.17 	f1: 83.19 	 1205 	 1375 	 1522

[32m iter_1[0m
ga 	p: 82.55 	r: 75.34 	f1: 78.78 	 4647 	 5629 	 6168
wo 	p: 93.14 	r: 86.19 	f1: 89.53 	 2920 	 3135 	 3388
ni 	p: 88.02 	r: 79.17 	f1: 83.36 	 1205 	 1369 	 1522

[32m iter_2[0m
ga 	p: 82.55 	r: 75.32 	f1: 78.77 	 4646 	 5628 	 6168
wo 	p: 93.37 	r: 85.98 	f1: 89.52 	 2913 	 3120 	 3388
ni 	p: 88.08 	r: 79.17 	f1: 83.39 	 1205 	 1368 	 1522
best_thres [[0.43, 0.31, 0.26], [0.4, 0.3, 0.26], [0.4, 0.32, 0.26]]
f [0.8273, 0.8272, 0.8271]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 4 	 [0.4, 0.32, 0.26] 	 lr: 0.0001 	 f: 82.71356151012313
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 5 [0m
Train...
loss: tensor(1264.9142) lr: 0.0001 time: 1134.25
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.05 	r: 74.94 	f1: 77.87 	 4622 	 5703 	 6168
wo 	p: 92.19 	r: 86.39 	f1: 89.2 	 2927 	 3175 	 3388
ni 	p: 85.37 	r: 74.77 	f1: 79.72 	 1138 	 1333 	 1522

[32m iter_1[0m
ga 	p: 80.49 	r: 75.45 	f1: 77.89 	 4654 	 5782 	 6168
wo 	p: 94.16 	r: 84.71 	f1: 89.19 	 2870 	 3048 	 3388
ni 	p: 84.65 	r: 75.76 	f1: 79.96 	 1153 	 1362 	 1522

[32m iter_2[0m
ga 	p: 80.46 	r: 75.39 	f1: 77.84 	 4650 	 5779 	 6168
wo 	p: 94.06 	r: 84.65 	f1: 89.11 	 2868 	 3049 	 3388
ni 	p: 84.72 	r: 75.76 	f1: 79.99 	 1153 	 1361 	 1522
best_thres [[0.33, 0.34, 0.19], [0.29, 0.73, 0.15], [0.29, 0.77, 0.15]]
f [0.8161, 0.816, 0.8158]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 5 	 [0.36, 0.44, 0.26] 	 lr: 0.0001 	 f: 82.64095186250874
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 8 [0m
Train...

[32m iter_0[0m
ga 	p: 80.83 	r: 76.13 	f1: 78.41 	 4696 	 5810 	 6168
wo 	p: 94.02 	r: 85.33 	f1: 89.46 	 2891 	 3075 	 3388
ni 	p: 86.44 	r: 79.17 	f1: 82.65 	 1205 	 1394 	 1522

[32m iter_1[0m
ga 	p: 80.01 	r: 76.85 	f1: 78.4 	 4740 	 5924 	 6168
wo 	p: 93.84 	r: 85.42 	f1: 89.43 	 2894 	 3084 	 3388
ni 	p: 85.99 	r: 79.43 	f1: 82.58 	 1209 	 1406 	 1522

[32m iter_2[0m
ga 	p: 79.49 	r: 77.37 	f1: 78.42 	 4772 	 6003 	 6168
wo 	p: 93.81 	r: 85.39 	f1: 89.4 	 2893 	 3084 	 3388
ni 	p: 86.05 	r: 79.43 	f1: 82.61 	 1209 	 1405 	 1522
best_thres [[0.32, 0.43, 0.34], [0.32, 0.48, 0.33], [0.3, 0.5, 0.33]]
f [0.8233, 0.8231, 0.823]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 4 	 [0.4, 0.39, 0.21] 	 lr: 0.0001 	 f: 82.49882573978394
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 6 [0m
Train...
loss: tensor(1249.6823) lr: 0.0001 time: 1188.98
pred_count_train 41644

Test...
loss: tensor(588.2811) lr: 0.0001 time: 1211.94
pred_count_train 41644

Test...
loss: tensor(1010.3385) lr: 0.0001 time: 1153.2
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.86 	r: 76.31 	f1: 78.52 	 4707 	 5821 	 6168
wo 	p: 91.74 	r: 87.19 	f1: 89.41 	 2954 	 3220 	 3388
ni 	p: 87.28 	r: 78.91 	f1: 82.88 	 1201 	 1376 	 1522

[32m iter_1[0m
ga 	p: 80.25 	r: 76.83 	f1: 78.51 	 4739 	 5905 	 6168
wo 	p: 92.35 	r: 86.63 	f1: 89.4 	 2935 	 3178 	 3388
ni 	p: 83.18 	r: 81.54 	f1: 82.35 	 1241 	 1492 	 1522

[32m iter_2[0m
ga 	p: 81.31 	r: 75.89 	f1: 78.51 	 4681 	 5757 	 6168
wo 	p: 94.9 	r: 84.5 	f1: 89.4 	 2863 	 3017 	 3388
ni 	p: 83.18 	r: 81.54 	f1: 82.35 	 1241 	 1492 	 1522
best_thres [[0.36, 0.31, 0.37], [0.33, 0.38, 0.23], [0.38, 0.62, 0.23]]
f [0.8246, 0.824, 0.8237]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 4 	 [0.4, 0.32, 0.26] 	 lr: 0.0001 	 f: 82.71356151012313
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 6 [0m
Train...

[32m iter_0[0m
ga 	p: 81.48 	r: 74.68 	f1: 77.93 	 4606 	 5653 	 6168
wo 	p: 91.71 	r: 85.92 	f1: 88.72 	 2911 	 3174 	 3388
ni 	p: 85.65 	r: 72.93 	f1: 78.78 	 1110 	 1296 	 1522

[32m iter_1[0m
ga 	p: 79.64 	r: 76.22 	f1: 77.89 	 4701 	 5903 	 6168
wo 	p: 92.3 	r: 85.3 	f1: 88.66 	 2890 	 3131 	 3388
ni 	p: 85.06 	r: 73.32 	f1: 78.76 	 1116 	 1312 	 1522

[32m iter_2[0m
ga 	p: 79.74 	r: 76.25 	f1: 77.95 	 4703 	 5898 	 6168
wo 	p: 91.65 	r: 85.89 	f1: 88.68 	 2910 	 3175 	 3388
ni 	p: 87.35 	r: 71.68 	f1: 78.74 	 1091 	 1249 	 1522
best_thres [[0.48, 0.43, 0.15], [0.33, 0.68, 0.12], [0.33, 0.4, 0.16]]
f [0.8138, 0.8133, 0.8134]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 5 	 [0.36, 0.44, 0.26] 	 lr: 0.0001 	 f: 82.64095186250874
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 9 [0m
Train...

[32m iter_0[0m
ga 	p: 81.39 	r: 75.66 	f1: 78.42 	 4667 	 5734 	 6168
wo 	p: 93.73 	r: 85.12 	f1: 89.22 	 2884 	 3077 	 3388
ni 	p: 85.4 	r: 77.99 	f1: 81.52 	 1187 	 1390 	 1522

[32m iter_1[0m
ga 	p: 80.65 	r: 75.68 	f1: 78.09 	 4668 	 5788 	 6168
wo 	p: 93.43 	r: 85.18 	f1: 89.12 	 2886 	 3089 	 3388
ni 	p: 89.0 	r: 75.49 	f1: 81.69 	 1149 	 1291 	 1522

[32m iter_2[0m
ga 	p: 80.07 	r: 76.23 	f1: 78.11 	 4702 	 5872 	 6168
wo 	p: 93.59 	r: 85.27 	f1: 89.24 	 2889 	 3087 	 3388
ni 	p: 89.91 	r: 74.97 	f1: 81.76 	 1141 	 1269 	 1522
best_thres [[0.41, 0.45, 0.13], [0.47, 0.54, 0.21], [0.46, 0.58, 0.24]]
f [0.8213, 0.8203, 0.8201]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 4 	 [0.4, 0.39, 0.21] 	 lr: 0.0001 	 f: 82.49882573978394
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 7 [0m
Train...
loss: tensor(986.1265) lr: 0.0001 time: 1205.47
pred_count_train 41644

Test...
loss: tensor(463.3457) lr: 0.0001 time: 1233.47
pred_count_train 41644

Test...
loss: tensor(786.8970) lr: 0.0001 time: 1178.96
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.18 	r: 76.36 	f1: 78.23 	 4710 	 5874 	 6168
wo 	p: 92.62 	r: 86.3 	f1: 89.35 	 2924 	 3157 	 3388
ni 	p: 87.54 	r: 77.99 	f1: 82.49 	 1187 	 1356 	 1522

[32m iter_1[0m
ga 	p: 80.45 	r: 76.39 	f1: 78.37 	 4712 	 5857 	 6168
wo 	p: 92.59 	r: 86.33 	f1: 89.35 	 2925 	 3159 	 3388
ni 	p: 88.58 	r: 77.46 	f1: 82.65 	 1179 	 1331 	 1522

[32m iter_2[0m
ga 	p: 80.39 	r: 76.38 	f1: 78.33 	 4711 	 5860 	 6168
wo 	p: 92.53 	r: 86.33 	f1: 89.33 	 2925 	 3161 	 3388
ni 	p: 88.58 	r: 77.46 	f1: 82.65 	 1179 	 1331 	 1522
best_thres [[0.43, 0.48, 0.2], [0.44, 0.47, 0.22], [0.44, 0.47, 0.22]]
f [0.8219, 0.8224, 0.8225]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 4 	 [0.4, 0.32, 0.26] 	 lr: 0.0001 	 f: 82.71356151012313
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 7 [0m
Train...

[32m iter_0[0m
ga 	p: 79.98 	r: 74.56 	f1: 77.18 	 4599 	 5750 	 6168
wo 	p: 93.28 	r: 85.21 	f1: 89.06 	 2887 	 3095 	 3388
ni 	p: 82.77 	r: 72.27 	f1: 77.17 	 1100 	 1329 	 1522

[32m iter_1[0m
ga 	p: 79.52 	r: 74.68 	f1: 77.02 	 4606 	 5792 	 6168
wo 	p: 92.77 	r: 85.63 	f1: 89.06 	 2901 	 3127 	 3388
ni 	p: 82.63 	r: 72.21 	f1: 77.07 	 1099 	 1330 	 1522

[32m iter_2[0m
ga 	p: 79.6 	r: 74.66 	f1: 77.05 	 4605 	 5785 	 6168
wo 	p: 92.61 	r: 85.83 	f1: 89.09 	 2908 	 3140 	 3388
ni 	p: 82.63 	r: 72.21 	f1: 77.07 	 1099 	 1330 	 1522
best_thres [[0.48, 0.67, 0.13], [0.47, 0.82, 0.11], [0.47, 0.85, 0.11]]
f [0.808, 0.8075, 0.8075]
load model: epoch5
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 5 	 [0.36, 0.44, 0.26] 	 lr: 0.0001 	 f: 82.64095186250874
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 10 [0m
Train...

[32m iter_0[0m
ga 	p: 80.97 	r: 75.19 	f1: 77.98 	 4638 	 5728 	 6168
wo 	p: 93.74 	r: 85.27 	f1: 89.3 	 2889 	 3082 	 3388
ni 	p: 84.51 	r: 75.62 	f1: 79.82 	 1151 	 1362 	 1522

[32m iter_1[0m
ga 	p: 80.44 	r: 76.09 	f1: 78.2 	 4693 	 5834 	 6168
wo 	p: 94.21 	r: 85.04 	f1: 89.39 	 2881 	 3058 	 3388
ni 	p: 84.77 	r: 75.69 	f1: 79.97 	 1152 	 1359 	 1522

[32m iter_2[0m
ga 	p: 80.85 	r: 75.7 	f1: 78.19 	 4669 	 5775 	 6168
wo 	p: 94.04 	r: 85.15 	f1: 89.37 	 2885 	 3068 	 3388
ni 	p: 85.11 	r: 75.49 	f1: 80.01 	 1149 	 1350 	 1522
best_thres [[0.34, 0.43, 0.16], [0.36, 0.6, 0.15], [0.41, 0.63, 0.15]]
f [0.8168, 0.8175, 0.8178]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 4 	 [0.4, 0.39, 0.21] 	 lr: 0.0001 	 f: 82.49882573978394
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 8 [0m
Train...
loss: tensor(749.3804) lr: 0.0001 time: 1235.22
pred_count_train 41644

Test...
loss: tensor(820.2161) lr: 5e-05 time: 1256.19
pred_count_train 41644

Test...
loss: tensor(609.3645) lr: 0.0001 time: 1204.65
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.78 	r: 75.76 	f1: 78.66 	 4673 	 5714 	 6168
wo 	p: 92.54 	r: 86.42 	f1: 89.38 	 2928 	 3164 	 3388
ni 	p: 87.83 	r: 74.44 	f1: 80.58 	 1133 	 1290 	 1522

[32m iter_1[0m
ga 	p: 81.25 	r: 76.36 	f1: 78.73 	 4710 	 5797 	 6168
wo 	p: 92.72 	r: 86.13 	f1: 89.3 	 2918 	 3147 	 3388
ni 	p: 87.88 	r: 73.85 	f1: 80.26 	 1124 	 1279 	 1522

[32m iter_2[0m
ga 	p: 81.23 	r: 76.36 	f1: 78.72 	 4710 	 5798 	 6168
wo 	p: 92.69 	r: 86.13 	f1: 89.29 	 2918 	 3148 	 3388
ni 	p: 87.81 	r: 73.85 	f1: 80.23 	 1124 	 1280 	 1522
best_thres [[0.37, 0.39, 0.28], [0.32, 0.41, 0.28], [0.32, 0.41, 0.28]]
f [0.8222, 0.822, 0.8219]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 4 	 [0.4, 0.32, 0.26] 	 lr: 0.0001 	 f: 82.71356151012313
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 8 [0m
Train...

[32m iter_0[0m
ga 	p: 80.69 	r: 75.36 	f1: 77.93 	 4648 	 5760 	 6168
wo 	p: 93.42 	r: 85.48 	f1: 89.27 	 2896 	 3100 	 3388
ni 	p: 86.74 	r: 76.48 	f1: 81.28 	 1164 	 1342 	 1522

[32m iter_1[0m
ga 	p: 79.9 	r: 76.44 	f1: 78.13 	 4715 	 5901 	 6168
wo 	p: 92.68 	r: 85.95 	f1: 89.19 	 2912 	 3142 	 3388
ni 	p: 85.67 	r: 77.79 	f1: 81.54 	 1184 	 1382 	 1522

[32m iter_2[0m
ga 	p: 79.95 	r: 76.46 	f1: 78.16 	 4716 	 5899 	 6168
wo 	p: 92.38 	r: 86.22 	f1: 89.19 	 2921 	 3162 	 3388
ni 	p: 85.71 	r: 77.66 	f1: 81.49 	 1182 	 1379 	 1522
best_thres [[0.33, 0.36, 0.21], [0.27, 0.29, 0.16], [0.27, 0.26, 0.16]]
f [0.8184, 0.819, 0.8192]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 5 	 [0.36, 0.44, 0.26] 	 lr: 0.0001 	 f: 82.64095186250874
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 11 [0m
Train...

[32m iter_0[0m
ga 	p: 80.32 	r: 75.58 	f1: 77.88 	 4662 	 5804 	 6168
wo 	p: 91.4 	r: 86.22 	f1: 88.73 	 2921 	 3196 	 3388
ni 	p: 84.03 	r: 77.79 	f1: 80.79 	 1184 	 1409 	 1522

[32m iter_1[0m
ga 	p: 79.64 	r: 76.36 	f1: 77.97 	 4710 	 5914 	 6168
wo 	p: 92.33 	r: 85.3 	f1: 88.68 	 2890 	 3130 	 3388
ni 	p: 83.29 	r: 77.92 	f1: 80.52 	 1186 	 1424 	 1522

[32m iter_2[0m
ga 	p: 80.54 	r: 75.54 	f1: 77.96 	 4659 	 5785 	 6168
wo 	p: 93.2 	r: 84.56 	f1: 88.67 	 2865 	 3074 	 3388
ni 	p: 83.5 	r: 77.79 	f1: 80.54 	 1184 	 1418 	 1522
best_thres [[0.33, 0.31, 0.15], [0.37, 0.53, 0.12], [0.52, 0.77, 0.12]]
f [0.816, 0.8158, 0.8157]
load model: epoch4
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 4 	 [0.4, 0.39, 0.21] 	 lr: 0.0001 	 f: 82.49882573978394
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 9 [0m
Train...
loss: tensor(569.9547) lr: 0.0001 time: 1265.05
pred_count_train 41644

Test...
loss: tensor(605.1066) lr: 5e-05 time: 1279.26
pred_count_train 41644

Test...
loss: tensor(1098.2582) lr: 5e-05 time: 1232.33
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.23 	r: 76.93 	f1: 78.06 	 4745 	 5989 	 6168
wo 	p: 94.37 	r: 84.09 	f1: 88.93 	 2849 	 3019 	 3388
ni 	p: 84.64 	r: 78.19 	f1: 81.28 	 1190 	 1406 	 1522

[32m iter_1[0m
ga 	p: 79.41 	r: 76.59 	f1: 77.97 	 4724 	 5949 	 6168
wo 	p: 92.57 	r: 85.71 	f1: 89.01 	 2904 	 3137 	 3388
ni 	p: 87.35 	r: 75.76 	f1: 81.14 	 1153 	 1320 	 1522

[32m iter_2[0m
ga 	p: 79.39 	r: 76.61 	f1: 77.97 	 4725 	 5952 	 6168
wo 	p: 92.51 	r: 85.63 	f1: 88.93 	 2901 	 3136 	 3388
ni 	p: 86.5 	r: 76.22 	f1: 81.03 	 1160 	 1341 	 1522
best_thres [[0.28, 0.73, 0.12], [0.29, 0.49, 0.18], [0.29, 0.49, 0.16]]
f [0.8174, 0.8174, 0.8173]
load model: epoch4
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 4 	 [0.4, 0.32, 0.26] 	 lr: 0.0001 	 f: 82.71356151012313
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 9 [0m
Train...

[32m iter_0[0m
ga 	p: 82.74 	r: 73.23 	f1: 77.7 	 4517 	 5459 	 6168
wo 	p: 91.67 	r: 86.75 	f1: 89.14 	 2939 	 3206 	 3388
ni 	p: 84.4 	r: 76.08 	f1: 80.03 	 1158 	 1372 	 1522

[32m iter_1[0m
ga 	p: 82.19 	r: 73.27 	f1: 77.47 	 4519 	 5498 	 6168
wo 	p: 91.43 	r: 86.89 	f1: 89.1 	 2944 	 3220 	 3388
ni 	p: 84.3 	r: 76.54 	f1: 80.23 	 1165 	 1382 	 1522

[32m iter_2[0m
ga 	p: 82.16 	r: 73.3 	f1: 77.47 	 4521 	 5503 	 6168
wo 	p: 91.13 	r: 87.07 	f1: 89.06 	 2950 	 3237 	 3388
ni 	p: 85.23 	r: 75.82 	f1: 80.25 	 1154 	 1354 	 1522
best_thres [[0.52, 0.62, 0.28], [0.58, 0.82, 0.24], [0.65, 0.83, 0.26]]
f [0.8159, 0.8154, 0.8152]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 5 	 [0.36, 0.44, 0.26] 	 lr: 0.0001 	 f: 82.64095186250874
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 12 [0m
Train...

[32m iter_0[0m
ga 	p: 82.15 	r: 75.05 	f1: 78.44 	 4629 	 5635 	 6168
wo 	p: 93.7 	r: 85.57 	f1: 89.45 	 2899 	 3094 	 3388
ni 	p: 85.0 	r: 78.19 	f1: 81.45 	 1190 	 1400 	 1522

[32m iter_1[0m
ga 	p: 81.06 	r: 76.28 	f1: 78.6 	 4705 	 5804 	 6168
wo 	p: 92.69 	r: 86.51 	f1: 89.5 	 2931 	 3162 	 3388
ni 	p: 83.67 	r: 79.11 	f1: 81.32 	 1204 	 1439 	 1522

[32m iter_2[0m
ga 	p: 81.57 	r: 75.86 	f1: 78.61 	 4679 	 5736 	 6168
wo 	p: 93.88 	r: 85.54 	f1: 89.51 	 2898 	 3087 	 3388
ni 	p: 83.15 	r: 79.43 	f1: 81.25 	 1209 	 1454 	 1522
best_thres [[0.4, 0.6, 0.26], [0.4, 0.56, 0.21], [0.45, 0.75, 0.19]]
f [0.8222, 0.8226, 0.8227]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 4 	 [0.4, 0.39, 0.21] 	 lr: 0.0001 	 f: 82.49882573978394
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 10 [0m
Train...
loss: tensor(1087.1741) lr: 5e-05 time: 1298.17
pred_count_train 41644

Test...
loss: tensor(453.6385) lr: 5e-05 time: 1301.61
pred_count_train 41644

Test...
loss: tensor(870.3374) lr: 5e-05 time: 1259.15
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.5 	r: 74.37 	f1: 77.77 	 4587 	 5628 	 6168
wo 	p: 92.28 	r: 85.77 	f1: 88.91 	 2906 	 3149 	 3388
ni 	p: 84.46 	r: 73.19 	f1: 78.42 	 1114 	 1319 	 1522

[32m iter_1[0m
ga 	p: 81.11 	r: 74.9 	f1: 77.88 	 4620 	 5696 	 6168
wo 	p: 91.68 	r: 86.25 	f1: 88.88 	 2922 	 3187 	 3388
ni 	p: 84.63 	r: 73.78 	f1: 78.83 	 1123 	 1327 	 1522

[32m iter_2[0m
ga 	p: 80.43 	r: 75.47 	f1: 77.87 	 4655 	 5788 	 6168
wo 	p: 91.68 	r: 86.19 	f1: 88.85 	 2920 	 3185 	 3388
ni 	p: 84.87 	r: 73.72 	f1: 78.9 	 1122 	 1322 	 1522
best_thres [[0.42, 0.35, 0.29], [0.38, 0.25, 0.23], [0.34, 0.25, 0.23]]
f [0.813, 0.8135, 0.8136]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 5 	 [0.36, 0.44, 0.26] 	 lr: 0.0001 	 f: 82.64095186250874
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 13 [0m
Train...

[32m iter_0[0m
ga 	p: 80.72 	r: 76.98 	f1: 78.8 	 4748 	 5882 	 6168
wo 	p: 93.14 	r: 86.13 	f1: 89.5 	 2918 	 3133 	 3388
ni 	p: 84.04 	r: 78.91 	f1: 81.4 	 1201 	 1429 	 1522

[32m iter_1[0m
ga 	p: 80.19 	r: 77.45 	f1: 78.8 	 4777 	 5957 	 6168
wo 	p: 92.93 	r: 86.19 	f1: 89.43 	 2920 	 3142 	 3388
ni 	p: 82.3 	r: 80.95 	f1: 81.62 	 1232 	 1497 	 1522

[32m iter_2[0m
ga 	p: 80.18 	r: 77.48 	f1: 78.81 	 4779 	 5960 	 6168
wo 	p: 93.41 	r: 85.8 	f1: 89.45 	 2907 	 3112 	 3388
ni 	p: 82.29 	r: 80.88 	f1: 81.58 	 1231 	 1496 	 1522
best_thres [[0.34, 0.6, 0.21], [0.31, 0.58, 0.16], [0.31, 0.62, 0.16]]
f [0.824, 0.824, 0.8239]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 4 	 [0.4, 0.32, 0.26] 	 lr: 0.0001 	 f: 82.71356151012313
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 10 [0m
Train...

[32m iter_0[0m
ga 	p: 81.04 	r: 74.71 	f1: 77.75 	 4608 	 5686 	 6168
wo 	p: 92.41 	r: 86.66 	f1: 89.44 	 2936 	 3177 	 3388
ni 	p: 85.91 	r: 78.91 	f1: 82.26 	 1201 	 1398 	 1522

[32m iter_1[0m
ga 	p: 81.21 	r: 74.98 	f1: 77.97 	 4625 	 5695 	 6168
wo 	p: 92.39 	r: 86.78 	f1: 89.5 	 2940 	 3182 	 3388
ni 	p: 86.19 	r: 78.71 	f1: 82.28 	 1198 	 1390 	 1522

[32m iter_2[0m
ga 	p: 81.3 	r: 74.98 	f1: 78.01 	 4625 	 5689 	 6168
wo 	p: 92.04 	r: 87.04 	f1: 89.47 	 2949 	 3204 	 3388
ni 	p: 85.24 	r: 79.3 	f1: 82.16 	 1207 	 1416 	 1522
best_thres [[0.37, 0.33, 0.19], [0.49, 0.38, 0.18], [0.54, 0.36, 0.15]]
f [0.8196, 0.8204, 0.8206]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 4 	 [0.4, 0.39, 0.21] 	 lr: 0.0001 	 f: 82.49882573978394
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 11 [0m
Train...
loss: tensor(349.6905) lr: 5e-05 time: 1318.19
pred_count_train 41644

Test...
loss: tensor(854.4760) lr: 5e-05 time: 1323.58
pred_count_train 41644

Test...
loss: tensor(696.0585) lr: 5e-05 time: 1284.0
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.18 	r: 75.03 	f1: 77.52 	 4628 	 5772 	 6168
wo 	p: 92.3 	r: 85.95 	f1: 89.01 	 2912 	 3155 	 3388
ni 	p: 82.62 	r: 72.14 	f1: 77.03 	 1098 	 1329 	 1522

[32m iter_1[0m
ga 	p: 79.54 	r: 75.57 	f1: 77.5 	 4661 	 5860 	 6168
wo 	p: 92.89 	r: 85.63 	f1: 89.11 	 2901 	 3123 	 3388
ni 	p: 84.94 	r: 70.43 	f1: 77.01 	 1072 	 1262 	 1522

[32m iter_2[0m
ga 	p: 79.17 	r: 75.79 	f1: 77.45 	 4675 	 5905 	 6168
wo 	p: 92.14 	r: 86.16 	f1: 89.05 	 2919 	 3168 	 3388
ni 	p: 85.3 	r: 70.17 	f1: 77.0 	 1068 	 1252 	 1522
best_thres [[0.57, 0.54, 0.18], [0.69, 0.85, 0.19], [0.7, 0.73, 0.19]]
f [0.8098, 0.8098, 0.8097]
load model: epoch5
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 5 	 [0.36, 0.44, 0.26] 	 lr: 0.0001 	 f: 82.64095186250874
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 14 [0m
Train...

[32m iter_0[0m
ga 	p: 81.9 	r: 75.15 	f1: 78.38 	 4635 	 5659 	 6168
wo 	p: 90.95 	r: 87.75 	f1: 89.32 	 2973 	 3269 	 3388
ni 	p: 86.33 	r: 77.99 	f1: 81.95 	 1187 	 1375 	 1522

[32m iter_1[0m
ga 	p: 81.83 	r: 75.26 	f1: 78.41 	 4642 	 5673 	 6168
wo 	p: 91.46 	r: 87.22 	f1: 89.29 	 2955 	 3231 	 3388
ni 	p: 86.87 	r: 77.79 	f1: 82.08 	 1184 	 1363 	 1522

[32m iter_2[0m
ga 	p: 81.84 	r: 75.19 	f1: 78.38 	 4638 	 5667 	 6168
wo 	p: 91.48 	r: 87.19 	f1: 89.29 	 2954 	 3229 	 3388
ni 	p: 86.8 	r: 77.79 	f1: 82.05 	 1184 	 1364 	 1522
best_thres [[0.45, 0.2, 0.25], [0.43, 0.23, 0.24], [0.43, 0.23, 0.24]]
f [0.8227, 0.8227, 0.8227]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 4 	 [0.4, 0.32, 0.26] 	 lr: 0.0001 	 f: 82.71356151012313
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 11 [0m
Train...

[32m iter_0[0m
ga 	p: 81.7 	r: 73.05 	f1: 77.14 	 4506 	 5515 	 6168
wo 	p: 92.38 	r: 85.86 	f1: 89.0 	 2909 	 3149 	 3388
ni 	p: 85.44 	r: 74.44 	f1: 79.56 	 1133 	 1326 	 1522

[32m iter_1[0m
ga 	p: 80.16 	r: 74.24 	f1: 77.09 	 4579 	 5712 	 6168
wo 	p: 91.35 	r: 86.66 	f1: 88.94 	 2936 	 3214 	 3388
ni 	p: 86.2 	r: 74.31 	f1: 79.82 	 1131 	 1312 	 1522

[32m iter_2[0m
ga 	p: 81.01 	r: 73.67 	f1: 77.17 	 4544 	 5609 	 6168
wo 	p: 91.14 	r: 86.81 	f1: 88.92 	 2941 	 3227 	 3388
ni 	p: 85.94 	r: 74.7 	f1: 79.93 	 1137 	 1323 	 1522
best_thres [[0.38, 0.6, 0.24], [0.43, 0.7, 0.27], [0.56, 0.73, 0.25]]
f [0.8115, 0.8113, 0.8116]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 4 	 [0.4, 0.39, 0.21] 	 lr: 0.0001 	 f: 82.49882573978394
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 12 [0m
Train...
loss: tensor(761.5530) lr: 2.5e-05 time: 1310.27
pred_count_train 41644

Test...
loss: tensor(671.3031) lr: 5e-05 time: 1326.95
pred_count_train 41644

Test...
loss: tensor(543.9390) lr: 5e-05 time: 1299.56
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.0 	r: 75.19 	f1: 78.45 	 4638 	 5656 	 6168
wo 	p: 93.42 	r: 86.28 	f1: 89.7 	 2923 	 3129 	 3388
ni 	p: 81.65 	r: 77.2 	f1: 79.37 	 1175 	 1439 	 1522

[32m iter_1[0m
ga 	p: 79.59 	r: 77.37 	f1: 78.46 	 4772 	 5996 	 6168
wo 	p: 93.68 	r: 86.16 	f1: 89.76 	 2919 	 3116 	 3388
ni 	p: 83.08 	r: 76.48 	f1: 79.64 	 1164 	 1401 	 1522

[32m iter_2[0m
ga 	p: 79.57 	r: 77.27 	f1: 78.4 	 4766 	 5990 	 6168
wo 	p: 93.22 	r: 86.45 	f1: 89.71 	 2929 	 3142 	 3388
ni 	p: 83.21 	r: 76.54 	f1: 79.74 	 1165 	 1400 	 1522
best_thres [[0.4, 0.62, 0.08], [0.27, 0.81, 0.08], [0.27, 0.81, 0.08]]
f [0.8202, 0.8202, 0.8201]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 5 	 [0.36, 0.44, 0.26] 	 lr: 0.0001 	 f: 82.64095186250874
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 15 [0m
Train...

[32m iter_0[0m
ga 	p: 80.6 	r: 74.77 	f1: 77.58 	 4612 	 5722 	 6168
wo 	p: 92.95 	r: 85.98 	f1: 89.33 	 2913 	 3134 	 3388
ni 	p: 86.82 	r: 73.59 	f1: 79.66 	 1120 	 1290 	 1522

[32m iter_1[0m
ga 	p: 80.75 	r: 74.61 	f1: 77.56 	 4602 	 5699 	 6168
wo 	p: 93.78 	r: 84.98 	f1: 89.16 	 2879 	 3070 	 3388
ni 	p: 85.44 	r: 74.77 	f1: 79.75 	 1138 	 1332 	 1522

[32m iter_2[0m
ga 	p: 80.76 	r: 74.58 	f1: 77.55 	 4600 	 5696 	 6168
wo 	p: 91.95 	r: 86.6 	f1: 89.19 	 2934 	 3191 	 3388
ni 	p: 84.97 	r: 75.03 	f1: 79.69 	 1142 	 1344 	 1522
best_thres [[0.42, 0.69, 0.31], [0.42, 0.79, 0.23], [0.42, 0.6, 0.22]]
f [0.8146, 0.8143, 0.8143]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 4 	 [0.4, 0.32, 0.26] 	 lr: 0.0001 	 f: 82.71356151012313
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 12 [0m
Train...

[32m iter_0[0m
ga 	p: 81.2 	r: 74.32 	f1: 77.61 	 4584 	 5645 	 6168
wo 	p: 91.44 	r: 86.1 	f1: 88.69 	 2917 	 3190 	 3388
ni 	p: 82.68 	r: 75.89 	f1: 79.14 	 1155 	 1397 	 1522

[32m iter_1[0m
ga 	p: 81.21 	r: 74.5 	f1: 77.71 	 4595 	 5658 	 6168
wo 	p: 92.04 	r: 85.63 	f1: 88.72 	 2901 	 3152 	 3388
ni 	p: 86.21 	r: 73.92 	f1: 79.59 	 1125 	 1305 	 1522

[32m iter_2[0m
ga 	p: 81.19 	r: 74.46 	f1: 77.68 	 4593 	 5657 	 6168
wo 	p: 91.24 	r: 86.33 	f1: 88.72 	 2925 	 3206 	 3388
ni 	p: 85.09 	r: 74.97 	f1: 79.71 	 1141 	 1341 	 1522
best_thres [[0.37, 0.34, 0.17], [0.57, 0.58, 0.3], [0.68, 0.43, 0.21]]
f [0.8124, 0.813, 0.8132]
load model: epoch4
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 4 	 [0.4, 0.39, 0.21] 	 lr: 0.0001 	 f: 82.49882573978394
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 13 [0m
Train...
loss: tensor(583.4603) lr: 2.5e-05 time: 1297.32
pred_count_train 41644

Test...
loss: tensor(509.7968) lr: 5e-05 time: 1324.11
pred_count_train 41644

Test...
loss: tensor(1032.0933) lr: 2.5e-05 time: 1310.72
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.24 	r: 74.72 	f1: 78.3 	 4609 	 5604 	 6168
wo 	p: 93.43 	r: 86.07 	f1: 89.6 	 2916 	 3121 	 3388
ni 	p: 85.39 	r: 75.3 	f1: 80.03 	 1146 	 1342 	 1522

[32m iter_1[0m
ga 	p: 81.62 	r: 75.52 	f1: 78.45 	 4658 	 5707 	 6168
wo 	p: 93.34 	r: 86.1 	f1: 89.57 	 2917 	 3125 	 3388
ni 	p: 85.88 	r: 75.95 	f1: 80.61 	 1156 	 1346 	 1522

[32m iter_2[0m
ga 	p: 81.76 	r: 75.34 	f1: 78.42 	 4647 	 5684 	 6168
wo 	p: 93.06 	r: 86.28 	f1: 89.54 	 2923 	 3141 	 3388
ni 	p: 86.33 	r: 75.95 	f1: 80.81 	 1156 	 1339 	 1522
best_thres [[0.44, 0.57, 0.16], [0.39, 0.75, 0.13], [0.4, 0.76, 0.13]]
f [0.8201, 0.8208, 0.8211]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 5 	 [0.36, 0.44, 0.26] 	 lr: 0.0001 	 f: 82.64095186250874
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 16 [0m
Train...

[32m iter_0[0m
ga 	p: 82.15 	r: 75.71 	f1: 78.8 	 4670 	 5685 	 6168
wo 	p: 93.37 	r: 86.45 	f1: 89.78 	 2929 	 3137 	 3388
ni 	p: 84.69 	r: 80.68 	f1: 82.64 	 1228 	 1450 	 1522

[32m iter_1[0m
ga 	p: 81.24 	r: 76.83 	f1: 78.98 	 4739 	 5833 	 6168
wo 	p: 94.17 	r: 85.86 	f1: 89.83 	 2909 	 3089 	 3388
ni 	p: 84.17 	r: 81.41 	f1: 82.77 	 1239 	 1472 	 1522

[32m iter_2[0m
ga 	p: 81.88 	r: 76.26 	f1: 78.97 	 4704 	 5745 	 6168
wo 	p: 94.47 	r: 85.68 	f1: 89.86 	 2903 	 3073 	 3388
ni 	p: 84.28 	r: 81.34 	f1: 82.78 	 1238 	 1469 	 1522
best_thres [[0.46, 0.44, 0.17], [0.49, 0.62, 0.15], [0.55, 0.67, 0.15]]
f [0.8269, 0.8273, 0.8276]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 13 	 [0.55, 0.67, 0.15] 	 lr: 2.5e-05 	 f: 82.75507501518999
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 14 [0m
Train...

[32m iter_0[0m
ga 	p: 83.56 	r: 73.85 	f1: 78.41 	 4555 	 5451 	 6168
wo 	p: 93.08 	r: 85.36 	f1: 89.05 	 2892 	 3107 	 3388
ni 	p: 85.46 	r: 75.69 	f1: 80.28 	 1152 	 1348 	 1522

[32m iter_1[0m
ga 	p: 83.94 	r: 73.62 	f1: 78.44 	 4541 	 5410 	 6168
wo 	p: 92.26 	r: 86.16 	f1: 89.1 	 2919 	 3164 	 3388
ni 	p: 84.5 	r: 77.0 	f1: 80.58 	 1172 	 1387 	 1522

[32m iter_2[0m
ga 	p: 83.98 	r: 73.59 	f1: 78.44 	 4539 	 5405 	 6168
wo 	p: 92.28 	r: 86.1 	f1: 89.08 	 2917 	 3161 	 3388
ni 	p: 84.5 	r: 77.0 	f1: 80.58 	 1172 	 1387 	 1522
best_thres [[0.67, 0.5, 0.28], [0.69, 0.37, 0.18], [0.69, 0.37, 0.18]]
f [0.8196, 0.8201, 0.8202]
load model: epoch4
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 4 	 [0.4, 0.32, 0.26] 	 lr: 0.0001 	 f: 82.71356151012313
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 13 [0m
Train...
loss: tensor(459.2535) lr: 2.5e-05 time: 1278.03
pred_count_train 41644

Test...
loss: tensor(862.5208) lr: 2.5e-05 time: 1307.57
pred_count_train 41644

Test...
loss: tensor(1021.8024) lr: 2.5e-05 time: 1313.06
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.22 	r: 74.29 	f1: 78.05 	 4582 	 5573 	 6168
wo 	p: 92.84 	r: 85.74 	f1: 89.15 	 2905 	 3129 	 3388
ni 	p: 83.28 	r: 73.32 	f1: 77.99 	 1116 	 1340 	 1522

[32m iter_1[0m
ga 	p: 81.86 	r: 74.84 	f1: 78.19 	 4616 	 5639 	 6168
wo 	p: 92.7 	r: 85.86 	f1: 89.15 	 2909 	 3138 	 3388
ni 	p: 81.5 	r: 75.56 	f1: 78.42 	 1150 	 1411 	 1522

[32m iter_2[0m
ga 	p: 81.81 	r: 74.81 	f1: 78.15 	 4614 	 5640 	 6168
wo 	p: 93.09 	r: 85.48 	f1: 89.12 	 2896 	 3111 	 3388
ni 	p: 81.36 	r: 75.43 	f1: 78.28 	 1148 	 1411 	 1522
best_thres [[0.46, 0.53, 0.16], [0.42, 0.68, 0.1], [0.42, 0.82, 0.1]]
f [0.8147, 0.8153, 0.8153]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 5 	 [0.36, 0.44, 0.26] 	 lr: 0.0001 	 f: 82.64095186250874
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 17 [0m
Train...

[32m iter_0[0m
ga 	p: 82.94 	r: 74.19 	f1: 78.32 	 4576 	 5517 	 6168
wo 	p: 92.3 	r: 86.98 	f1: 89.56 	 2947 	 3193 	 3388
ni 	p: 86.25 	r: 75.03 	f1: 80.25 	 1142 	 1324 	 1522

[32m iter_1[0m
ga 	p: 83.57 	r: 73.88 	f1: 78.43 	 4557 	 5453 	 6168
wo 	p: 92.88 	r: 86.69 	f1: 89.68 	 2937 	 3162 	 3388
ni 	p: 85.51 	r: 75.62 	f1: 80.26 	 1151 	 1346 	 1522

[32m iter_2[0m
ga 	p: 83.06 	r: 74.25 	f1: 78.41 	 4580 	 5514 	 6168
wo 	p: 92.83 	r: 86.78 	f1: 89.7 	 2940 	 3167 	 3388
ni 	p: 85.62 	r: 75.49 	f1: 80.24 	 1149 	 1342 	 1522
best_thres [[0.4, 0.43, 0.14], [0.54, 0.62, 0.11], [0.55, 0.64, 0.11]]
f [0.8209, 0.8213, 0.8214]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 13 	 [0.55, 0.67, 0.15] 	 lr: 2.5e-05 	 f: 82.75507501518999
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 15 [0m
Train...

[32m iter_0[0m
ga 	p: 81.48 	r: 76.98 	f1: 79.17 	 4748 	 5827 	 6168
wo 	p: 94.08 	r: 86.25 	f1: 89.99 	 2922 	 3106 	 3388
ni 	p: 86.94 	r: 79.17 	f1: 82.87 	 1205 	 1386 	 1522

[32m iter_1[0m
ga 	p: 81.16 	r: 77.19 	f1: 79.13 	 4761 	 5866 	 6168
wo 	p: 94.14 	r: 86.3 	f1: 90.05 	 2924 	 3106 	 3388
ni 	p: 86.02 	r: 80.03 	f1: 82.91 	 1218 	 1416 	 1522

[32m iter_2[0m
ga 	p: 81.2 	r: 77.22 	f1: 79.16 	 4763 	 5866 	 6168
wo 	p: 94.11 	r: 86.3 	f1: 90.04 	 2924 	 3107 	 3388
ni 	p: 86.03 	r: 80.09 	f1: 82.95 	 1219 	 1417 	 1522
best_thres [[0.46, 0.56, 0.22], [0.44, 0.55, 0.19], [0.44, 0.55, 0.19]]
f [0.8296, 0.8295, 0.8296]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 13 	 [0.44, 0.55, 0.19] 	 lr: 2.5e-05 	 f: 82.95844927018078
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 14 [0m
Train...
loss: tensor(362.8105) lr: 2.5e-05 time: 1251.84
pred_count_train 41644

Test...
loss: tensor(723.2512) lr: 2.5e-05 time: 1284.23
pred_count_train 41644

Test...
loss: tensor(853.1380) lr: 2.5e-05 time: 1280.79
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.39 	r: 74.66 	f1: 77.88 	 4605 	 5658 	 6168
wo 	p: 90.63 	r: 87.07 	f1: 88.82 	 2950 	 3255 	 3388
ni 	p: 84.56 	r: 74.11 	f1: 78.99 	 1128 	 1334 	 1522

[32m iter_1[0m
ga 	p: 81.23 	r: 75.1 	f1: 78.05 	 4632 	 5702 	 6168
wo 	p: 91.78 	r: 86.07 	f1: 88.83 	 2916 	 3177 	 3388
ni 	p: 85.11 	r: 73.98 	f1: 79.16 	 1126 	 1323 	 1522

[32m iter_2[0m
ga 	p: 81.35 	r: 74.98 	f1: 78.04 	 4625 	 5685 	 6168
wo 	p: 91.9 	r: 86.04 	f1: 88.87 	 2915 	 3172 	 3388
ni 	p: 85.15 	r: 73.85 	f1: 79.1 	 1124 	 1320 	 1522
best_thres [[0.47, 0.25, 0.26], [0.45, 0.35, 0.2], [0.46, 0.35, 0.2]]
f [0.8143, 0.8148, 0.8149]
load model: epoch5
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 5 	 [0.36, 0.44, 0.26] 	 lr: 0.0001 	 f: 82.64095186250874
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 18 [0m
Train...

[32m iter_0[0m
ga 	p: 81.38 	r: 75.26 	f1: 78.2 	 4642 	 5704 	 6168
wo 	p: 92.64 	r: 86.6 	f1: 89.52 	 2934 	 3167 	 3388
ni 	p: 86.33 	r: 76.74 	f1: 81.25 	 1168 	 1353 	 1522

[32m iter_1[0m
ga 	p: 81.22 	r: 75.32 	f1: 78.16 	 4646 	 5720 	 6168
wo 	p: 92.51 	r: 86.81 	f1: 89.57 	 2941 	 3179 	 3388
ni 	p: 84.95 	r: 78.65 	f1: 81.68 	 1197 	 1409 	 1522

[32m iter_2[0m
ga 	p: 80.79 	r: 75.62 	f1: 78.12 	 4664 	 5773 	 6168
wo 	p: 92.26 	r: 87.25 	f1: 89.68 	 2956 	 3204 	 3388
ni 	p: 85.92 	r: 78.19 	f1: 81.87 	 1190 	 1385 	 1522
best_thres [[0.37, 0.46, 0.18], [0.53, 0.59, 0.12], [0.56, 0.59, 0.13]]
f [0.821, 0.8212, 0.8214]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 13 	 [0.55, 0.67, 0.15] 	 lr: 2.5e-05 	 f: 82.75507501518999
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 16 [0m
Train...

[32m iter_0[0m
ga 	p: 82.82 	r: 75.28 	f1: 78.87 	 4643 	 5606 	 6168
wo 	p: 93.05 	r: 86.48 	f1: 89.64 	 2930 	 3149 	 3388
ni 	p: 87.76 	r: 74.44 	f1: 80.55 	 1133 	 1291 	 1522

[32m iter_1[0m
ga 	p: 82.83 	r: 75.39 	f1: 78.93 	 4650 	 5614 	 6168
wo 	p: 93.07 	r: 86.42 	f1: 89.62 	 2928 	 3146 	 3388
ni 	p: 87.64 	r: 74.51 	f1: 80.54 	 1134 	 1294 	 1522

[32m iter_2[0m
ga 	p: 82.62 	r: 75.55 	f1: 78.93 	 4660 	 5640 	 6168
wo 	p: 93.16 	r: 86.36 	f1: 89.63 	 2926 	 3141 	 3388
ni 	p: 87.09 	r: 74.9 	f1: 80.54 	 1140 	 1309 	 1522
best_thres [[0.45, 0.55, 0.16], [0.44, 0.55, 0.15], [0.43, 0.56, 0.14]]
f [0.8243, 0.8244, 0.8244]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 13 	 [0.44, 0.55, 0.19] 	 lr: 2.5e-05 	 f: 82.95844927018078
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 15 [0m
Train...
loss: tensor(748.3027) lr: 1.25e-05 time: 1228.96
pred_count_train 41644

Test...
loss: tensor(600.3288) lr: 2.5e-05 time: 1262.13
pred_count_train 41644

Test...
loss: tensor(711.7047) lr: 2.5e-05 time: 1251.1
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.1 	r: 76.09 	f1: 78.98 	 4693 	 5716 	 6168
wo 	p: 93.1 	r: 86.42 	f1: 89.64 	 2928 	 3145 	 3388
ni 	p: 85.79 	r: 78.52 	f1: 81.99 	 1195 	 1393 	 1522

[32m iter_1[0m
ga 	p: 82.3 	r: 76.07 	f1: 79.06 	 4692 	 5701 	 6168
wo 	p: 93.07 	r: 86.48 	f1: 89.66 	 2930 	 3148 	 3388
ni 	p: 87.43 	r: 77.66 	f1: 82.25 	 1182 	 1352 	 1522

[32m iter_2[0m
ga 	p: 82.32 	r: 76.1 	f1: 79.09 	 4694 	 5702 	 6168
wo 	p: 93.16 	r: 86.42 	f1: 89.66 	 2928 	 3143 	 3388
ni 	p: 87.29 	r: 77.6 	f1: 82.16 	 1181 	 1353 	 1522
best_thres [[0.39, 0.43, 0.15], [0.39, 0.41, 0.16], [0.39, 0.43, 0.16]]
f [0.8266, 0.827, 0.8272]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 18 	 [0.39, 0.43, 0.16] 	 lr: 1.25e-05 	 f: 82.71792383426988
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 19 [0m
Train...

[32m iter_0[0m
ga 	p: 82.55 	r: 73.25 	f1: 77.62 	 4518 	 5473 	 6168
wo 	p: 93.5 	r: 84.95 	f1: 89.02 	 2878 	 3078 	 3388
ni 	p: 84.53 	r: 71.42 	f1: 77.42 	 1087 	 1286 	 1522

[32m iter_1[0m
ga 	p: 82.33 	r: 73.88 	f1: 77.88 	 4557 	 5535 	 6168
wo 	p: 93.32 	r: 85.33 	f1: 89.15 	 2891 	 3098 	 3388
ni 	p: 82.98 	r: 72.4 	f1: 77.33 	 1102 	 1328 	 1522

[32m iter_2[0m
ga 	p: 81.46 	r: 74.51 	f1: 77.83 	 4596 	 5642 	 6168
wo 	p: 93.5 	r: 85.36 	f1: 89.25 	 2892 	 3093 	 3388
ni 	p: 84.53 	r: 71.42 	f1: 77.42 	 1087 	 1286 	 1522
best_thres [[0.46, 0.56, 0.15], [0.65, 0.75, 0.11], [0.7, 0.84, 0.12]]
f [0.8112, 0.812, 0.8123]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 13 	 [0.55, 0.67, 0.15] 	 lr: 2.5e-05 	 f: 82.75507501518999
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 17 [0m
Train...

[32m iter_0[0m
ga 	p: 82.67 	r: 74.63 	f1: 78.44 	 4603 	 5568 	 6168
wo 	p: 92.65 	r: 86.69 	f1: 89.57 	 2937 	 3170 	 3388
ni 	p: 88.26 	r: 76.08 	f1: 81.72 	 1158 	 1312 	 1522

[32m iter_1[0m
ga 	p: 83.18 	r: 74.38 	f1: 78.53 	 4588 	 5516 	 6168
wo 	p: 92.35 	r: 86.89 	f1: 89.54 	 2944 	 3188 	 3388
ni 	p: 88.42 	r: 76.28 	f1: 81.9 	 1161 	 1313 	 1522

[32m iter_2[0m
ga 	p: 83.01 	r: 74.51 	f1: 78.53 	 4596 	 5537 	 6168
wo 	p: 92.32 	r: 86.89 	f1: 89.52 	 2944 	 3189 	 3388
ni 	p: 88.36 	r: 76.28 	f1: 81.88 	 1161 	 1314 	 1522
best_thres [[0.52, 0.51, 0.22], [0.54, 0.47, 0.2], [0.53, 0.47, 0.2]]
f [0.8234, 0.8238, 0.8239]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 13 	 [0.44, 0.55, 0.19] 	 lr: 2.5e-05 	 f: 82.95844927018078
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 16 [0m
Train...
loss: tensor(618.5364) lr: 1.25e-05 time: 1201.45
pred_count_train 41644

Test...
loss: tensor(491.7569) lr: 2.5e-05 time: 1239.2
pred_count_train 41644

Test...
loss: tensor(585.7191) lr: 2.5e-05 time: 1219.87
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.86 	r: 76.82 	f1: 78.31 	 4738 	 5933 	 6168
wo 	p: 93.53 	r: 86.19 	f1: 89.71 	 2920 	 3122 	 3388
ni 	p: 85.55 	r: 78.19 	f1: 81.7 	 1190 	 1391 	 1522

[32m iter_1[0m
ga 	p: 80.38 	r: 76.67 	f1: 78.48 	 4729 	 5883 	 6168
wo 	p: 93.53 	r: 86.25 	f1: 89.74 	 2922 	 3124 	 3388
ni 	p: 84.2 	r: 79.83 	f1: 81.96 	 1215 	 1443 	 1522

[32m iter_2[0m
ga 	p: 83.51 	r: 73.96 	f1: 78.45 	 4562 	 5463 	 6168
wo 	p: 93.79 	r: 86.07 	f1: 89.76 	 2916 	 3109 	 3388
ni 	p: 84.13 	r: 79.76 	f1: 81.89 	 1214 	 1443 	 1522
best_thres [[0.33, 0.53, 0.19], [0.34, 0.65, 0.13], [0.7, 0.76, 0.13]]
f [0.8222, 0.8229, 0.8233]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 18 	 [0.39, 0.43, 0.16] 	 lr: 1.25e-05 	 f: 82.71792383426988
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 20 [0m
Train...

[32m iter_0[0m
ga 	p: 81.36 	r: 73.75 	f1: 77.37 	 4549 	 5591 	 6168
wo 	p: 93.05 	r: 84.62 	f1: 88.64 	 2867 	 3081 	 3388
ni 	p: 85.14 	r: 72.27 	f1: 78.18 	 1100 	 1292 	 1522

[32m iter_1[0m
ga 	p: 81.93 	r: 73.35 	f1: 77.4 	 4524 	 5522 	 6168
wo 	p: 92.17 	r: 85.86 	f1: 88.91 	 2909 	 3156 	 3388
ni 	p: 85.6 	r: 72.67 	f1: 78.61 	 1106 	 1292 	 1522

[32m iter_2[0m
ga 	p: 79.81 	r: 75.13 	f1: 77.4 	 4634 	 5806 	 6168
wo 	p: 92.52 	r: 85.83 	f1: 89.05 	 2908 	 3143 	 3388
ni 	p: 86.56 	r: 71.94 	f1: 78.58 	 1095 	 1265 	 1522
best_thres [[0.38, 0.46, 0.21], [0.7, 0.46, 0.19], [0.61, 0.59, 0.23]]
f [0.8094, 0.8104, 0.8107]
load model: epoch13
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 13 	 [0.55, 0.67, 0.15] 	 lr: 2.5e-05 	 f: 82.75507501518999
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 18 [0m
Train...

[32m iter_0[0m
ga 	p: 83.27 	r: 73.59 	f1: 78.13 	 4539 	 5451 	 6168
wo 	p: 92.72 	r: 86.1 	f1: 89.29 	 2917 	 3146 	 3388
ni 	p: 85.02 	r: 73.06 	f1: 78.59 	 1112 	 1308 	 1522

[32m iter_1[0m
ga 	p: 83.36 	r: 73.51 	f1: 78.13 	 4534 	 5439 	 6168
wo 	p: 92.78 	r: 86.1 	f1: 89.31 	 2917 	 3144 	 3388
ni 	p: 83.99 	r: 74.11 	f1: 78.74 	 1128 	 1343 	 1522

[32m iter_2[0m
ga 	p: 83.52 	r: 73.36 	f1: 78.11 	 4525 	 5418 	 6168
wo 	p: 92.81 	r: 86.13 	f1: 89.34 	 2918 	 3144 	 3388
ni 	p: 83.39 	r: 74.57 	f1: 78.74 	 1135 	 1361 	 1522
best_thres [[0.62, 0.55, 0.19], [0.61, 0.54, 0.15], [0.62, 0.54, 0.14]]
f [0.8167, 0.8168, 0.8168]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 13 	 [0.44, 0.55, 0.19] 	 lr: 2.5e-05 	 f: 82.95844927018078
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 17 [0m
Train...
loss: tensor(525.8293) lr: 1.25e-05 time: 1176.81
pred_count_train 41644

Test...
loss: tensor(824.8430) lr: 1.25e-05 time: 1219.07
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.07 	r: 74.87 	f1: 78.3 	 4618 	 5627 	 6168
wo 	p: 92.85 	r: 86.66 	f1: 89.65 	 2936 	 3162 	 3388
ni 	p: 85.44 	r: 76.35 	f1: 80.64 	 1162 	 1360 	 1522

[32m iter_1[0m
ga 	p: 81.66 	r: 75.31 	f1: 78.36 	 4645 	 5688 	 6168
wo 	p: 92.72 	r: 86.84 	f1: 89.68 	 2942 	 3173 	 3388
ni 	p: 85.64 	r: 77.2 	f1: 81.2 	 1175 	 1372 	 1522

[32m iter_2[0m
ga 	p: 82.04 	r: 75.03 	f1: 78.38 	 4628 	 5641 	 6168
wo 	p: 92.72 	r: 86.84 	f1: 89.68 	 2942 	 3173 	 3388
ni 	p: 85.81 	r: 77.07 	f1: 81.2 	 1173 	 1367 	 1522
best_thres [[0.39, 0.46, 0.14], [0.35, 0.44, 0.1], [0.37, 0.44, 0.1]]
f [0.8212, 0.8218, 0.822]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 18 	 [0.39, 0.43, 0.16] 	 lr: 1.25e-05 	 f: 82.71792383426988
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 21 [0m
Train...
loss: tensor(477.3695) lr: 2.5e-05 time: 1195.42
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.58 	r: 75.11 	f1: 78.67 	 4633 	 5610 	 6168
wo 	p: 92.13 	r: 87.4 	f1: 89.7 	 2961 	 3214 	 3388
ni 	p: 84.07 	r: 80.09 	f1: 82.03 	 1219 	 1450 	 1522

[32m iter_1[0m
ga 	p: 82.3 	r: 75.62 	f1: 78.82 	 4664 	 5667 	 6168
wo 	p: 93.29 	r: 86.6 	f1: 89.82 	 2934 	 3145 	 3388
ni 	p: 86.38 	r: 78.32 	f1: 82.15 	 1192 	 1380 	 1522

[32m iter_2[0m
ga 	p: 82.83 	r: 75.29 	f1: 78.88 	 4644 	 5607 	 6168
wo 	p: 93.19 	r: 86.78 	f1: 89.87 	 2940 	 3155 	 3388
ni 	p: 84.89 	r: 79.37 	f1: 82.04 	 1208 	 1423 	 1522
best_thres [[0.4, 0.33, 0.12], [0.49, 0.53, 0.15], [0.56, 0.53, 0.12]]
f [0.8255, 0.826, 0.8263]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 13 	 [0.55, 0.67, 0.15] 	 lr: 2.5e-05 	 f: 82.75507501518999
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 19 [0m
Train...

[32m iter_0[0m
ga 	p: 83.21 	r: 73.51 	f1: 78.06 	 4534 	 5449 	 6168
wo 	p: 92.85 	r: 85.8 	f1: 89.19 	 2907 	 3131 	 3388
ni 	p: 84.38 	r: 74.9 	f1: 79.36 	 1140 	 1351 	 1522

[32m iter_1[0m
ga 	p: 81.43 	r: 75.02 	f1: 78.09 	 4627 	 5682 	 6168
wo 	p: 92.06 	r: 86.25 	f1: 89.06 	 2922 	 3174 	 3388
ni 	p: 84.01 	r: 75.62 	f1: 79.6 	 1151 	 1370 	 1522

[32m iter_2[0m
ga 	p: 81.46 	r: 75.02 	f1: 78.11 	 4627 	 5680 	 6168
wo 	p: 92.03 	r: 86.28 	f1: 89.06 	 2923 	 3176 	 3388
ni 	p: 84.51 	r: 75.3 	f1: 79.64 	 1146 	 1356 	 1522
best_thres [[0.69, 0.56, 0.24], [0.57, 0.46, 0.19], [0.57, 0.45, 0.2]]
f [0.8169, 0.8168, 0.8168]
load model: epoch13
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 13 	 [0.44, 0.55, 0.19] 	 lr: 2.5e-05 	 f: 82.95844927018078
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 18 [0m
Train...
loss: tensor(452.0611) lr: 1.25e-05 time: 1154.98
pred_count_train 41644

Test...
loss: tensor(735.9783) lr: 1.25e-05 time: 1196.45
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.79 	r: 74.01 	f1: 78.15 	 4565 	 5514 	 6168
wo 	p: 93.19 	r: 85.98 	f1: 89.44 	 2913 	 3126 	 3388
ni 	p: 81.6 	r: 77.2 	f1: 79.34 	 1175 	 1440 	 1522

[32m iter_1[0m
ga 	p: 81.15 	r: 75.54 	f1: 78.24 	 4659 	 5741 	 6168
wo 	p: 93.18 	r: 85.92 	f1: 89.4 	 2911 	 3124 	 3388
ni 	p: 84.12 	r: 75.89 	f1: 79.79 	 1155 	 1373 	 1522

[32m iter_2[0m
ga 	p: 80.84 	r: 75.79 	f1: 78.24 	 4675 	 5783 	 6168
wo 	p: 92.88 	r: 86.22 	f1: 89.42 	 2921 	 3145 	 3388
ni 	p: 83.86 	r: 76.15 	f1: 79.82 	 1159 	 1382 	 1522
best_thres [[0.47, 0.57, 0.14], [0.35, 0.81, 0.14], [0.33, 0.8, 0.13]]
f [0.8179, 0.8183, 0.8184]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 18 	 [0.39, 0.43, 0.16] 	 lr: 1.25e-05 	 f: 82.71792383426988
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 22 [0m
Train...
loss: tensor(816.2125) lr: 1.25e-05 time: 1178.11
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.28 	r: 74.9 	f1: 78.42 	 4620 	 5615 	 6168
wo 	p: 93.39 	r: 86.3 	f1: 89.71 	 2924 	 3131 	 3388
ni 	p: 86.75 	r: 77.46 	f1: 81.85 	 1179 	 1359 	 1522

[32m iter_1[0m
ga 	p: 81.52 	r: 75.88 	f1: 78.6 	 4680 	 5741 	 6168
wo 	p: 92.97 	r: 86.98 	f1: 89.87 	 2947 	 3170 	 3388
ni 	p: 87.59 	r: 77.0 	f1: 81.96 	 1172 	 1338 	 1522

[32m iter_2[0m
ga 	p: 79.9 	r: 77.32 	f1: 78.59 	 4769 	 5969 	 6168
wo 	p: 92.94 	r: 86.98 	f1: 89.86 	 2947 	 3171 	 3388
ni 	p: 87.3 	r: 77.2 	f1: 81.94 	 1175 	 1346 	 1522
best_thres [[0.42, 0.54, 0.24], [0.5, 0.59, 0.27], [0.41, 0.63, 0.26]]
f [0.8236, 0.8244, 0.8245]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 13 	 [0.55, 0.67, 0.15] 	 lr: 2.5e-05 	 f: 82.75507501518999
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 20 [0m
Train...

[32m iter_0[0m
ga 	p: 83.08 	r: 75.47 	f1: 79.09 	 4655 	 5603 	 6168
wo 	p: 93.9 	r: 86.39 	f1: 89.99 	 2927 	 3117 	 3388
ni 	p: 86.92 	r: 78.19 	f1: 82.32 	 1190 	 1369 	 1522

[32m iter_1[0m
ga 	p: 82.98 	r: 75.57 	f1: 79.1 	 4661 	 5617 	 6168
wo 	p: 93.81 	r: 86.33 	f1: 89.92 	 2925 	 3118 	 3388
ni 	p: 86.57 	r: 78.38 	f1: 82.28 	 1193 	 1378 	 1522

[32m iter_2[0m
ga 	p: 83.0 	r: 75.57 	f1: 79.11 	 4661 	 5616 	 6168
wo 	p: 92.62 	r: 87.37 	f1: 89.91 	 2960 	 3196 	 3388
ni 	p: 86.13 	r: 78.71 	f1: 82.25 	 1198 	 1391 	 1522
best_thres [[0.46, 0.54, 0.18], [0.45, 0.53, 0.16], [0.45, 0.41, 0.15]]
f [0.8288, 0.8287, 0.8287]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 13 	 [0.44, 0.55, 0.19] 	 lr: 2.5e-05 	 f: 82.95844927018078
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 19 [0m
Train...
loss: tensor(382.9460) lr: 1.25e-05 time: 1143.9
pred_count_train 41644

Test...
loss: tensor(652.6098) lr: 1.25e-05 time: 1178.23
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.92 	r: 74.27 	f1: 77.91 	 4581 	 5592 	 6168
wo 	p: 93.72 	r: 84.98 	f1: 89.13 	 2879 	 3072 	 3388
ni 	p: 85.37 	r: 73.59 	f1: 79.04 	 1120 	 1312 	 1522

[32m iter_1[0m
ga 	p: 82.45 	r: 74.11 	f1: 78.06 	 4571 	 5544 	 6168
wo 	p: 93.5 	r: 85.39 	f1: 89.26 	 2893 	 3094 	 3388
ni 	p: 85.74 	r: 73.85 	f1: 79.35 	 1124 	 1311 	 1522

[32m iter_2[0m
ga 	p: 82.07 	r: 74.42 	f1: 78.05 	 4590 	 5593 	 6168
wo 	p: 93.35 	r: 85.36 	f1: 89.18 	 2892 	 3098 	 3388
ni 	p: 86.01 	r: 73.52 	f1: 79.28 	 1119 	 1301 	 1522
best_thres [[0.39, 0.62, 0.2], [0.41, 0.8, 0.16], [0.38, 0.85, 0.16]]
f [0.815, 0.816, 0.8161]
load model: epoch18
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 18 	 [0.39, 0.43, 0.16] 	 lr: 1.25e-05 	 f: 82.71792383426988
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 23 [0m
Train...
loss: tensor(727.1508) lr: 1.25e-05 time: 1162.58
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.26 	r: 74.66 	f1: 78.28 	 4605 	 5598 	 6168
wo 	p: 93.24 	r: 86.33 	f1: 89.66 	 2925 	 3137 	 3388
ni 	p: 85.69 	r: 77.53 	f1: 81.41 	 1180 	 1377 	 1522

[32m iter_1[0m
ga 	p: 81.52 	r: 75.58 	f1: 78.44 	 4662 	 5719 	 6168
wo 	p: 93.24 	r: 86.3 	f1: 89.64 	 2924 	 3136 	 3388
ni 	p: 86.08 	r: 77.6 	f1: 81.62 	 1181 	 1372 	 1522

[32m iter_2[0m
ga 	p: 82.34 	r: 74.84 	f1: 78.41 	 4616 	 5606 	 6168
wo 	p: 92.38 	r: 86.95 	f1: 89.58 	 2946 	 3189 	 3388
ni 	p: 84.37 	r: 79.11 	f1: 81.65 	 1204 	 1427 	 1522
best_thres [[0.38, 0.53, 0.16], [0.48, 0.7, 0.14], [0.65, 0.64, 0.1]]
f [0.8221, 0.8225, 0.8227]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 13 	 [0.55, 0.67, 0.15] 	 lr: 2.5e-05 	 f: 82.75507501518999
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 21 [0m
Train...

[32m iter_0[0m
ga 	p: 81.88 	r: 75.81 	f1: 78.73 	 4676 	 5711 	 6168
wo 	p: 93.57 	r: 86.33 	f1: 89.81 	 2925 	 3126 	 3388
ni 	p: 87.41 	r: 77.53 	f1: 82.17 	 1180 	 1350 	 1522

[32m iter_1[0m
ga 	p: 81.7 	r: 76.05 	f1: 78.77 	 4691 	 5742 	 6168
wo 	p: 93.74 	r: 86.13 	f1: 89.77 	 2918 	 3113 	 3388
ni 	p: 88.19 	r: 77.0 	f1: 82.22 	 1172 	 1329 	 1522

[32m iter_2[0m
ga 	p: 81.62 	r: 76.15 	f1: 78.79 	 4697 	 5755 	 6168
wo 	p: 93.77 	r: 86.13 	f1: 89.78 	 2918 	 3112 	 3388
ni 	p: 85.46 	r: 79.17 	f1: 82.2 	 1205 	 1410 	 1522
best_thres [[0.47, 0.59, 0.26], [0.45, 0.6, 0.28], [0.44, 0.6, 0.19]]
f [0.8259, 0.8259, 0.826]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 13 	 [0.44, 0.55, 0.19] 	 lr: 2.5e-05 	 f: 82.95844927018078
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 20 [0m
Train...
loss: tensor(605.6203) lr: 6.25e-06 time: 1131.91
pred_count_train 41644

Test...
loss: tensor(578.3471) lr: 1.25e-05 time: 1158.63
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.09 	r: 75.75 	f1: 78.79 	 4672 	 5691 	 6168
wo 	p: 93.62 	r: 86.16 	f1: 89.73 	 2919 	 3118 	 3388
ni 	p: 84.63 	r: 79.24 	f1: 81.85 	 1206 	 1425 	 1522

[32m iter_1[0m
ga 	p: 81.54 	r: 76.35 	f1: 78.86 	 4709 	 5775 	 6168
wo 	p: 93.48 	r: 86.33 	f1: 89.77 	 2925 	 3129 	 3388
ni 	p: 86.02 	r: 78.84 	f1: 82.28 	 1200 	 1395 	 1522

[32m iter_2[0m
ga 	p: 81.61 	r: 76.33 	f1: 78.88 	 4708 	 5769 	 6168
wo 	p: 93.14 	r: 86.57 	f1: 89.74 	 2933 	 3149 	 3388
ni 	p: 84.28 	r: 80.29 	f1: 82.23 	 1222 	 1450 	 1522
best_thres [[0.39, 0.5, 0.15], [0.35, 0.49, 0.15], [0.35, 0.46, 0.11]]
f [0.8255, 0.826, 0.8262]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 18 	 [0.39, 0.43, 0.16] 	 lr: 1.25e-05 	 f: 82.71792383426988
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 24 [0m
Train...
loss: tensor(646.2133) lr: 1.25e-05 time: 1146.87
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.54 	r: 75.6 	f1: 77.99 	 4663 	 5790 	 6168
wo 	p: 93.78 	r: 85.48 	f1: 89.44 	 2896 	 3088 	 3388
ni 	p: 83.67 	r: 76.08 	f1: 79.7 	 1158 	 1384 	 1522

[32m iter_1[0m
ga 	p: 81.94 	r: 74.5 	f1: 78.04 	 4595 	 5608 	 6168
wo 	p: 93.17 	r: 86.1 	f1: 89.49 	 2917 	 3131 	 3388
ni 	p: 85.53 	r: 75.36 	f1: 80.13 	 1147 	 1341 	 1522

[32m iter_2[0m
ga 	p: 82.26 	r: 74.22 	f1: 78.04 	 4578 	 5565 	 6168
wo 	p: 93.39 	r: 85.92 	f1: 89.5 	 2911 	 3117 	 3388
ni 	p: 88.3 	r: 73.39 	f1: 80.16 	 1117 	 1265 	 1522
best_thres [[0.29, 0.61, 0.17], [0.55, 0.72, 0.19], [0.69, 0.83, 0.3]]
f [0.817, 0.8177, 0.818]
load model: epoch13
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 13 	 [0.55, 0.67, 0.15] 	 lr: 2.5e-05 	 f: 82.75507501518999
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 22 [0m
Train...

[32m iter_0[0m
ga 	p: 82.9 	r: 74.58 	f1: 78.52 	 4600 	 5549 	 6168
wo 	p: 93.67 	r: 86.1 	f1: 89.73 	 2917 	 3114 	 3388
ni 	p: 86.12 	r: 78.25 	f1: 82.0 	 1191 	 1383 	 1522

[32m iter_1[0m
ga 	p: 81.41 	r: 76.09 	f1: 78.66 	 4693 	 5765 	 6168
wo 	p: 93.5 	r: 86.13 	f1: 89.66 	 2918 	 3121 	 3388
ni 	p: 85.42 	r: 78.52 	f1: 81.82 	 1195 	 1399 	 1522

[32m iter_2[0m
ga 	p: 81.41 	r: 76.12 	f1: 78.68 	 4695 	 5767 	 6168
wo 	p: 93.53 	r: 86.16 	f1: 89.69 	 2919 	 3121 	 3388
ni 	p: 85.3 	r: 78.52 	f1: 81.77 	 1195 	 1401 	 1522
best_thres [[0.5, 0.63, 0.16], [0.38, 0.61, 0.13], [0.38, 0.61, 0.13]]
f [0.8245, 0.8244, 0.8245]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 13 	 [0.44, 0.55, 0.19] 	 lr: 2.5e-05 	 f: 82.95844927018078
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 21 [0m
Train...
loss: tensor(552.8090) lr: 6.25e-06 time: 1123.16
pred_count_train 41644

Test...
loss: tensor(808.0954) lr: 6.25e-06 time: 1140.81
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.56 	r: 75.75 	f1: 78.55 	 4672 	 5728 	 6168
wo 	p: 94.17 	r: 85.74 	f1: 89.76 	 2905 	 3085 	 3388
ni 	p: 82.58 	r: 79.11 	f1: 80.81 	 1204 	 1458 	 1522

[32m iter_1[0m
ga 	p: 80.77 	r: 76.75 	f1: 78.71 	 4734 	 5861 	 6168
wo 	p: 93.45 	r: 86.28 	f1: 89.72 	 2923 	 3128 	 3388
ni 	p: 83.9 	r: 79.11 	f1: 81.43 	 1204 	 1435 	 1522

[32m iter_2[0m
ga 	p: 80.76 	r: 76.69 	f1: 78.67 	 4730 	 5857 	 6168
wo 	p: 93.45 	r: 86.3 	f1: 89.73 	 2924 	 3129 	 3388
ni 	p: 83.3 	r: 79.63 	f1: 81.42 	 1212 	 1455 	 1522
best_thres [[0.39, 0.59, 0.11], [0.33, 0.66, 0.1], [0.33, 0.72, 0.09]]
f [0.8226, 0.8234, 0.8236]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 18 	 [0.39, 0.43, 0.16] 	 lr: 1.25e-05 	 f: 82.71792383426988
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 25 [0m
Train...
loss: tensor(574.6782) lr: 1.25e-05 time: 1130.1
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.45 	r: 75.39 	f1: 78.76 	 4650 	 5640 	 6168
wo 	p: 92.46 	r: 87.63 	f1: 89.98 	 2969 	 3211 	 3388
ni 	p: 87.24 	r: 78.58 	f1: 82.68 	 1196 	 1371 	 1522

[32m iter_1[0m
ga 	p: 83.32 	r: 75.02 	f1: 78.95 	 4627 	 5553 	 6168
wo 	p: 92.32 	r: 87.69 	f1: 89.95 	 2971 	 3218 	 3388
ni 	p: 87.51 	r: 78.71 	f1: 82.88 	 1198 	 1369 	 1522

[32m iter_2[0m
ga 	p: 82.83 	r: 75.41 	f1: 78.94 	 4651 	 5615 	 6168
wo 	p: 92.29 	r: 87.66 	f1: 89.92 	 2970 	 3218 	 3388
ni 	p: 87.16 	r: 78.98 	f1: 82.87 	 1202 	 1379 	 1522
best_thres [[0.39, 0.38, 0.18], [0.54, 0.43, 0.17], [0.55, 0.44, 0.16]]
f [0.8277, 0.8284, 0.8285]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 22 	 [0.55, 0.44, 0.16] 	 lr: 6.25e-06 	 f: 82.85481444333
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 23 [0m
Train...
loss: tensor(502.3841) lr: 6.25e-06 time: 1128.86
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.74 	r: 76.04 	f1: 78.32 	 4690 	 5809 	 6168
wo 	p: 93.09 	r: 86.28 	f1: 89.55 	 2923 	 3140 	 3388
ni 	p: 84.95 	r: 76.74 	f1: 80.64 	 1168 	 1375 	 1522

[32m iter_1[0m
ga 	p: 81.12 	r: 75.81 	f1: 78.38 	 4676 	 5764 	 6168
wo 	p: 93.9 	r: 85.45 	f1: 89.48 	 2895 	 3083 	 3388
ni 	p: 84.66 	r: 76.87 	f1: 80.58 	 1170 	 1382 	 1522

[32m iter_2[0m
ga 	p: 81.14 	r: 75.81 	f1: 78.38 	 4676 	 5763 	 6168
wo 	p: 94.4 	r: 85.04 	f1: 89.47 	 2881 	 3052 	 3388
ni 	p: 84.72 	r: 76.87 	f1: 80.61 	 1170 	 1381 	 1522
best_thres [[0.39, 0.62, 0.2], [0.41, 0.71, 0.18], [0.41, 0.75, 0.18]]
f [0.8206, 0.8205, 0.8205]
load model: epoch13
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 13 	 [0.44, 0.55, 0.19] 	 lr: 2.5e-05 	 f: 82.95844927018078
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 22 [0m
Train...
loss: tensor(754.0327) lr: 6.25e-06 time: 1129.26
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.28 	r: 75.0 	f1: 78.47 	 4626 	 5622 	 6168
wo 	p: 93.87 	r: 85.83 	f1: 89.67 	 2908 	 3098 	 3388
ni 	p: 83.87 	r: 76.87 	f1: 80.22 	 1170 	 1395 	 1522

[32m iter_1[0m
ga 	p: 80.45 	r: 76.88 	f1: 78.63 	 4742 	 5894 	 6168
wo 	p: 93.99 	r: 85.83 	f1: 89.73 	 2908 	 3094 	 3388
ni 	p: 84.51 	r: 78.12 	f1: 81.19 	 1189 	 1407 	 1522

[32m iter_2[0m
ga 	p: 80.52 	r: 76.86 	f1: 78.65 	 4741 	 5888 	 6168
wo 	p: 93.69 	r: 85.95 	f1: 89.66 	 2912 	 3108 	 3388
ni 	p: 85.11 	r: 77.73 	f1: 81.25 	 1183 	 1390 	 1522
best_thres [[0.43, 0.61, 0.15], [0.31, 0.8, 0.12], [0.31, 0.81, 0.13]]
f [0.8214, 0.8223, 0.8227]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 18 	 [0.39, 0.43, 0.16] 	 lr: 1.25e-05 	 f: 82.71792383426988
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 26 [0m
Train...
loss: tensor(800.3573) lr: 6.25e-06 time: 1130.84
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.73 	r: 76.54 	f1: 78.58 	 4721 	 5848 	 6168
wo 	p: 93.44 	r: 86.19 	f1: 89.67 	 2920 	 3125 	 3388
ni 	p: 85.98 	r: 78.19 	f1: 81.9 	 1190 	 1384 	 1522

[32m iter_1[0m
ga 	p: 83.23 	r: 74.61 	f1: 78.69 	 4602 	 5529 	 6168
wo 	p: 93.53 	r: 86.19 	f1: 89.71 	 2920 	 3122 	 3388
ni 	p: 84.59 	r: 79.7 	f1: 82.07 	 1213 	 1434 	 1522

[32m iter_2[0m
ga 	p: 83.76 	r: 74.17 	f1: 78.68 	 4575 	 5462 	 6168
wo 	p: 93.36 	r: 86.36 	f1: 89.73 	 2926 	 3134 	 3388
ni 	p: 86.56 	r: 78.25 	f1: 82.19 	 1191 	 1376 	 1522
best_thres [[0.28, 0.5, 0.19], [0.52, 0.63, 0.14], [0.61, 0.63, 0.18]]
f [0.824, 0.8247, 0.8251]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 22 	 [0.55, 0.44, 0.16] 	 lr: 6.25e-06 	 f: 82.85481444333
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 24 [0m
Train...
loss: tensor(459.9192) lr: 6.25e-06 time: 1139.47
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.89 	r: 78.21 	f1: 79.04 	 4824 	 6038 	 6168
wo 	p: 93.92 	r: 86.19 	f1: 89.89 	 2920 	 3109 	 3388
ni 	p: 87.11 	r: 79.04 	f1: 82.88 	 1203 	 1381 	 1522

[32m iter_1[0m
ga 	p: 79.8 	r: 78.47 	f1: 79.13 	 4840 	 6065 	 6168
wo 	p: 92.61 	r: 87.34 	f1: 89.9 	 2959 	 3195 	 3388
ni 	p: 87.12 	r: 79.11 	f1: 82.92 	 1204 	 1382 	 1522

[32m iter_2[0m
ga 	p: 79.77 	r: 78.45 	f1: 79.11 	 4839 	 6066 	 6168
wo 	p: 92.45 	r: 87.49 	f1: 89.9 	 2964 	 3206 	 3388
ni 	p: 86.98 	r: 79.04 	f1: 82.82 	 1203 	 1383 	 1522
best_thres [[0.31, 0.56, 0.18], [0.29, 0.42, 0.17], [0.29, 0.41, 0.17]]
f [0.8282, 0.8286, 0.8287]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 13 	 [0.44, 0.55, 0.19] 	 lr: 2.5e-05 	 f: 82.95844927018078
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 23 [0m
Train...
loss: tensor(710.6591) lr: 6.25e-06 time: 1120.25
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.76 	r: 75.13 	f1: 78.3 	 4634 	 5668 	 6168
wo 	p: 93.52 	r: 86.04 	f1: 89.62 	 2915 	 3117 	 3388
ni 	p: 83.88 	r: 76.22 	f1: 79.86 	 1160 	 1383 	 1522

[32m iter_1[0m
ga 	p: 81.14 	r: 75.75 	f1: 78.35 	 4672 	 5758 	 6168
wo 	p: 94.08 	r: 85.42 	f1: 89.54 	 2894 	 3076 	 3388
ni 	p: 85.32 	r: 76.35 	f1: 80.58 	 1162 	 1362 	 1522

[32m iter_2[0m
ga 	p: 81.16 	r: 75.71 	f1: 78.34 	 4670 	 5754 	 6168
wo 	p: 93.08 	r: 86.22 	f1: 89.52 	 2921 	 3138 	 3388
ni 	p: 85.04 	r: 76.54 	f1: 80.57 	 1165 	 1370 	 1522
best_thres [[0.4, 0.57, 0.17], [0.35, 0.84, 0.15], [0.35, 0.73, 0.14]]
f [0.8198, 0.8202, 0.8203]
load model: epoch18
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 18 	 [0.39, 0.43, 0.16] 	 lr: 1.25e-05 	 f: 82.71792383426988
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 27 [0m
Train...

[32m iter_0[0m
ga 	p: 82.58 	r: 74.94 	f1: 78.57 	 4622 	 5597 	 6168
wo 	p: 92.89 	r: 86.72 	f1: 89.7 	 2938 	 3163 	 3388
ni 	p: 86.96 	r: 76.68 	f1: 81.49 	 1167 	 1342 	 1522

[32m iter_1[0m
ga 	p: 83.22 	r: 74.46 	f1: 78.6 	 4593 	 5519 	 6168
wo 	p: 92.21 	r: 87.34 	f1: 89.71 	 2959 	 3209 	 3388
ni 	p: 88.64 	r: 75.89 	f1: 81.77 	 1155 	 1303 	 1522

[32m iter_2[0m
ga 	p: 81.88 	r: 75.62 	f1: 78.62 	 4664 	 5696 	 6168
wo 	p: 92.45 	r: 87.07 	f1: 89.68 	 2950 	 3191 	 3388
ni 	p: 88.62 	r: 75.76 	f1: 81.69 	 1153 	 1301 	 1522
best_thres [[0.38, 0.43, 0.21], [0.57, 0.45, 0.25], [0.52, 0.52, 0.25]]
f [0.8241, 0.8245, 0.8245]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 22 	 [0.55, 0.44, 0.16] 	 lr: 6.25e-06 	 f: 82.85481444333
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 25 [0m
Train...
loss: tensor(747.4321) lr: 6.25e-06 time: 1135.63
pred_count_train 41644

Test...
loss: tensor(602.3525) lr: 5e-06 time: 1151.92
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.77 	r: 75.0 	f1: 78.69 	 4626 	 5589 	 6168
wo 	p: 94.02 	r: 85.8 	f1: 89.72 	 2907 	 3092 	 3388
ni 	p: 86.35 	r: 78.58 	f1: 82.28 	 1196 	 1385 	 1522

[32m iter_1[0m
ga 	p: 79.75 	r: 77.71 	f1: 78.72 	 4793 	 6010 	 6168
wo 	p: 94.29 	r: 85.36 	f1: 89.6 	 2892 	 3067 	 3388
ni 	p: 87.23 	r: 78.12 	f1: 82.43 	 1189 	 1363 	 1522

[32m iter_2[0m
ga 	p: 79.75 	r: 77.72 	f1: 78.73 	 4794 	 6011 	 6168
wo 	p: 94.32 	r: 85.36 	f1: 89.62 	 2892 	 3066 	 3388
ni 	p: 87.12 	r: 78.19 	f1: 82.41 	 1190 	 1366 	 1522
best_thres [[0.46, 0.58, 0.2], [0.28, 0.63, 0.21], [0.28, 0.63, 0.21]]
f [0.8257, 0.8252, 0.8251]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 13 	 [0.44, 0.55, 0.19] 	 lr: 2.5e-05 	 f: 82.95844927018078
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 24 [0m
Train...
loss: tensor(662.4083) lr: 6.25e-06 time: 1117.8
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.34 	r: 76.56 	f1: 78.88 	 4722 	 5805 	 6168
wo 	p: 93.82 	r: 86.01 	f1: 89.74 	 2914 	 3106 	 3388
ni 	p: 84.19 	r: 78.38 	f1: 81.18 	 1193 	 1417 	 1522

[32m iter_1[0m
ga 	p: 81.86 	r: 76.25 	f1: 78.96 	 4703 	 5745 	 6168
wo 	p: 93.32 	r: 86.57 	f1: 89.82 	 2933 	 3143 	 3388
ni 	p: 84.67 	r: 79.11 	f1: 81.79 	 1204 	 1422 	 1522

[32m iter_2[0m
ga 	p: 82.34 	r: 75.84 	f1: 78.96 	 4678 	 5681 	 6168
wo 	p: 93.21 	r: 86.69 	f1: 89.83 	 2937 	 3151 	 3388
ni 	p: 84.71 	r: 78.98 	f1: 81.74 	 1202 	 1419 	 1522
best_thres [[0.38, 0.58, 0.14], [0.4, 0.64, 0.12], [0.43, 0.66, 0.12]]
f [0.8249, 0.8258, 0.8261]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 18 	 [0.39, 0.43, 0.16] 	 lr: 1.25e-05 	 f: 82.71792383426988
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 28 [0m
Train...

[32m iter_0[0m
ga 	p: 82.78 	r: 74.04 	f1: 78.17 	 4567 	 5517 	 6168
wo 	p: 92.26 	r: 86.95 	f1: 89.53 	 2946 	 3193 	 3388
ni 	p: 85.95 	r: 76.74 	f1: 81.08 	 1168 	 1359 	 1522

[32m iter_1[0m
ga 	p: 83.26 	r: 73.8 	f1: 78.25 	 4552 	 5467 	 6168
wo 	p: 93.21 	r: 86.33 	f1: 89.64 	 2925 	 3138 	 3388
ni 	p: 87.75 	r: 75.76 	f1: 81.31 	 1153 	 1314 	 1522

[32m iter_2[0m
ga 	p: 81.87 	r: 75.1 	f1: 78.34 	 4632 	 5658 	 6168
wo 	p: 93.45 	r: 85.95 	f1: 89.54 	 2912 	 3116 	 3388
ni 	p: 87.74 	r: 76.15 	f1: 81.53 	 1159 	 1321 	 1522
best_thres [[0.43, 0.43, 0.21], [0.64, 0.65, 0.25], [0.59, 0.76, 0.24]]
f [0.821, 0.8215, 0.8217]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 22 	 [0.55, 0.44, 0.16] 	 lr: 6.25e-06 	 f: 82.85481444333
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 26 [0m
Train...
loss: tensor(704.3907) lr: 6.25e-06 time: 1150.95
pred_count_train 41644

Test...
loss: tensor(558.5732) lr: 5e-06 time: 1172.07
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.29 	r: 76.44 	f1: 78.79 	 4715 	 5800 	 6168
wo 	p: 93.31 	r: 86.81 	f1: 89.94 	 2941 	 3152 	 3388
ni 	p: 88.82 	r: 76.22 	f1: 82.04 	 1160 	 1306 	 1522

[32m iter_1[0m
ga 	p: 82.63 	r: 75.1 	f1: 78.68 	 4632 	 5606 	 6168
wo 	p: 93.76 	r: 86.1 	f1: 89.77 	 2917 	 3111 	 3388
ni 	p: 85.58 	r: 78.38 	f1: 81.82 	 1193 	 1394 	 1522

[32m iter_2[0m
ga 	p: 81.73 	r: 75.88 	f1: 78.7 	 4680 	 5726 	 6168
wo 	p: 93.73 	r: 86.1 	f1: 89.75 	 2917 	 3112 	 3388
ni 	p: 85.52 	r: 78.38 	f1: 81.8 	 1193 	 1395 	 1522
best_thres [[0.38, 0.48, 0.26], [0.46, 0.54, 0.17], [0.4, 0.54, 0.17]]
f [0.8264, 0.8258, 0.8255]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 13 	 [0.44, 0.55, 0.19] 	 lr: 2.5e-05 	 f: 82.95844927018078
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 25 [0m
Train...
loss: tensor(617.9948) lr: 6.25e-06 time: 1129.87
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.94 	r: 75.42 	f1: 78.55 	 4652 	 5677 	 6168
wo 	p: 94.24 	r: 85.42 	f1: 89.61 	 2894 	 3071 	 3388
ni 	p: 86.77 	r: 77.14 	f1: 81.67 	 1174 	 1353 	 1522

[32m iter_1[0m
ga 	p: 80.4 	r: 76.96 	f1: 78.64 	 4747 	 5904 	 6168
wo 	p: 92.52 	r: 86.84 	f1: 89.59 	 2942 	 3180 	 3388
ni 	p: 83.49 	r: 80.75 	f1: 82.1 	 1229 	 1472 	 1522

[32m iter_2[0m
ga 	p: 80.43 	r: 76.96 	f1: 78.66 	 4747 	 5902 	 6168
wo 	p: 92.52 	r: 86.87 	f1: 89.6 	 2943 	 3181 	 3388
ni 	p: 84.84 	r: 79.43 	f1: 82.05 	 1209 	 1425 	 1522
best_thres [[0.41, 0.64, 0.2], [0.31, 0.43, 0.1], [0.31, 0.43, 0.12]]
f [0.8235, 0.824, 0.8241]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 18 	 [0.39, 0.43, 0.16] 	 lr: 1.25e-05 	 f: 82.71792383426988
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 29 [0m
Train...

[32m iter_0[0m
ga 	p: 82.55 	r: 73.95 	f1: 78.01 	 4561 	 5525 	 6168
wo 	p: 93.3 	r: 85.95 	f1: 89.48 	 2912 	 3121 	 3388
ni 	p: 84.86 	r: 76.22 	f1: 80.3 	 1160 	 1367 	 1522

[32m iter_1[0m
ga 	p: 81.84 	r: 74.9 	f1: 78.22 	 4620 	 5645 	 6168
wo 	p: 92.81 	r: 86.51 	f1: 89.55 	 2931 	 3158 	 3388
ni 	p: 87.39 	r: 75.16 	f1: 80.82 	 1144 	 1309 	 1522

[32m iter_2[0m
ga 	p: 82.16 	r: 74.72 	f1: 78.26 	 4609 	 5610 	 6168
wo 	p: 93.65 	r: 85.71 	f1: 89.51 	 2904 	 3101 	 3388
ni 	p: 86.69 	r: 75.76 	f1: 80.86 	 1153 	 1330 	 1522
best_thres [[0.41, 0.52, 0.18], [0.53, 0.6, 0.21], [0.63, 0.76, 0.18]]
f [0.8186, 0.8197, 0.82]
load model: epoch22
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 22 	 [0.55, 0.44, 0.16] 	 lr: 6.25e-06 	 f: 82.85481444333
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 27 [0m
Train...
loss: tensor(658.6403) lr: 6.25e-06 time: 1165.38
pred_count_train 41644

Test...
loss: tensor(517.2542) lr: 5e-06 time: 1191.95
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 79.99 	r: 77.01 	f1: 78.47 	 4750 	 5938 	 6168
wo 	p: 92.79 	r: 86.66 	f1: 89.62 	 2936 	 3164 	 3388
ni 	p: 85.04 	r: 78.06 	f1: 81.4 	 1188 	 1397 	 1522

[32m iter_1[0m
ga 	p: 80.08 	r: 77.09 	f1: 78.56 	 4755 	 5938 	 6168
wo 	p: 93.78 	r: 85.89 	f1: 89.66 	 2910 	 3103 	 3388
ni 	p: 86.16 	r: 77.33 	f1: 81.51 	 1177 	 1366 	 1522

[32m iter_2[0m
ga 	p: 80.14 	r: 77.09 	f1: 78.59 	 4755 	 5933 	 6168
wo 	p: 93.72 	r: 85.92 	f1: 89.65 	 2911 	 3106 	 3388
ni 	p: 86.05 	r: 77.4 	f1: 81.49 	 1178 	 1369 	 1522
best_thres [[0.35, 0.52, 0.19], [0.34, 0.6, 0.2], [0.34, 0.6, 0.2]]
f [0.8225, 0.8228, 0.823]
load model: epoch13
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 13 	 [0.44, 0.55, 0.19] 	 lr: 2.5e-05 	 f: 82.95844927018078
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 26 [0m
Train...
loss: tensor(750.7007) lr: 5e-06 time: 1143.57
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.1 	r: 75.31 	f1: 78.56 	 4645 	 5658 	 6168
wo 	p: 94.23 	r: 85.83 	f1: 89.84 	 2908 	 3086 	 3388
ni 	p: 84.65 	r: 76.08 	f1: 80.14 	 1158 	 1368 	 1522

[32m iter_1[0m
ga 	p: 81.07 	r: 76.49 	f1: 78.71 	 4718 	 5820 	 6168
wo 	p: 94.31 	r: 85.54 	f1: 89.71 	 2898 	 3073 	 3388
ni 	p: 83.76 	r: 77.92 	f1: 80.74 	 1186 	 1416 	 1522

[32m iter_2[0m
ga 	p: 81.14 	r: 76.44 	f1: 78.72 	 4715 	 5811 	 6168
wo 	p: 93.7 	r: 85.98 	f1: 89.67 	 2913 	 3109 	 3388
ni 	p: 83.86 	r: 77.86 	f1: 80.75 	 1185 	 1413 	 1522
best_thres [[0.42, 0.62, 0.16], [0.34, 0.82, 0.11], [0.34, 0.78, 0.11]]
f [0.8222, 0.8227, 0.8228]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	current best epoch 18 	 [0.39, 0.43, 0.16] 	 lr: 1.25e-05 	 f: 82.71792383426988
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse[0m [33m epoch 30 [0m
Train...

[32m iter_0[0m
ga 	p: 82.38 	r: 75.13 	f1: 78.59 	 4634 	 5625 	 6168
wo 	p: 92.64 	r: 86.98 	f1: 89.72 	 2947 	 3181 	 3388
ni 	p: 85.45 	r: 78.71 	f1: 81.94 	 1198 	 1402 	 1522

[32m iter_1[0m
ga 	p: 82.36 	r: 75.41 	f1: 78.73 	 4651 	 5647 	 6168
wo 	p: 92.17 	r: 87.6 	f1: 89.83 	 2968 	 3220 	 3388
ni 	p: 85.01 	r: 79.37 	f1: 82.09 	 1208 	 1421 	 1522

[32m iter_2[0m
ga 	p: 82.11 	r: 75.75 	f1: 78.8 	 4672 	 5690 	 6168
wo 	p: 91.92 	r: 87.63 	f1: 89.72 	 2969 	 3230 	 3388
ni 	p: 86.09 	r: 78.45 	f1: 82.09 	 1194 	 1387 	 1522
best_thres [[0.41, 0.44, 0.18], [0.52, 0.47, 0.15], [0.55, 0.48, 0.17]]
f [0.8249, 0.8256, 0.8258]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 22 	 [0.55, 0.44, 0.16] 	 lr: 6.25e-06 	 f: 82.85481444333
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 28 [0m
Train...
loss: tensor(799.4312) lr: 5e-06 time: 1183.78
pred_count_train 41644

Test...
loss: tensor(480.5315) lr: 5e-06 time: 1212.33
pred_count_train 41644

Test...
loss: tensor(713.7969) lr: 5e-06 time: 1157.45
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.18 	r: 77.19 	f1: 79.13 	 4761 	 5865 	 6168
wo 	p: 94.19 	r: 86.19 	f1: 90.01 	 2920 	 3100 	 3388
ni 	p: 88.13 	r: 77.6 	f1: 82.53 	 1181 	 1340 	 1522

[32m iter_1[0m
ga 	p: 81.26 	r: 76.98 	f1: 79.06 	 4748 	 5843 	 6168
wo 	p: 93.92 	r: 86.13 	f1: 89.85 	 2918 	 3107 	 3388
ni 	p: 88.74 	r: 77.14 	f1: 82.53 	 1174 	 1323 	 1522

[32m iter_2[0m
ga 	p: 81.25 	r: 76.98 	f1: 79.05 	 4748 	 5844 	 6168
wo 	p: 93.89 	r: 86.13 	f1: 89.84 	 2918 	 3108 	 3388
ni 	p: 88.6 	r: 77.14 	f1: 82.47 	 1174 	 1325 	 1522
best_thres [[0.38, 0.56, 0.22], [0.38, 0.54, 0.23], [0.38, 0.54, 0.23]]
f [0.8289, 0.8285, 0.8283]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 13 	 [0.44, 0.55, 0.19] 	 lr: 2.5e-05 	 f: 82.95844927018078
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 27 [0m
Train...

[32m iter_0[0m
ga 	p: 82.41 	r: 74.45 	f1: 78.23 	 4592 	 5572 	 6168
wo 	p: 93.74 	r: 85.77 	f1: 89.58 	 2906 	 3100 	 3388
ni 	p: 83.32 	r: 75.82 	f1: 79.39 	 1154 	 1385 	 1522

[32m iter_1[0m
ga 	p: 82.92 	r: 74.22 	f1: 78.33 	 4578 	 5521 	 6168
wo 	p: 92.89 	r: 86.33 	f1: 89.49 	 2925 	 3149 	 3388
ni 	p: 82.99 	r: 77.27 	f1: 80.03 	 1176 	 1417 	 1522

[32m iter_2[0m
ga 	p: 83.27 	r: 73.9 	f1: 78.3 	 4558 	 5474 	 6168
wo 	p: 93.64 	r: 85.66 	f1: 89.47 	 2902 	 3099 	 3388
ni 	p: 82.98 	r: 77.2 	f1: 79.99 	 1175 	 1416 	 1522
best_thres [[0.4, 0.54, 0.11], [0.42, 0.43, 0.08], [0.44, 0.75, 0.08]]
f [0.8187, 0.8194, 0.8195]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse 	best in epoch 18 	 [0.39, 0.43, 0.16] 	 lr: 1.25e-05 	 f: 82.71792383426988
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 1 [0m
Train...

[32m iter_0[0m
ga 	p: 81.83 	r: 75.44 	f1: 78.51 	 4653 	 5686 	 6168
wo 	p: 93.05 	r: 86.48 	f1: 89.64 	 2930 	 3149 	 3388
ni 	p: 86.99 	r: 77.73 	f1: 82.1 	 1183 	 1360 	 1522

[32m iter_1[0m
ga 	p: 82.02 	r: 75.66 	f1: 78.71 	 4667 	 5690 	 6168
wo 	p: 93.03 	r: 86.72 	f1: 89.76 	 2938 	 3158 	 3388
ni 	p: 85.83 	r: 78.78 	f1: 82.15 	 1199 	 1397 	 1522

[32m iter_2[0m
ga 	p: 82.71 	r: 75.39 	f1: 78.88 	 4650 	 5622 	 6168
wo 	p: 93.3 	r: 86.69 	f1: 89.87 	 2937 	 3148 	 3388
ni 	p: 84.45 	r: 80.29 	f1: 82.32 	 1222 	 1447 	 1522
best_thres [[0.37, 0.49, 0.23], [0.5, 0.57, 0.18], [0.59, 0.65, 0.14]]
f [0.8241, 0.825, 0.8258]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 22 	 [0.55, 0.44, 0.16] 	 lr: 6.25e-06 	 f: 82.85481444333
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 29 [0m
Train...
loss: tensor(753.6482) lr: 5e-06 time: 1197.65
pred_count_train 41644

Test...
loss: tensor(3623.8530) lr: 0.0001 time: 1237.57
pred_count_train 41644

Test...
loss: tensor(674.4711) lr: 5e-06 time: 1182.57
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.19 	r: 77.69 	f1: 78.92 	 4792 	 5976 	 6168
wo 	p: 94.23 	r: 85.83 	f1: 89.84 	 2908 	 3086 	 3388
ni 	p: 87.49 	r: 77.66 	f1: 82.28 	 1182 	 1351 	 1522

[32m iter_1[0m
ga 	p: 79.92 	r: 77.93 	f1: 78.91 	 4807 	 6015 	 6168
wo 	p: 92.36 	r: 87.37 	f1: 89.79 	 2960 	 3205 	 3388
ni 	p: 86.17 	r: 78.58 	f1: 82.2 	 1196 	 1388 	 1522

[32m iter_2[0m
ga 	p: 80.09 	r: 77.74 	f1: 78.9 	 4795 	 5987 	 6168
wo 	p: 92.36 	r: 87.37 	f1: 89.79 	 2960 	 3205 	 3388
ni 	p: 86.12 	r: 78.65 	f1: 82.21 	 1197 	 1390 	 1522
best_thres [[0.34, 0.62, 0.22], [0.32, 0.44, 0.18], [0.33, 0.44, 0.18]]
f [0.8266, 0.8266, 0.8266]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 13 	 [0.44, 0.55, 0.19] 	 lr: 2.5e-05 	 f: 82.95844927018078
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 28 [0m
Train...

[32m iter_0[0m
ga 	p: 75.34 	r: 71.13 	f1: 73.17 	 4387 	 5823 	 6168
wo 	p: 90.01 	r: 82.67 	f1: 86.18 	 2801 	 3112 	 3388
ni 	p: 80.15 	r: 79.57 	f1: 79.85 	 1211 	 1511 	 1522

[32m iter_1[0m
ga 	p: 75.41 	r: 71.09 	f1: 73.19 	 4385 	 5815 	 6168
wo 	p: 90.65 	r: 82.14 	f1: 86.19 	 2783 	 3070 	 3388
ni 	p: 82.65 	r: 77.33 	f1: 79.9 	 1177 	 1424 	 1522

[32m iter_2[0m
ga 	p: 75.39 	r: 71.08 	f1: 73.17 	 4384 	 5815 	 6168
wo 	p: 91.25 	r: 81.58 	f1: 86.15 	 2764 	 3029 	 3388
ni 	p: 82.65 	r: 77.33 	f1: 79.9 	 1177 	 1424 	 1522
best_thres [[0.31, 0.22, 0.17], [0.31, 0.24, 0.22], [0.31, 0.27, 0.22]]
f [0.7804, 0.7804, 0.7803]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 1 	 [0.31, 0.27, 0.22] 	 lr: 0.0001 	 f: 78.02729663694227
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 2 [0m
Train...

[32m iter_0[0m
ga 	p: 81.38 	r: 75.73 	f1: 78.45 	 4671 	 5740 	 6168
wo 	p: 92.2 	r: 87.28 	f1: 89.67 	 2957 	 3207 	 3388
ni 	p: 86.79 	r: 75.56 	f1: 80.79 	 1150 	 1325 	 1522

[32m iter_1[0m
ga 	p: 81.84 	r: 75.84 	f1: 78.73 	 4678 	 5716 	 6168
wo 	p: 93.25 	r: 86.51 	f1: 89.76 	 2931 	 3143 	 3388
ni 	p: 83.78 	r: 78.38 	f1: 80.99 	 1193 	 1424 	 1522

[32m iter_2[0m
ga 	p: 81.86 	r: 75.73 	f1: 78.68 	 4671 	 5706 	 6168
wo 	p: 92.26 	r: 87.28 	f1: 89.7 	 2957 	 3205 	 3388
ni 	p: 88.24 	r: 74.9 	f1: 81.02 	 1140 	 1292 	 1522
best_thres [[0.36, 0.37, 0.2], [0.51, 0.6, 0.12], [0.58, 0.51, 0.22]]
f [0.8223, 0.8232, 0.8235]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	current best epoch 22 	 [0.55, 0.44, 0.16] 	 lr: 6.25e-06 	 f: 82.85481444333
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse[0m [33m epoch 30 [0m
Train...
loss: tensor(716.5469) lr: 5e-06 time: 1221.27
pred_count_train 41644

Test...
loss: tensor(2227.2188) lr: 0.0001 time: 1252.53
pred_count_train 41644

Test...
loss: tensor(638.9974) lr: 5e-06 time: 1202.63
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.69 	r: 77.08 	f1: 78.84 	 4754 	 5892 	 6168
wo 	p: 94.21 	r: 85.95 	f1: 89.89 	 2912 	 3091 	 3388
ni 	p: 86.17 	r: 79.04 	f1: 82.45 	 1203 	 1396 	 1522

[32m iter_1[0m
ga 	p: 80.96 	r: 76.88 	f1: 78.87 	 4742 	 5857 	 6168
wo 	p: 93.56 	r: 86.19 	f1: 89.72 	 2920 	 3121 	 3388
ni 	p: 86.63 	r: 78.78 	f1: 82.52 	 1199 	 1384 	 1522

[32m iter_2[0m
ga 	p: 80.95 	r: 76.86 	f1: 78.85 	 4741 	 5857 	 6168
wo 	p: 93.64 	r: 86.04 	f1: 89.68 	 2915 	 3113 	 3388
ni 	p: 86.57 	r: 78.78 	f1: 82.49 	 1199 	 1385 	 1522
best_thres [[0.36, 0.61, 0.2], [0.36, 0.55, 0.21], [0.36, 0.56, 0.21]]
f [0.8267, 0.8266, 0.8265]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	current best epoch 13 	 [0.44, 0.55, 0.19] 	 lr: 2.5e-05 	 f: 82.95844927018078
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse[0m [33m epoch 29 [0m
Train...

[32m iter_0[0m
ga 	p: 77.9 	r: 73.27 	f1: 75.51 	 4519 	 5801 	 6168
wo 	p: 90.44 	r: 85.48 	f1: 87.89 	 2896 	 3202 	 3388
ni 	p: 83.37 	r: 80.68 	f1: 82.0 	 1228 	 1473 	 1522

[32m iter_1[0m
ga 	p: 78.25 	r: 73.07 	f1: 75.57 	 4507 	 5760 	 6168
wo 	p: 90.58 	r: 85.42 	f1: 87.92 	 2894 	 3195 	 3388
ni 	p: 83.55 	r: 80.42 	f1: 81.96 	 1224 	 1465 	 1522

[32m iter_2[0m
ga 	p: 78.27 	r: 73.04 	f1: 75.56 	 4505 	 5756 	 6168
wo 	p: 90.58 	r: 85.39 	f1: 87.91 	 2893 	 3194 	 3388
ni 	p: 83.48 	r: 80.35 	f1: 81.89 	 1223 	 1465 	 1522
best_thres [[0.3, 0.29, 0.3], [0.31, 0.29, 0.31], [0.31, 0.29, 0.31]]
f [0.802, 0.8022, 0.8022]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 2 	 [0.31, 0.29, 0.31] 	 lr: 0.0001 	 f: 80.22000154930669
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 3 [0m
Train...

[32m iter_0[0m
ga 	p: 82.61 	r: 74.11 	f1: 78.13 	 4571 	 5533 	 6168
wo 	p: 93.2 	r: 86.19 	f1: 89.56 	 2920 	 3133 	 3388
ni 	p: 86.67 	r: 75.16 	f1: 80.51 	 1144 	 1320 	 1522

[32m iter_1[0m
ga 	p: 82.65 	r: 74.21 	f1: 78.2 	 4577 	 5538 	 6168
wo 	p: 93.35 	r: 86.19 	f1: 89.63 	 2920 	 3128 	 3388
ni 	p: 85.54 	r: 76.94 	f1: 81.01 	 1171 	 1369 	 1522

[32m iter_2[0m
ga 	p: 82.46 	r: 74.61 	f1: 78.34 	 4602 	 5581 	 6168
wo 	p: 93.43 	r: 86.04 	f1: 89.58 	 2915 	 3120 	 3388
ni 	p: 85.87 	r: 76.68 	f1: 81.01 	 1167 	 1359 	 1522
best_thres [[0.38, 0.42, 0.17], [0.54, 0.56, 0.12], [0.58, 0.63, 0.12]]
f [0.8199, 0.8205, 0.8209]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse 	best in epoch 22 	 [0.55, 0.44, 0.16] 	 lr: 6.25e-06 	 f: 82.85481444333
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 1 [0m
Train...
loss: tensor(679.0052) lr: 5e-06 time: 1249.28
pred_count_train 41644

Test...
loss: tensor(1837.3610) lr: 0.0001 time: 1275.29
pred_count_train 41644

Test...
loss: tensor(3623.5181) lr: 0.0001 time: 1232.77
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.51 	r: 75.32 	f1: 78.75 	 4646 	 5631 	 6168
wo 	p: 93.98 	r: 86.16 	f1: 89.9 	 2919 	 3106 	 3388
ni 	p: 88.25 	r: 75.03 	f1: 81.11 	 1142 	 1294 	 1522

[32m iter_1[0m
ga 	p: 83.05 	r: 74.9 	f1: 78.77 	 4620 	 5563 	 6168
wo 	p: 93.87 	r: 85.89 	f1: 89.7 	 2910 	 3100 	 3388
ni 	p: 84.61 	r: 78.38 	f1: 81.38 	 1193 	 1410 	 1522

[32m iter_2[0m
ga 	p: 82.89 	r: 75.08 	f1: 78.79 	 4631 	 5587 	 6168
wo 	p: 93.87 	r: 85.89 	f1: 89.7 	 2910 	 3100 	 3388
ni 	p: 84.49 	r: 78.38 	f1: 81.32 	 1193 	 1412 	 1522
best_thres [[0.49, 0.57, 0.24], [0.52, 0.57, 0.14], [0.51, 0.57, 0.14]]
f [0.825, 0.8249, 0.8249]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse 	best in epoch 13 	 [0.44, 0.55, 0.19] 	 lr: 2.5e-05 	 f: 82.95844927018078
vocab size:  29396
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 1 [0m
Train...

[32m iter_0[0m
ga 	p: 79.3 	r: 75.96 	f1: 77.59 	 4685 	 5908 	 6168
wo 	p: 92.27 	r: 85.57 	f1: 88.79 	 2899 	 3142 	 3388
ni 	p: 84.93 	r: 81.47 	f1: 83.17 	 1240 	 1460 	 1522

[32m iter_1[0m
ga 	p: 80.78 	r: 74.74 	f1: 77.64 	 4610 	 5707 	 6168
wo 	p: 92.14 	r: 85.45 	f1: 88.67 	 2895 	 3142 	 3388
ni 	p: 84.38 	r: 81.6 	f1: 82.97 	 1242 	 1472 	 1522

[32m iter_2[0m
ga 	p: 80.81 	r: 74.74 	f1: 77.66 	 4610 	 5705 	 6168
wo 	p: 92.36 	r: 85.24 	f1: 88.66 	 2888 	 3127 	 3388
ni 	p: 84.43 	r: 81.6 	f1: 82.99 	 1242 	 1471 	 1522
best_thres [[0.42, 0.26, 0.27], [0.48, 0.26, 0.26], [0.48, 0.27, 0.26]]
f [0.8175, 0.8175, 0.8175]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 3 	 [0.48, 0.27, 0.26] 	 lr: 0.0001 	 f: 81.75180213770817
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 4 [0m
Train...

[32m iter_0[0m
ga 	p: 75.27 	r: 71.19 	f1: 73.17 	 4391 	 5834 	 6168
wo 	p: 91.8 	r: 81.29 	f1: 86.22 	 2754 	 3000 	 3388
ni 	p: 80.09 	r: 79.57 	f1: 79.83 	 1211 	 1512 	 1522

[32m iter_1[0m
ga 	p: 75.55 	r: 70.78 	f1: 73.09 	 4366 	 5779 	 6168
wo 	p: 91.36 	r: 81.73 	f1: 86.28 	 2769 	 3031 	 3388
ni 	p: 80.33 	r: 79.43 	f1: 79.88 	 1209 	 1505 	 1522

[32m iter_2[0m
ga 	p: 75.55 	r: 70.8 	f1: 73.1 	 4367 	 5780 	 6168
wo 	p: 91.36 	r: 81.73 	f1: 86.28 	 2769 	 3031 	 3388
ni 	p: 80.33 	r: 79.43 	f1: 79.88 	 1209 	 1505 	 1522
best_thres [[0.3, 0.3, 0.17], [0.32, 0.28, 0.17], [0.32, 0.28, 0.17]]
f [0.7801, 0.7801, 0.7801]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 1 	 [0.32, 0.28, 0.17] 	 lr: 0.0001 	 f: 78.00844092133747
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 2 [0m
Train...
loss: tensor(3623.0378) lr: 0.0001 time: 1283.34
pred_count_train 41644

Test...
loss: tensor(1541.7111) lr: 0.0001 time: 1296.14
pred_count_train 41644

Test...
loss: tensor(2227.3108) lr: 0.0001 time: 1258.34
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 76.16 	r: 70.51 	f1: 73.23 	 4349 	 5710 	 6168
wo 	p: 90.25 	r: 82.53 	f1: 86.22 	 2796 	 3098 	 3388
ni 	p: 82.42 	r: 77.33 	f1: 79.8 	 1177 	 1428 	 1522

[32m iter_1[0m
ga 	p: 77.67 	r: 69.15 	f1: 73.16 	 4265 	 5491 	 6168
wo 	p: 90.96 	r: 82.0 	f1: 86.25 	 2778 	 3054 	 3388
ni 	p: 82.76 	r: 77.27 	f1: 79.92 	 1176 	 1421 	 1522

[32m iter_2[0m
ga 	p: 76.1 	r: 70.46 	f1: 73.17 	 4346 	 5711 	 6168
wo 	p: 90.96 	r: 81.97 	f1: 86.23 	 2777 	 3053 	 3388
ni 	p: 82.76 	r: 77.27 	f1: 79.92 	 1176 	 1421 	 1522
best_thres [[0.34, 0.23, 0.23], [0.38, 0.26, 0.23], [0.34, 0.26, 0.23]]
f [0.7809, 0.781, 0.7809]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 1 	 [0.34, 0.26, 0.23] 	 lr: 0.0001 	 f: 78.08742396378555
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 2 [0m
Train...

[32m iter_0[0m
ga 	p: 81.06 	r: 75.91 	f1: 78.4 	 4682 	 5776 	 6168
wo 	p: 92.68 	r: 86.01 	f1: 89.22 	 2914 	 3144 	 3388
ni 	p: 88.17 	r: 80.81 	f1: 84.33 	 1230 	 1395 	 1522

[32m iter_1[0m
ga 	p: 81.09 	r: 75.84 	f1: 78.38 	 4678 	 5769 	 6168
wo 	p: 92.32 	r: 86.25 	f1: 89.18 	 2922 	 3165 	 3388
ni 	p: 88.23 	r: 80.75 	f1: 84.32 	 1229 	 1393 	 1522

[32m iter_2[0m
ga 	p: 81.14 	r: 75.88 	f1: 78.42 	 4680 	 5768 	 6168
wo 	p: 92.32 	r: 86.22 	f1: 89.16 	 2921 	 3164 	 3388
ni 	p: 88.29 	r: 80.75 	f1: 84.35 	 1229 	 1392 	 1522
best_thres [[0.39, 0.43, 0.34], [0.39, 0.4, 0.33], [0.39, 0.4, 0.33]]
f [0.8251, 0.825, 0.8251]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 4 	 [0.39, 0.4, 0.33] 	 lr: 0.0001 	 f: 82.50778816199377
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 5 [0m
Train...

[32m iter_0[0m
ga 	p: 77.69 	r: 73.28 	f1: 75.42 	 4520 	 5818 	 6168
wo 	p: 92.25 	r: 83.91 	f1: 87.88 	 2843 	 3082 	 3388
ni 	p: 83.63 	r: 80.55 	f1: 82.06 	 1226 	 1466 	 1522

[32m iter_1[0m
ga 	p: 77.73 	r: 73.17 	f1: 75.38 	 4513 	 5806 	 6168
wo 	p: 92.11 	r: 84.09 	f1: 87.92 	 2849 	 3093 	 3388
ni 	p: 83.46 	r: 80.55 	f1: 81.98 	 1226 	 1469 	 1522

[32m iter_2[0m
ga 	p: 76.46 	r: 74.42 	f1: 75.43 	 4590 	 6003 	 6168
wo 	p: 92.14 	r: 84.12 	f1: 87.95 	 2850 	 3093 	 3388
ni 	p: 83.61 	r: 80.42 	f1: 81.98 	 1224 	 1464 	 1522
best_thres [[0.3, 0.4, 0.31], [0.31, 0.41, 0.29], [0.26, 0.41, 0.3]]
f [0.8011, 0.801, 0.8009]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 2 	 [0.26, 0.41, 0.3] 	 lr: 0.0001 	 f: 80.09236300520705
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 3 [0m
Train...
loss: tensor(2223.6084) lr: 0.0001 time: 1313.23
pred_count_train 41644

Test...
loss: tensor(1258.9264) lr: 0.0001 time: 1319.08
pred_count_train 41644

Test...
loss: tensor(1838.1957) lr: 0.0001 time: 1284.15
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.98 	r: 73.75 	f1: 78.09 	 4549 	 5482 	 6168
wo 	p: 92.73 	r: 85.89 	f1: 89.18 	 2910 	 3138 	 3388
ni 	p: 85.26 	r: 79.04 	f1: 82.03 	 1203 	 1411 	 1522

[32m iter_1[0m
ga 	p: 80.39 	r: 75.97 	f1: 78.12 	 4686 	 5829 	 6168
wo 	p: 92.35 	r: 86.22 	f1: 89.18 	 2921 	 3163 	 3388
ni 	p: 84.52 	r: 79.3 	f1: 81.83 	 1207 	 1428 	 1522

[32m iter_2[0m
ga 	p: 80.38 	r: 75.99 	f1: 78.12 	 4687 	 5831 	 6168
wo 	p: 92.35 	r: 86.22 	f1: 89.18 	 2921 	 3163 	 3388
ni 	p: 84.89 	r: 78.98 	f1: 81.82 	 1202 	 1416 	 1522
best_thres [[0.36, 0.46, 0.27], [0.25, 0.41, 0.24], [0.25, 0.41, 0.25]]
f [0.8207, 0.8203, 0.8202]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 4 	 [0.39, 0.4, 0.33] 	 lr: 0.0001 	 f: 82.50778816199377
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 6 [0m
Train...

[32m iter_0[0m
ga 	p: 77.92 	r: 73.14 	f1: 75.45 	 4511 	 5789 	 6168
wo 	p: 91.79 	r: 84.53 	f1: 88.01 	 2864 	 3120 	 3388
ni 	p: 83.23 	r: 80.55 	f1: 81.87 	 1226 	 1473 	 1522

[32m iter_1[0m
ga 	p: 77.85 	r: 73.07 	f1: 75.39 	 4507 	 5789 	 6168
wo 	p: 91.56 	r: 84.5 	f1: 87.89 	 2863 	 3127 	 3388
ni 	p: 83.15 	r: 80.75 	f1: 81.93 	 1229 	 1478 	 1522

[32m iter_2[0m
ga 	p: 76.32 	r: 74.48 	f1: 75.39 	 4594 	 6019 	 6168
wo 	p: 91.59 	r: 84.5 	f1: 87.9 	 2863 	 3126 	 3388
ni 	p: 83.14 	r: 80.68 	f1: 81.89 	 1228 	 1477 	 1522
best_thres [[0.31, 0.39, 0.29], [0.31, 0.38, 0.28], [0.25, 0.38, 0.28]]
f [0.8016, 0.8013, 0.801]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 2 	 [0.25, 0.38, 0.28] 	 lr: 0.0001 	 f: 80.09964104468375
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 3 [0m
Train...

[32m iter_0[0m
ga 	p: 79.86 	r: 75.54 	f1: 77.64 	 4659 	 5834 	 6168
wo 	p: 92.29 	r: 85.48 	f1: 88.75 	 2896 	 3138 	 3388
ni 	p: 85.11 	r: 81.14 	f1: 83.08 	 1235 	 1451 	 1522

[32m iter_1[0m
ga 	p: 80.86 	r: 74.68 	f1: 77.65 	 4606 	 5696 	 6168
wo 	p: 92.69 	r: 84.92 	f1: 88.63 	 2877 	 3104 	 3388
ni 	p: 85.39 	r: 81.01 	f1: 83.14 	 1233 	 1444 	 1522

[32m iter_2[0m
ga 	p: 79.47 	r: 75.89 	f1: 77.64 	 4681 	 5890 	 6168
wo 	p: 92.69 	r: 84.95 	f1: 88.65 	 2878 	 3105 	 3388
ni 	p: 85.27 	r: 81.01 	f1: 83.09 	 1233 	 1446 	 1522
best_thres [[0.43, 0.29, 0.3], [0.5, 0.35, 0.29], [0.45, 0.35, 0.29]]
f [0.8176, 0.8176, 0.8174]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 3 	 [0.45, 0.35, 0.29] 	 lr: 0.0001 	 f: 81.74442821174351
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 4 [0m
Train...
loss: tensor(993.4202) lr: 0.0001 time: 1333.57
pred_count_train 41644

Test...
loss: tensor(1834.3726) lr: 0.0001 time: 1339.52
pred_count_train 41644

Test...
loss: tensor(1543.2814) lr: 0.0001 time: 1307.36
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.49 	r: 74.09 	f1: 78.07 	 4570 	 5540 	 6168
wo 	p: 92.29 	r: 85.54 	f1: 88.79 	 2898 	 3140 	 3388
ni 	p: 85.43 	r: 76.28 	f1: 80.6 	 1161 	 1359 	 1522

[32m iter_1[0m
ga 	p: 82.56 	r: 74.21 	f1: 78.16 	 4577 	 5544 	 6168
wo 	p: 92.67 	r: 85.12 	f1: 88.74 	 2884 	 3112 	 3388
ni 	p: 86.05 	r: 75.36 	f1: 80.35 	 1147 	 1333 	 1522

[32m iter_2[0m
ga 	p: 82.51 	r: 74.17 	f1: 78.12 	 4575 	 5545 	 6168
wo 	p: 92.87 	r: 85.01 	f1: 88.77 	 2880 	 3101 	 3388
ni 	p: 83.39 	r: 77.53 	f1: 80.35 	 1180 	 1415 	 1522
best_thres [[0.45, 0.44, 0.29], [0.45, 0.49, 0.31], [0.45, 0.59, 0.24]]
f [0.8173, 0.8172, 0.8171]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 4 	 [0.39, 0.4, 0.33] 	 lr: 0.0001 	 f: 82.50778816199377
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 7 [0m
Train...

[32m iter_0[0m
ga 	p: 79.09 	r: 76.22 	f1: 77.63 	 4701 	 5944 	 6168
wo 	p: 91.93 	r: 85.77 	f1: 88.75 	 2906 	 3161 	 3388
ni 	p: 85.35 	r: 80.35 	f1: 82.77 	 1223 	 1433 	 1522

[32m iter_1[0m
ga 	p: 80.49 	r: 74.94 	f1: 77.62 	 4622 	 5742 	 6168
wo 	p: 92.42 	r: 85.3 	f1: 88.72 	 2890 	 3127 	 3388
ni 	p: 84.79 	r: 80.55 	f1: 82.61 	 1226 	 1446 	 1522

[32m iter_2[0m
ga 	p: 80.51 	r: 74.95 	f1: 77.63 	 4623 	 5742 	 6168
wo 	p: 92.42 	r: 85.27 	f1: 88.7 	 2889 	 3126 	 3388
ni 	p: 84.79 	r: 80.55 	f1: 82.61 	 1226 	 1446 	 1522
best_thres [[0.42, 0.26, 0.3], [0.47, 0.29, 0.28], [0.47, 0.29, 0.28]]
f [0.817, 0.8169, 0.8169]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 3 	 [0.47, 0.29, 0.28] 	 lr: 0.0001 	 f: 81.69438362758342
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 4 [0m
Train...

[32m iter_0[0m
ga 	p: 82.43 	r: 75.13 	f1: 78.61 	 4634 	 5622 	 6168
wo 	p: 92.48 	r: 86.36 	f1: 89.32 	 2926 	 3164 	 3388
ni 	p: 87.47 	r: 81.67 	f1: 84.47 	 1243 	 1421 	 1522

[32m iter_1[0m
ga 	p: 80.83 	r: 76.31 	f1: 78.51 	 4707 	 5823 	 6168
wo 	p: 92.39 	r: 86.3 	f1: 89.24 	 2924 	 3165 	 3388
ni 	p: 87.37 	r: 81.34 	f1: 84.25 	 1238 	 1417 	 1522

[32m iter_2[0m
ga 	p: 81.43 	r: 75.78 	f1: 78.5 	 4674 	 5740 	 6168
wo 	p: 92.9 	r: 85.77 	f1: 89.2 	 2906 	 3128 	 3388
ni 	p: 87.48 	r: 81.27 	f1: 84.26 	 1237 	 1414 	 1522
best_thres [[0.42, 0.38, 0.34], [0.4, 0.41, 0.33], [0.43, 0.47, 0.33]]
f [0.8272, 0.8264, 0.8261]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 4 	 [0.43, 0.47, 0.33] 	 lr: 0.0001 	 f: 82.61289920159682
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 5 [0m
Train...
loss: tensor(768.0162) lr: 0.0001 time: 1328.92
pred_count_train 41644

Test...
loss: tensor(1535.5872) lr: 0.0001 time: 1341.66
pred_count_train 41644

Test...
loss: tensor(1262.2753) lr: 0.0001 time: 1321.92
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.03 	r: 74.17 	f1: 77.45 	 4575 	 5646 	 6168
wo 	p: 93.47 	r: 84.03 	f1: 88.5 	 2847 	 3046 	 3388
ni 	p: 86.34 	r: 76.41 	f1: 81.07 	 1163 	 1347 	 1522

[32m iter_1[0m
ga 	p: 80.19 	r: 75.19 	f1: 77.61 	 4638 	 5784 	 6168
wo 	p: 92.97 	r: 84.68 	f1: 88.63 	 2869 	 3086 	 3388
ni 	p: 86.35 	r: 76.48 	f1: 81.11 	 1164 	 1348 	 1522

[32m iter_2[0m
ga 	p: 80.27 	r: 75.24 	f1: 77.67 	 4641 	 5782 	 6168
wo 	p: 93.0 	r: 84.71 	f1: 88.66 	 2870 	 3086 	 3388
ni 	p: 86.46 	r: 76.35 	f1: 81.09 	 1162 	 1344 	 1522
best_thres [[0.43, 0.45, 0.33], [0.37, 0.38, 0.31], [0.37, 0.38, 0.31]]
f [0.8131, 0.8137, 0.8141]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 4 	 [0.39, 0.4, 0.33] 	 lr: 0.0001 	 f: 82.50778816199377
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 8 [0m
Train...

[32m iter_0[0m
ga 	p: 81.66 	r: 75.49 	f1: 78.45 	 4656 	 5702 	 6168
wo 	p: 93.28 	r: 85.21 	f1: 89.06 	 2887 	 3095 	 3388
ni 	p: 87.47 	r: 81.21 	f1: 84.22 	 1236 	 1413 	 1522

[32m iter_1[0m
ga 	p: 82.26 	r: 74.82 	f1: 78.37 	 4615 	 5610 	 6168
wo 	p: 92.63 	r: 85.71 	f1: 89.04 	 2904 	 3135 	 3388
ni 	p: 87.83 	r: 80.62 	f1: 84.07 	 1227 	 1397 	 1522

[32m iter_2[0m
ga 	p: 81.84 	r: 75.16 	f1: 78.36 	 4636 	 5665 	 6168
wo 	p: 93.13 	r: 85.27 	f1: 89.03 	 2889 	 3102 	 3388
ni 	p: 87.83 	r: 80.62 	f1: 84.07 	 1227 	 1397 	 1522
best_thres [[0.41, 0.5, 0.32], [0.44, 0.43, 0.33], [0.42, 0.48, 0.33]]
f [0.8248, 0.8246, 0.8244]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 4 	 [0.42, 0.48, 0.33] 	 lr: 0.0001 	 f: 82.43764705882353
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 5 [0m
Train...

[32m iter_0[0m
ga 	p: 81.23 	r: 75.23 	f1: 78.11 	 4640 	 5712 	 6168
wo 	p: 94.0 	r: 85.06 	f1: 89.31 	 2882 	 3066 	 3388
ni 	p: 87.58 	r: 77.4 	f1: 82.18 	 1178 	 1345 	 1522

[32m iter_1[0m
ga 	p: 81.78 	r: 74.95 	f1: 78.22 	 4623 	 5653 	 6168
wo 	p: 94.52 	r: 84.56 	f1: 89.27 	 2865 	 3031 	 3388
ni 	p: 87.44 	r: 77.73 	f1: 82.3 	 1183 	 1353 	 1522

[32m iter_2[0m
ga 	p: 81.88 	r: 74.95 	f1: 78.26 	 4623 	 5646 	 6168
wo 	p: 94.52 	r: 84.45 	f1: 89.2 	 2861 	 3027 	 3388
ni 	p: 87.37 	r: 77.73 	f1: 82.27 	 1183 	 1354 	 1522
best_thres [[0.26, 0.59, 0.3], [0.31, 0.73, 0.3], [0.32, 0.76, 0.3]]
f [0.8207, 0.821, 0.8211]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 4 	 [0.43, 0.47, 0.33] 	 lr: 0.0001 	 f: 82.61289920159682
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 6 [0m
Train...
loss: tensor(588.7797) lr: 0.0001 time: 1314.73
pred_count_train 41644

Test...
loss: tensor(1250.1487) lr: 0.0001 time: 1341.6
pred_count_train 41644

Test...
loss: tensor(1002.3574) lr: 0.0001 time: 1331.52
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.56 	r: 74.72 	f1: 77.99 	 4609 	 5651 	 6168
wo 	p: 92.72 	r: 85.39 	f1: 88.91 	 2893 	 3120 	 3388
ni 	p: 85.52 	r: 77.99 	f1: 81.58 	 1187 	 1388 	 1522

[32m iter_1[0m
ga 	p: 81.41 	r: 74.92 	f1: 78.03 	 4621 	 5676 	 6168
wo 	p: 93.5 	r: 84.98 	f1: 89.04 	 2879 	 3079 	 3388
ni 	p: 86.07 	r: 77.92 	f1: 81.79 	 1186 	 1378 	 1522

[32m iter_2[0m
ga 	p: 81.11 	r: 75.18 	f1: 78.03 	 4637 	 5717 	 6168
wo 	p: 93.05 	r: 85.36 	f1: 89.04 	 2892 	 3108 	 3388
ni 	p: 86.04 	r: 77.73 	f1: 81.67 	 1183 	 1375 	 1522
best_thres [[0.56, 0.55, 0.25], [0.7, 0.85, 0.23], [0.7, 0.84, 0.23]]
f [0.8183, 0.8186, 0.8187]
load model: epoch4
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 4 	 [0.39, 0.4, 0.33] 	 lr: 0.0001 	 f: 82.50778816199377
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 9 [0m
Train...

[32m iter_0[0m
ga 	p: 81.25 	r: 75.68 	f1: 78.37 	 4668 	 5745 	 6168
wo 	p: 93.2 	r: 84.56 	f1: 88.67 	 2865 	 3074 	 3388
ni 	p: 82.4 	r: 75.36 	f1: 78.72 	 1147 	 1392 	 1522

[32m iter_1[0m
ga 	p: 82.34 	r: 74.77 	f1: 78.38 	 4612 	 5601 	 6168
wo 	p: 93.74 	r: 84.03 	f1: 88.62 	 2847 	 3037 	 3388
ni 	p: 84.62 	r: 74.11 	f1: 79.02 	 1128 	 1333 	 1522

[32m iter_2[0m
ga 	p: 82.65 	r: 74.46 	f1: 78.35 	 4593 	 5557 	 6168
wo 	p: 92.23 	r: 85.15 	f1: 88.55 	 2885 	 3128 	 3388
ni 	p: 82.83 	r: 75.76 	f1: 79.14 	 1153 	 1392 	 1522
best_thres [[0.36, 0.49, 0.14], [0.49, 0.7, 0.16], [0.53, 0.56, 0.13]]
f [0.8154, 0.8157, 0.8158]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 4 	 [0.43, 0.47, 0.33] 	 lr: 0.0001 	 f: 82.61289920159682
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 7 [0m
Train...

[32m iter_0[0m
ga 	p: 80.27 	r: 77.16 	f1: 78.68 	 4759 	 5929 	 6168
wo 	p: 92.38 	r: 86.28 	f1: 89.22 	 2923 	 3164 	 3388
ni 	p: 85.95 	r: 78.38 	f1: 81.99 	 1193 	 1388 	 1522

[32m iter_1[0m
ga 	p: 80.73 	r: 76.95 	f1: 78.79 	 4746 	 5879 	 6168
wo 	p: 92.75 	r: 86.13 	f1: 89.32 	 2918 	 3146 	 3388
ni 	p: 86.27 	r: 78.45 	f1: 82.17 	 1194 	 1384 	 1522

[32m iter_2[0m
ga 	p: 80.73 	r: 76.95 	f1: 78.79 	 4746 	 5879 	 6168
wo 	p: 92.75 	r: 86.13 	f1: 89.32 	 2918 	 3146 	 3388
ni 	p: 86.27 	r: 78.45 	f1: 82.17 	 1194 	 1384 	 1522
best_thres [[0.29, 0.41, 0.32], [0.3, 0.43, 0.33], [0.3, 0.43, 0.33]]
f [0.8233, 0.8239, 0.8241]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 4 	 [0.42, 0.48, 0.33] 	 lr: 0.0001 	 f: 82.43764705882353
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 6 [0m
Train...
loss: tensor(1095.5197) lr: 5e-05 time: 1292.95
pred_count_train 41644

Test...
loss: tensor(787.1298) lr: 0.0001 time: 1314.23
pred_count_train 41644

Test...
loss: tensor(982.1230) lr: 0.0001 time: 1314.46
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.95 	r: 76.09 	f1: 78.91 	 4693 	 5727 	 6168
wo 	p: 92.11 	r: 87.13 	f1: 89.55 	 2952 	 3205 	 3388
ni 	p: 86.72 	r: 78.98 	f1: 82.67 	 1202 	 1386 	 1522

[32m iter_1[0m
ga 	p: 82.73 	r: 75.24 	f1: 78.81 	 4641 	 5610 	 6168
wo 	p: 93.2 	r: 86.16 	f1: 89.54 	 2919 	 3132 	 3388
ni 	p: 85.06 	r: 80.42 	f1: 82.67 	 1224 	 1439 	 1522

[32m iter_2[0m
ga 	p: 82.74 	r: 75.21 	f1: 78.79 	 4639 	 5607 	 6168
wo 	p: 93.0 	r: 86.28 	f1: 89.51 	 2923 	 3143 	 3388
ni 	p: 85.23 	r: 80.35 	f1: 82.72 	 1223 	 1435 	 1522
best_thres [[0.37, 0.48, 0.19], [0.41, 0.75, 0.14], [0.41, 0.76, 0.14]]
f [0.827, 0.8267, 0.8266]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 9 	 [0.41, 0.76, 0.14] 	 lr: 5e-05 	 f: 82.6559028755593
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 10 [0m
Train...

[32m iter_0[0m
ga 	p: 79.88 	r: 74.53 	f1: 77.11 	 4597 	 5755 	 6168
wo 	p: 92.09 	r: 83.85 	f1: 87.78 	 2841 	 3085 	 3388
ni 	p: 85.94 	r: 77.14 	f1: 81.3 	 1174 	 1366 	 1522

[32m iter_1[0m
ga 	p: 79.61 	r: 75.32 	f1: 77.41 	 4646 	 5836 	 6168
wo 	p: 92.2 	r: 83.74 	f1: 87.76 	 2837 	 3077 	 3388
ni 	p: 86.48 	r: 77.33 	f1: 81.65 	 1177 	 1361 	 1522

[32m iter_2[0m
ga 	p: 83.0 	r: 72.89 	f1: 77.62 	 4496 	 5417 	 6168
wo 	p: 92.56 	r: 83.32 	f1: 87.7 	 2823 	 3050 	 3388
ni 	p: 86.21 	r: 77.66 	f1: 81.71 	 1182 	 1371 	 1522
best_thres [[0.36, 0.35, 0.23], [0.43, 0.47, 0.25], [0.68, 0.56, 0.24]]
f [0.8092, 0.8102, 0.8111]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 4 	 [0.43, 0.47, 0.33] 	 lr: 0.0001 	 f: 82.61289920159682
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 8 [0m
Train...

[32m iter_0[0m
ga 	p: 82.61 	r: 73.95 	f1: 78.04 	 4561 	 5521 	 6168
wo 	p: 92.98 	r: 84.5 	f1: 88.54 	 2863 	 3079 	 3388
ni 	p: 86.06 	r: 76.28 	f1: 80.88 	 1161 	 1349 	 1522

[32m iter_1[0m
ga 	p: 81.93 	r: 74.59 	f1: 78.09 	 4601 	 5616 	 6168
wo 	p: 93.05 	r: 84.59 	f1: 88.62 	 2866 	 3080 	 3388
ni 	p: 86.08 	r: 76.41 	f1: 80.96 	 1163 	 1351 	 1522

[32m iter_2[0m
ga 	p: 82.0 	r: 74.61 	f1: 78.13 	 4602 	 5612 	 6168
wo 	p: 93.14 	r: 84.5 	f1: 88.61 	 2863 	 3074 	 3388
ni 	p: 86.16 	r: 76.48 	f1: 81.03 	 1164 	 1351 	 1522
best_thres [[0.46, 0.6, 0.25], [0.43, 0.6, 0.24], [0.43, 0.61, 0.24]]
f [0.8166, 0.8168, 0.817]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 4 	 [0.42, 0.48, 0.33] 	 lr: 0.0001 	 f: 82.43764705882353
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 7 [0m
Train...
loss: tensor(869.6993) lr: 5e-05 time: 1269.38
pred_count_train 41644

Test...
loss: tensor(609.5280) lr: 0.0001 time: 1292.12
pred_count_train 41644

Test...
loss: tensor(752.4322) lr: 0.0001 time: 1282.34
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.14 	r: 75.06 	f1: 78.9 	 4630 	 5569 	 6168
wo 	p: 92.06 	r: 86.57 	f1: 89.23 	 2933 	 3186 	 3388
ni 	p: 84.04 	r: 77.14 	f1: 80.44 	 1174 	 1397 	 1522

[32m iter_1[0m
ga 	p: 82.85 	r: 75.42 	f1: 78.96 	 4652 	 5615 	 6168
wo 	p: 93.64 	r: 85.24 	f1: 89.25 	 2888 	 3084 	 3388
ni 	p: 85.93 	r: 75.82 	f1: 80.56 	 1154 	 1343 	 1522

[32m iter_2[0m
ga 	p: 82.9 	r: 75.45 	f1: 79.0 	 4654 	 5614 	 6168
wo 	p: 93.71 	r: 85.27 	f1: 89.29 	 2889 	 3083 	 3388
ni 	p: 86.99 	r: 75.1 	f1: 80.61 	 1143 	 1314 	 1522
best_thres [[0.56, 0.48, 0.1], [0.67, 0.8, 0.11], [0.7, 0.84, 0.12]]
f [0.8231, 0.8232, 0.8234]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 9 	 [0.41, 0.76, 0.14] 	 lr: 5e-05 	 f: 82.6559028755593
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 11 [0m
Train...

[32m iter_0[0m
ga 	p: 78.81 	r: 75.68 	f1: 77.21 	 4668 	 5923 	 6168
wo 	p: 94.67 	r: 83.29 	f1: 88.62 	 2822 	 2981 	 3388
ni 	p: 88.25 	r: 73.52 	f1: 80.22 	 1119 	 1268 	 1522

[32m iter_1[0m
ga 	p: 81.35 	r: 73.96 	f1: 77.48 	 4562 	 5608 	 6168
wo 	p: 93.25 	r: 84.36 	f1: 88.58 	 2858 	 3065 	 3388
ni 	p: 87.61 	r: 73.85 	f1: 80.14 	 1124 	 1283 	 1522

[32m iter_2[0m
ga 	p: 81.26 	r: 74.01 	f1: 77.46 	 4565 	 5618 	 6168
wo 	p: 93.31 	r: 84.39 	f1: 88.62 	 2859 	 3064 	 3388
ni 	p: 87.3 	r: 74.05 	f1: 80.13 	 1127 	 1291 	 1522
best_thres [[0.33, 0.67, 0.28], [0.63, 0.71, 0.29], [0.7, 0.78, 0.28]]
f [0.8103, 0.8113, 0.8117]
load model: epoch4
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 4 	 [0.43, 0.47, 0.33] 	 lr: 0.0001 	 f: 82.61289920159682
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 9 [0m
Train...

[32m iter_0[0m
ga 	p: 82.15 	r: 74.16 	f1: 77.95 	 4574 	 5568 	 6168
wo 	p: 91.24 	r: 85.45 	f1: 88.25 	 2895 	 3173 	 3388
ni 	p: 87.84 	r: 74.51 	f1: 80.63 	 1134 	 1291 	 1522

[32m iter_1[0m
ga 	p: 82.43 	r: 73.87 	f1: 77.91 	 4556 	 5527 	 6168
wo 	p: 91.53 	r: 85.15 	f1: 88.23 	 2885 	 3152 	 3388
ni 	p: 83.71 	r: 77.33 	f1: 80.4 	 1177 	 1406 	 1522

[32m iter_2[0m
ga 	p: 81.8 	r: 74.4 	f1: 77.92 	 4589 	 5610 	 6168
wo 	p: 91.53 	r: 85.12 	f1: 88.21 	 2884 	 3151 	 3388
ni 	p: 83.77 	r: 77.33 	f1: 80.42 	 1177 	 1405 	 1522
best_thres [[0.49, 0.46, 0.33], [0.5, 0.48, 0.19], [0.47, 0.48, 0.19]]
f [0.8151, 0.8148, 0.8146]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 4 	 [0.42, 0.48, 0.33] 	 lr: 0.0001 	 f: 82.43764705882353
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 8 [0m
Train...
loss: tensor(685.9142) lr: 5e-05 time: 1246.52
pred_count_train 41644

Test...
loss: tensor(1099.8875) lr: 5e-05 time: 1272.81
pred_count_train 41644

Test...
loss: tensor(570.3453) lr: 0.0001 time: 1254.91
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.84 	r: 74.97 	f1: 77.79 	 4624 	 5720 	 6168
wo 	p: 92.07 	r: 86.04 	f1: 88.95 	 2915 	 3166 	 3388
ni 	p: 84.34 	r: 75.03 	f1: 79.42 	 1142 	 1354 	 1522

[32m iter_1[0m
ga 	p: 84.29 	r: 72.18 	f1: 77.76 	 4452 	 5282 	 6168
wo 	p: 92.38 	r: 85.83 	f1: 88.98 	 2908 	 3148 	 3388
ni 	p: 85.31 	r: 74.38 	f1: 79.47 	 1132 	 1327 	 1522

[32m iter_2[0m
ga 	p: 84.31 	r: 72.15 	f1: 77.76 	 4450 	 5278 	 6168
wo 	p: 92.46 	r: 85.83 	f1: 89.02 	 2908 	 3145 	 3388
ni 	p: 84.14 	r: 75.62 	f1: 79.65 	 1151 	 1368 	 1522
best_thres [[0.28, 0.36, 0.12], [0.47, 0.37, 0.11], [0.47, 0.37, 0.09]]
f [0.8144, 0.8148, 0.815]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 9 	 [0.41, 0.76, 0.14] 	 lr: 5e-05 	 f: 82.6559028755593
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 12 [0m
Train...

[32m iter_0[0m
ga 	p: 81.28 	r: 76.46 	f1: 78.8 	 4716 	 5802 	 6168
wo 	p: 92.69 	r: 86.51 	f1: 89.5 	 2931 	 3162 	 3388
ni 	p: 87.13 	r: 78.32 	f1: 82.49 	 1192 	 1368 	 1522

[32m iter_1[0m
ga 	p: 81.45 	r: 76.72 	f1: 79.01 	 4732 	 5810 	 6168
wo 	p: 94.05 	r: 85.45 	f1: 89.55 	 2895 	 3078 	 3388
ni 	p: 87.59 	r: 77.92 	f1: 82.48 	 1186 	 1354 	 1522

[32m iter_2[0m
ga 	p: 82.76 	r: 75.55 	f1: 78.99 	 4660 	 5631 	 6168
wo 	p: 93.79 	r: 85.63 	f1: 89.52 	 2901 	 3093 	 3388
ni 	p: 87.32 	r: 78.25 	f1: 82.54 	 1191 	 1364 	 1522
best_thres [[0.33, 0.5, 0.19], [0.4, 0.78, 0.19], [0.5, 0.79, 0.18]]
f [0.8257, 0.8262, 0.8265]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 9 	 [0.5, 0.79, 0.18] 	 lr: 5e-05 	 f: 82.64680105170903
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 10 [0m
Train...

[32m iter_0[0m
ga 	p: 80.02 	r: 75.73 	f1: 77.82 	 4671 	 5837 	 6168
wo 	p: 92.16 	r: 85.74 	f1: 88.84 	 2905 	 3152 	 3388
ni 	p: 88.78 	r: 72.8 	f1: 80.0 	 1108 	 1248 	 1522

[32m iter_1[0m
ga 	p: 79.38 	r: 76.59 	f1: 77.96 	 4724 	 5951 	 6168
wo 	p: 93.05 	r: 85.01 	f1: 88.85 	 2880 	 3095 	 3388
ni 	p: 83.48 	r: 76.68 	f1: 79.93 	 1167 	 1398 	 1522

[32m iter_2[0m
ga 	p: 80.4 	r: 75.62 	f1: 77.93 	 4664 	 5801 	 6168
wo 	p: 93.34 	r: 84.83 	f1: 88.88 	 2874 	 3079 	 3388
ni 	p: 83.67 	r: 76.74 	f1: 80.05 	 1168 	 1396 	 1522
best_thres [[0.61, 0.53, 0.5], [0.55, 0.67, 0.27], [0.63, 0.72, 0.27]]
f [0.8148, 0.8149, 0.8151]
load model: epoch4
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 4 	 [0.42, 0.48, 0.33] 	 lr: 0.0001 	 f: 82.43764705882353
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 9 [0m
Train...
loss: tensor(530.0828) lr: 5e-05 time: 1225.53
pred_count_train 41644

Test...
loss: tensor(877.0377) lr: 5e-05 time: 1252.7
pred_count_train 41644

Test...
loss: tensor(1090.0117) lr: 5e-05 time: 1228.33
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.45 	r: 74.01 	f1: 78.0 	 4565 	 5537 	 6168
wo 	p: 92.23 	r: 85.12 	f1: 88.53 	 2884 	 3127 	 3388
ni 	p: 86.1 	r: 73.65 	f1: 79.39 	 1121 	 1302 	 1522

[32m iter_1[0m
ga 	p: 80.91 	r: 75.13 	f1: 77.92 	 4634 	 5727 	 6168
wo 	p: 92.19 	r: 84.98 	f1: 88.43 	 2879 	 3123 	 3388
ni 	p: 86.16 	r: 74.05 	f1: 79.65 	 1127 	 1308 	 1522

[32m iter_2[0m
ga 	p: 80.89 	r: 75.1 	f1: 77.89 	 4632 	 5726 	 6168
wo 	p: 92.51 	r: 84.89 	f1: 88.53 	 2876 	 3109 	 3388
ni 	p: 84.8 	r: 75.16 	f1: 79.69 	 1144 	 1349 	 1522
best_thres [[0.48, 0.47, 0.13], [0.39, 0.47, 0.11], [0.39, 0.77, 0.09]]
f [0.8145, 0.8141, 0.814]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 9 	 [0.41, 0.76, 0.14] 	 lr: 5e-05 	 f: 82.6559028755593
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 13 [0m
Train...

[32m iter_0[0m
ga 	p: 83.02 	r: 75.06 	f1: 78.84 	 4630 	 5577 	 6168
wo 	p: 93.2 	r: 85.39 	f1: 89.13 	 2893 	 3104 	 3388
ni 	p: 87.68 	r: 75.3 	f1: 81.02 	 1146 	 1307 	 1522

[32m iter_1[0m
ga 	p: 83.2 	r: 75.31 	f1: 79.06 	 4645 	 5583 	 6168
wo 	p: 91.34 	r: 87.22 	f1: 89.23 	 2955 	 3235 	 3388
ni 	p: 85.59 	r: 76.48 	f1: 80.78 	 1164 	 1360 	 1522

[32m iter_2[0m
ga 	p: 82.47 	r: 75.81 	f1: 79.0 	 4676 	 5670 	 6168
wo 	p: 91.79 	r: 86.81 	f1: 89.23 	 2941 	 3204 	 3388
ni 	p: 86.55 	r: 75.69 	f1: 80.76 	 1152 	 1331 	 1522
best_thres [[0.5, 0.55, 0.13], [0.67, 0.42, 0.09], [0.67, 0.52, 0.1]]
f [0.823, 0.8238, 0.8239]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 9 	 [0.5, 0.79, 0.18] 	 lr: 5e-05 	 f: 82.64680105170903
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 11 [0m
Train...

[32m iter_0[0m
ga 	p: 82.29 	r: 75.7 	f1: 78.85 	 4669 	 5674 	 6168
wo 	p: 92.71 	r: 86.69 	f1: 89.6 	 2937 	 3168 	 3388
ni 	p: 84.69 	r: 80.68 	f1: 82.64 	 1228 	 1450 	 1522

[32m iter_1[0m
ga 	p: 82.05 	r: 75.91 	f1: 78.86 	 4682 	 5706 	 6168
wo 	p: 92.85 	r: 86.6 	f1: 89.62 	 2934 	 3160 	 3388
ni 	p: 84.94 	r: 80.42 	f1: 82.62 	 1224 	 1441 	 1522

[32m iter_2[0m
ga 	p: 80.65 	r: 77.19 	f1: 78.88 	 4761 	 5903 	 6168
wo 	p: 92.82 	r: 86.6 	f1: 89.6 	 2934 	 3161 	 3388
ni 	p: 83.89 	r: 81.41 	f1: 82.63 	 1239 	 1477 	 1522
best_thres [[0.46, 0.53, 0.14], [0.45, 0.53, 0.14], [0.37, 0.53, 0.11]]
f [0.8268, 0.8268, 0.8267]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 9 	 [0.37, 0.53, 0.11] 	 lr: 5e-05 	 f: 82.66691521421691
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 10 [0m
Train...
loss: tensor(409.7659) lr: 5e-05 time: 1205.18
pred_count_train 41644

Test...
loss: tensor(694.7648) lr: 5e-05 time: 1234.53
pred_count_train 41644

Test...
loss: tensor(860.4369) lr: 5e-05 time: 1202.19
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.15 	r: 73.05 	f1: 77.34 	 4506 	 5485 	 6168
wo 	p: 93.54 	r: 84.15 	f1: 88.6 	 2851 	 3048 	 3388
ni 	p: 84.68 	r: 74.11 	f1: 79.05 	 1128 	 1332 	 1522

[32m iter_1[0m
ga 	p: 82.18 	r: 72.91 	f1: 77.27 	 4497 	 5472 	 6168
wo 	p: 93.31 	r: 84.36 	f1: 88.61 	 2858 	 3063 	 3388
ni 	p: 86.99 	r: 72.47 	f1: 79.07 	 1103 	 1268 	 1522

[32m iter_2[0m
ga 	p: 82.18 	r: 72.88 	f1: 77.25 	 4495 	 5470 	 6168
wo 	p: 92.96 	r: 84.59 	f1: 88.58 	 2866 	 3083 	 3388
ni 	p: 86.17 	r: 73.26 	f1: 79.19 	 1115 	 1294 	 1522
best_thres [[0.35, 0.59, 0.05], [0.35, 0.84, 0.06], [0.35, 0.83, 0.05]]
f [0.8103, 0.8102, 0.8102]
load model: epoch9
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 9 	 [0.41, 0.76, 0.14] 	 lr: 5e-05 	 f: 82.6559028755593
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 14 [0m
Train...

[32m iter_0[0m
ga 	p: 82.18 	r: 73.85 	f1: 77.79 	 4555 	 5543 	 6168
wo 	p: 93.13 	r: 85.57 	f1: 89.19 	 2899 	 3113 	 3388
ni 	p: 82.65 	r: 77.0 	f1: 79.73 	 1172 	 1418 	 1522

[32m iter_1[0m
ga 	p: 82.32 	r: 73.88 	f1: 77.87 	 4557 	 5536 	 6168
wo 	p: 92.73 	r: 85.83 	f1: 89.15 	 2908 	 3136 	 3388
ni 	p: 86.31 	r: 74.57 	f1: 80.01 	 1135 	 1315 	 1522

[32m iter_2[0m
ga 	p: 82.85 	r: 73.56 	f1: 77.93 	 4537 	 5476 	 6168
wo 	p: 93.83 	r: 84.86 	f1: 89.12 	 2875 	 3064 	 3388
ni 	p: 84.21 	r: 76.41 	f1: 80.12 	 1163 	 1381 	 1522
best_thres [[0.3, 0.4, 0.09], [0.43, 0.51, 0.12], [0.54, 0.74, 0.08]]
f [0.8156, 0.8161, 0.8163]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 9 	 [0.5, 0.79, 0.18] 	 lr: 5e-05 	 f: 82.64680105170903
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 12 [0m
Train...

[32m iter_0[0m
ga 	p: 81.06 	r: 76.88 	f1: 78.91 	 4742 	 5850 	 6168
wo 	p: 93.41 	r: 85.39 	f1: 89.22 	 2893 	 3097 	 3388
ni 	p: 84.2 	r: 77.0 	f1: 80.44 	 1172 	 1392 	 1522

[32m iter_1[0m
ga 	p: 82.06 	r: 75.94 	f1: 78.88 	 4684 	 5708 	 6168
wo 	p: 92.94 	r: 85.83 	f1: 89.24 	 2908 	 3129 	 3388
ni 	p: 87.01 	r: 74.84 	f1: 80.47 	 1139 	 1309 	 1522

[32m iter_2[0m
ga 	p: 82.12 	r: 75.97 	f1: 78.93 	 4686 	 5706 	 6168
wo 	p: 93.82 	r: 85.12 	f1: 89.26 	 2884 	 3074 	 3388
ni 	p: 86.21 	r: 75.56 	f1: 80.53 	 1150 	 1334 	 1522
best_thres [[0.46, 0.67, 0.12], [0.52, 0.61, 0.16], [0.52, 0.7, 0.14]]
f [0.8224, 0.8226, 0.8227]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 9 	 [0.37, 0.53, 0.11] 	 lr: 5e-05 	 f: 82.66691521421691
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 11 [0m
Train...
loss: tensor(784.2318) lr: 2.5e-05 time: 1183.53
pred_count_train 41644

Test...
loss: tensor(543.4224) lr: 5e-05 time: 1217.4
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.73 	r: 73.87 	f1: 78.49 	 4556 	 5441 	 6168
wo 	p: 93.82 	r: 85.18 	f1: 89.29 	 2886 	 3076 	 3388
ni 	p: 85.28 	r: 79.57 	f1: 82.32 	 1211 	 1420 	 1522

[32m iter_1[0m
ga 	p: 82.62 	r: 74.69 	f1: 78.46 	 4607 	 5576 	 6168
wo 	p: 93.77 	r: 85.36 	f1: 89.37 	 2892 	 3084 	 3388
ni 	p: 85.91 	r: 79.3 	f1: 82.47 	 1207 	 1405 	 1522

[32m iter_2[0m
ga 	p: 82.9 	r: 74.53 	f1: 78.49 	 4597 	 5545 	 6168
wo 	p: 93.62 	r: 85.39 	f1: 89.32 	 2893 	 3090 	 3388
ni 	p: 85.89 	r: 79.17 	f1: 82.39 	 1205 	 1403 	 1522
best_thres [[0.54, 0.64, 0.16], [0.5, 0.81, 0.15], [0.61, 0.84, 0.15]]
f [0.8235, 0.8235, 0.8235]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 9 	 [0.41, 0.76, 0.14] 	 lr: 5e-05 	 f: 82.6559028755593
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 15 [0m
Train...
loss: tensor(675.7062) lr: 5e-05 time: 1184.73
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 80.78 	r: 74.76 	f1: 77.65 	 4611 	 5708 	 6168
wo 	p: 94.03 	r: 84.59 	f1: 89.06 	 2866 	 3048 	 3388
ni 	p: 84.57 	r: 74.9 	f1: 79.44 	 1140 	 1348 	 1522

[32m iter_1[0m
ga 	p: 81.03 	r: 74.85 	f1: 77.82 	 4617 	 5698 	 6168
wo 	p: 94.31 	r: 84.56 	f1: 89.17 	 2865 	 3038 	 3388
ni 	p: 84.87 	r: 75.16 	f1: 79.72 	 1144 	 1348 	 1522

[32m iter_2[0m
ga 	p: 80.95 	r: 74.97 	f1: 77.85 	 4624 	 5712 	 6168
wo 	p: 93.05 	r: 85.42 	f1: 89.07 	 2894 	 3110 	 3388
ni 	p: 85.82 	r: 74.38 	f1: 79.69 	 1132 	 1319 	 1522
best_thres [[0.27, 0.48, 0.1], [0.4, 0.76, 0.08], [0.45, 0.61, 0.09]]
f [0.8136, 0.8144, 0.8147]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 9 	 [0.5, 0.79, 0.18] 	 lr: 5e-05 	 f: 82.64680105170903
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 13 [0m
Train...

[32m iter_0[0m
ga 	p: 81.28 	r: 75.6 	f1: 78.34 	 4663 	 5737 	 6168
wo 	p: 94.6 	r: 84.21 	f1: 89.1 	 2853 	 3016 	 3388
ni 	p: 86.74 	r: 74.77 	f1: 80.31 	 1138 	 1312 	 1522

[32m iter_1[0m
ga 	p: 81.49 	r: 75.36 	f1: 78.3 	 4648 	 5704 	 6168
wo 	p: 94.53 	r: 84.09 	f1: 89.0 	 2849 	 3014 	 3388
ni 	p: 86.07 	r: 75.1 	f1: 80.21 	 1143 	 1328 	 1522

[32m iter_2[0m
ga 	p: 81.25 	r: 75.5 	f1: 78.27 	 4657 	 5732 	 6168
wo 	p: 94.26 	r: 84.36 	f1: 89.03 	 2858 	 3032 	 3388
ni 	p: 84.48 	r: 76.54 	f1: 80.32 	 1165 	 1379 	 1522
best_thres [[0.31, 0.7, 0.14], [0.32, 0.7, 0.11], [0.31, 0.66, 0.09]]
f [0.8186, 0.8183, 0.8182]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 9 	 [0.37, 0.53, 0.11] 	 lr: 5e-05 	 f: 82.66691521421691
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 12 [0m
Train...
loss: tensor(641.8786) lr: 2.5e-05 time: 1167.95
pred_count_train 41644

Test...
loss: tensor(426.3207) lr: 5e-05 time: 1198.61
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.8 	r: 74.76 	f1: 78.57 	 4611 	 5569 	 6168
wo 	p: 93.61 	r: 85.66 	f1: 89.46 	 2902 	 3100 	 3388
ni 	p: 87.9 	r: 74.44 	f1: 80.61 	 1133 	 1289 	 1522

[32m iter_1[0m
ga 	p: 84.47 	r: 73.56 	f1: 78.64 	 4537 	 5371 	 6168
wo 	p: 91.85 	r: 87.16 	f1: 89.44 	 2953 	 3215 	 3388
ni 	p: 89.42 	r: 73.85 	f1: 80.89 	 1124 	 1257 	 1522

[32m iter_2[0m
ga 	p: 84.51 	r: 73.57 	f1: 78.66 	 4538 	 5370 	 6168
wo 	p: 91.82 	r: 87.13 	f1: 89.41 	 2952 	 3215 	 3388
ni 	p: 88.98 	r: 74.24 	f1: 80.95 	 1130 	 1270 	 1522
best_thres [[0.38, 0.64, 0.17], [0.47, 0.41, 0.18], [0.47, 0.41, 0.16]]
f [0.822, 0.8227, 0.823]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 9 	 [0.41, 0.76, 0.14] 	 lr: 5e-05 	 f: 82.6559028755593
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 16 [0m
Train...
loss: tensor(512.2899) lr: 5e-05 time: 1168.95
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.25 	r: 73.96 	f1: 77.43 	 4562 	 5615 	 6168
wo 	p: 91.79 	r: 86.45 	f1: 89.04 	 2929 	 3191 	 3388
ni 	p: 83.25 	r: 76.41 	f1: 79.68 	 1163 	 1397 	 1522

[32m iter_1[0m
ga 	p: 81.66 	r: 74.01 	f1: 77.65 	 4565 	 5590 	 6168
wo 	p: 91.61 	r: 86.66 	f1: 89.06 	 2936 	 3205 	 3388
ni 	p: 88.87 	r: 72.4 	f1: 79.8 	 1102 	 1240 	 1522

[32m iter_2[0m
ga 	p: 82.01 	r: 73.54 	f1: 77.55 	 4536 	 5531 	 6168
wo 	p: 92.08 	r: 86.19 	f1: 89.04 	 2920 	 3171 	 3388
ni 	p: 87.98 	r: 73.13 	f1: 79.87 	 1113 	 1265 	 1522
best_thres [[0.29, 0.37, 0.05], [0.5, 0.55, 0.1], [0.69, 0.82, 0.08]]
f [0.8133, 0.8141, 0.8142]
load model: epoch9
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 9 	 [0.5, 0.79, 0.18] 	 lr: 5e-05 	 f: 82.64680105170903
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 14 [0m
Train...

[32m iter_0[0m
ga 	p: 81.25 	r: 74.77 	f1: 77.88 	 4612 	 5676 	 6168
wo 	p: 92.95 	r: 85.24 	f1: 88.93 	 2888 	 3107 	 3388
ni 	p: 87.07 	r: 74.77 	f1: 80.45 	 1138 	 1307 	 1522

[32m iter_1[0m
ga 	p: 80.99 	r: 75.15 	f1: 77.96 	 4635 	 5723 	 6168
wo 	p: 92.07 	r: 86.01 	f1: 88.94 	 2914 	 3165 	 3388
ni 	p: 86.2 	r: 75.49 	f1: 80.49 	 1149 	 1333 	 1522

[32m iter_2[0m
ga 	p: 80.94 	r: 75.13 	f1: 77.93 	 4634 	 5725 	 6168
wo 	p: 92.01 	r: 85.98 	f1: 88.89 	 2913 	 3166 	 3388
ni 	p: 86.32 	r: 75.43 	f1: 80.5 	 1148 	 1330 	 1522
best_thres [[0.49, 0.64, 0.15], [0.46, 0.51, 0.12], [0.46, 0.51, 0.12]]
f [0.8161, 0.8164, 0.8165]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 9 	 [0.37, 0.53, 0.11] 	 lr: 5e-05 	 f: 82.66691521421691
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 13 [0m
Train...
loss: tensor(522.2747) lr: 2.5e-05 time: 1159.12
pred_count_train 41644

Test...
loss: tensor(790.3950) lr: 2.5e-05 time: 1183.67
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.25 	r: 73.49 	f1: 77.63 	 4533 	 5511 	 6168
wo 	p: 92.8 	r: 85.54 	f1: 89.02 	 2898 	 3123 	 3388
ni 	p: 85.37 	r: 72.86 	f1: 78.62 	 1109 	 1299 	 1522

[32m iter_1[0m
ga 	p: 83.13 	r: 72.88 	f1: 77.67 	 4495 	 5407 	 6168
wo 	p: 93.09 	r: 85.48 	f1: 89.12 	 2896 	 3111 	 3388
ni 	p: 85.52 	r: 72.93 	f1: 78.72 	 1110 	 1298 	 1522

[32m iter_2[0m
ga 	p: 82.98 	r: 72.96 	f1: 77.65 	 4500 	 5423 	 6168
wo 	p: 93.31 	r: 85.24 	f1: 89.09 	 2888 	 3095 	 3388
ni 	p: 87.87 	r: 71.42 	f1: 78.8 	 1087 	 1237 	 1522
best_thres [[0.45, 0.42, 0.17], [0.65, 0.46, 0.15], [0.67, 0.49, 0.19]]
f [0.8129, 0.8133, 0.8134]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 9 	 [0.41, 0.76, 0.14] 	 lr: 5e-05 	 f: 82.6559028755593
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 17 [0m
Train...
loss: tensor(393.9891) lr: 5e-05 time: 1156.98
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.79 	r: 75.54 	f1: 78.54 	 4659 	 5696 	 6168
wo 	p: 93.52 	r: 85.57 	f1: 89.36 	 2899 	 3100 	 3388
ni 	p: 87.75 	r: 77.66 	f1: 82.4 	 1182 	 1347 	 1522

[32m iter_1[0m
ga 	p: 80.69 	r: 76.91 	f1: 78.76 	 4744 	 5879 	 6168
wo 	p: 92.79 	r: 86.25 	f1: 89.4 	 2922 	 3149 	 3388
ni 	p: 86.24 	r: 79.04 	f1: 82.48 	 1203 	 1395 	 1522

[32m iter_2[0m
ga 	p: 80.28 	r: 77.46 	f1: 78.84 	 4778 	 5952 	 6168
wo 	p: 92.39 	r: 86.69 	f1: 89.45 	 2937 	 3179 	 3388
ni 	p: 86.52 	r: 78.84 	f1: 82.5 	 1200 	 1387 	 1522
best_thres [[0.39, 0.6, 0.2], [0.43, 0.67, 0.15], [0.43, 0.66, 0.15]]
f [0.8237, 0.8244, 0.8248]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 9 	 [0.5, 0.79, 0.18] 	 lr: 5e-05 	 f: 82.64680105170903
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 15 [0m
Train...

[32m iter_0[0m
ga 	p: 81.67 	r: 74.17 	f1: 77.74 	 4575 	 5602 	 6168
wo 	p: 92.94 	r: 84.65 	f1: 88.6 	 2868 	 3086 	 3388
ni 	p: 85.64 	r: 72.86 	f1: 78.74 	 1109 	 1295 	 1522

[32m iter_1[0m
ga 	p: 82.45 	r: 73.74 	f1: 77.85 	 4548 	 5516 	 6168
wo 	p: 92.96 	r: 84.62 	f1: 88.6 	 2867 	 3084 	 3388
ni 	p: 84.25 	r: 74.18 	f1: 78.9 	 1129 	 1340 	 1522

[32m iter_2[0m
ga 	p: 82.21 	r: 73.93 	f1: 77.85 	 4560 	 5547 	 6168
wo 	p: 92.96 	r: 84.59 	f1: 88.58 	 2866 	 3083 	 3388
ni 	p: 84.3 	r: 74.11 	f1: 78.88 	 1128 	 1338 	 1522
best_thres [[0.39, 0.7, 0.08], [0.44, 0.68, 0.05], [0.42, 0.68, 0.05]]
f [0.8121, 0.8126, 0.8127]
load model: epoch9
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 9 	 [0.37, 0.53, 0.11] 	 lr: 5e-05 	 f: 82.66691521421691
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 14 [0m
Train...
loss: tensor(419.6187) lr: 2.5e-05 time: 1152.81
pred_count_train 41644

Test...
loss: tensor(648.4290) lr: 2.5e-05 time: 1170.52
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.41 	r: 73.77 	f1: 77.4 	 4550 	 5589 	 6168
wo 	p: 92.33 	r: 85.3 	f1: 88.68 	 2890 	 3130 	 3388
ni 	p: 85.85 	r: 70.17 	f1: 77.22 	 1068 	 1244 	 1522

[32m iter_1[0m
ga 	p: 81.87 	r: 73.72 	f1: 77.58 	 4547 	 5554 	 6168
wo 	p: 92.85 	r: 85.09 	f1: 88.8 	 2883 	 3105 	 3388
ni 	p: 83.72 	r: 71.94 	f1: 77.39 	 1095 	 1308 	 1522

[32m iter_2[0m
ga 	p: 81.96 	r: 73.64 	f1: 77.57 	 4542 	 5542 	 6168
wo 	p: 92.57 	r: 85.27 	f1: 88.77 	 2889 	 3121 	 3388
ni 	p: 83.25 	r: 72.47 	f1: 77.49 	 1103 	 1325 	 1522
best_thres [[0.39, 0.52, 0.2], [0.41, 0.83, 0.12], [0.42, 0.85, 0.11]]
f [0.8087, 0.8094, 0.8097]
load model: epoch9
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 9 	 [0.41, 0.76, 0.14] 	 lr: 5e-05 	 f: 82.6559028755593
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 18 [0m
Train...
loss: tensor(776.4031) lr: 2.5e-05 time: 1144.43
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.95 	r: 74.92 	f1: 78.28 	 4621 	 5639 	 6168
wo 	p: 93.62 	r: 85.74 	f1: 89.51 	 2905 	 3103 	 3388
ni 	p: 88.05 	r: 74.57 	f1: 80.75 	 1135 	 1289 	 1522

[32m iter_1[0m
ga 	p: 82.48 	r: 74.64 	f1: 78.37 	 4604 	 5582 	 6168
wo 	p: 93.22 	r: 86.04 	f1: 89.49 	 2915 	 3127 	 3388
ni 	p: 87.21 	r: 75.3 	f1: 80.82 	 1146 	 1314 	 1522

[32m iter_2[0m
ga 	p: 83.72 	r: 73.69 	f1: 78.38 	 4545 	 5429 	 6168
wo 	p: 92.85 	r: 86.3 	f1: 89.46 	 2924 	 3149 	 3388
ni 	p: 90.44 	r: 73.32 	f1: 80.99 	 1116 	 1234 	 1522
best_thres [[0.3, 0.56, 0.15], [0.46, 0.71, 0.11], [0.64, 0.73, 0.18]]
f [0.8206, 0.8209, 0.8213]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 9 	 [0.5, 0.79, 0.18] 	 lr: 5e-05 	 f: 82.64680105170903
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 16 [0m
Train...

[32m iter_0[0m
ga 	p: 82.56 	r: 75.13 	f1: 78.67 	 4634 	 5613 	 6168
wo 	p: 93.31 	r: 86.01 	f1: 89.51 	 2914 	 3123 	 3388
ni 	p: 86.74 	r: 79.11 	f1: 82.75 	 1204 	 1388 	 1522

[32m iter_1[0m
ga 	p: 83.05 	r: 75.13 	f1: 78.89 	 4634 	 5580 	 6168
wo 	p: 93.67 	r: 85.66 	f1: 89.49 	 2902 	 3098 	 3388
ni 	p: 86.87 	r: 78.65 	f1: 82.55 	 1197 	 1378 	 1522

[32m iter_2[0m
ga 	p: 83.05 	r: 75.16 	f1: 78.91 	 4636 	 5582 	 6168
wo 	p: 93.56 	r: 85.77 	f1: 89.5 	 2906 	 3106 	 3388
ni 	p: 86.64 	r: 78.84 	f1: 82.56 	 1200 	 1385 	 1522
best_thres [[0.54, 0.64, 0.19], [0.55, 0.67, 0.19], [0.55, 0.65, 0.18]]
f [0.8256, 0.826, 0.8262]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 9 	 [0.37, 0.53, 0.11] 	 lr: 5e-05 	 f: 82.66691521421691
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 15 [0m
Train...
loss: tensor(752.1550) lr: 1.25e-05 time: 1146.55
pred_count_train 41644

Test...
loss: tensor(530.2165) lr: 2.5e-05 time: 1156.15
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.34 	r: 74.71 	f1: 78.79 	 4608 	 5529 	 6168
wo 	p: 92.14 	r: 87.49 	f1: 89.75 	 2964 	 3217 	 3388
ni 	p: 89.2 	r: 76.54 	f1: 82.39 	 1165 	 1306 	 1522

[32m iter_1[0m
ga 	p: 82.98 	r: 75.26 	f1: 78.93 	 4642 	 5594 	 6168
wo 	p: 91.96 	r: 87.75 	f1: 89.81 	 2973 	 3233 	 3388
ni 	p: 87.38 	r: 78.25 	f1: 82.56 	 1191 	 1363 	 1522

[32m iter_2[0m
ga 	p: 83.01 	r: 75.18 	f1: 78.9 	 4637 	 5586 	 6168
wo 	p: 91.98 	r: 87.66 	f1: 89.77 	 2970 	 3229 	 3388
ni 	p: 87.48 	r: 78.06 	f1: 82.5 	 1188 	 1358 	 1522
best_thres [[0.45, 0.38, 0.19], [0.42, 0.34, 0.14], [0.42, 0.34, 0.14]]
f [0.827, 0.8275, 0.8276]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 18 	 [0.42, 0.34, 0.14] 	 lr: 1.25e-05 	 f: 82.76013762981351
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 19 [0m
Train...
loss: tensor(638.1101) lr: 2.5e-05 time: 1131.83
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.59 	r: 72.2 	f1: 77.48 	 4453 	 5327 	 6168
wo 	p: 91.91 	r: 86.81 	f1: 89.28 	 2941 	 3200 	 3388
ni 	p: 82.27 	r: 75.62 	f1: 78.81 	 1151 	 1399 	 1522

[32m iter_1[0m
ga 	p: 81.65 	r: 74.01 	f1: 77.64 	 4565 	 5591 	 6168
wo 	p: 91.88 	r: 86.81 	f1: 89.27 	 2941 	 3201 	 3388
ni 	p: 89.84 	r: 70.89 	f1: 79.25 	 1079 	 1201 	 1522

[32m iter_2[0m
ga 	p: 81.47 	r: 74.34 	f1: 77.74 	 4585 	 5628 	 6168
wo 	p: 93.23 	r: 85.8 	f1: 89.36 	 2907 	 3118 	 3388
ni 	p: 89.81 	r: 71.22 	f1: 79.44 	 1084 	 1207 	 1522
best_thres [[0.47, 0.28, 0.12], [0.58, 0.37, 0.31], [0.67, 0.74, 0.3]]
f [0.8137, 0.8143, 0.8147]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 9 	 [0.5, 0.79, 0.18] 	 lr: 5e-05 	 f: 82.64680105170903
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 17 [0m
Train...

[32m iter_0[0m
ga 	p: 81.79 	r: 75.75 	f1: 78.65 	 4672 	 5712 	 6168
wo 	p: 93.9 	r: 85.89 	f1: 89.72 	 2910 	 3099 	 3388
ni 	p: 85.92 	r: 77.0 	f1: 81.22 	 1172 	 1364 	 1522

[32m iter_1[0m
ga 	p: 83.74 	r: 74.04 	f1: 78.59 	 4567 	 5454 	 6168
wo 	p: 93.03 	r: 86.66 	f1: 89.73 	 2936 	 3156 	 3388
ni 	p: 87.43 	r: 75.89 	f1: 81.25 	 1155 	 1321 	 1522

[32m iter_2[0m
ga 	p: 83.71 	r: 74.04 	f1: 78.58 	 4567 	 5456 	 6168
wo 	p: 93.03 	r: 86.66 	f1: 89.73 	 2936 	 3156 	 3388
ni 	p: 87.45 	r: 76.02 	f1: 81.34 	 1157 	 1323 	 1522
best_thres [[0.37, 0.7, 0.13], [0.49, 0.58, 0.15], [0.49, 0.58, 0.15]]
f [0.8238, 0.824, 0.8241]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 9 	 [0.37, 0.53, 0.11] 	 lr: 5e-05 	 f: 82.66691521421691
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 16 [0m
Train...
loss: tensor(652.4771) lr: 1.25e-05 time: 1141.68
pred_count_train 41644

Test...
loss: tensor(429.2719) lr: 2.5e-05 time: 1142.33
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.47 	r: 75.13 	f1: 78.63 	 4634 	 5619 	 6168
wo 	p: 91.43 	r: 87.81 	f1: 89.58 	 2975 	 3254 	 3388
ni 	p: 87.46 	r: 76.54 	f1: 81.64 	 1165 	 1332 	 1522

[32m iter_1[0m
ga 	p: 82.57 	r: 74.66 	f1: 78.42 	 4605 	 5577 	 6168
wo 	p: 91.92 	r: 87.28 	f1: 89.54 	 2957 	 3217 	 3388
ni 	p: 86.96 	r: 77.14 	f1: 81.75 	 1174 	 1350 	 1522

[32m iter_2[0m
ga 	p: 82.41 	r: 74.84 	f1: 78.44 	 4616 	 5601 	 6168
wo 	p: 91.92 	r: 87.25 	f1: 89.52 	 2956 	 3216 	 3388
ni 	p: 87.89 	r: 76.28 	f1: 81.67 	 1161 	 1321 	 1522
best_thres [[0.37, 0.32, 0.17], [0.38, 0.35, 0.14], [0.37, 0.35, 0.16]]
f [0.8245, 0.8239, 0.8237]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 18 	 [0.42, 0.34, 0.14] 	 lr: 1.25e-05 	 f: 82.76013762981351
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 20 [0m
Train...
loss: tensor(518.2200) lr: 2.5e-05 time: 1117.25
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.99 	r: 72.97 	f1: 77.22 	 4501 	 5490 	 6168
wo 	p: 94.06 	r: 84.18 	f1: 88.85 	 2852 	 3032 	 3388
ni 	p: 84.31 	r: 69.91 	f1: 76.44 	 1064 	 1262 	 1522

[32m iter_1[0m
ga 	p: 81.97 	r: 73.17 	f1: 77.32 	 4513 	 5506 	 6168
wo 	p: 92.51 	r: 85.66 	f1: 88.95 	 2902 	 3137 	 3388
ni 	p: 87.55 	r: 68.4 	f1: 76.8 	 1041 	 1189 	 1522

[32m iter_2[0m
ga 	p: 81.58 	r: 73.61 	f1: 77.39 	 4540 	 5565 	 6168
wo 	p: 91.28 	r: 86.81 	f1: 88.99 	 2941 	 3222 	 3388
ni 	p: 87.75 	r: 68.73 	f1: 77.08 	 1046 	 1192 	 1522
best_thres [[0.3, 0.66, 0.15], [0.49, 0.7, 0.21], [0.55, 0.53, 0.17]]
f [0.8069, 0.8079, 0.8085]
load model: epoch9
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 9 	 [0.5, 0.79, 0.18] 	 lr: 5e-05 	 f: 82.64680105170903
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 18 [0m
Train...
loss: tensor(572.8145) lr: 1.25e-05 time: 1144.9
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.0 	r: 73.44 	f1: 77.93 	 4530 	 5458 	 6168
wo 	p: 92.0 	r: 86.54 	f1: 89.19 	 2932 	 3187 	 3388
ni 	p: 87.71 	r: 73.59 	f1: 80.03 	 1120 	 1277 	 1522

[32m iter_1[0m
ga 	p: 84.88 	r: 71.97 	f1: 77.89 	 4439 	 5230 	 6168
wo 	p: 92.68 	r: 85.89 	f1: 89.15 	 2910 	 3140 	 3388
ni 	p: 88.31 	r: 73.0 	f1: 79.93 	 1111 	 1258 	 1522

[32m iter_2[0m
ga 	p: 82.27 	r: 73.93 	f1: 77.88 	 4560 	 5543 	 6168
wo 	p: 92.02 	r: 86.45 	f1: 89.15 	 2929 	 3183 	 3388
ni 	p: 90.04 	r: 71.88 	f1: 79.94 	 1094 	 1215 	 1522
best_thres [[0.53, 0.4, 0.23], [0.66, 0.49, 0.23], [0.48, 0.4, 0.28]]
f [0.8173, 0.8172, 0.8171]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 9 	 [0.37, 0.53, 0.11] 	 lr: 5e-05 	 f: 82.66691521421691
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 17 [0m
Train...
loss: tensor(757.0370) lr: 1.25e-05 time: 1137.68
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 84.4 	r: 73.25 	f1: 78.43 	 4518 	 5353 	 6168
wo 	p: 92.34 	r: 86.51 	f1: 89.33 	 2931 	 3174 	 3388
ni 	p: 86.79 	r: 73.39 	f1: 79.53 	 1117 	 1287 	 1522

[32m iter_1[0m
ga 	p: 84.35 	r: 73.31 	f1: 78.45 	 4522 	 5361 	 6168
wo 	p: 92.65 	r: 86.3 	f1: 89.36 	 2924 	 3156 	 3388
ni 	p: 86.99 	r: 73.78 	f1: 79.84 	 1123 	 1291 	 1522

[32m iter_2[0m
ga 	p: 84.1 	r: 73.49 	f1: 78.44 	 4533 	 5390 	 6168
wo 	p: 92.68 	r: 86.3 	f1: 89.38 	 2924 	 3155 	 3388
ni 	p: 87.46 	r: 73.32 	f1: 79.77 	 1116 	 1276 	 1522
best_thres [[0.49, 0.43, 0.16], [0.53, 0.45, 0.14], [0.49, 0.45, 0.15]]
f [0.82, 0.8203, 0.8203]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 18 	 [0.42, 0.34, 0.14] 	 lr: 1.25e-05 	 f: 82.76013762981351
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 21 [0m
Train...

[32m iter_0[0m
ga 	p: 85.16 	r: 73.3 	f1: 78.78 	 4521 	 5309 	 6168
wo 	p: 93.25 	r: 86.51 	f1: 89.76 	 2931 	 3143 	 3388
ni 	p: 88.76 	r: 77.79 	f1: 82.91 	 1184 	 1334 	 1522

[32m iter_1[0m
ga 	p: 84.18 	r: 74.3 	f1: 78.94 	 4583 	 5444 	 6168
wo 	p: 93.65 	r: 86.13 	f1: 89.73 	 2918 	 3116 	 3388
ni 	p: 88.74 	r: 77.66 	f1: 82.83 	 1182 	 1332 	 1522

[32m iter_2[0m
ga 	p: 83.35 	r: 75.05 	f1: 78.98 	 4629 	 5554 	 6168
wo 	p: 93.64 	r: 86.07 	f1: 89.7 	 2916 	 3114 	 3388
ni 	p: 89.0 	r: 77.6 	f1: 82.91 	 1181 	 1327 	 1522
best_thres [[0.47, 0.47, 0.16], [0.55, 0.63, 0.15], [0.54, 0.69, 0.15]]
f [0.8278, 0.828, 0.828]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 18 	 [0.54, 0.69, 0.15] 	 lr: 1.25e-05 	 f: 82.8047753032254
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 19 [0m
Train...
loss: tensor(414.9655) lr: 2.5e-05 time: 1120.2
pred_count_train 41644

Test...
loss: tensor(500.8884) lr: 1.25e-05 time: 1154.44
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.16 	r: 74.12 	f1: 77.48 	 4572 	 5633 	 6168
wo 	p: 92.84 	r: 84.95 	f1: 88.72 	 2878 	 3100 	 3388
ni 	p: 87.46 	r: 70.11 	f1: 77.83 	 1067 	 1220 	 1522

[32m iter_1[0m
ga 	p: 80.48 	r: 74.95 	f1: 77.62 	 4623 	 5744 	 6168
wo 	p: 92.9 	r: 85.01 	f1: 88.78 	 2880 	 3100 	 3388
ni 	p: 85.53 	r: 71.09 	f1: 77.65 	 1082 	 1265 	 1522

[32m iter_2[0m
ga 	p: 80.51 	r: 74.95 	f1: 77.63 	 4623 	 5742 	 6168
wo 	p: 92.9 	r: 84.98 	f1: 88.76 	 2879 	 3099 	 3388
ni 	p: 85.66 	r: 71.02 	f1: 77.66 	 1081 	 1262 	 1522
best_thres [[0.43, 0.7, 0.24], [0.36, 0.69, 0.17], [0.36, 0.69, 0.17]]
f [0.8099, 0.8102, 0.8103]
load model: epoch9
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 9 	 [0.37, 0.53, 0.11] 	 lr: 5e-05 	 f: 82.66691521421691
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 18 [0m
Train...
loss: tensor(657.3154) lr: 1.25e-05 time: 1139.02
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.63 	r: 74.16 	f1: 77.72 	 4574 	 5603 	 6168
wo 	p: 92.6 	r: 85.74 	f1: 89.04 	 2905 	 3137 	 3388
ni 	p: 86.96 	r: 74.51 	f1: 80.25 	 1134 	 1304 	 1522

[32m iter_1[0m
ga 	p: 81.09 	r: 74.59 	f1: 77.71 	 4601 	 5674 	 6168
wo 	p: 91.74 	r: 86.54 	f1: 89.06 	 2932 	 3196 	 3388
ni 	p: 88.61 	r: 74.11 	f1: 80.72 	 1128 	 1273 	 1522

[32m iter_2[0m
ga 	p: 81.1 	r: 74.59 	f1: 77.71 	 4601 	 5673 	 6168
wo 	p: 91.7 	r: 86.45 	f1: 89.0 	 2929 	 3194 	 3388
ni 	p: 88.72 	r: 73.92 	f1: 80.65 	 1125 	 1268 	 1522
best_thres [[0.36, 0.56, 0.18], [0.32, 0.45, 0.19], [0.32, 0.45, 0.19]]
f [0.8155, 0.8159, 0.8159]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 18 	 [0.42, 0.34, 0.14] 	 lr: 1.25e-05 	 f: 82.76013762981351
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 22 [0m
Train...

[32m iter_0[0m
ga 	p: 83.39 	r: 73.99 	f1: 78.41 	 4564 	 5473 	 6168
wo 	p: 93.49 	r: 86.07 	f1: 89.63 	 2916 	 3119 	 3388
ni 	p: 89.64 	r: 75.62 	f1: 82.04 	 1151 	 1284 	 1522

[32m iter_1[0m
ga 	p: 81.88 	r: 75.62 	f1: 78.62 	 4664 	 5696 	 6168
wo 	p: 92.45 	r: 87.07 	f1: 89.68 	 2950 	 3191 	 3388
ni 	p: 88.54 	r: 76.15 	f1: 81.88 	 1159 	 1309 	 1522

[32m iter_2[0m
ga 	p: 83.07 	r: 74.72 	f1: 78.68 	 4609 	 5548 	 6168
wo 	p: 93.0 	r: 86.66 	f1: 89.72 	 2936 	 3157 	 3388
ni 	p: 88.97 	r: 75.76 	f1: 81.83 	 1153 	 1296 	 1522
best_thres [[0.36, 0.52, 0.22], [0.37, 0.48, 0.18], [0.51, 0.65, 0.19]]
f [0.8238, 0.8243, 0.8246]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 18 	 [0.54, 0.69, 0.15] 	 lr: 1.25e-05 	 f: 82.8047753032254
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 20 [0m
Train...
loss: tensor(746.3635) lr: 1.25e-05 time: 1132.15
pred_count_train 41644

Test...
loss: tensor(435.7270) lr: 1.25e-05 time: 1167.16
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.52 	r: 75.1 	f1: 79.08 	 4632 	 5546 	 6168
wo 	p: 92.13 	r: 87.43 	f1: 89.72 	 2962 	 3215 	 3388
ni 	p: 89.25 	r: 76.94 	f1: 82.64 	 1171 	 1312 	 1522

[32m iter_1[0m
ga 	p: 83.7 	r: 75.11 	f1: 79.18 	 4633 	 5535 	 6168
wo 	p: 92.35 	r: 87.34 	f1: 89.78 	 2959 	 3204 	 3388
ni 	p: 89.07 	r: 77.07 	f1: 82.63 	 1173 	 1317 	 1522

[32m iter_2[0m
ga 	p: 84.16 	r: 74.77 	f1: 79.19 	 4612 	 5480 	 6168
wo 	p: 92.32 	r: 87.34 	f1: 89.76 	 2959 	 3205 	 3388
ni 	p: 89.07 	r: 77.07 	f1: 82.63 	 1173 	 1317 	 1522
best_thres [[0.5, 0.4, 0.19], [0.5, 0.41, 0.18], [0.52, 0.41, 0.18]]
f [0.8288, 0.8291, 0.8293]
save model
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 18 	 [0.52, 0.41, 0.18] 	 lr: 1.25e-05 	 f: 82.92906178489703
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 19 [0m
Train...
loss: tensor(578.4814) lr: 1.25e-05 time: 1144.21
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.41 	r: 72.71 	f1: 77.7 	 4485 	 5377 	 6168
wo 	p: 93.37 	r: 85.18 	f1: 89.09 	 2886 	 3091 	 3388
ni 	p: 82.43 	r: 75.23 	f1: 78.67 	 1145 	 1389 	 1522

[32m iter_1[0m
ga 	p: 82.34 	r: 73.61 	f1: 77.73 	 4540 	 5514 	 6168
wo 	p: 92.63 	r: 86.04 	f1: 89.21 	 2915 	 3147 	 3388
ni 	p: 82.73 	r: 75.56 	f1: 78.98 	 1150 	 1390 	 1522

[32m iter_2[0m
ga 	p: 82.34 	r: 73.69 	f1: 77.77 	 4545 	 5520 	 6168
wo 	p: 92.63 	r: 86.04 	f1: 89.21 	 2915 	 3147 	 3388
ni 	p: 83.04 	r: 75.62 	f1: 79.16 	 1151 	 1386 	 1522
best_thres [[0.51, 0.62, 0.12], [0.45, 0.75, 0.09], [0.44, 0.81, 0.09]]
f [0.8136, 0.814, 0.8144]
load model: epoch18
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 18 	 [0.42, 0.34, 0.14] 	 lr: 1.25e-05 	 f: 82.76013762981351
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 23 [0m
Train...

[32m iter_0[0m
ga 	p: 82.03 	r: 74.81 	f1: 78.25 	 4614 	 5625 	 6168
wo 	p: 93.07 	r: 86.04 	f1: 89.42 	 2915 	 3132 	 3388
ni 	p: 87.44 	r: 73.65 	f1: 79.96 	 1121 	 1282 	 1522

[32m iter_1[0m
ga 	p: 82.9 	r: 74.37 	f1: 78.4 	 4587 	 5533 	 6168
wo 	p: 92.88 	r: 86.28 	f1: 89.46 	 2923 	 3147 	 3388
ni 	p: 88.52 	r: 73.46 	f1: 80.29 	 1118 	 1263 	 1522

[32m iter_2[0m
ga 	p: 83.14 	r: 74.21 	f1: 78.42 	 4577 	 5505 	 6168
wo 	p: 92.92 	r: 86.33 	f1: 89.5 	 2925 	 3148 	 3388
ni 	p: 88.35 	r: 73.72 	f1: 80.37 	 1122 	 1270 	 1522
best_thres [[0.34, 0.5, 0.17], [0.56, 0.64, 0.16], [0.67, 0.7, 0.15]]
f [0.8192, 0.8201, 0.8205]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 18 	 [0.54, 0.69, 0.15] 	 lr: 1.25e-05 	 f: 82.8047753032254
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 21 [0m
Train...
loss: tensor(650.9977) lr: 1.25e-05 time: 1143.19
pred_count_train 41644

Test...
loss: tensor(634.9773) lr: 6.25e-06 time: 1182.28
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.37 	r: 74.59 	f1: 78.74 	 4601 	 5519 	 6168
wo 	p: 92.44 	r: 86.95 	f1: 89.61 	 2946 	 3187 	 3388
ni 	p: 88.12 	r: 76.48 	f1: 81.89 	 1164 	 1321 	 1522

[32m iter_1[0m
ga 	p: 82.46 	r: 75.55 	f1: 78.86 	 4660 	 5651 	 6168
wo 	p: 92.88 	r: 86.66 	f1: 89.66 	 2936 	 3161 	 3388
ni 	p: 88.91 	r: 75.82 	f1: 81.84 	 1154 	 1298 	 1522

[32m iter_2[0m
ga 	p: 82.45 	r: 75.54 	f1: 78.84 	 4659 	 5651 	 6168
wo 	p: 92.2 	r: 87.25 	f1: 89.66 	 2956 	 3206 	 3388
ni 	p: 88.97 	r: 75.82 	f1: 81.87 	 1154 	 1297 	 1522
best_thres [[0.47, 0.48, 0.19], [0.4, 0.52, 0.2], [0.4, 0.43, 0.2]]
f [0.8255, 0.8257, 0.8258]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 18 	 [0.52, 0.41, 0.18] 	 lr: 1.25e-05 	 f: 82.92906178489703
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 20 [0m
Train...
loss: tensor(505.7385) lr: 1.25e-05 time: 1153.0
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.91 	r: 74.64 	f1: 78.56 	 4604 	 5553 	 6168
wo 	p: 91.99 	r: 87.4 	f1: 89.63 	 2961 	 3219 	 3388
ni 	p: 87.88 	r: 76.68 	f1: 81.89 	 1167 	 1328 	 1522

[32m iter_1[0m
ga 	p: 83.5 	r: 74.11 	f1: 78.53 	 4571 	 5474 	 6168
wo 	p: 91.94 	r: 87.51 	f1: 89.67 	 2965 	 3225 	 3388
ni 	p: 87.07 	r: 77.4 	f1: 81.95 	 1178 	 1353 	 1522

[32m iter_2[0m
ga 	p: 83.51 	r: 74.12 	f1: 78.54 	 4572 	 5475 	 6168
wo 	p: 92.29 	r: 87.25 	f1: 89.7 	 2956 	 3203 	 3388
ni 	p: 87.19 	r: 77.33 	f1: 81.96 	 1177 	 1350 	 1522
best_thres [[0.41, 0.36, 0.2], [0.44, 0.33, 0.16], [0.44, 0.36, 0.16]]
f [0.8246, 0.8247, 0.8248]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 18 	 [0.42, 0.34, 0.14] 	 lr: 1.25e-05 	 f: 82.76013762981351
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 24 [0m
Train...

[32m iter_0[0m
ga 	p: 80.27 	r: 75.26 	f1: 77.68 	 4642 	 5783 	 6168
wo 	p: 93.61 	r: 85.21 	f1: 89.22 	 2887 	 3084 	 3388
ni 	p: 88.58 	r: 74.44 	f1: 80.9 	 1133 	 1279 	 1522

[32m iter_1[0m
ga 	p: 81.85 	r: 74.56 	f1: 78.04 	 4599 	 5619 	 6168
wo 	p: 93.12 	r: 85.89 	f1: 89.36 	 2910 	 3125 	 3388
ni 	p: 89.06 	r: 74.31 	f1: 81.02 	 1131 	 1270 	 1522

[32m iter_2[0m
ga 	p: 82.29 	r: 74.42 	f1: 78.15 	 4590 	 5578 	 6168
wo 	p: 92.89 	r: 85.95 	f1: 89.28 	 2912 	 3135 	 3388
ni 	p: 89.03 	r: 74.11 	f1: 80.89 	 1128 	 1267 	 1522
best_thres [[0.26, 0.63, 0.2], [0.52, 0.79, 0.2], [0.68, 0.84, 0.2]]
f [0.8162, 0.8178, 0.8184]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 18 	 [0.54, 0.69, 0.15] 	 lr: 1.25e-05 	 f: 82.8047753032254
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 22 [0m
Train...
loss: tensor(574.6939) lr: 1.25e-05 time: 1153.85
pred_count_train 41644

Test...
loss: tensor(586.4336) lr: 6.25e-06 time: 1196.2
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.32 	r: 75.47 	f1: 78.74 	 4655 	 5655 	 6168
wo 	p: 92.75 	r: 86.51 	f1: 89.52 	 2931 	 3160 	 3388
ni 	p: 87.56 	r: 73.52 	f1: 79.93 	 1119 	 1278 	 1522

[32m iter_1[0m
ga 	p: 83.33 	r: 74.74 	f1: 78.8 	 4610 	 5532 	 6168
wo 	p: 92.84 	r: 86.45 	f1: 89.53 	 2929 	 3155 	 3388
ni 	p: 87.35 	r: 73.52 	f1: 79.84 	 1119 	 1281 	 1522

[32m iter_2[0m
ga 	p: 83.47 	r: 74.59 	f1: 78.78 	 4601 	 5512 	 6168
wo 	p: 92.81 	r: 86.42 	f1: 89.5 	 2928 	 3155 	 3388
ni 	p: 87.42 	r: 73.52 	f1: 79.87 	 1119 	 1280 	 1522
best_thres [[0.43, 0.54, 0.17], [0.48, 0.53, 0.16], [0.49, 0.53, 0.16]]
f [0.8224, 0.8226, 0.8226]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 18 	 [0.52, 0.41, 0.18] 	 lr: 1.25e-05 	 f: 82.92906178489703
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 21 [0m
Train...
loss: tensor(438.3582) lr: 1.25e-05 time: 1160.34
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.3 	r: 74.77 	f1: 78.36 	 4612 	 5604 	 6168
wo 	p: 92.82 	r: 86.57 	f1: 89.58 	 2933 	 3160 	 3388
ni 	p: 86.16 	r: 76.48 	f1: 81.03 	 1164 	 1351 	 1522

[32m iter_1[0m
ga 	p: 82.36 	r: 74.66 	f1: 78.32 	 4605 	 5591 	 6168
wo 	p: 91.91 	r: 87.13 	f1: 89.45 	 2952 	 3212 	 3388
ni 	p: 87.0 	r: 76.48 	f1: 81.4 	 1164 	 1338 	 1522

[32m iter_2[0m
ga 	p: 82.78 	r: 74.37 	f1: 78.35 	 4587 	 5541 	 6168
wo 	p: 91.9 	r: 87.04 	f1: 89.4 	 2949 	 3209 	 3388
ni 	p: 87.03 	r: 76.28 	f1: 81.3 	 1161 	 1334 	 1522
best_thres [[0.4, 0.44, 0.13], [0.4, 0.33, 0.11], [0.42, 0.33, 0.11]]
f [0.8219, 0.8219, 0.8219]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 18 	 [0.42, 0.34, 0.14] 	 lr: 1.25e-05 	 f: 82.76013762981351
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 25 [0m
Train...

[32m iter_0[0m
ga 	p: 80.51 	r: 74.87 	f1: 77.59 	 4618 	 5736 	 6168
wo 	p: 93.08 	r: 85.71 	f1: 89.24 	 2904 	 3120 	 3388
ni 	p: 88.41 	r: 71.16 	f1: 78.85 	 1083 	 1225 	 1522

[32m iter_1[0m
ga 	p: 82.42 	r: 73.8 	f1: 77.87 	 4552 	 5523 	 6168
wo 	p: 93.51 	r: 85.45 	f1: 89.3 	 2895 	 3096 	 3388
ni 	p: 88.62 	r: 71.62 	f1: 79.22 	 1090 	 1230 	 1522

[32m iter_2[0m
ga 	p: 81.7 	r: 74.38 	f1: 77.87 	 4588 	 5616 	 6168
wo 	p: 92.91 	r: 85.83 	f1: 89.23 	 2908 	 3130 	 3388
ni 	p: 88.28 	r: 71.75 	f1: 79.16 	 1092 	 1237 	 1522
best_thres [[0.28, 0.51, 0.22], [0.61, 0.82, 0.22], [0.68, 0.85, 0.21]]
f [0.8134, 0.8146, 0.8149]
load model: epoch18
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 18 	 [0.54, 0.69, 0.15] 	 lr: 1.25e-05 	 f: 82.8047753032254
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 23 [0m
Train...
loss: tensor(503.2594) lr: 1.25e-05 time: 1164.95
pred_count_train 41644

Test...
loss: tensor(545.1979) lr: 6.25e-06 time: 1211.33
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.42 	r: 75.16 	f1: 78.17 	 4636 	 5694 	 6168
wo 	p: 91.98 	r: 86.66 	f1: 89.24 	 2936 	 3192 	 3388
ni 	p: 84.96 	r: 77.92 	f1: 81.29 	 1186 	 1396 	 1522

[32m iter_1[0m
ga 	p: 82.71 	r: 74.24 	f1: 78.25 	 4579 	 5536 	 6168
wo 	p: 91.87 	r: 86.69 	f1: 89.2 	 2937 	 3197 	 3388
ni 	p: 88.75 	r: 74.64 	f1: 81.08 	 1136 	 1280 	 1522

[32m iter_2[0m
ga 	p: 82.71 	r: 74.24 	f1: 78.25 	 4579 	 5536 	 6168
wo 	p: 91.79 	r: 86.81 	f1: 89.23 	 2941 	 3204 	 3388
ni 	p: 89.09 	r: 74.57 	f1: 81.19 	 1135 	 1274 	 1522
best_thres [[0.39, 0.54, 0.14], [0.47, 0.51, 0.22], [0.47, 0.5, 0.23]]
f [0.82, 0.8202, 0.8204]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 18 	 [0.52, 0.41, 0.18] 	 lr: 1.25e-05 	 f: 82.92906178489703
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 22 [0m
Train...
loss: tensor(641.7456) lr: 6.25e-06 time: 1171.4
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.47 	r: 74.27 	f1: 78.15 	 4581 	 5555 	 6168
wo 	p: 91.71 	r: 87.22 	f1: 89.41 	 2955 	 3222 	 3388
ni 	p: 85.88 	r: 76.35 	f1: 80.83 	 1162 	 1353 	 1522

[32m iter_1[0m
ga 	p: 84.0 	r: 73.12 	f1: 78.18 	 4510 	 5369 	 6168
wo 	p: 92.74 	r: 86.36 	f1: 89.44 	 2926 	 3155 	 3388
ni 	p: 83.82 	r: 77.92 	f1: 80.76 	 1186 	 1415 	 1522

[32m iter_2[0m
ga 	p: 83.76 	r: 73.41 	f1: 78.24 	 4528 	 5406 	 6168
wo 	p: 92.71 	r: 86.33 	f1: 89.41 	 2925 	 3155 	 3388
ni 	p: 86.76 	r: 75.76 	f1: 80.88 	 1153 	 1329 	 1522
best_thres [[0.43, 0.38, 0.17], [0.67, 0.47, 0.11], [0.69, 0.47, 0.16]]
f [0.8203, 0.8204, 0.8205]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 18 	 [0.42, 0.34, 0.14] 	 lr: 1.25e-05 	 f: 82.76013762981351
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 26 [0m
Train...

[32m iter_0[0m
ga 	p: 84.1 	r: 73.85 	f1: 78.64 	 4555 	 5416 	 6168
wo 	p: 93.42 	r: 86.36 	f1: 89.75 	 2926 	 3132 	 3388
ni 	p: 88.29 	r: 76.81 	f1: 82.15 	 1169 	 1324 	 1522

[32m iter_1[0m
ga 	p: 84.02 	r: 74.09 	f1: 78.75 	 4570 	 5439 	 6168
wo 	p: 94.13 	r: 85.68 	f1: 89.71 	 2903 	 3084 	 3388
ni 	p: 89.08 	r: 76.61 	f1: 82.37 	 1166 	 1309 	 1522

[32m iter_2[0m
ga 	p: 83.46 	r: 74.51 	f1: 78.73 	 4596 	 5507 	 6168
wo 	p: 93.08 	r: 86.6 	f1: 89.72 	 2934 	 3152 	 3388
ni 	p: 89.14 	r: 76.54 	f1: 82.36 	 1165 	 1307 	 1522
best_thres [[0.42, 0.48, 0.21], [0.57, 0.72, 0.21], [0.59, 0.62, 0.2]]
f [0.8258, 0.826, 0.8261]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 18 	 [0.54, 0.69, 0.15] 	 lr: 1.25e-05 	 f: 82.8047753032254
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 24 [0m
Train...
loss: tensor(437.5445) lr: 1.25e-05 time: 1173.08
pred_count_train 41644

Test...
loss: tensor(504.3843) lr: 6.25e-06 time: 1225.26
pred_count_train 41644

Test...
loss: tensor(592.2073) lr: 6.25e-06 time: 1188.56
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.99 	r: 74.42 	f1: 78.02 	 4590 	 5598 	 6168
wo 	p: 92.14 	r: 86.45 	f1: 89.2 	 2929 	 3179 	 3388
ni 	p: 88.52 	r: 71.94 	f1: 79.38 	 1095 	 1237 	 1522

[32m iter_1[0m
ga 	p: 80.29 	r: 75.97 	f1: 78.07 	 4686 	 5836 	 6168
wo 	p: 92.07 	r: 86.39 	f1: 89.14 	 2927 	 3179 	 3388
ni 	p: 88.16 	r: 71.94 	f1: 79.23 	 1095 	 1242 	 1522

[32m iter_2[0m
ga 	p: 80.88 	r: 75.37 	f1: 78.03 	 4649 	 5748 	 6168
wo 	p: 92.18 	r: 86.28 	f1: 89.13 	 2923 	 3171 	 3388
ni 	p: 88.17 	r: 72.01 	f1: 79.28 	 1096 	 1243 	 1522
best_thres [[0.5, 0.55, 0.27], [0.35, 0.53, 0.25], [0.4, 0.55, 0.25]]
f [0.8168, 0.8166, 0.8164]
load model: epoch18
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 18 	 [0.52, 0.41, 0.18] 	 lr: 1.25e-05 	 f: 82.92906178489703
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 23 [0m
Train...

[32m iter_0[0m
ga 	p: 81.05 	r: 74.95 	f1: 77.88 	 4623 	 5704 	 6168
wo 	p: 91.62 	r: 87.13 	f1: 89.32 	 2952 	 3222 	 3388
ni 	p: 87.28 	r: 72.14 	f1: 78.99 	 1098 	 1258 	 1522

[32m iter_1[0m
ga 	p: 81.9 	r: 74.16 	f1: 77.84 	 4574 	 5585 	 6168
wo 	p: 92.94 	r: 85.83 	f1: 89.24 	 2908 	 3129 	 3388
ni 	p: 86.39 	r: 73.0 	f1: 79.13 	 1111 	 1286 	 1522

[32m iter_2[0m
ga 	p: 83.66 	r: 72.78 	f1: 77.84 	 4489 	 5366 	 6168
wo 	p: 91.4 	r: 87.25 	f1: 89.28 	 2956 	 3234 	 3388
ni 	p: 87.22 	r: 72.67 	f1: 79.28 	 1106 	 1268 	 1522
best_thres [[0.34, 0.38, 0.18], [0.39, 0.72, 0.14], [0.67, 0.33, 0.15]]
f [0.8158, 0.8156, 0.8159]
load model: epoch18
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 18 	 [0.42, 0.34, 0.14] 	 lr: 1.25e-05 	 f: 82.76013762981351
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 27 [0m
Train...

[32m iter_0[0m
ga 	p: 82.02 	r: 74.98 	f1: 78.34 	 4625 	 5639 	 6168
wo 	p: 93.91 	r: 85.51 	f1: 89.51 	 2897 	 3085 	 3388
ni 	p: 89.11 	r: 75.23 	f1: 81.58 	 1145 	 1285 	 1522

[32m iter_1[0m
ga 	p: 84.34 	r: 73.46 	f1: 78.53 	 4531 	 5372 	 6168
wo 	p: 93.54 	r: 85.89 	f1: 89.55 	 2910 	 3111 	 3388
ni 	p: 89.54 	r: 75.36 	f1: 81.84 	 1147 	 1281 	 1522

[32m iter_2[0m
ga 	p: 82.93 	r: 74.61 	f1: 78.55 	 4602 	 5549 	 6168
wo 	p: 93.89 	r: 85.68 	f1: 89.6 	 2903 	 3092 	 3388
ni 	p: 89.66 	r: 75.23 	f1: 81.81 	 1145 	 1277 	 1522
best_thres [[0.34, 0.58, 0.17], [0.64, 0.7, 0.15], [0.61, 0.78, 0.15]]
f [0.822, 0.8231, 0.8234]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 18 	 [0.54, 0.69, 0.15] 	 lr: 1.25e-05 	 f: 82.8047753032254
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 25 [0m
Train...
loss: tensor(634.8600) lr: 6.25e-06 time: 1191.09
pred_count_train 41644

Test...
loss: tensor(632.9229) lr: 5e-06 time: 1241.66
pred_count_train 41644

Test...
loss: tensor(549.1714) lr: 6.25e-06 time: 1208.45
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.56 	r: 75.37 	f1: 78.8 	 4649 	 5631 	 6168
wo 	p: 92.35 	r: 87.31 	f1: 89.76 	 2958 	 3203 	 3388
ni 	p: 89.0 	r: 76.54 	f1: 82.3 	 1165 	 1309 	 1522

[32m iter_1[0m
ga 	p: 82.45 	r: 75.7 	f1: 78.93 	 4669 	 5663 	 6168
wo 	p: 92.63 	r: 87.19 	f1: 89.83 	 2954 	 3189 	 3388
ni 	p: 88.82 	r: 76.74 	f1: 82.34 	 1168 	 1315 	 1522

[32m iter_2[0m
ga 	p: 82.6 	r: 75.5 	f1: 78.89 	 4657 	 5638 	 6168
wo 	p: 92.6 	r: 87.22 	f1: 89.83 	 2955 	 3191 	 3388
ni 	p: 88.83 	r: 76.81 	f1: 82.38 	 1169 	 1316 	 1522
best_thres [[0.43, 0.43, 0.24], [0.41, 0.47, 0.22], [0.42, 0.47, 0.22]]
f [0.8267, 0.8272, 0.8273]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 18 	 [0.52, 0.41, 0.18] 	 lr: 1.25e-05 	 f: 82.92906178489703
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 24 [0m
Train...

[32m iter_0[0m
ga 	p: 82.9 	r: 74.69 	f1: 78.58 	 4607 	 5557 	 6168
wo 	p: 92.3 	r: 87.43 	f1: 89.8 	 2962 	 3209 	 3388
ni 	p: 89.26 	r: 74.84 	f1: 81.42 	 1139 	 1276 	 1522

[32m iter_1[0m
ga 	p: 82.68 	r: 75.0 	f1: 78.65 	 4626 	 5595 	 6168
wo 	p: 92.5 	r: 87.04 	f1: 89.69 	 2949 	 3188 	 3388
ni 	p: 85.82 	r: 77.53 	f1: 81.46 	 1180 	 1375 	 1522

[32m iter_2[0m
ga 	p: 83.17 	r: 74.53 	f1: 78.61 	 4597 	 5527 	 6168
wo 	p: 92.59 	r: 87.1 	f1: 89.76 	 2951 	 3187 	 3388
ni 	p: 86.01 	r: 77.53 	f1: 81.55 	 1180 	 1372 	 1522
best_thres [[0.43, 0.4, 0.22], [0.41, 0.41, 0.13], [0.44, 0.41, 0.13]]
f [0.8246, 0.8246, 0.8247]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 18 	 [0.42, 0.34, 0.14] 	 lr: 1.25e-05 	 f: 82.76013762981351
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 28 [0m
Train...

[32m iter_0[0m
ga 	p: 81.95 	r: 74.66 	f1: 78.14 	 4605 	 5619 	 6168
wo 	p: 93.71 	r: 85.8 	f1: 89.58 	 2907 	 3102 	 3388
ni 	p: 87.47 	r: 75.23 	f1: 80.89 	 1145 	 1309 	 1522

[32m iter_1[0m
ga 	p: 82.6 	r: 74.45 	f1: 78.31 	 4592 	 5559 	 6168
wo 	p: 93.95 	r: 85.74 	f1: 89.66 	 2905 	 3092 	 3388
ni 	p: 86.51 	r: 76.28 	f1: 81.08 	 1161 	 1342 	 1522

[32m iter_2[0m
ga 	p: 82.61 	r: 74.58 	f1: 78.39 	 4600 	 5568 	 6168
wo 	p: 93.44 	r: 86.13 	f1: 89.63 	 2918 	 3123 	 3388
ni 	p: 90.18 	r: 73.59 	f1: 81.04 	 1120 	 1242 	 1522
best_thres [[0.35, 0.56, 0.2], [0.56, 0.77, 0.16], [0.66, 0.76, 0.26]]
f [0.8203, 0.821, 0.8214]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 18 	 [0.54, 0.69, 0.15] 	 lr: 1.25e-05 	 f: 82.8047753032254
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 26 [0m
Train...
loss: tensor(590.1313) lr: 6.25e-06 time: 1210.44
pred_count_train 41644

Test...
loss: tensor(592.4340) lr: 5e-06 time: 1254.82
pred_count_train 41644

Test...
loss: tensor(507.2106) lr: 6.25e-06 time: 1224.27
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.43 	r: 74.38 	f1: 78.65 	 4588 	 5499 	 6168
wo 	p: 92.42 	r: 86.78 	f1: 89.51 	 2940 	 3181 	 3388
ni 	p: 87.62 	r: 76.28 	f1: 81.56 	 1161 	 1325 	 1522

[32m iter_1[0m
ga 	p: 83.6 	r: 74.22 	f1: 78.63 	 4578 	 5476 	 6168
wo 	p: 93.17 	r: 86.16 	f1: 89.53 	 2919 	 3133 	 3388
ni 	p: 87.21 	r: 76.61 	f1: 81.57 	 1166 	 1337 	 1522

[32m iter_2[0m
ga 	p: 83.56 	r: 74.19 	f1: 78.6 	 4576 	 5476 	 6168
wo 	p: 93.17 	r: 86.16 	f1: 89.53 	 2919 	 3133 	 3388
ni 	p: 87.21 	r: 76.61 	f1: 81.57 	 1166 	 1337 	 1522
best_thres [[0.51, 0.47, 0.16], [0.51, 0.54, 0.14], [0.51, 0.54, 0.14]]
f [0.8243, 0.8242, 0.8241]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 18 	 [0.52, 0.41, 0.18] 	 lr: 1.25e-05 	 f: 82.92906178489703
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 25 [0m
Train...

[32m iter_0[0m
ga 	p: 83.22 	r: 74.48 	f1: 78.61 	 4594 	 5520 	 6168
wo 	p: 91.46 	r: 87.6 	f1: 89.49 	 2968 	 3245 	 3388
ni 	p: 86.36 	r: 76.15 	f1: 80.94 	 1159 	 1342 	 1522

[32m iter_1[0m
ga 	p: 82.19 	r: 75.28 	f1: 78.58 	 4643 	 5649 	 6168
wo 	p: 91.52 	r: 87.6 	f1: 89.52 	 2968 	 3243 	 3388
ni 	p: 86.39 	r: 76.74 	f1: 81.28 	 1168 	 1352 	 1522

[32m iter_2[0m
ga 	p: 82.61 	r: 74.87 	f1: 78.55 	 4618 	 5590 	 6168
wo 	p: 91.55 	r: 87.57 	f1: 89.52 	 2967 	 3241 	 3388
ni 	p: 86.56 	r: 76.61 	f1: 81.28 	 1166 	 1347 	 1522
best_thres [[0.43, 0.33, 0.17], [0.37, 0.31, 0.14], [0.39, 0.31, 0.14]]
f [0.8233, 0.8234, 0.8234]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 18 	 [0.42, 0.34, 0.14] 	 lr: 1.25e-05 	 f: 82.76013762981351
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 29 [0m
Train...

[32m iter_0[0m
ga 	p: 80.32 	r: 75.58 	f1: 77.88 	 4662 	 5804 	 6168
wo 	p: 93.55 	r: 85.68 	f1: 89.45 	 2903 	 3103 	 3388
ni 	p: 87.26 	r: 72.47 	f1: 79.18 	 1103 	 1264 	 1522

[32m iter_1[0m
ga 	p: 82.67 	r: 74.09 	f1: 78.15 	 4570 	 5528 	 6168
wo 	p: 93.67 	r: 85.6 	f1: 89.45 	 2900 	 3096 	 3388
ni 	p: 90.54 	r: 71.02 	f1: 79.6 	 1081 	 1194 	 1522

[32m iter_2[0m
ga 	p: 82.24 	r: 74.5 	f1: 78.18 	 4595 	 5587 	 6168
wo 	p: 93.38 	r: 85.71 	f1: 89.38 	 2904 	 3110 	 3388
ni 	p: 90.25 	r: 71.16 	f1: 79.57 	 1083 	 1200 	 1522
best_thres [[0.27, 0.56, 0.18], [0.6, 0.77, 0.24], [0.67, 0.81, 0.23]]
f [0.8159, 0.8171, 0.8175]
load model: epoch18
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 18 	 [0.54, 0.69, 0.15] 	 lr: 1.25e-05 	 f: 82.8047753032254
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 27 [0m
Train...
loss: tensor(548.8614) lr: 6.25e-06 time: 1229.94
pred_count_train 41644

Test...
loss: tensor(556.7521) lr: 5e-06 time: 1269.28
pred_count_train 41644

Test...
loss: tensor(639.3115) lr: 5e-06 time: 1242.91
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.28 	r: 75.15 	f1: 78.55 	 4635 	 5633 	 6168
wo 	p: 91.92 	r: 87.28 	f1: 89.54 	 2957 	 3217 	 3388
ni 	p: 87.38 	r: 75.95 	f1: 81.27 	 1156 	 1323 	 1522

[32m iter_1[0m
ga 	p: 81.74 	r: 75.78 	f1: 78.65 	 4674 	 5718 	 6168
wo 	p: 92.35 	r: 86.95 	f1: 89.57 	 2946 	 3190 	 3388
ni 	p: 87.9 	r: 75.43 	f1: 81.19 	 1148 	 1306 	 1522

[32m iter_2[0m
ga 	p: 82.17 	r: 75.39 	f1: 78.63 	 4650 	 5659 	 6168
wo 	p: 92.35 	r: 86.95 	f1: 89.57 	 2946 	 3190 	 3388
ni 	p: 88.84 	r: 74.77 	f1: 81.2 	 1138 	 1281 	 1522
best_thres [[0.47, 0.42, 0.22], [0.42, 0.45, 0.22], [0.45, 0.45, 0.25]]
f [0.8233, 0.8234, 0.8235]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 18 	 [0.52, 0.41, 0.18] 	 lr: 1.25e-05 	 f: 82.92906178489703
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 26 [0m
Train...

[32m iter_0[0m
ga 	p: 81.75 	r: 75.1 	f1: 78.28 	 4632 	 5666 	 6168
wo 	p: 92.16 	r: 86.78 	f1: 89.39 	 2940 	 3190 	 3388
ni 	p: 88.83 	r: 73.65 	f1: 80.53 	 1121 	 1262 	 1522

[32m iter_1[0m
ga 	p: 82.28 	r: 74.66 	f1: 78.28 	 4605 	 5597 	 6168
wo 	p: 92.25 	r: 86.78 	f1: 89.43 	 2940 	 3187 	 3388
ni 	p: 87.11 	r: 75.03 	f1: 80.62 	 1142 	 1311 	 1522

[32m iter_2[0m
ga 	p: 81.97 	r: 74.9 	f1: 78.28 	 4620 	 5636 	 6168
wo 	p: 93.15 	r: 85.89 	f1: 89.37 	 2910 	 3124 	 3388
ni 	p: 87.66 	r: 74.7 	f1: 80.67 	 1137 	 1297 	 1522
best_thres [[0.38, 0.4, 0.2], [0.4, 0.38, 0.14], [0.38, 0.68, 0.15]]
f [0.8202, 0.8204, 0.8203]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	current best epoch 18 	 [0.42, 0.34, 0.14] 	 lr: 1.25e-05 	 f: 82.76013762981351
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse[0m [33m epoch 30 [0m
Train...

[32m iter_0[0m
ga 	p: 84.21 	r: 73.51 	f1: 78.5 	 4534 	 5384 	 6168
wo 	p: 94.16 	r: 85.71 	f1: 89.74 	 2904 	 3084 	 3388
ni 	p: 90.06 	r: 75.03 	f1: 81.86 	 1142 	 1268 	 1522

[32m iter_1[0m
ga 	p: 84.87 	r: 73.56 	f1: 78.81 	 4537 	 5346 	 6168
wo 	p: 93.5 	r: 86.22 	f1: 89.71 	 2921 	 3124 	 3388
ni 	p: 88.58 	r: 76.48 	f1: 82.09 	 1164 	 1314 	 1522

[32m iter_2[0m
ga 	p: 84.08 	r: 74.17 	f1: 78.82 	 4575 	 5441 	 6168
wo 	p: 92.73 	r: 87.01 	f1: 89.78 	 2948 	 3179 	 3388
ni 	p: 89.19 	r: 75.89 	f1: 82.0 	 1155 	 1295 	 1522
best_thres [[0.46, 0.6, 0.23], [0.65, 0.66, 0.17], [0.66, 0.59, 0.18]]
f [0.8244, 0.8255, 0.8259]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 18 	 [0.54, 0.69, 0.15] 	 lr: 1.25e-05 	 f: 82.8047753032254
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 28 [0m
Train...
loss: tensor(509.4273) lr: 6.25e-06 time: 1250.08
pred_count_train 41644

Test...
loss: tensor(521.8452) lr: 5e-06 time: 1283.96
pred_count_train 41644

Test...
loss: tensor(598.1581) lr: 5e-06 time: 1259.4
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 82.11 	r: 74.81 	f1: 78.29 	 4614 	 5619 	 6168
wo 	p: 92.92 	r: 86.33 	f1: 89.5 	 2925 	 3148 	 3388
ni 	p: 85.98 	r: 74.11 	f1: 79.6 	 1128 	 1312 	 1522

[32m iter_1[0m
ga 	p: 81.7 	r: 75.32 	f1: 78.38 	 4646 	 5687 	 6168
wo 	p: 92.88 	r: 86.25 	f1: 89.44 	 2922 	 3146 	 3388
ni 	p: 85.14 	r: 74.51 	f1: 79.47 	 1134 	 1332 	 1522

[32m iter_2[0m
ga 	p: 81.84 	r: 75.18 	f1: 78.37 	 4637 	 5666 	 6168
wo 	p: 92.85 	r: 86.25 	f1: 89.43 	 2922 	 3147 	 3388
ni 	p: 85.29 	r: 74.64 	f1: 79.61 	 1136 	 1332 	 1522
best_thres [[0.47, 0.56, 0.15], [0.43, 0.55, 0.13], [0.44, 0.55, 0.13]]
f [0.8193, 0.8193, 0.8193]
load model: epoch18
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 18 	 [0.52, 0.41, 0.18] 	 lr: 1.25e-05 	 f: 82.92906178489703
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 27 [0m
Train...

[32m iter_0[0m
ga 	p: 82.47 	r: 73.67 	f1: 77.82 	 4544 	 5510 	 6168
wo 	p: 92.57 	r: 86.45 	f1: 89.41 	 2929 	 3164 	 3388
ni 	p: 88.16 	r: 73.85 	f1: 80.37 	 1124 	 1275 	 1522

[32m iter_1[0m
ga 	p: 83.78 	r: 73.12 	f1: 78.09 	 4510 	 5383 	 6168
wo 	p: 93.54 	r: 85.51 	f1: 89.34 	 2897 	 3097 	 3388
ni 	p: 86.35 	r: 75.23 	f1: 80.41 	 1145 	 1326 	 1522

[32m iter_2[0m
ga 	p: 83.33 	r: 73.35 	f1: 78.02 	 4524 	 5429 	 6168
wo 	p: 93.42 	r: 85.51 	f1: 89.29 	 2897 	 3101 	 3388
ni 	p: 86.94 	r: 74.77 	f1: 80.4 	 1138 	 1309 	 1522
best_thres [[0.46, 0.45, 0.22], [0.69, 0.77, 0.14], [0.68, 0.8, 0.15]]
f [0.8177, 0.8184, 0.8184]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse 	best in epoch 18 	 [0.42, 0.34, 0.14] 	 lr: 1.25e-05 	 f: 82.76013762981351
2016 {'GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2016_preFalse': {'best_epoch': 18, 'best_thres': [0.31, 0.46, 0.15], 'best_lr': 1.25e-05, 'best_performance': 82.59886124327275}}
2018 {'GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2018_preFalse': {'best_epoch': 18, 'best_thres': [0.39, 0.43, 0.16], 'best_lr': 1.25e-05, 'best_performance': 82.71792383426988}}
2020 {'GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.5_it3_rs2020_preFalse': {'best_epoch': 18, 'best_thres': [0.42, 0.34, 0.14], 'best_lr': 1.25e-05, 'best_performance': 82.76013762981351}}

[32m iter_0[0m
ga 	p: 83.31 	r: 74.12 	f1: 78.45 	 4572 	 5488 	 6168
wo 	p: 93.28 	r: 86.1 	f1: 89.55 	 2917 	 3127 	 3388
ni 	p: 88.71 	r: 75.36 	f1: 81.49 	 1147 	 1293 	 1522

[32m iter_1[0m
ga 	p: 84.55 	r: 73.57 	f1: 78.68 	 4538 	 5367 	 6168
wo 	p: 93.63 	r: 85.83 	f1: 89.56 	 2908 	 3106 	 3388
ni 	p: 88.74 	r: 75.62 	f1: 81.66 	 1151 	 1297 	 1522

[32m iter_2[0m
ga 	p: 84.11 	r: 74.04 	f1: 78.75 	 4567 	 5430 	 6168
wo 	p: 93.49 	r: 85.98 	f1: 89.58 	 2913 	 3116 	 3388
ni 	p: 89.25 	r: 75.3 	f1: 81.68 	 1146 	 1284 	 1522
best_thres [[0.4, 0.52, 0.21], [0.64, 0.72, 0.19], [0.68, 0.74, 0.19]]
f [0.823, 0.8239, 0.8243]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 18 	 [0.54, 0.69, 0.15] 	 lr: 1.25e-05 	 f: 82.8047753032254
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 29 [0m
Train...
loss: tensor(632.8792) lr: 5e-06 time: 1106.41
pred_count_train 41644

Test...
loss: tensor(560.5609) lr: 5e-06 time: 1090.68
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 83.88 	r: 74.42 	f1: 78.87 	 4590 	 5472 	 6168
wo 	p: 93.14 	r: 86.54 	f1: 89.72 	 2932 	 3148 	 3388
ni 	p: 88.86 	r: 76.02 	f1: 81.94 	 1157 	 1302 	 1522

[32m iter_1[0m
ga 	p: 83.96 	r: 74.42 	f1: 78.9 	 4590 	 5467 	 6168
wo 	p: 92.78 	r: 86.81 	f1: 89.69 	 2941 	 3170 	 3388
ni 	p: 88.85 	r: 75.95 	f1: 81.9 	 1156 	 1301 	 1522

[32m iter_2[0m
ga 	p: 83.94 	r: 74.38 	f1: 78.87 	 4588 	 5466 	 6168
wo 	p: 92.37 	r: 87.16 	f1: 89.69 	 2953 	 3197 	 3388
ni 	p: 89.16 	r: 75.69 	f1: 81.88 	 1152 	 1292 	 1522
best_thres [[0.53, 0.53, 0.21], [0.53, 0.48, 0.2], [0.53, 0.43, 0.21]]
f [0.8266, 0.8266, 0.8266]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 18 	 [0.52, 0.41, 0.18] 	 lr: 1.25e-05 	 f: 82.92906178489703
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 28 [0m
Train...

[32m iter_0[0m
ga 	p: 81.81 	r: 74.82 	f1: 78.16 	 4615 	 5641 	 6168
wo 	p: 93.25 	r: 86.07 	f1: 89.52 	 2916 	 3127 	 3388
ni 	p: 87.41 	r: 75.76 	f1: 81.17 	 1153 	 1319 	 1522

[32m iter_1[0m
ga 	p: 83.08 	r: 74.29 	f1: 78.44 	 4582 	 5515 	 6168
wo 	p: 93.81 	r: 85.86 	f1: 89.66 	 2909 	 3101 	 3388
ni 	p: 89.41 	r: 74.31 	f1: 81.16 	 1131 	 1265 	 1522

[32m iter_2[0m
ga 	p: 83.19 	r: 74.27 	f1: 78.48 	 4581 	 5507 	 6168
wo 	p: 93.54 	r: 85.92 	f1: 89.57 	 2911 	 3112 	 3388
ni 	p: 90.67 	r: 73.46 	f1: 81.16 	 1118 	 1233 	 1522
best_thres [[0.34, 0.51, 0.15], [0.57, 0.73, 0.18], [0.66, 0.77, 0.2]]
f [0.8206, 0.8217, 0.822]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	current best epoch 18 	 [0.54, 0.69, 0.15] 	 lr: 1.25e-05 	 f: 82.8047753032254
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse[0m [33m epoch 30 [0m
Train...
loss: tensor(594.9974) lr: 5e-06 time: 1114.24
pred_count_train 41644

Test...
loss: tensor(526.4872) lr: 5e-06 time: 1104.0
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.89 	r: 75.88 	f1: 78.77 	 4680 	 5715 	 6168
wo 	p: 91.94 	r: 87.51 	f1: 89.67 	 2965 	 3225 	 3388
ni 	p: 88.71 	r: 75.36 	f1: 81.49 	 1147 	 1293 	 1522

[32m iter_1[0m
ga 	p: 82.57 	r: 75.34 	f1: 78.79 	 4647 	 5628 	 6168
wo 	p: 91.89 	r: 87.6 	f1: 89.69 	 2968 	 3230 	 3388
ni 	p: 90.04 	r: 74.24 	f1: 81.38 	 1130 	 1255 	 1522

[32m iter_2[0m
ga 	p: 82.94 	r: 75.1 	f1: 78.82 	 4632 	 5585 	 6168
wo 	p: 91.86 	r: 87.54 	f1: 89.65 	 2966 	 3229 	 3388
ni 	p: 90.1 	r: 74.18 	f1: 81.37 	 1129 	 1253 	 1522
best_thres [[0.4, 0.4, 0.22], [0.44, 0.38, 0.24], [0.46, 0.38, 0.24]]
f [0.8251, 0.8252, 0.8253]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 18 	 [0.52, 0.41, 0.18] 	 lr: 1.25e-05 	 f: 82.92906178489703
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 29 [0m
Train...

[32m iter_0[0m
ga 	p: 81.66 	r: 74.55 	f1: 77.94 	 4598 	 5631 	 6168
wo 	p: 92.81 	r: 86.51 	f1: 89.55 	 2931 	 3158 	 3388
ni 	p: 88.32 	r: 74.51 	f1: 80.83 	 1134 	 1284 	 1522

[32m iter_1[0m
ga 	p: 81.85 	r: 74.92 	f1: 78.23 	 4621 	 5646 	 6168
wo 	p: 93.42 	r: 85.92 	f1: 89.51 	 2911 	 3116 	 3388
ni 	p: 87.77 	r: 74.97 	f1: 80.86 	 1141 	 1300 	 1522

[32m iter_2[0m
ga 	p: 82.8 	r: 74.3 	f1: 78.32 	 4583 	 5535 	 6168
wo 	p: 93.3 	r: 85.92 	f1: 89.46 	 2911 	 3120 	 3388
ni 	p: 88.26 	r: 74.57 	f1: 80.84 	 1135 	 1286 	 1522
best_thres [[0.35, 0.45, 0.2], [0.52, 0.76, 0.17], [0.7, 0.81, 0.17]]
f [0.8192, 0.8198, 0.8202]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse 	best in epoch 18 	 [0.54, 0.69, 0.15] 	 lr: 1.25e-05 	 f: 82.8047753032254
2016 {'GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2016_preFalse': {'best_epoch': 15, 'best_thres': [0.54, 0.66, 0.21], 'best_lr': 2.5e-05, 'best_performance': 81.98224898220218}}
2018 {'GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2018_preFalse': {'best_epoch': 22, 'best_thres': [0.55, 0.44, 0.16], 'best_lr': 6.25e-06, 'best_performance': 82.85481444333}}
2020 {'GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.2_it3_rs2020_preFalse': {'best_epoch': 18, 'best_thres': [0.54, 0.69, 0.15], 'best_lr': 1.25e-05, 'best_performance': 82.8047753032254}}
loss: tensor(560.3677) lr: 5e-06 time: 1028.2
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.87 	r: 75.49 	f1: 78.55 	 4656 	 5687 	 6168
wo 	p: 93.14 	r: 86.13 	f1: 89.5 	 2918 	 3133 	 3388
ni 	p: 87.4 	r: 75.62 	f1: 81.08 	 1151 	 1317 	 1522

[32m iter_1[0m
ga 	p: 81.01 	r: 76.2 	f1: 78.53 	 4700 	 5802 	 6168
wo 	p: 92.81 	r: 86.45 	f1: 89.52 	 2929 	 3156 	 3388
ni 	p: 87.53 	r: 75.16 	f1: 80.88 	 1144 	 1307 	 1522

[32m iter_2[0m
ga 	p: 81.31 	r: 75.94 	f1: 78.53 	 4684 	 5761 	 6168
wo 	p: 92.37 	r: 86.84 	f1: 89.52 	 2942 	 3185 	 3388
ni 	p: 87.48 	r: 75.3 	f1: 80.93 	 1146 	 1310 	 1522
best_thres [[0.43, 0.55, 0.17], [0.37, 0.5, 0.16], [0.39, 0.45, 0.16]]
f [0.8225, 0.8223, 0.8223]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	current best epoch 18 	 [0.52, 0.41, 0.18] 	 lr: 1.25e-05 	 f: 82.92906178489703
[34mGATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse[0m [33m epoch 30 [0m
Train...
loss: tensor(528.3471) lr: 5e-06 time: 1035.59
pred_count_train 41644

Test...

[32m iter_0[0m
ga 	p: 81.84 	r: 75.05 	f1: 78.3 	 4629 	 5656 	 6168
wo 	p: 91.66 	r: 87.22 	f1: 89.38 	 2955 	 3224 	 3388
ni 	p: 86.53 	r: 75.95 	f1: 80.9 	 1156 	 1336 	 1522

[32m iter_1[0m
ga 	p: 81.58 	r: 75.54 	f1: 78.44 	 4659 	 5711 	 6168
wo 	p: 92.06 	r: 86.87 	f1: 89.38 	 2943 	 3197 	 3388
ni 	p: 86.81 	r: 75.23 	f1: 80.61 	 1145 	 1319 	 1522

[32m iter_2[0m
ga 	p: 81.54 	r: 75.5 	f1: 78.41 	 4657 	 5711 	 6168
wo 	p: 92.02 	r: 86.84 	f1: 89.35 	 2942 	 3197 	 3388
ni 	p: 86.81 	r: 75.23 	f1: 80.61 	 1145 	 1319 	 1522
best_thres [[0.46, 0.43, 0.18], [0.43, 0.46, 0.18], [0.43, 0.46, 0.18]]
f [0.8209, 0.821, 0.821]
GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse 	best in epoch 18 	 [0.52, 0.41, 0.18] 	 lr: 1.25e-05 	 f: 82.92906178489703
2016 {'GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2016_preFalse': {'best_epoch': 14, 'best_thres': [0.41, 0.56, 0.16], 'best_lr': 2.5e-05, 'best_performance': 82.41023652578518}}
2018 {'GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2018_preFalse': {'best_epoch': 13, 'best_thres': [0.44, 0.55, 0.19], 'best_lr': 2.5e-05, 'best_performance': 82.95844927018078}}
2020 {'GATE_e2e-stack_ve256_vu256_depth6_adam_lr0.0001_du0.1_dh0.0_True_size60_sub1118_th0.8_it3_rs2020_preFalse': {'best_epoch': 18, 'best_thres': [0.52, 0.41, 0.18], 'best_lr': 1.25e-05, 'best_performance': 82.92906178489703}}
